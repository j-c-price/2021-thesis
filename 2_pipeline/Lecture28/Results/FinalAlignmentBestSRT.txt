{"0": [0, 30, "The following content is\nprovided under a Creative\nCommons license.\nYour support will help\nMIT OpenCourseWare\ncontinue to offer high quality\neducational resources for free.\nTo make a donation, or\nview additional materials\nfrom hundreds of MIT courses,\nvisit MIT OpenCourseWare\nat ocw.mit.edu.\nJAMES DICARLO: So let\nme start by first--\nI already alluded to\nthis, but let's talk"], "30": [1, 110, "This is just one\ncomputational challenge\nthat our brains solve, but\nit's one that many of us\nare very fascinated by.\nAs you'll hear in the\nrest of the course,\nthere are other problems\nthat are equally fascinating.\nBut I'm going to talk\nabout problems of vision.\nI'm going to talk about a\nspecific problem of vision,\nand that's the problem\nof object recognition.\nSo I will try to\noperationalize that for you.\nAnd one thing you'll\nsee when I talk\nis that our field,\neven though we\ncan be motivated by words like\nvision and object recognition,\nwe're going to\nonly make progress\nif we start to\noperationally define things\nand then decide in what domain\nmodels are going to apply.\nAnd I think that's an\nimportant lesson that I hope\nwill come across in my talk.\nSo this is the way computer\nvision operationally\ndefines part of the problem of\nobject recognition and vision.\nIt's as if you take\na scene like this\nand you want to do\nthings like come up\nwith an answer space that\nlooks like this, where you\nhave noun labels, say a car.\nAnd you have what are called\nbounding boxes around the cars,\nsimilarly for people,\nor buildings, or trees,\nor whatever nouns that\nyou or DARPA or whoever\nwants to actually label.\nRight, so this is just one way\nof operationalizing vision.\nBut I think it gets at the crux\nof what we're after, which is,\nthere is what's called\nlatent content in this image\nthat all of us instantly\nbring to our memories,\nthat we can say, aha, that's\na car, that's a building.\nThere are nouns that\npop into our heads.\nWe also know other latent\ninformation about these things,\nlike the pose of this car,\nthe position of the car,\nthe size of the car.\nThe key point that I'm\ngoing to tell you today\nabout this problem is that\nthat information feels to us\nthat it's obvious,\nbut it's quite\nlatent in the image--\nthat's implicit\nin the pixel representation.\nThose of you who have\nworked on this problem\nwill understand this and those\nof you who haven't, I hopefully\nwill give you some flavor for\nwhat that problem feels like.\nSo I want to back up a bit."], "140": [2, 60, "or a human brain perspective,\nto ask, why would we\neven bother worrying about this\nproblem of object recognition?\nAnd maybe this is obvious\nthat those of you-- and I\ndon't need to say\nthis, but I like\nto point out that we think\nof the representations\nof the tokens of what's\nout there in the world\nas being the substrates\nof what you might do,\nwhat's called higher level\ncognition, things like memory,\nvalue judgments, decisions\nand actions in the world.\nImagine building\na robot and having\nit try to act in the\nworld and it doesn't even\nreally know what's out there.\nSo these are the sort of\nsubstrate of these kind\nof cognitive processes.\nAgain, from an\nengineering perspective,\nthese are processes\nor behaviors.\nThis is just a\nshort list of them\nthat might depend on\nyour good abilities\nto recognize and discriminate\namong different objects.\nI think if you look\nthrough this list,\nyou could imagine things that\nwould go terribly wrong if you\ndidn't actually do a\ngood job at identifying\nwhat's out there in the world.\nSo that's just to\nthink about, again,\nas an engineer building a robot.\nThis is a slide\nI stuck in that I"], "200": [3, 100, "that I know many of you are\nfrom maybe these backgrounds,\nor from this background.\nAnd when I think\nabout the brain,\nI have this coin\nhere to say, really\nthese are kind of two sides--\nwe're studying the same coin\nfrom two directions here.\nAnd really the\nquestion that we have\nto all be excited about,\nI hope many of you\nare excited about it is,\nhow does the brain work?\nAnd you could do\ncomputer science\nand not care at all\nabout this question.\nI think it's a little\nharder to do these and not\ncare about this question.\nBut it's possible, I guess.\nSo these are all trying\nto answer this question.\nAnd this is maybe\npretty obvious,\nbut when you have\nbiological brains that\nare performing tasks better\nthan current computer\nsystems, machines that\nhumans have built,\nthen the flow tends to\nwant to go this way.\nYou discover phenomena\nor constraints over here.\nThese lead to ideas that can be\nbuilt into computer code that\ncan say, hey, can I build\na better machine based\non what we discover over here?\nAnd many of us who came into\nthe field excited to do this\nand are still excited of\nthis kind of direction.\nBut an equally\nimportant direction\nis that when you\nhave systems that\nare matched with our\nabilities, or that\ncan compute some of the things\nthat we think the brain has\nto compute, then the\nflow goes more this way,\nwhere there's many possible\nways to implement an idea\nand these become falsifiable.\nThat is, that they can be\ntested against experimental data\nto ask which of these many ways\nof implementing a computation\nare the ones that are actually\noccurring in the brain.\nAnd that's important\nif you say you\nwant to build\nbrain-machine interfaces,\nor fix diseases, or\ndo something that's\non the level of interacting\nwith the brain directly.\nI hope that you guys\nkeep this picture in mind\nbecause I think it's sort\nof the spirit of the course\nthat both of these\ndirections are important.\nAnd it's not as if we work on\nthis for 20 years and then work\non this for 20 years."], "300": [4, 70, "is the most exciting to us.\nSo just to connect to that,\na little bit of history\nof where was the field on this\nproblem of visual recognition.\nI don't know if many\nof you heard this,\nbut here you are\nat summer school,\nso there was a Summer\nVision Project--\nit was called, at MIT.\nI used to think this\nstory was apocryphal.\nIn 1966, there was a\nproject that the final goal\nwas object identification,\nwhich we'll actually name,\nObjects by Matching the\nVocabulary of Known Objects.\nSo this was essentially\na summer project to say,\nwe're going to get a couple\nundergraduate students together\nand we're going to build a\nrecognition system in 1966.\nAnd this was the\nexcitement of AI,\nwe can build anything\nthat we want.\nAnd of course, those\nof you who know this,\nthis problem turned\nout to be much, much\nharder than anticipated.\nSo sometimes problems that\nseem easy for us are actually\nquite difficult. If\nany of you wants this,\nI would be happy to share\nthis document with you.\nIt's interesting, the space\nof objects that they describe\nthings like recognizing--\nof course, I would say like\ncoffee cups on your desk.\nBut they also say packs of\ncigarettes on your desk.\nSo this sort of dates\nthe time of this here.\nSo it's a little bit like\nMad Men or something.\nSo now, here we are today.\nAnd I guess I just can't help\nbut sort of get excited about,"], "370": [5, 40, "that does these computations.\nThe things got-- I can't tell\nyou all this because of the 100\nbillion computing elements,\nsolves problems not solveable\nby any previous machine.\nAnd the thing, it looks\ncrazy, but it only\nrequires 20 watts of power.\nThose of you who\nhave seen this slide,\nI'm not talking\nabout this thing.\nI'm talking about that\nthing right there.\nSo this is a scale\nof what we're after.\nAnd we often talk about power,\nbut this is something engineers\nare especially interested in\nas they build these systems, is\nhow does our brains solve these\nproblems at such a low wattage,\nso to speak.\nThis is, again, the spirit\nof many of the things\nthat I hope that you guys are\nexcited about in the future\nof this field.\nHere's another slide\nthat I pulled out"], "410": [6, 80, "point of view, we\noften try to say,\nwell, we want to build\nmachines that are as\ngood or better than our brain.\nSo machines today, you\nguys know this, beat us\nat many things,\nstraight calculation,\nthey beat us at chess.\nWhen I was a grad student,\nthey recently won at Jeopardy.\nIn memory, they've\nalways beaten us.\nMachines are way better\nat memory than us\nin the simple form of memory.\nSeeing, in pattern\nmatching, go to the grocery\nstore, hey, what's\nthat bar code done?\nI don't know what that\nwas, but it just scans in\nand somehow it does\npattern matching, right?\nSo there's forms of\nvision that machines\nare way better than us.\nBut some forms of vision that\nare more complicated that\nrequire generalization,\nlike object recognition,\nor more broadly,\nscene understanding,\nwe like to think that we\nare still the winners at.\nAnd even things that we take\nfor granted, like walking,\nthis is quite a\nchallenging problem.\nSo engineers really want\nto move this over here.\nSo our goal is to discover\nhow the brain solves\nobject recognition.\nAnd the reason I put this\nup is, from an engineering\npoint of view, that just doesn't\nmean write a bunch of papers\nin a textbook that says,\nthis part of the brain\ndoes it, but actually help to\nimplement a system where this\nis, at least, matched with\nus and I assume someday,\nwill be better than us.\nAnd this is also\na gateway problem.\nThat is, even if it's\njust this domain,\nwe think that the\nsystems we're studying\nmight generalize to other,\nfor instance, sensory domains.\nGabriel told me you\nwere going to do"], "490": [7, 100, "That's an engineer's\npoint of view,\nhow do I just build\nbetter systems?\nLet's step back and talk from\na scientist's point of view.\nSo this is really now to\nintroduce the talk that I'm\ngoing to give you today.\nSo when you're a\nscientist, what's our job?\nWe say we want to understand.\nWe all write that, understand.\nWhat does that mean?\nWell, what it really\nmeans if you boil it down,\nand I would love to\ndiscuss this if you like,\nis that you have some\nmeasurements in some domain.\nSo you can think of this\nas a state space here.\nThis is like the position\nof the planets today.\nAnd this is like the position\nof the planets tomorrow.\nOr you could say, this is the\nDNA sequence inside a cell.\nAnd this is some protein\nthat's going to get made.\nSo you're searching for\nmappings that are predictive\nfrom one domain to another.\nAnd we can give lots\nof examples of what\nwe call successful\nscience, where that's true.\nThis is the core of science\nis to predict, given\nsome measurements or\nobservations, what's\ngoing to happen either in\nthe future or some other set\nof measurements.\nSo predictive power is\nthe core of all science\nand the core of understanding.\nAnd I think it would be fun\nif you want to debate that,\nthat you think\nthere's another way.\nBut this is what I come to in\nthinking about this problem.\nAnd the reason I'm\nbringing this up\nis because the accuracy\nof this predictive mapping\nis a measure of the strength\nof any scientific field.\nAnd some fields are\nfurther along than others.\nAnd I would say ours is\nstill not very far along.\nOur job is to bring it\nfrom a nonpredictive state\nto a very predictive state.\nAnd so that means building\nmodels that can be falsified\nand that can predict things.\nAnd you'll hear that\nthrough my talk.\nAs Gabriel mentioned,\nwhat we try to do\nis build models that can predict\neither behavior or neural\nactivity.\nAnd that's what we think is\nwhat progress looks like.\nSo now let's translate\nthis to the problem I gave"], "590": [8, 100, "object recognition.\nYou could imagine, there's\na domain of images.\nSo just to slow down\nhere, just so everybody's\non the same page,\neach dot here might be\nall the pixels in this image.\nIn this dot, all the\npixels in this image.\nSo there's a set of\npossible pixel-images\nthat you could see.\nAnd we imagine that they\ngive rise to, in the brain,\nsome state space.\nThink of this as the whole brain\nfor now, to just fix ideas,\nthat you could imagine that this\nimage, one you're looking at,\nit gives rise to some\npattern of activity\nacross your whole brain.\nAnd this image gives rise to a\ndifferent pattern of activity\nacross your whole brain.\nAnd loosely, we call this\nthe neural representation\nof this thing.\nBut then what we do is\nsomehow when we ask you\nfor behavior reports,\nthere's a mapping\nbetween that neural\nstate space and what\nwe measure as the output.\nWhether you say it or\nwrite it, you might say,\nthat's a face, these\nare both faces,\nif I asked you for\nnouns among them.\nOK, so this is another\ndomain of measurement.\nSo now you can see I'm setting\nup the notion of predictivity.\nAnd what we want\nto do is, we have\nthis complex thing over\nhere of images that somehow\nmap internally into\nneural activity\nand then somehow\nmap to the thing\nwe call perceptual reports.\nAnd notice I've\nalready put things\nthat we call nouns\nthat we usually\nassociate with objects, cars,\nface, dogs, cats, clocks,\nand so forth.\nOK, so understanding this\nmapping in a predictive sense\nis really a summary of what\nour part of the field is about.\nAnd again, accurate\npredictivity is the core product\nof the science that\nunderlies our ability\nto build a system like\nthis-- many of you\nare interested, to fix\na system like this,\nor to perhaps even\naugment our own systems.\nIf we want to inject signals\nhere and have them give rise\nto percepts, we have\nto know how this works.\nA big part of the\nfield of vision\nis spent-- a lot of\nthe last three decades,"], "690": [9, 180, "That's usually called encoding,\npredictive encoding mechanisms.\nAnd it's driven by\nHubel and Wiesel's work.\nThe people saw this as\na great way forward.\nIt's like, let's go\nstudy the neurons\nand try to understand what\nin the image is driving them.\nThat is, what's an\nimage computable\nmodel in the world that\nwould go from images\nto neural responses?\nThe other part is that there's\nsome linkage, we think,\nbetween the neural\nactivity and these reports.\nAnd notice, this is\nactually why most of us\nget into neuroscience\nbecause you\nnotice this arrow is two-way.\nThis is actually\nquite deep here.\nFrom an engineer's\npoint of view,\nyou go, well, there's\ngot to be some mapping\nbetween the neural\nactivity and the button\npresses on my fingers or\nmy saying the word noun.\nThere's some causal linkage\nbetween this and the things\nthat we observe\nobjectively in a subject.\nBut this is where philosophers\ndebate about like, well, you\nknow in some sense\nthese are sort\nof two sides of the same coin.\nWe say our own\nperception, there's\nsome aspects of the\ninternal activity\nthat are the thing that we\ncall awareness or perception.\nNow I'm not going to\nget into all that,\nbut I just want to point\nout that if you're just\nbuilding models, you\ncan't approach that.\nIt's this sort of strange\nthing between neurons\nand these reported states that\nmany of us are fascinated by.\nSo this is called predictive\ndecoding mechanisms.\nFor me, it's all going\nto be operationalized\nin terms of reports\nfrom humans or animals.\nAnd I'll not do that\nphilosophical part,\nbut I thought I'd mention\nthat for those you like\nto think about those things.\nSo for visual\nobject perception, I\nwant to point out that, again,\nthe history of the field\nhas been mostly here.\nThis link has been\nneglected or dominated\nby weakly predictive\nword models.\nThat doesn't mean they're\nnot useful starting points,\nbut they're weakly predictive.\nAnd so a weakly predictive\nword model would be--\nand for temporal cortex,\na part of the brain\nI'm going to tell you about\ntoday, does object recognition.\nThat model has been\naround for a long time.\nIt is somewhat predictive\nbecause it says,\nyou take that out and\nall object recognition\nwill get destroyed,\nwould be a prediction.\nTurns out that doesn't\nactually happen.\nWe can discuss that.\nBut it doesn't tell you how it\ndoes it, how to inject signals,\nwhich tasks are more\nor less affected,\nso that's what I mean\nby weakly predictive.\nIt's a word model.\nFace neurons do\nface task, that's\nprobably true to some extent.\nBut again, it doesn't\ntell us-- it's more tight.\nIt sort of says, oh, I'll\ntake out these smaller regions\nand there'll be some set of\ntasks that involve faces.\nI don't know, I won't say\nanything about other tasks.\nSo that's a somewhat more\nstrongly predictive model,\nbut still pretty\nweakly predictive.\nAnd my personal favorite that\ncomes in from reviewers a lot\nis, attention solves that.\nSo this is just a\nstatement that--\njust to be on the\nlookout for word models\nthat don't actually have\ncontent in terms of prediction.\nI don't know what that means.\nI read this as, hand\nof God reaches in\nand solves the problem.\nSo there's got to be an\nactual predictive model that\ncan be falsified.\nOK, so I don't mean to doubt\nthe importance of these.\nBefore people start\ngiving me a hard time,\nthere are attentional phenomena,\nthere are face neurons,\nthere is an IT,\nthat's what we study.\nI'm just trying to\nemphasize for you that we\nneed to go beyond word\nmodels into actual testable\nmodels that make predictions,\nthat would stand even\nif the person claiming those\nmodels is no longer around,\nit would make a prediction.\nLet me try to define a domain.\nI said we're going to\ntry to define stuff.\nIt's hard to define stuff.\nIt's big, vision,\nit's a big area."], "870": [10, 10, "And when I say this, I\ninclude faces as an object,\na socially important when.\nYou'll hear this\nfrom Winrich I think."], "880": [11, 40, "that's still a big domain.\nAnd so we tried early on to\nreduce the problem even further\nto something that is\nmore, again, naturalistic,\nthat we think can\ngive us more traction,\nthis predictive sense.\nSo we started by saying, when\nyou take a scene like this\nand you analyze it,\nyou may not notice it\nbut your ventral stream, really\nyour retina has high acuity\nin say the central 10 degrees.\nThere's anatomy that\nI'll show you later\nthat the ventral\nstream is especially\ninterested in processing\nthe central 10\ndegrees of information.\nSo that's about two\nhands at arm's length,\nfor those you see in the room.\nSo you may have the sense that\nyou know what's out there,\nbut you don't really.\nYou kind of stitch\nthat together.\nAnd lots of people\nhave shown this,"], "920": [12, 20, "movements around,\ncalled saccades,\nfollowed by fixations, which\nare 200 to 500 milliseconds\nin duration.\nYou don't really see\nduring this time here.\nIt's not as if your\nbrain shuts down,\nit's just that the movement\nis too fast for your retina\nto really keep up with this.\nSo you make these\nrapid eye movements,\nyou fixate, fixate, fixate.\nAnd what you do is,\nthat brings this sort"], "940": [13, 40, "that might look\nsomething like this.\nSo those are 200\nmillisecond snapshots\nacross that scan path.\nAnd I'll play it for\nyou one more time.\nNow, you should\nnotice that there's\none or more objects in\neach and every image\nthat you probably said,\noh, there's a sign.\nThere's a person.\nThere's a car.\nYou might have gotten\ntwo out of each one.\nBut you were sort of\nextracting, at least\nintuitively to me, at least\none or more foreground\nor central objects when\nI show you those images.\nAnd that ability to do what I\njust showed you there, we think\nis the core of how you\nanalyze or build up\na scene like this, at least how\nthe ventral stream contributes.\nAnd therefore, we call\nthat core recognition,\nwhich I defined as a central\n10 degrees of visual field,"], "980": [14, 40, "And again, it's not all\nof object recognition,\nbut we think it's a\ngood starting point.\nAnd a way that we probably got\ninto this is because of a rapid\nserial visual presentation\nmovies from the 70's.\nMolly Potter showed\nthis really nicely.\nThis is a movie that I've\nbeen showing for 15 years now.\nNotice that this is just\na sequence of images\nwhere there is typically one\nor more foreground objects.\nAnd you should be quickly\nmapping those to memory,\neven though I'm not\ntelling you what to expect.\nLike Leaning Tower\nof Pisa, right, I'm\nnot going to tell you that\nyou're going to see Star Wars\ncharacters-- well, I just did.\nBut you quickly are\nable to map those things\nto some noun or even a more\nprecise subordinate noun.\nI know this is Yoda."], "1020": [15, 90, "Notice you didn't need\na lot of pre-cueing,\nyet you're still\nable to do that.\nAnd that is really what\nfascinates us about vision\nand object recognition\nin particular.\nEven without featural\nattention or pre-cueing,\nyou're able to do a remarkable\namount of processing.\nAnd I think that's a great\ndemonstration of that.\nAnd just to quantify\nthis for you,\nbecause sometimes\npeople say, well you're\nshowing it too short.\nYour vision system\ndoesn't do much.\nHere's an eight-way\ncategorization task\nI'll show you later under\nrange of transformation.\nThese are just\nthe example images\nof eight different\ncategories of objects.\nIt doesn't really matter\nwhat I much do here,\nyou get a very similar curve.\nAnd that is, you get most\nof the performance gain\nin about the first\n100 milliseconds.\nThis is accuracy, you're\nabout 85% correct.\nThis is a challenging task,\nas I'll show you earlier.\nIt looks easy here, but\nit's quite challenging.\n85% correct, if I let you\nlook at the image longer,\nup to two seconds, you can\nbump up to around 90's.\nSo there is some gain with\nlonger viewing duration,\nbut you get--\nchance is 50, so you\nget this huge ability.\nAnd we're not the\nfirst to show this.\nThis is just to show you\nin our own kind of task\nthat the data I'm going\nto tell you about,\nwhere we show the image for\n100 or 200 milliseconds,\nthis is the typical\nprimate viewing duration\nthat I pin this on.\nWe use this for\nreasons of efficiency.\nBut you see, the performance\nis similar across that time.\nYou get a lot done.\nYour visual system does a lot\nof work in that first glimpse.\nAnd that's core recognition that\nwe are trying to study here.\nAnd I know it's not all\nof object recognition\nor all of vision,\nbut it's now, we\nthink, a much more\ndefined domain\nthat we can make progress on.\nAnd that's what we've\nbeen working on.\nAnd that's essentially what\nI'm going to talk about today."], "1110": [16, 10, "within that core recognition.\nThis is David Marr.\nDavid and Tommy Poggio, I\nstudied with a long time.\nAnd Tommy wrote the introduction\nto David's-- if you guys"], "1120": [17, 90, "has anybody, guys\nknow this book?\nIt's really a classic\nbook in our field.\nIt's the first\ncouple chapters that\nare the part you\nshould really read.\nThat's the best\npart of the book.\nAnd one of the things that\nyou take from this book,\nthat I think David\nand Tommy helped\nto lay out a long time\nago, is that there\nis this challenge of level.\nI think one of the\nthings I take from this\nis, they would try to\ndefine three clean levels.\nIt turns out not to be\nthis clean in practice.\nBut there's one level called\ncomputational theory, what's\nthe goal, what's appropriate,\nwhat's the logic,\nand by what strategy\ncan it be carried out.\nThere's another\nlevel which is, OK,\nnow once you decide that, how\nshould you represent the data?\nHow can you implement\nan algorithm to do it?\nAnd then there's this\nactually, how do you run it,\nhow do you build it in hardware?\nAnd neuroscientists often\ncome in, they're like,\nI'm going to study\nneurons and it's sort of\nlike jumping into your\niPhone and saying,\nI'm going to study transistors.\nThey often tend to start\nat the hardware level.\nAnd I think that's the biggest\nlesson you take from this like,\noh wait, there's\nsomething going on here,\nthese transistors are flying.\nAnd you make some\nstory about it if you\nwere recording from\nthe brain or measuring\ntransistors in my iPhone.\nBut I think the important\npoint to take from this\nis it helps to start\nthinking about what's\nthe point of the system.\nWhat might it be doing?\nHow might you\nsolve that problem?\nAnd that leads you\nthen to algorithm.\nAnd then you think\nabout representations.\nSo it's sort of a\ntop down approach,\nrather than just\ndigging into the brain\nand hoping that the\nanswers will emerge.\nSo I'm going to try to give\nyou that top down approach\nin this problem that\nI'm talking about.\nI've already given you a\nbit of it by introducing you\nto the problem.\nI'll say a little bit more about\nthat and step down a little bit\nthis way.\nAnd so this kind of\nthinking, I think,\nis important to\nmaking progress in how"], "1210": [18, 120, "So here's a related slide\nthat I made a long time ago\nthat, again, I\npulled out for you\nguys, that I think helps\nbridge between what I just\nsaid about the Marr\nlevels of analysis\nand whether you're a\nneuroscientist or cognitive\nscientist, and are a computer\nvision or machine learning\nperson.\nSo the first is, what is the\nproblem we're trying to solve?\nSo that's Marr\ncomputational level one.\nSo computational vision--\nnow operationally,\nyou'll hear folks\nin machine learning,\nthey might say, well, there's\nsome benchmarks, that's good.\nThere's a ImageNet\nChallenge or whatever\nchallenge they want to solve.\nSometimes they'll say,\nwell the brain solves it.\nThat's not good because\nthey didn't really\ndefine the problem.\nNeuroscientists\nwill say, well, it's\nsomething like\nperception or behavior\nor there's some sort of\nbehavior that they imagined,\nalthough characterizing\nthat behavior\nis not usually\ntheir primary goal.\nBut I think there is at least\nsome progress in that regard.\nNow what does a\nsolution look like?\nThis is really just to\ntalk about language.\nSo useful image representations\nfor machine learning, like what\nwe might call features--\nbut neuroscientists will talk\nabout explicit neuronal spiking\npopulations.\nYou heard this in Haim's talk.\nHe was using these\nwords interchangeably.\nAgain, this may be\nobvious to you guys,\nbut I thought it's\nworth going through.\nSo this is like Marr\nlevel two, representation.\nHow do we instantiate\nthese solutions?\nSo this is still\nlevel two algorithms,\nor mechanisms that actually\nbuild useful feature\nrepresentations.\nNeuroscientists will think about\nneuronal wiring and weighting\npatterns that are actually\nexecuting those algorithms.\nThis is what we think is\na bridging language there.\nAnd then there's\nthis deeper level\nthat came up in the\nquestions, which\nis, how would you construct\nit from the beginning?\nLearning rules, initial\nconditions, training images,\nare words that are used here.\nThere is a learning machine.\nHere, neuroscientists talk\nabout plasticity, architecture,\nand experience.\nBut again, those are\nsimilar questions just\nwith different language.\nAnd I'm doing this because I\nthink the spirit of this course\nis to try to build these links\nat all these different levels\nhere.\nOK, so hopefully\nthat kind of helps\norient you to how\nwe think about it.\nLet me just go and say, I\nwant to talk about number one.\nWhat is a problem we're trying\nto solve and why is it hard?"], "1330": [19, 30, "and I showed you that MIT\nChallenge and it was difficult.\nMaybe it's hard because\nthere's lots of objects.\nWho thinks that's why it's hard?\nWho thinks that's\nnot why it's hard?\nYou think computers can\nlist a bunch of objects?\nIt's easy, right?\nThis is what I\nsaid about memory,\nit's a big long list of stuff.\nComputers are good at that.\nThere's going to be\nthousands of objects.\nA list of objects is not a\nhard thing for a machine to do."], "1360": [20, 50, "infinite number of images.\nAnd so you somehow\nhave to be able to take\nsome samples of certain\nviews or poses of an object,\nthis is a car under\ndifferent poses,\nand be able to generalize or\nto predict what the car might\nlook like in another view.\nThis is what's called\nthe invariance problem.\nand it's due to the fact\nthat, again, there's\nidentity preserving\nimage variation.\nThis is why the bar code\nreader in your supermarket\nworks fine, because the code\nis always laid out very simply.\nBut when you have to\nbe able to generalize\nacross a bunch of conditions,\npotentially things\nlike background clutter, even\nmore severely occlusion, things\nyou heard from Gabriel, or you\nmay even want to generalize\nacross the class of cars\nwhere the cars have slightly\ndifferent geometry but\nthey're still cars,\nthese kind of generalizations\nare what make the problem hard.\nSo I'm lumping them\nall together in what"], "1410": [21, 140, "Many of you in the room know\nthis is the hard problem.\nAnd I think that hopefully\nit fixes ideas of, that's\nwhat you should think about.\nIt's not the number of\nobjects, but it's the fact\nthat it has to deal with\nthat invariance problem.\nHaim was talking\nabout manifolds,\nand this is my version of that.\nSo this is to introduce\nyou to the problem of,\nwhy that invariance problem--\nwhat it looks like\nor feels like.\nI'm not going to give you\nmath on how to solve it.\nIt's just a geometric\nfeel for the problem.\nSo if you imagine\nyou're a camera--\nor your retina,\nwhich is capturing\nan image of an object,\nlet's call this a person,\nI think I called him Joe.\nSo when you see this image of\nJoe, and this is the retina,\nso now this is a state space of\nwhat's going on in your retina.\nSo it's a million\nretinal ganglion cells.\nThink of them as being an\nanalog value out of each,\nso this is a million\ndimensional state space.\nSo when you see\nthis image of Joe,\nhe activates every retinal\nganglion cell, some a lot,\nsome a little, but he's\nsome point of that million\ndimensional space.\nOK, everybody with me?\nIf everybody's heard all this\nbefore and wants me to go on,\neverybody wave your\nhand and I'll move on.\nAUDIENCE: No, it's good.\nJAMES DICARLO: Keep going, OK.\nSo the basic idea is that if\nJoe undergoes a transformation,\nlike a change in\npose, what that does\nis, it's only a 1\ndegree of freedom\nI'm turning under the hood\none of those latent variables.\nIf I had a graphics\nengine, I'm changing\nthe pose of latent variables.\nIt's only one knob that\nI'm turning, so to speak.\nAnd that means there's\none line through here\nas Joe projects across\nthese different images here.\nAnd I'm ignoring\nnoise and things.\nThis is just the\ndeterministic mapping\nonto the retinal ganglion cells.\nSo Joe goes--\n[MOVING NOISE]\n--and he goes over here.\nAnd if I turn the other\nknob, he goes over here.\nAnd so I could imagine,\nif I turned those two\nknobs of two axis\nopposed always possible\nand plotted this in the million\ndimensional state space,\nthere'd be this curved\nup sheet of points,\nwhich you could think of\nJoe's identity manifold\nover those two degrees\nof view change.\nIt's only two dimensions,\nit's hard to start\nshowing more than this.\nBut it's this curved\nup sheet of points.\nEverybody with me so far?\nYou don't actually\nget to see all those.\nYou could imagine a machine\nactually running them all,\nbut you don't really\nget to see them.\nYou've got to get\nsamples of them.\nBut there's some underlying\nmanifold structure here.\nNow, what's interesting and\nwhat's important to point out\nis that this thing,\neven though I've drawn\nit and it's a little curve,\nbut it's highly complicated\nin this native pixel space."], "1550": [22, 130, "And the reason that\nmatters, and this\nis what Haim\nintroduced you to, is\nthat if you want to be\nable to separate Joe\nfrom another object, say\nnot Joe, another person say,\nthen you need a representation.\nI showed you retinal\nganglion cells.\nThis is another\nimaginary state space\nwhere you can take simple tools\nto extract the information.\nAnd the simple tools\nthat we like to use\nare linear classifiers.\nBut you can use\nother simple tools.\nHaim used the exact\nsame description to you\nguys in his talk, that you\nhave some linear decoder\non the state space that can\nsay, oh, they can separate\ncleanly Joe from not Joe.\nSo these manifolds\nare nicely separated\nby a separating hyperplane.\nThat's what these tools tend to\ndo is they like to cut planes.\nThis is one thing\nthey like to do,\nor they want to find\nlocations or regions,\nlike compact regions\nin this space,\ndepending on what\nkind of tool you use.\nBut you don't want\nthe tool having\nto do all kinds of complicated\ntracing through this space.\nThat's basically the\noriginal problem itself.\nSo what you need is, you\nhave a simple tool box,\nwhich we think of as\ndownstream neurons.\nSo a linear classifier,\nas an approximation,\nit's like a dot product.\nIt's a weighted sum, which is\nwhat we think, neuroscientists,\nof downstream neurons doing.\nSo it's a weighted sum.\nAnd if we want an\nexplicit representation\nin some neural\nstate space, then we\nneed to be able to take\nweighted sums of some population\nrepresentation to be able to\nseparate Joe from not Joe,\nand Sam from Jill, and\neverything from everything else\nthat we want to separate.\nIf we had such a space\nof neural population,\nwe'd call that a\ngood set of features\nor an explicit representation\nof object shape.\nAnd for any\naficionados here, it's\nnot just cleanly\nlinear separation,\nit's actually being\nable to find this\nwith a low number of\ntraining examples.\nSo that turns out\nto be important.\nBut it helps to fix ideas to\nthink about linear separation,\nideally with a low number\nof training examples.\nSo that's a good representation.\nAnd notice, I'm starting\nto mix up terms here.\nI am assuming, when\nI talk about shape,\nthat that will map\ncleanly to identity,\nor what you might call\nbroadly, category.\nThat's another topic I won't\ntalk about, if you just\nthink about the shape of Joe,\nor separating one geometry\nfrom another."], "1680": [23, 70, "Cox, who's now at Harvard, did.\nThis is a number of years old.\nThis takes these two\nface objects, render them\nunder changes, and view.\nAnd then he actually\nsimulated the manifolds\nin a 14,000 dimensional space.\nAnd then he wanted\nto visualize it.\nAnd because we\nwanted to try to make\nthe point that these\nmanifolds of these two objects\nare highly curved\nand highly tangled,\nthis is a three\ndimensional view.\nRemember, it's sitting on a\n14,000 dimensional simulation\nspace.\nYou can't view that space.\nThis is a three\ndimensional view of it.\nAnd the point is that it's\nlike two sheets of paper\nbeing all crumpled up together\nand they're not fused.\nThey look fused here because\nit's in three dimensions.\nBut they're not actually fused.\nBut they're complicated,\nyou can't easily\nfind a separating hyperplane\nto separate these two objects.\nWe call these tangled\nobject manifolds.\nAnd really, they're tangled\ndue to image variation.\nRemember, if I didn't change\nthose knobs of view or position\nor scale, there would just\nbe two points in the space\nand it would be easy.\nThat's the easy problem\nof listing objects.\nBut if they have to undergo\nall this transformation,\nthey become these\ncomplicated structures"], "1750": [24, 70, "So the problem\nthat's being solved\nis, you have this\nretina sampling data,\nlike a camera on the front\nend, where things look\ncomplicated with respect\nto the latent variables,\nin this case shape or\nidentity, Sam or Joe.\nAnd that they somehow are\ntransformed, as Haim mentioned,\nthey're transformed by some\nnon-linear transformation,\nsome other neural population\nstate space, shown here, where\nthe things look more like this.\nThe latent variable\nstructure is more explicit,\nthat you can easily take things\nlike separating hyperplanes\nto identify things like\nshape, which again, roughly\ncorresponds to identity or\nother latent parameters,\nlike position and scale.\nYou maybe haven't thrown\naway all these other latent\nparameters.\nAnd if I have time, I'll\nsay something about that\nso you don't just get identity.\nBut if you can\nuntangle this, you\nwould have a very nice\nrepresentation with regard\nto those originally\nlatent parameters.\nThat's the dream of\nwhat you'd like to do.\nIt's like reverse\ngraphics, if you will.\nSo this is what we call an\nuntangled explicit object\ninformation.\nAnd we think it lives\nsomewhere in the brain,\nat least to some degree.\nAnd I'll show you the\nevidence for that later on.\nSo what you have then is you\nhave a poor encoding basis,\nthe pixel space.\nAnd somewhere in the brain\nis a powerful encoding basis,\na good set of features.\nAnd as Haim mentioned,\nas I already said,\nthis must be a\nnon-linear transformation"], "1820": [25, 120, "rotations of that\noriginal space.\nSo now let's go down\nto-- actually this\nwould be Marr level three.\nLet's go to instantiation.\nLet's get into\nthe hardware here.\nWe're supposed to be\ntalking about brains.\nSo I'm going to give you a\ntour of the ventral stream.\nSo we would love to know\nhow this brain solves it.\nThis is the human brain.\nThis is a non-human primate.\nThis is not shown to scale.\nThis is blown up\nto show you it's\na similar structure,\ntemporal lobe, frontal lobes,\noccipital lobe.\nThere is a non-human primate.\nWe like this model for\na number of reasons.\nOne reason that we\nlike it is that they\nare very visual\ncreatures, their acuity\nis very well matched to ours.\nIn fact, even their object\nrecognition abilities\nare actually quite\nsimilar to our own.\nThis may be surprising\nto you, but let me just\nshow you some data for that.\nThis is actually data from\nRishi Rajalingham, in my lab.\nIt says, impressed,\nbut this just came out.\nThis is the confusion\nmatrix patterns\nof humans trying to\ndiscriminate different objects\nunder those transformations\nthat I showed you earlier,\nwhere they're not\njust seeing images,\nbut they have to deal\nwith these invariances.\nAnd this is rhesus monkey data\ntrying to do the same thing.\nAnd the task goes, I'll\ngive you a test image\nand then you get choice images.\nWas it a car or a dog?\nI'll show you an image, what\nchoice was it, a dog or a tree?\nAnd you're trying to entertain\nmany objects all at once,\nand you get an image under\nsome unpredictable view\nand unpredictable\nbackground, and then\nyou have to make a choice.\nSo this is the\nconfusion difficulty.\nAnd when you look at\nthis, it's intuitive\nthat these are sort\nof geometry similar.\nCamel is confused with dog, and\ntank is confused with truck,\nand that's true of both\nmonkeys and humans.\nAnd to some level, this\nshouldn't be surprising to you.\nThe same tasks that are\ndifficult for humans\nare difficult for monkeys\nbecause probably they\nshare very similar\nprocessing structures.\nThey don't have to bring\nin a bunch of knowledge\nabout tanks are driven by people\nor that, they just have to say,\nwas there a tank or a truck.\nAnd under those conditions,\nthey make very similar patterns\nof confusion.\nAnd these patterns are\nvery different from those\nthat you get when\nyou run classifiers"], "1940": [26, 60, "But they're very similar\nto each other, in fact,\nare statistically\nindistinguishable,\nmonkeys and humans, on these\nkind of patterns of confusion.\nOK, so that's one reason we like\nthis subject, the monkey model,\nis that the behavior is very\nwell matched to the humans.\nThe other reason is that we\nknow from a lot of previous work\nthat I alluded to, that some\nstudies have shown that lesions\nin these parts of the brain can\nlead to deficits in recognition\ntask.\nSo again, we think the ventral\nstream solves recognition.\nSo we know a weak word\nmodel of where to look,\nwe just don't know exactly\nwhat's going on there.\nJust to orient\nyou, these ventral\nareas, V1, V2, V4, and infer\ntemporal cortex, or IT cortex--\nIT projects anatomically\nto the frontal lobe\nto regions involved in\ndecision and action,\nand around the bend to the\nmedial temporal lobe to regions\ninvolved in formation\nof long-term memory.\nBecause these are\nmonkeys and not humans,\nand Gabriel mentioned this\nin his talk, we can go in\nand we can record\nfrom their brains,\nand we can perturb neural\nactivity in their brains\ndirectly.\nAnd we can do that\nin a systematic way."], "2000": [27, 90, "to a human model.\nOK, as neuroscientists\nnow, we've\ntaken a problem,\ntranslated it to behavior,\ntaken that behavior into\na species we can study,\nwe know roughly where\nto look, and now\nwe want to try to\nunderstand what's going on.\nSo as engineers, we take these\ncurled up sheets of cortex\nand think of them as I've\nalready been showing you,\nas populations of neurons.\nSo there's millions of neurons\non each of these sheets.\nI'll give you numbers\non a slide coming up.\nThere's some sort of processing\nthat may be common here,\nI put these T's\nin, there might be\nsome common cortical algorithm\nprocessing forward this way.\nThere's also\ninter-cortical processing.\nAnd there's also some feedback\nprocessing going on in here.\nSo all that's schematically\nillustrated in this slide\nthat I'll keep\nbringing up here when\nwe talk about these different\nlevels of the ventral stream.\nNow I'm most going to be\ntalking about IT cortex here\nat the end.\nWhy do we call these\ndifferent areas?\nOne reason is that there's\na complete retina topic\nmap, a map of the whole\nvisual space in each\nof these different levels.\nIn retina, there's one.\nIn LGN-- in the thalamus,\nthere's another.\nIn V1, there's another map.\nIn V2, there's another map.\nIn V4, there's another map.\nIn IT, it's less clear\nthat it's retinotopic,\nwe're not even sure\nthat IT is one area.\nMaybe we'll have time, I'll\nsay more about that detail.\nSo it's not that\nretinotopic in IT,\nexcept the most\nposterior parts of IT.\nBut that's why\nneuroscientists divide these\ninto different areas.\nSo a key concept, though,\nfor you computationally is,\nthink of each of these as\na population representation\nthat's retransforming the data\nfrom that complicated space\nto some nicer space.\nAnd it's doing this probably\nin a stepwise, gradual manner."], "2090": [28, 80, "basis that I alluded\nto earlier, where\nyou have these nice\nflattened object manifolds.\nAnd I'll show you the\nevidence for that.\nThis is recently from a\nreview I did that gives\nmore numbers on these things.\nAnd I've sized the\nareas according\nto their relative cortical\narea in the monkey.\nHere's V1, V2, V4, IT.\nIT is a complex of areas.\nAnd I'm showing you\nthese latencies.\nThese are the\naverage latencies in\nthese different visual areas.\nYou can see, it's\nabout 50 milliseconds\nfrom when an image\nhits the retina\nuntil you get activity in V1.\n60 in V2, 70-- there's\nabout a 10 millisecond step\nacross these different areas.\nSo it's about 100 millisecond\nlag between an image it's here,\nand you start to see changes\nin activity at this level\nup here that I'm referring to.\nWhen I say IT, I'm referring\nto AIT and CIT together.\nThat's my usage of the\nword IT for the aficionados\nin the room.\nAnd that's about 10 million\noutput neurons in IT\njust to fix numbers.\nIn V1 here, you have like\n37 million output neurons.\nThere's about 200 million\nneurons in V1, similar in V2.\nAnd many of you probably\nheard about other parts\nof the visual system.\nHere's MT, many of you\nprobably heard about MT.\nSo you can see it's tiny\ncompared to some of these areas\nthat I'm talking about here.\nI'm going to show\nyou some neural dam--\nI'm just going to\ngive you a brief tour"], "2170": [29, 80, "But at least those of\nyou who haven't seen this\nshould at least be exposed.\nSo in the retina--\nyou guys know in\nthe retina there's\na bunch of cell\nlayers in the retina.\nThe retina is a\ncomplicated device.\nI think of it as a\nbeautiful camera.\nSo you're down in the retina.\nTo me, the key\nthing in the retina\nis in the end you've got some\ncells that are going to project\nback along the optic nerve.\nSo these are the\nretinal ganglion cells,\nthey actually live\non the surface.\nThe light comes through,\nphoto receptors are here,\nthere is processing in\nthese intermediate layers,\nand then there's a bunch of\nretinal ganglion cell types.\nThere's thought to be\nabout 20 types or so.\nThe original\nphysiology, there are\ntwo functional central\ntypes where they\nhave on center or off center.\nLet's take an on center\ncell, you shine light\nin the middle of\na spot-- now this\nis a tiny little\nspot on the retina,\nthe size depends on where\nyou are in the visual field.\nBut you shine a little bit\nof light in the center,\nthe response goes up.\nSee the spike rate\ngoing up here.\nPut light in the surround,\nthe response rate goes down.\nSo it has an on center,\noff surround profile.\nAnd then there's\na flip type here.\nSo that's the basic\nfunctional type.\nWhen you think\nabout the retina, it\nis tiled with all of\nthese point detectors that\nhave some nice center\nsurround effects.\nThere's some nice gain control\nfor overall illumination\nconditions.\nBut my toy model\nof the retina, it's\nbasically a really nice\npixel map coming back down"], "2250": [30, 10, "OK, I'm going to skip the\nLGN and go straight to V1.\nPeople have known for a\nlong time, functionally V1"], "2260": [31, 80, "They have what's called\norientation selectivity.\nHopefully this isn't\nnew to you guys.\nHere's a simple cell in V1.\nIf you shine a bar\nof a light on it\ninside it's receptive field--\ndoes everyone know what\na receptive field is?\nI don't want to go--\nOK.\nIt's OK if you\nask, because I want\nto make sure you guys are OK.\nSo the receptive field, you\nshine a bar light in it,\nturn it on in the\nright orientation,\ngives good response\nout of the cell.\nMove it off this position,\nnow not much response,\nthere's a little bit of\nan off response here.\nChange the orientation,\nnothing happens.\nFull field illumination,\nnothing happens.\nOK, so this is\ncalled selectivity.\nThat is, there's some\nportion of the image space\nthat it cares about.\nIt doesn't just\nrespond to any light\nat that spot like the pixel\nwise, retinal ganglion cell\nwould.\nSo now there's this\ncomplex cell that's\nalso in V1, which\nmaintains this orientation\nselectivity across a\nchange in position,\nas shown here, also across\nsome changes in scale.\nSo it maintains it, meaning\nthat you have this tolerance--\nso that's called position\ntolerance, for position.\nYou can move the bar around it,\nstill likes that oriented bar.\nBut you change its\nangle and it goes down,\nso it still maintains\nthe same selectivity here\nbut it has some tolerance.\nSo you get this build up of\nsome orientation sensitivity\nfollowed by some tolerance."], "2340": [32, 60, "that they thought that\nyou could build this first\nand then you build\nthese out of these,\nthat's the simple version.\nAnd here they are.\nThese are the Hubel\nand Wiesel models,\nhow you build these and like\noperators to build selectivity\nfrom pixel-wise cells with\nan and like operator lining\nthese up correctly.\nYou can imagine oriented\ntuned cells built this way.\nThere's evidence for\nthis in physiology\nthat this is how\nthese are constructed.\nThe tolerance of\nthese complex cells\nis thought to build by a\ncombination of simple cells.\nAnd there's some\nevidence for this.\nAnd this is again, all the\nway from Hubel and Wiesel,\nwho won a Nobel Prize for this\nand related work in the 1960s.\nAnd then there were a bunch\nof computational models\nthat are really\ninspired by this and I\nthink are still the core\nmodels of how the system works.\nAnd some of the original\nones that were written down\nare Fukushima in\nthe '80s, and then\nTommy Poggio and others built\nwhat's called an HMAX Model,\nyou guys have\nprobably heard about,\nthat's off of these\nsimilar ideas, much more\nrefined and much more\nmatched to the neural data.\nBut I'm just try to\npoint out that these kind"], "2400": [33, 10, "what inspired this class of\nlargely feedforward models\nthat you heard about much today."], "2410": [34, 160, "Now, what's going on in V2?\nFor a long time,\npeople thought it\nwas hard to tell the\ndifference from V1 and V2.\nAnd I just thought\nI'd show you guys,\nthis is a slide I stuck\nin, this is from Eero\nSimoncelli and Tony Movshon.\nAnd I think you guys have Eero\nteaching in the course a bit\nlater, so he may\nsay some of this.\nBut V2 cells have some\nsensitivity to natural image\nstatistics that V1 cells don't.\nAnd maybe I'll see if I\ncan take you through this.\nSo the way that they did\nthis is you can simulate--\nso this is all driven off of\nwork that Eero and Tony have\ndone--\nespecially Eero has done\non texture synthesis.\nSo you have these\noriginal images,\nand if you run them through a\nbunch of V1-like filter banks,\nand then you take a new\nimage, a random seed, which\nis like white noise,\nand you try to make sure\nthat it would\nactivate populations\nof V1 cells in a\nsimilar way, there's\na large set of images that would\ndo that because you're just\ndoing summary\nstatistics, but these\nare some examples of them.\nFor this image, this is one\nthat one might look like.\nSo you can see, to you, it\ndoesn't look the same as this.\nBut to V1, these are\nmetamers, they're\nvery similar in the\nsummary statistics in V1.\nAnd then you start taking cross\nproducts of these V1 summary\nstatistics and then\nyou try to match those.\nAnd what's interesting\nis you start\nto get something that looks,\ntexture wise, much more\nlike this original image.\nAnd this is a big part of\nwhat Eero and others did\nin that work.\nAnd the reason I'm\nshowing you this\nis that Tony's lab has gone\nand recorded in V1 and V2\nwith these kinds of stimuli, and\nthe main observation they have\nis that V1 doesn't care whether\nyou show it this or this.\nTo V1, these are\nboth the same, which\nsays we have the\nsummary statistics\nfor V1 right in terms of\nthe average V1 response.\nThat's all I'm showing you here.\nThe paper, if you want\nit, is much more detailed.\nBut you go to V2 and\nthere's a big difference\nbetween this, which V2 cells\nrespond to more, and this,\nwhich they respond to less.\nAnd really one inference\nyou can take from this\nis that V2 neurons apply a\nrepeated-- another and like\noperator on V1.\nThat's a simple inference\nthat these kinds of data seem\nto support .\nAnd they also tell you that\nthese and-like operators,\nthese conjunctions\nof V1 statistics\ntend to be in the\ndirection of the statistics\nof the natural world, that's\nnaturalistic statistics.\nNow lots of controls\nhaven't been done here\nto narrow in exactly\nwhat kinds ands,\nbut that's the spirit\nof where the field is\nin trying to understand V2.\nEverybody thinks\nit has something\nto do with corners or a\nmore complicated structure.\nBut this is a way that\ncurrent in the field\nto try to move these image\ncomputing models forward\nin V1 and V2.\nAnd Tony likes to point out that\nthis is one of the strongest\ndifferences that you\nsee between V1 and V2,\nother than the\nreceptive field sizes.\nSo I think that's quite\nsome exciting work if you"], "2570": [35, 10, "OK, then you get up into V4\nand things get much murkier.\nSo what's going on in V4?\nWell, let me just briefly say\nthat one of my post-docs-- this"], "2580": [36, 50, "This is Nicole Rust, when she\nwas a post-doc in the lab,\ncompared V4.\nShe actually compared it to IT.\nI'll skip that.\nBut she was using these\nSimoncelli scrambled images.\nThese are actually the\ntexture images from--\nthese are the original\nimages and these\nare the texture versions.\nSo this should look like a\ntextured version of that.\nYou can see that these\nalgorithms don't actually\ncapture the object\ncontent of these images.\nAnd what Nicole actually\nshowed is that similar to what\nyou just saw there, in\nthe earlier work like V1,\nV4 doesn't care about the\ndifferences between these.\nIt responds similarly, as a\npopulation, to this and this,\nand this and this,\nand this and this.\nBut IT cares a lot\nabout this versus this.\nSo this is just repeating the\nsame theme, the general idea\nthat you have and -like\noperators that we think\nare aligned along the\nventral stream that\nare tuned to the\nkind of statistics"], "2630": [37, 50, "And this is some of the\nevidence for it in V2,\nand then later in V4, and\nIT, and Nicole's work,\nif you piece that all together.\nWhen you go to a place\nlike V4, remember V4\nis now like three levels up.\nAnd what does V4 do?\nLook, this is\nJack's work in 1996.\nThis is from Jack\nGallant when he was\nworking with David Van Essen.\nAnd people had some\nideas that maybe there\nare these certain functions\nthat V4 neurons like,\nand they would show these--\nthe same thing people\nhave done in V2,\nthey would show a bunch\nof images like this\nand figure out, well, does it\nlike these Cartesian gratings\nor these curved ones.\nAnd you know what, you\nget out of this is,\nyou could tell some\nstory about it,\nbut you get a bunch of\nresponses out of it.\nThe color indicates\nthe response.\nAnd you kind of look at it, and\npeople would tell some stories,\nbut it really was just\nkind of like tea leaves.\nHere's a bunch of\ndata, we don't really\nknow what these V4\nneurons were doing."], "2680": [38, 40, "And then Ed Connor\nand Anitha Pasupathy\nworked together a\nfew years after that\nto try to figure out more\nabout what V4 neurons do.\nAnd they did things\nlike take images\nlike this, which were\nisolated, and try\nto cut them into parts, like\ncurved parts, pointy parts,\ncurved, concave, convex.\nAnd this was motivated off of\nsome psychology literature.\nAnd they would\ndefine these based\non the center of the object.\nSo this wasn't an\nimage computable model,\nit was just a\nbasis set that they\nbuilt around these\nsilhouette objects.\nAnd so they made this basis set\nabout any kind of silhouetted"], "2720": [39, 50, "They hypothesized\nthat they could\nfit the responses of V4\nneurons in this basis set.\nAnd this was their\nattempt to do it.\nThey could actually\nfit quite well.\nAnd that's kind of\nwhat's being shown here.\nHere's the response\nof a V4 neuron.\nThe color indicates the\ndepth of the response.\nYou can see, this is sort\nof like that previous slide,\nyou're looking at tea leaves.\nIt looks complicated,\nbut under this model\nthey were able to, in the\nshape space, explain about half\nof the response\nvariants of V4 neurons.\nThe upshot is, that V4 curve\nis about some combination\nof curves.\nAnd then later, Scott\nBrincat, with Ed,\nwent on into posterior\nIT and showed\nthat maybe some combinations\nof these V4 cells\ncould fit posterior IT\nresponses quite well.\nSo if you read the\nliterature in V4 and IT,\nyou'll come across\nthese studies.\nAnd they are important\nones to look at.\nUnfortunately,\nthey don't give you\nan image computable model of\nwhat these neurons are doing.\nBut it's some of the work\nthat you should know about"], "2770": [40, 10, "so I'm telling it to you.\nSo let me go on to IT,\nwhich is what I want to talk\nabout for the rest of today.\nAgain, I'm talking\nabout AIT and CIT."], "2780": [41, 40, "And I'll just quickly say\nthat the anatomy, again,\nsuggests that the IT is\nthe central 10 degrees.\nAnd even though V1, V2, and V4\ncover the whole visual field,\nif you make injections\nin V4, that's\nshown here, where\nyou make injections\nin the more peripheral parts\nof the V4 representation, which\nis up here, that you don't get\nmuch projection into IT, which\nis here.\nYou don't see much green color,\nwhereas, you make projections\nin the center part of\nV4, these red sites here,\nyou see much more coverage\ninto IT, which is shown here.\nSo when I say 10\ndegrees, that's rough.\nEverything in biology is messy."], "2820": [42, 130, "there's anatomical evidence\nthat as you go down into IT,\nyou are more and more focused\non the central 10 degrees.\nOK, let me talk about a little\nbit of the history of IT\nrecordings.\nThis is when people got\nexcited about IT, in the 70s.\nThis is work by Charlie Gross,\nwho's one of the first people\nto record an IT cortex.\nAnd I'll show you\nwhat they did here.\nThis was in an era where,\nremember, Hubel and Wiesel\nhad just done their\nwork in the '60s.\nAnd they recorded from\nthe cat visual cortex.\nAnd they had found\nthese edge cells,\nand they ended up winning\nthe Nobel Prize for that.\nSo it was the heyday\nof like, let's record\nand figure out what\nmakes cells go.\nSo they were brave enough to put\nan electrode down an IT cortex\nin 1970 and said, what\nmakes this neuron go.\nRemember, that's an\nencoding question,\nwhat's the image content\nthat will drive this neuron.\nAnd it's fun to just\nlook back on this\nand what they were doing.\nSo they didn't have\ncomputer monitors.\nThey were actually\nwaving around stimuli\nin front of the animals.\nThis is an anesthetized\nanimal on a table.\nThis is a monkey.\nActually, they\nstarted with a cat\nand then they later\nwent to monkey.\nThe use of these stimuli\nwas begun one day when,\nhaving failed to drive a unit\nwith any light stimulus-- that\nprobably means spots\nof light, edges things\nthat Hubel and Wiesel\nhad been using.\nWe waved a hand at\nthe stimulus screen,\nthey waved in front\nof the monkey,\nand elicited a very\nvigorous response\nfrom the previously\nunresponsive neuron.\nAnd then we spent the next\n12 hours-- so the animal's\nanesthetized on the table, their\nrecording from this neuron.\nIt's 12 hours because\nnothing's moving,\nso you can record for\na long period of time.\nSo singular neuron,\nthey're recording,\nlistening to the spikes.\nWe spent the next 12 hours\ntesting various paper cut\nouts in attempt to find\nthe trigger feature.\nYou can see, that's a\nHubel and Wiesel idea,\nwhat makes this neuron go.\nWhat's the best\nthing, that's become\na lot of what the\nfield spent time doing.\nTrigger feature for this unit,\nwhen the entire stimulus set\nwere used, were ranked according\nto the strength of the response\nthat they produced.\nWe could not find a\nsimple physical dimension\nthat correlated with\nthis rank order.\nHowever, the rank order\nof adequate stimuli\ndid correlate with\nsimilarity for us,\nthat means\npsychophysical judged,\nto the shadow of a monkey hand.\nSo these are their rank\norder of the stimuli.\nAnd they say look, it looks like\nit's some sort of hand neuron."], "2950": [43, 70, "I can't find some\nsimple thing on here.\nSo this kind of study then\nlaunched a whole domain\nwhere people started to go\nin to record these neurons\nand they found interesting\ndifferent types.\nBob Desimone, who worked\nwith Charlie Gross,\nlater showed much more\nnicely under more controlled\nconditions, yes, there are\nindeed neurons that respond.\nYou can see more to these hand--\nthis is the post stimulus time\nhistogram, lots of spikes, lots\nof spikes, lots of spikes--\nrespond more to these hands\nthan to these other kind\nof stimuli here.\nSo you could say,\nthese neurons have\ntuned to specific combinations\nof high selectivity.\nYou'll hear from\nWinrich that others\nhad shown that you\ncould record some\nof the neurons are really like\nfaces that you could find,\nand not so much hands.\nSo you could find\nneurons that seem\nto have some interesting\nselectivity in IT cortex.\nAnd then others\nlater went on to show\nin a number of studies-- this\nis from Nico Logothetis' work\nof a number of years later.\nIt's just one example that this\nselectivity had some tolerance\nto, say, the position\nof the stimulus, that's\nwhat's shown here.\nThe fact that these\nbars are high just\nmeans that it tolerates\nmovement in where the--\nsorry, this is size,\ndegrees of visual angle.\nThis is position, moving\nthe stimulus around.\nSo this was known\nfor a number of years\nthat there's some tolerance\nto position and size\nchanges at least."], "3020": [44, 10, "there's some selectivity\nand there's some tolerance.\nAnd that should remind you of\nwhat we already said in V1,\nthere's some selectivity,\nsimple cells.\nThere's some tolerance,\ncomplex cells."], "3030": [45, 60, "just different kinds of\ntypes of stimuli being used.\nThen people really went on, in\nthe 80s especially, and said,\nlet's go after this\ntrigger feature.\nAnd Tanaka's group really\nwent after this really hard.\nTanaka's group would\nfind the best stimulus\nthey would find, dangle\na bunch of objects\nin front of a recorded\nneuron, find the best out\nof a whole set of\nobjects, and then they\ntry to do a reduction.\nThey'd try to figure out,\nhow can I reduce this.\nThis is their attempt to reduce\nthe stimulus to its features\nwithout lowering\nthe neural response.\nSo high response, high response,\nhigh response, high response,\nhigh response, suddenly I\ndo this, the response drops.\nI do this, the response drops.\nAnd they have lots\nof examples of this.\nAnd they want you to try to\nget to the simplest thing that\ncould capture the response.\nAnd when they did this, they\nwould take stimuli like this,\nand end up with stimuli\nthat looked like that.\nNow, many of you should\nprobably start to wonder here,\nthere's lots of paths\nfor stimulus space.\nIt's not clear that these\nare elemental in any way.\nThere's lots of ways that\nyou can show with modeling"], "3090": [46, 60, "here.\nThis is just, again,\na history of the work.\nThis is the kind of things\nthat people were doing.\nAnd then from that,\nthey presented\nwhat we think of as the\nice cube model of IT,\nthat I think is actually still\na very reasonable approximation.\nThey not only\nshowed that neurons\ntended to like certain\nrelatively reduced stimulus\nfeatures, not full\nobjects, but that they\nare gathered together.\nSo these are millimeter\nscale regions of IT\nthat nearby neurons,\nwithin a millimeter or so,\nhave similar preferences.\nThey're not just\nscattered willy-nilly\nthroughout the tissue.\nWhen you go record nearby\nneurons, they're similar.\nSo there's some mapping\nwithin IT cortex.\nThis is schematic here.\nThis is optical imaging\ndata of IT cortex also\nfrom Tanaka's\ngroup that show you\nthat these different\nblobs of tissue\nget activated by different\nimages shown here.\nAnd I'm just showing\nyou the scale of this,\nit's around a little\nless than a millimeter.\nAnd our lab has\nevidence of this too."], "3150": [47, 70, "but we really don't really\nyet understand the features,\nthese elemental features yet,\nor at least, not at this time.\nThen later, there's lots\nof beautiful work in IT.\nAgain, I'm probably not\ntelling you all of it.\nSome of the most\nexciting work recently--\nand you'll hear about\nthis from Winrich,\nthat people started\nto use fMRIs.\nSo Doris Tsao and Winrich\nFreiwald and Marge Livingstone\nall together started to\nuse fMRI data to compare\nfaces versus objects.\nThis was motivated\nfrom human work,\nby work like Nancy\nKanwisher lab and others.\nWhat they found was\nthat in monkeys,\nyou could find different\nparts that would show up,\nwhat are called\nface patches, where\nyou have a relative preference\nfor faces over objects.\nAgain, I don't want to take\nall of Winrich's talk here,\nbut you have these\ndifferent patches here.\nAnd then what's really cool\nis, you go in and record\nfrom these patches and then you\nfind a very enriched locations\nfor face neurons.\nAnd these enriched\nlocations were known\nfrom a number of other studies.\nBut this is a nice correlation\nbetween functional imaging\nand this enrichment\nof these face cells.\nAnd that's what's shown here,\nthat these neurons respond\nmostly to faces and not\nso much other objects.\nAlthough, you see they still\nsort of respond to these.\nSo this kind of says\nfMRI and physiology are\ntelling you similar things."], "3220": [48, 20, "for face-like objects, at a\nscale of a few millimeters\nor so, the size\nof these patches.\nOK, so that's larger\nscale organization.\nThis is data from our own lab\nthat shows the same thing.\nMaybe I'll just skip through\nthis in the interest of time--\nthat we can map and record\nthe neurons very precisely,"], "3240": [49, 20, "So this is just a larger field\nof view maps of the same idea.\nSo what we have\nthen, just to wrap up\nthis whirlwind tour\nof the ventral stream,\nis that we had some untangled\nexplicit information.\nAnd what I want to try to\nconvince you of now, is that--\nI've told you about\nthe ventral stream,"], "3260": [50, 130, "this is a powerful\nrepresentation for encoding\nobject information.\nAnd then we'll take a break\nbecause we've already probably\nbeen going a while.\nYeah, about 10 more minutes\nand then we'll take a break.\nSo what I've told you is, I've\nled you up the ventral stream,\nI've given you a\nbit of the history,\nso now let's talk about\nIT more precisely.\nSo now this is work\nfrom my own lab.\nYou go in and record IT.\nYou go record extracellularly.\nYou travel down into IT\ncortex, which is down here.\nAnd you record from this.\nAnd similar to what you\nsaw, another version of what\nyou saw from Charlie\nGross or Bob Desimone,\nyou show a bunch of images.\nAnd they could be\narbitrary images.\nYou take an IT recording site,\nand see these little dots,\nthose are action potential\nspikes out of a particular IT\nsite.\nAnd these are repeatable.\nYou have some Poisson\nvariability here.\nBut you see that there's\nmore spikes here,\nthere's little more here,\nless here, less there.\nThese images are all\nrandomly interleaved\nwhen you collect the data,\nas I'll show you in a minute.\nAnd you go to different sites\nand it likes different images.\nSo there is certainly\nsome image selectivity.\nThis should not be surprising\nbecause I already showed\nyou this from previous work.\nThis is just data\nfrom our own lab.\nYou can also see now that\nyou are looking closely\nat the time lag, remember, I\nsaid around 100 milliseconds\nstimulus on.\nStimulus off, the\nstimulus is actually off\nbefore the spikes actually\nstart to occur out\nhere in IT because,\nagain, there's a long time\nlag, 100 milliseconds.\nOK, so that's what the\nneural responses look like.\nI don't know if you\nguys can hear this,\nmaybe I should have\nhooked up audio.\nMaybe you might\nbe able to hear--\nthis is actually a\nrecording that Chou Hung did\nwhen he collected his data in\nmy lab for the early studies\nwe did in the lab.\nI don't know if\nyou guys can hear.\n[STATIC]\n[BEEP]\n[BEEP]\n[BEEP]\nThose high beeps are the\nanimal getting reward\nfor fixating on that dot.\nYou're not even going to\nbe able to parse that.\nI mean, you hear the\nspikes clicking by, those--\n[STATIC]\nThose are action potentials.\nAnd I don't expect you to look\nat anything like, oh, it's\na face neuron, or whatever.\nI just want you to get a\nfeel for how those data were\noriginally collected.\nThis is a pretty grainy video.\nBut you get the idea.\nYou collect data like that.\nAnd again, you can find\nselectivity in those population"], "3390": [51, 40, "But then, Gabriel and Tommy\nand I, so the three of us,\nI think all in this\nroom, way back when\nin 2005 said, well look,\nthe population of IT\nmight have good,\nuseful information\nfor solving this\ndifficult object manifold\ntangling problem.\nIt might be a good\nexplicit representation.\nSo we did a, what I call,\nearly test of this idea.\nWe took this simple image set\nfrom eight different categories\nthat we had chosen.\nAnd there's good stories of\nwhy we chose those objects,\nif you like to hear them.\nBut let me just say, simple\nobjects, we moved them\nacross position and scale,\nand we collected the responses\nof IT of a bunch\nof sites to changes"], "3430": [52, 30, "And we showed them\nas I just showed you.\nWe just showed them\nfor 100 milliseconds.\nThis is this core\nrecognition regime,\nwere just showing them\nfor 100 milliseconds.\nAnd then we show another\none, and they're just\nrandomly interleaved.\nAnd from this,\nwhat you do is you\ncould get a\npopulation set of data\nwhere we recorded 350 IT sites.\nHere's a sample of 63 sites.\nThis is 78 images, the\nmean neural response\nhere is the mean\nresponse to an image.\nThis is 78 of the\nimages we showed.\nThere's nothing for you to read\ninto here to say, other than,"], "3460": [53, 60, "And now our question is, well,\nwhat lives in this population\ndata that we've collected.\nIs it explicit with\nregard to categories?\nSo we come back to\nwhat I showed you\nearlier about those\ntangled manifolds and said,\nwe need simple decoding tools.\nCan a simple decoding tool\nlook at that population\nand tell me what's out there?\nAnd again, we were using\nlinear classifiers at the time,\nbecause we took\nthat, as you heard\nfrom Haim as our operational\ndefinition of what\na simple tool is.\nAnd if it could decode\ninformation about the object\nidentity, then we'd\nsay, well, that means,\nby that operational\ndefinition, this\nis explicit, available,\naccessible information, or just\ngenerally good.\nSo if you imagine that the\nactivity-- this is schematic.\nEach dot, this is\nneuron one, neuron two,\nand you could have a\nbunch of IT neurons.\nBut if you can\nseparate any object\nfrom all the other\nobject, these points\nrepresent the\npopulation response\nto each image of an object.\nRemember, there's many\nimages of each object.\nBut if you could\nlinearly separate that,\nthat would mean it was explicit.\nAnd if you had a hard\ntime separating it,\nthis would be implicit."], "3520": [54, 50, "This is Inaccessible,\nor bad, information.\nSo we just-- we,\nand when I mean we,\nI mean Chou Hung,\nwho led the study.\nGabriel, Tommy, and I did this.\nWe took the response of\nan image, like this one.\nIt produced a population vector.\nAgain, we recorded\na bunch of neurons.\nWe recorded them sequentially\nand then pieced together\nthis population vector.\nSo these are the\nspikes simulated off\na population of IT.\nWe could do various things.\nIn fact, I think Gabriel\ndid everything possible,\nas I remember at the time.\nAnd one of the things we\ndid was just count spikes.\nOne of the simple things, that\nturns out to work quite well,\nis count the spikes\nover 100 milliseconds.\nSo this neuron counts spikes.\nThat gives you a\nnumber, one number here,\ncount spikes get one number.\nSo you have n neurons,\nyou get n numbers.\nSo it's a point in a n\ndimensional state space where\nn is the number of neurons.\nAnd then we had already\npre-divided the images"], "3570": [55, 90, "These are the categories.\nAnd again, we just\nasked how well\nyou could do faces\nversus non-faces,\ntoys versus non-toys,\nso on and so forth.\nThese are old slides.\nBut you get the idea,\nis that basically, you\ndon't need that many\nsites to already get\nto very high levels\nof performance\non both categorization\nand identification.\nThe interesting\nthing about this was\nthat you could\nsolve simple forms\nof this invariance problem\nin this representation\nquite easily.\nThat if you just trained on\nthe central objects, the center\nand size, the simple three\ndegree size center position,\nand test it on the\nsame thing, just\nheld out repeats of this\ndata, you did quite well.\nThat's a baseline.\nBut what's interesting is you\ntest at different position\nand scale.\nAnd then you also do\nalmost nearly as well.\nSo you naturally generalize\nto these other conditions\nby training on these\nsimple conditions.\nSo this is evidence\nthat the population\nis a good basis set for\nsolving these kind of problems.\nA few number of training\nexamples on this population\nthen generalizes,\nwell, across conditions\nmakes the problem hard.\nSo again, we published\nthat a long time ago.\nThis was an early\nstep to say, look,\nthe phenomenology looks\nright for the story that I've\nbeen telling you so far.\nYou can't do this easily in\nearlier visual areas like V1,\nor simulated V1 or V4.\nAnd we later show\nthat a number of ways.\nThis is consistent with\nwork I was showing you\nwith Logothetis\nposition tolerance, size\ntolerance, the selectivity.\nIt's really just an explicit\ntest of the idea population"], "3660": [56, 100, "So the take home here is that\nthere's this explicit object\nrepresentation in IT.\nI didn't prove to\nyou that this is\nthe link, this predictive\nmodel to decoding yet.\nWe're going to talk\nabout that next.\nBut this was some of\nthe important population\nphenomenology that we did.\nWhat I try to tell you today--\nhopefully I've introduced you\nto the problem of visual object\nrecognition and the\nway we restricted it\nto core object recognition.\nWe talked a lot about predictive\nmodels as being the goal,\nalthough I haven't\npresented much to you yet.\nHopefully, that's the\nsecond part of the talk.\nI've given you a tour\nof the ventral stream.\nBut it was a poor tour.\nI'm sure everybody\ni work with would\nsay that you've neglected\nall this work because there's\nno way I can do that all\nin even a whole week.\nI just tried to hit some\nof the highlights for you.\nAnd I told you that\nthe IT population\nseems to have solved\na key problem,\nthis sort of invariance\nproblem that I set up.\nAnd one way to step back and\nsay, over the last 40 years\nor so, from those early\nstudies of Charlie Gross\nor even Hubel and Wiesel, we,\nthe field of ventral stream\nphysiology, we've largely\ndescribed important\nphenomenology.\nEven that last study is\npopulation phenomenology.\nAnd so now we need these\nmore advanced models.\nSo the next phase of the field\nis developing and testing\nthese predictive\nmodels that I've\nmotivated at the\nbeginning, but I\nhaven't given you much of yet.\nSo this was hopefully a bit\nof history and set context\nto where we are."]}