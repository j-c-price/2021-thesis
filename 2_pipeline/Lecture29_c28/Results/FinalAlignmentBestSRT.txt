{"0": [58, 10, "The following content is\nprovided under a Creative\nCommons license.\nYour support will help\nMIT OpenCourseWare"], "10": [59, 30, "To make a donation, or\nview additional materials\nfrom hundreds of MIT courses,\nvisit MIT OpenCourseWare\nat OCW.MIT.edu.\nJAMES DICARLO: I'm\ngoing to shift more\ntowards this decoding\nspace than we talked about,\nthe linkage between neural\nactivity and behavioral report.\nAnd I introduced that a bit.\nYou just saw that there's some\npopulation powerful activity\nin IT.\nAnd I'm going to expand\non that a bit here."], "40": [60, 40, "what I call an end\nto end understanding,\ngoing from the image all\nthe way to neural activity\nto the perceptual report, one\nof the things we want to do,\nagain, is just define\na decoding mechanism\nthat the brain uses to support\nthese perceptual reports.\nBasically what neural\nactivity are directly\nresponsible for these tasks?\nAnd I'll come back to\nlater this encoding side.\nIt's like, you know,\nand notice I'm putting\nthese in this order, right?\nSo once you know what\nthe relevant aspects\nof neural activity are in IT,\nor wherever you think they are,\nthen that sets a target\nfor what is the image\nto neural transformation that\nyou're trying to explain?\nNot predict any neural\nresponse, but those\nparticular aspects of\nthe neural response.\nSo that's what I mean by\nthe relevant ventral stream"], "80": [61, 50, "So we start here.\nWe work to here, and then\nwe work to here, rather than\nthe other way around.\nOK, so I'm going\nto try it again.\nKeep with the domain I set up.\nI talked about core recognition.\nI now need to start\nto define tasks.\nI'm going to talk about specific\ntasks that are, for now, let's\ncall them basic level nouns.\nI'm actually going to relax\nthat to subordinate tasks\nin a minute.\nBut here they are.\nCar, clock, cat.\nThese are not the actual nouns.\nI'll show you the ones we use.\nBut just to fix\nideas, we're imagining\na space of all possible\nnouns that you might use\nto describe what you just saw.\nAnd I'm going to have a\ngenerative image domain.\nSo I now have a\nspace of images here.\nI'm not just going to\ndraw these off the web.\nWe're going to\ngenerate our own image\ndomain that we think\nengages on the problem,\nbut gives us control of\nthe latent variables.\nSo I'll show you that now.\nSo the way we're\ngoing to do this\nis by generating one\nforeground object in each image"], "130": [62, 10, "And we just did this by\ntaking 3-D models like these--\nthis is a model of a car.\nWe can control it's other latent\nvariables beyond its identity.\nSo this is a car."], "140": [63, 10, "So there's a couple of latent\nvariables about identity here\nthat relate to the geometry.\nThen there's these position--\nother latent variables like\nposition, size, and\npose that I mentioned,"], "150": [64, 10, "And we can then just,\nlike, render this thing.\nAnd we could place it on any\nold background we wanted to.\nAnd what we did was we\ntended to place them"], "160": [65, 10, "And that creates these sort\nof weirdish looking images.\nSome of them may look\nsort of natural, hence,\nthis looks pretty unnatural.\nBut the reason we did this."], "170": [66, 50, "So-- so we did this because we\ncould add a generative space.\nAnd because it was-- so we know\nwhat's going on with the latent\nvariables we care about.\nAnd we also, when we built this,\nit was challenging for computer\nvision systems to\ndeal with this,\neven though humans could\nnaturally-- you know,\nthey don't have advantage\nof any contextual cues\nhere because by construction,\nthese are uncorrelated.\nWe just took natural\nimages and would randomly\nput objects on them.\nBut this was enough to fool\na lot of the computer vision\nsystems at the time that tended\nto rely on the contextual cues.\nLike blue in the background\nsignals or being an airplane,\nwe didn't want those kind\nof things being done.\nWe wanted the actual\nextraction of object identity.\nAnd again, humans\ncould do it quite well.\nSo that's why we\nended up in this sort\nof maybe this no man's\nland of image space, which\nis not very simple, but not\nImageNet just pulled off\noff the web."], "220": [67, 20, "And just to give you a\nsense that this is actually\nquite doable for humans,\nI'll show you a few images.\nI won't even cue\nyou what they are.\nI'm going to show them\nfor 100 milliseconds.\nYou can kind of shout\nout what object you see.\nAUDIENCE: Car.\nAUDIENCE: [INAUDIBLE]\nJAMES DICARIO: Right.\nSo see, it's pretty\nstraightforward, right?"], "240": [68, 30, "And you know, here's the kind of\nimages that we would generate.\nThis would be-- so when\nwe think of image bags,\nwe think of partitions\nof image space.\nThis is some images that\nwould correspond to faces.\nThese are all images of faces\nunder some transformations.\nAgain, different backgrounds.\nThese are not faces.\nThese are other objects\nagain, under transformations.\nAnd we can have as many\nof these as we want.\nWe call this one--\nthis distinction, when\nshown for 100 milliseconds--\nis one core recognition test.\nDiscriminate face for not face.\nHere is a subordinate task."], "270": [69, 10, "This is a particular\ntype of car.\nYou can see it's\nmore challenging.\nAgain, we don't show\nthese images like this.\nThis is just to\nshow you the set.\nWe show them one at a time."], "280": [70, 10, "we're going to try to make\na predictive model using\nthat kind of image space to\nsee if we can understand what\nare the relevant aspects\nof neural activity"], "290": [71, 20, "And when I say we, I\nmean Naiib Maiai and Ha\nHong, who are post-doc\nand graduate student\nthat were in the lab that\nled this experimental work.\nAnd Ethan Soloman and Dan Yamins\nalso contributed to the work.\nSo what we did was to try to\nrecord a bunch of IT activity"], "310": [72, 60, "as I showed you earlier, but\nnow in this more defined space\nwhere we're going to collect\na bunch of human behavior\nto compare possible\nways of reading IT\nwith the behavior of the human.\nThis is how we started.\nWe're now doing monkeys--\nwhere we're recording and\nthe monkey's doing a task.\nBut what we did here was we\njust passively fixating monkeys,\ncompared with behaving humans.\nAnd as I showed you\nearlier, monkeys and humans\nhave very similar\npatterns of behavior.\nSo what we record\nfrom IT, in this case,\nwe were using array\nrecording electrodes.\nThese are chronically implanted.\nThis shows them here.\nYou implant them\nduring a surgery,\nas kind of is shown here.\nDown in the IT cortex.\nYou can get their size here.\nThere are about hundred--\nthere's actually 96\nelectrodes on each of them.\nThey typically yield about\nhalf of the electrodes\nhaving active neurons on them.\nSo you get, you know, on the\norder of 150 recording sites.\nAnd you can lay them out.\nYou can lay-- we would\ntypically lay out\nthree of them across IT and\nV4 to record a population\nsample out of IT.\nAnd we would do this across\namong multiple monkeys."], "370": [73, 30, "This is 168 IT recording sites.\nThis is similar to what\nI showed you earlier.\nThis is the mean response\nin a particular time window\nout of IT, similar to\nwhat I showed you earlier\nin that study with Gabriel.\nAnd what we do here is, I'm just\nshowing you to give you feel.\nThat's one image.\nHere is eight more--\nhere's seven more images.\nAnd these are just\nthe population vectors\nin a graphic form.\nAnd but we actually\ncollected nearly 25--\nthis is 2,560 images.\nThis is sort of\nthe mean response"], "400": [74, 160, "And now you have this again,\nthis rich population data.\nAnd you can ask, what's\navailable in there\nto support these tasks?\nAnd how well does it predict\nhuman patterns of performance\non those tasks?\nSo in this study, that's\nall we were asking to do.\nWe're trying to do\nmore and more recently.\nBut let me show you what all\nwe were trying to do is to say,\nlook.\nOne thing we observed,\neven though you\nsaw that car-- you could\ndo car, you could do faces.\nIt seemed like you\nwere doing 100%.\nTurns out you're better at\nsome things than others.\nSo discriminate-- this is\na deep prime map of humans.\nSo red means good performance.\nHigh D prime.\nYou know, a D prime of\n3 is something like--\nI don't know, psychophysicists\nin the room may correct me.\nA D prime of 3 is sort of\non the order of 90 some 95%\ncorrect, in that range.\nSo these are very high\nperformance levels\nwhen you get up to 5.\n0 is chance.\nSo 50%-- well this\nis an eight way task.\nSo one over 8% correct.\nSo the subjects were doing\neither eight way basic level\ntasks, or eight way subordinate\ncars, or eight way faces.\nAnd these are the D prime\nlevels under different amounts\nof variation of those\nother latent variables\nposition size and pose.\nDon't worry about those details.\nWhat I want you to\nsee is the color here.\nSo look, it's tables versus--\ndiscriminating tables from\nall these other objects.\nYou do that at a\nvery high D prime.\nDiscriminating beetles\nfrom other cars,\nyou do it at slightly\nlower D prime.\nYou can see this, specially\nat a high variation,\nyou're actually starting to\nget down to lower performance.\nAnd faces-- one face\nversus another face,\nyou're actually\nquite poor at that.\nYou're a little bit\nbetter than chance.\nBut it's actually quite\nchallenging in 100 milliseconds\nwithout hair and\nglasses to discriminate\nthose 3-D kind of face models.\nI showed you Sam and\nJoe earlier as examples.\nYou're actually quite\nchallenging to do\nthat for humans in\nthat domain of faces.\nSo, what I want to\nshow you here is\nyou have this pattern of\nbehavioral performance.\nYou have all this IT activity.\nThis is humans.\nThis is monkeys.\nAnd what we wanted\nto do is say, look.\nWe can use this pattern.\nThis is very repeatable\nacross humans.\nCan we use this repeatable\nbehavioral pattern\nto understand what\naspects of this activity\ncould map to that?\nAnd again, this\npattern is reliable.\nI just said that.\nAnd it's not as if you\ncan predict this pattern\nby just running classifiers\non pixels or V1.\nIn fact, I'll show\nyou that a minute.\nBut we thought there's\nsome aspects of IT activity\nthat would predict this.\nAnd we wanted to try to\nfind those aspects to--\nso, again, this was motivated\nby that study I showed you\nearlier.\nSo which part of the\nIT population activity\ncould predict this behavior\nover all recognition tasks?\nWe're seeking a general\ndecoding model that would work.\nHere's some specific tasks.\nBut we'd like it to be--\nwork over any task that we\ncould imagine testing humans\nwithin this domain\nof taking 3D models,\nputting them under variation."], "560": [75, 60, "That was what we\nwere hoping to do.\nSo again, I'll briefly\ntake you through this.\nBecause I already\nshowed you this earlier.\nAgain, we've previously shown\nthat you could kind of take\nthis kind of state\nspace, and say hey,\ncan you separate images\nof faces from non-faces,\nusing these simple\nlinear classifiers, which\nare essentially weighted\nsums on the IT activity?\nAnd now we wanted\nto ask, could this\npredict human behavioral\nface performance,\nand monkey, because again,\nthey're very similar.\nAnd not only would\nthis class of decoding\nmodels that was motivated by the\nearlier work predict this task,\nbut would predict car detection?\nWould the same model predict\ncar one versus car two?\nThat's a subordinate task.\nAnd all such tasks.\nAgain, over the whole domain,\ncan you take a same decoding\nstrategy and take\nthe data and say,\nI'm going to just learn on\na certain number of training\nexamples, build a\nclassifier, and then I'll\nsay that's my model\nof how the human does\nevery one of these tasks.\nAnd if that's true,\nthen it should perfectly\npredict that pattern\nor performance\nthat I just showed you earlier."], "620": [76, 150, "Passively evoked spike rates\nusing single fixed time\nscale that are spatially\ndistributed, because they're\nsampled over IT, over a single\nfixed number of non-human--\nof non-human primate cortex.\nSo a single number of neurons.\nAnd learn from a reasonable\nnumber of training examples.\nSo all of that is a decoding\nclass of models that we thought\nmight work.\nAnd if this is correct--\nthis is what I just\nsaid-- it should predict\nthe behavioral data that we\ncollect.\nFor example, the D prime\ndata I just showed you.\nBut also more fine grained\nbehavioral data in principle.\nSo I want to just\nstep back to make\nit clear that it's not obvious\nthat this should work, right?\nI mean, it depends--\nin the audience, I get people\non completely different sides\nof this, whether this\nshould work or not.\nSo, you know, one thing\nis, like, well look,\nit's passively evoked.\nYou heard Gabriel say, well,\nyou didn't like passive tasks.\nAnd I agree with that.\nIn the ideal world,\nthe animal will\nbe actively doing the task.\nAnd then you'd say, well I'll\nmeasure while the animal's\ndoing the task.\nThat's going to be your\nbest chance of prediction.\nBut we also saw earlier\nthat that passively\nevoked monkey still--\nyou know, nobody would argue\nthat a passively evoked\nretinal data is not going to be\nsomewhat applicable to vision.\nAnd you know, the\nquestion is, how much\nof those arousal effects\nshow up in a place\nlike IT cortex, which\nis typically high?\nWhich is high up in\nthe ventral stream.\nSo you could argue\nboth sides of this.\nBut it's possible that\nattentional arousal mechanisms\nare needed to make this a good\npredictive linkage between that\nto sort of activate IT in this\nsort of crude way, if you like.\nSome people have pointed\nout that you need the trial\nby trial coordinated\nspike timing\nstructure to actually\nmake good predictions,\nthat those are critical.\nSome people have pointed out\nthat you have to kind of assign\ndifferent parts of IT to\nparticular roles, which is\na prior on the decoding space.\nFor instance, that you could\nbelieve that biologically,\nan animal's born.\nThere's some tissue that's\ngoing to be dedicated to faces.\nYou have to wire those neurons\ndownstream to that tissue.\nAnd that means you're going to\nrestrict the decoding space,\nrather than just letting them\nlearn from the space of IT\nas if they collected\nsamples off of all of IT.\nSo I think some\npeople implicitly\nbelieve that even if it's\nnot stated quite that way.\nIT does not directly\nunderlie recognition.\nYou could imagine that.\nI mean, it's not for sure known.\nAnd some lesions of IT\ndon't produce deficits\nin recognition.\nThat's a possibility.\nMaybe you need too\nmany training examples.\nMonkey neural codes cannot\nexplain human behavior.\nYou know, again, but\nI already showed you\nmonkeys and humans\nare very similar.\nSo these are the\nreasons that you\nmight say this is negative,\nand might not work.\nAnd probably\nalready have guessed\nthat I'm telling all these\nnegatives because it turns out\nthis simple thing works quite\nwell for the grain of behavior\nthat I've shown you so far.\nAnd here's my evidence of that."], "770": [77, 50, "that I showed you earlier.\nThis is mean D prime.\nThis is the predicted\nbehavior or performance\nof taking a classifier, reading\nfrom that IT population data\nthat I've shown you, which\ngives a predicted D prime.\nHere is-- we first\nchose a decoder.\nWe had to match things\nlike the number of neurons.\nWe had to get it in\nthe ballpark, so--\nbecause again, there's\na free variable,\nas I showed you earlier.\nThere's at least one.\nBut for now, let's think of\nmatching the number of neurons\nto get you near the\ndiagonal, so that you\nhave sufficient number\nof neural recordings\nto say, how well do you do\non a face detection task?\nAnd then, here's\nall the other tasks.\nThis is those 64 points\nthat I showed you earlier.\nHere's some examples like\nfruit versus other things, car\nversus other things.\nAnd you should see that all\nthese points kind of line up\nalong this diagonal, which\nsays, wow, this is actually\nquite predictive, that I\ncan take this simple thing\nand predict all the stuff\nthat we've collected so far."], "820": [78, 110, "about what is the\ninferred neural mechanism\nthat we're testing here?\nWell, I'll show you in a minute.\nThis is, for each\nnew object, we think\nwhat happens is some\ndownstream observer,\na downstream neuron, randomly\nsamples roughly 50,000\nsingle neurons, spatially\ndistributed over all of IT,\nnot biased to any compartments.\nListens to each IT sites.\nWhen I say listen\nin this case, we\nthink could average\nover 100 milliseconds.\nWe're not sure about this.\nThis is just the version\nthat's shown here.\nLearn an appropriate weighted\nsum of those IT spiking.\nAnd then listen at 10%.\nThat's basically,\nonce you learn,\nthere's a heavily weighted\nabout 10% of the IT neurons\nare heavily weighted\nfor each of the tasks.\nThat's just an observation\nthat we have in our data.\nBut this is trying to map it\nto neuroscientist language\nfrom these decoder\nversions out of IT.\nSo what that is a\nmodel that says,\nlearn weighted sums of 50,000\nrandom average 100 milliseconds\nsingle unit responses\ndistributed over all IT.\nSo a bunch of stuff in\nhere is what your model\nis sort of encapsulating.\nThat's still too long.\nSo I made a little\nacronym out of that.\nAnd that caught Laws of\nRAD IT decoding mechanism.\nSo this is just to say there's\na hypothesis of how everything\nmight work, but now can be make\npredictions for other objects\nand could potentially\nbe falsified.\nSo, so far, this model works\nquite well over these tasks.\nAnd in fact, the\ncorrelation is 0.92.\nYou might look at this and\nsay, oh, it's not perfect.\nBut it turns out that\nthat's about the level\nthat which humans\ndiffer from each other.\nSo it's passing a Turing test,\nthat this mechanism read off\nof the monkey IT hides\nin the distribution\nof the human population that\nwe're asking to also perform\nthese same tasks.\nSo it can't be\ndistinguished from being\na human in these tasks.\nYou guys, watch \"X Machina?\"\nWasn't that a movie I saw?\nDoesn't pass that test.\nPasses just a simple\ncore recognition test."], "930": [79, 50, "So OK, so, this is\nhere that I quantified.\nSo this is human to\nhuman consistency.\nThat's the range\nI just mentioned\nthat, you've got to get\ninto here to pass our Turing\ntest on this.\nAnd that's a decoding\nmechanism I just showed you.\nThere's other ways of reading\nout of IT that don't pass.\nThere's ways of reading out of\nV4, which you recorded from--\nnone of them we've tried are\nable to get you to this here.\nThat doesn't mean\nV4 isn't involved.\nV4 is the feeder to IT.\nIt just means you can't take\nsimple decodes off of V4\nand naturally\nproduces this pattern.\nAnd that's similar for like,\npixels or V1 representations.\nSo lower level representations\ndon't naturally\npredict this\npattern of behavior.\nAnd even some\ncomputer vision codes\nthat we tested at the time, as\nyou can see, if those of you\nknow these older computer\nvision models didn't do this.\nBut more recent computer\nvision models actually do."], "980": [80, 100, "OK.\nSo, this is a little\nbit for the aficionados\nto tell you how\nwe got there as we\nincrease the number of units in\nIT, that drives performance up.\nSo as you read more and\nmore units out of IT,\nyou get better and\nbetter performance.\nThat's also true out of V4.\nBut I'm trying to\nshow you this here,\nis it's like, not the\nabsolute performance\nthat is the good thing\nto compare a model\nwith actual behavioral data.\nIt's the pattern of\nperformance, which\nwe call the consistency\nwith the humans.\nThat's that correlation\nalong that diagonal\nthat I showed you\nearlier, that tasks\nthat are hard for the models\nare also hard for the humans.\nTasks that are easy for humans\nare also easy for the models.\nAnd you could\nimagine doing that,\nnot just at the task level,\nbut at the image level as well.\nAnd anyway, that's\nwhat's quantified here.\nAnd you see that when you get up\nto around you know, about 100--\nI showed you 168\nrecordings out of IT.\nThis point right there\nis about 500 IT features.\nAnd taking you\nthrough some things\nthat maybe I won't\nhave time for,\nthat's actually how we\napproximate that 50,000 single\nIT neuron number.\nThat's an inference\nfrom our data\nbased on if we didn't actually\nrecord 50,000 single neurons.\nBut from these kind\nof plots, we're\nable to make a pretty good guess\nthat this kind of model right\nhere would produce--\nwould land right there.\nTo be consistent with\nhumans, and would\nget the absolute\nlevel of performance\nwhich humans matched.\nAnd you know, the models\nwe tried out of V4,\nthis is one example of them.\nThey can get performance.\nBut they can never-- they\ndon't match this pattern\nof performance naturally.\nThey over perform on some tasks,\nand under-perform on others.\nThey sort of reveal\nthemselves as not\nbeing human like by being too\ngood at some things, right?"], "1080": [81, 10, "OK.\nMaybe I'll skip through this,\nit's sort of the same thing.\nThis is about training examples.\nIf those of you guys\ncare about this,\nI could kind of take you through\nhow we-- there's actually"], "1090": [82, 120, "And I'm just telling you about\none of them for simplicity.\nSo, let me then just take\nit down to another grain.\nSo that was the\npattern of performance,\nit's actually\nnaturally predicted\nby this first decoding\nmechanism that we tried.\nBut what about the\nconfusion pattern?\nSo not just the absolute D\nprimes for each of these tasks,\nbut there's finer grained data,\nlike how often an animal is\nconfused with a fruit, or an\nanimal's confused with a face.\nThese are the confusion\npattern data here.\nI'm sorry I don't have\nthe color bars up.\nAll I'm going to need you\nto do is say, well these\nare the confusion patterns\nthat we predicted.\nAnd this is what is the\npredicted confusion pattern,\nif I gave the machine, the\nIT, these ground truth labels.\nAnd it predicts this.\nThis is what actually\nhappened in human data.\nAnd what I want to sort of\nlook at this and this, and say,\nthere actually\nlook quite similar.\nTheir noise corrected\ncorrelation is 0.91.\nSo they were still quite good at\npredicting confusion patterns.\nAlthough this did\nnot hold up fully.\nWe're only at 0.68.\nI say only.\nSome people would\nsay this is success.\nWe're only at 0.68\non high variation.\nSo there's a failure\nhere of the model.\nThat should be at 1, because\nit's noise corrected.\nSo there's something\nabout this that's\nnot quite right at\npredicting the confusion\npatterns of humans at\nhigh variation images.\nAnd that to us, that's an\nopening to push forward, right?\nSo this is a strategy\ngoing forward\nas we have an initial guess\nof how you read out of IT.\nIt looks pretty good\nfor first grain test.\nBut now we can turn\nthe crank harder.\nWe need more neural data.\nWe need more psychophysics,\nfiner grained measurements\nto sort of distinguish\namong, not just\nsay IT's better than V4 or\nthose other representations.\nBut what exactly about\nthe IT representation?\nIs it 100 milliseconds?\nWhat time scale?\nMaybe those synchronous\ncodes do matter.\nSome of those things that\nI put on there earlier\nmight start to matter when\nwe push the code-- push\nthis even further.\nSo what I take home\nhere is that you\ndo quite well with this\nfirst order rate code\nreads out of IT.\nBut now there's an opportunity\nto try to dig in and say,\nwell at what point\ndo they break down?\nAnd what kind of\ndecoding models are you\ngoing to replace them with?"], "1210": [83, 30, "I've told you that IT\ndoes good at identity.\nBut remember I said\nearlier on, remember\nI showed you those\nmanifolds, and said\nthere's other latent variables\nlike position and scale.\nAnd I said those\ndon't get thrown away.\nThey just get unwrapped, right?\nRemember that manifold\npicture I showed earlier?\nAnd so one of the things\nwe've been doing recently\nis asking, because we\nbuilt these images,\nwe know these other\nlatent variables,\nlike position and\npose-- that was\none of the advantages of\nbuilding the images this way.\nAnd we've been asking how well\nIT encodes those other latent"], "1240": [84, 20, "the position of the object.\nAnd to make-- let me\njust skip through.\nTo make a long story short,\nIT actually encodes--\nnot only has information\nabout these kind of variables,\nwhich is really not\nsurprising, because others\nhave shown that\nthere's information\nabout those kind\nof things before.\nBut that's sort\nof what's on here."], "1260": [85, 20, "IT V4 simulated V1 in pixels.\nAnd always, everything goes\nup along the ventral stream\nfor the other\nvariables, which may be\nnon-intuitive to some of you.\nI mean, because position\nis supposed to be V1.\nBut position of an object\nin a complex background\nis better at IT.\nThat's one example.\nBut all these latent\nvariables go up\nalong the ventral\nstream in terms"], "1280": [86, 10, "But what I'm most\nexcited about is\nthat if you do this\ncomparison with humans again,\nyou actually get this sort\nof, again, pretty decent,"], "1290": [87, 30, "actual measured\nbehavioral performance\non making estimates of those\nother latent variables,\nand the predicted behavioral\nperformance out of IT.\nAnd again, much\nbetter correlations.\nIt's not perfect.\nSo again, there's some gap here,\nsome failure of understanding.\nBut much better than if you\nread out of V4, V1 or pixel.\nSo this says that the\nrepresentation again isn't just\nan identity thing.\nIt seems like this could\nbe representational\nunderlie some of these\nother judgments, at least\nat the central 10 degrees for\nsort of foreground objects"], "1320": [88, 10, "That's the-- don't worry about\nthe details on here-- that's\nthe upshot of what I'm trying\nto say with this slide.\nBut I just wanted to\nput that out there\nso you didn't forget that\nyou haven't thrown away"], "1330": [89, 140, "out there in the scene.\nOK.\nLet me kind of--\nI've sort of alluded\nto this a bit.\nI want to come back\nto kind of now,\nthis is like Marr\nlevel 3 stuff, right?\nSo you have this idea of\nwhat you're trying to solve.\nYou have a decode--\nyou have an algorithm that's\na decoder on a basis, that's\ntrying-- that looks like\nit predicts pretty well.\nIt's not perfect.\nThere's work to be done there.\nBut it actually does quite well.\nNow what does that mean on\nthe physical hardware level?\nSo that's Marr level 3.\nSo you think-- here's\nhow I visualize it.\nYou have IT cortex,\nwhich I mean AIT and CIT.\nSo it's about 150 square\nmillimeters in a monkey.\nAnd remember I told you\nthere was about 1 millimeter\nscale of organization?\nI showed you that earlier.\nAnd others have shown--\nI showed this earlier,\ntoo-- that there's\nsort of face regions.\nSo I've drawn them just\nfor sort of for scale here,\njust a schematic.\nThat they're slightly\nbigger organizations,\nthey're 2 to 5 millimeter.\nSo I think of IT as being\nthis sort of like 100 to 200\nlittle--\nsimilar to Tanaka.\nThis is not a new\nconceptual idea.\nBut there's sort of\njust the simple version\nwould be each millimeter does\nexactly the same thing, is\na feature.\nAnd if you sample off of\nthat, you take 5,000 neurons,\nbut they're really sampling\nfrom only about 150 IT\nfeatures at 1 millimeter scale.\nRemember, I don't know\nif you caught that.\nBut I showed 150--\n101-- 150.\nI showed you 168 IT neurons\npredicted the pattern\nof human performance.\nI showed that a few slides ago.\nBut I told you the real number\nof neurons is probably 50,000.\nMost of those are\nredundant copies\nof that 168 dimensional\nfeature set.\nThat's how we think about it.\nSo you could imagine, it's\njust a redundant set of about--\nI like to think of about\n100 features in IT which\nare sampled maybe randomly\ndownstream neurons that are\nthen learned.\nSo when you learn faces\nversus other things,\nhey, there's lots of good\ninformation about faces\nversus other things.\nAnd these face patches,\nthat's how they're defined.\nBut those neurons are\ngoing to lean heavily--\nthis downstream neuron\nis going to lean heavily\non those neurons.\nAnd then these-- so that would\nmake these regions causally\ninvolved.\nSo that doesn't mean you had\nto pre-build in anything here.\nYou just learn this at\na downstream version.\nAnd you would get\nsomething that looks\nlike it would explain our data.\nSo we like that, because\nit captures that case.\nBut it also captures\nthe more general case.\nIf you learn cars,\nyou're going to sample\nfrom a different\nsubset of neurons.\nBut you're following\nthe same learning rule.\nThat's what I said earlier on.\nSo you end up--\nwe think this is\nthe initial state."], "1470": [90, 160, "And so what we think is a\npost learning, what you have\nis again, about 100\nto 150 IT sub regions,\neach at 1 millimeter\nscale, that are\nsupporting a number\nof noun tasks\nread off this common basis here.\nThat's the model\nthat we like, given\nthe kind of data that\nI've been showing you.\nThe post learning\nmodel, as we call it.\nSo the reason I'm\nbringing this up\nis probably for\nthe neuroscientists\nto fix ideas about how we\nthink about IT as a basis set.\nAnd this is--\nI think Haim sort of\nset this up nicely,\nhe sort of implied\nsimilar things.\nThat somebody downstream\nreads from it.\nOK.\nBut now, we have\na more-- you know,\nwe're starting to have a more\nconcrete model, that we now,\nI'm trying to start\nto be physical\nabout it, about the size\nof these regions connecting\nto earlier data,\nhow many there are.\nSo we're gaining\ninference on that\nfrom these different\nexperiments.\nAnd now, if you believe this,\nit starts to make a prediction\nof what's-- now we can\ndo causality, right?\nSomebody mentioned that earlier.\nAnd so, one of the things\nwe've been doing recently\nis if we can start to silence--\nlook, the way I've drawn\nthis, this bit of tissue\nfor-- this is just\nschematic-- is somehow\ninvolved in this\ntask and that task.\nFace task and car task.\nBut this bit of\ntissue, only face task.\nAnd that bit of\ntissue, only car task.\nAnd this bit of tissue, neither.\nSo if you believe that,\nyou had the tools,\nyou should be able\nto go in and start\nto silence little bits of IT.\nAnd you should get\npredictable patterns out\nof the behavioral\ndeficits of the animal\nwhen you make those\nmanipulations, right?\nEverybody follow that?\nRight?\nOK.\nAnd now the models\ngive you a framework\nto build those\npredictions and to also\nestimate the magnitude of those\neffects that you should see.\nAnd so that's what we've\nbeen doing more recently.\nAnd I'll just give\nyou a taste of this,\nbecause this is really ongoing.\nBut I think it connects\nto what Gabriel\nsaid earlier about now there\nare these tools available to do\nthat.\nOh, I put that in from\nan earlier talk where--\nI think Google has a\nthing called Inception.\nAnd I don't know--\nwas it Google?\nOr somebody has it-- you can't\ndo Inception unless you're\nactually in a brain.\nSo are you going\nto try to insert--\nthe reason we do this is my\nstudent that is working on it\nreally wants to inject\nsignals in the brain.\nThere's a dream\nabout VMI, right?\nCould you kind of\ninject a percept?\nAnd to do that,\nyou're going to need\nto do experiments like this.\nAnd you understand this\nhardware to interact with it.\nIt's something we\ntalked about earlier.\nSo actually-- and Tonegawa's lab\nhas some cool Inception stuff\non memory.\nBut this is like inserting\nan object/person.\nSo to do that, this has\nbeen a dream for many of us\nfor a long time.\nCan we reliably\ndisrupt performance\nby suppressing 1\nmillimeter bits of IT?\nSo to do that,\nwhat we're doing is\ntesting a large battery\nof tasks and a battery\nof suppression patterns.\nSo not just sort\nof saying, can we\naffect face tasks or one task?\nBut let's imagine we\ntest a battery of tasks.\nAnd then, we--\nand the idea where\nwe'd have a whole bunch of tasks\nand we'd do every bit of IT one\nby one, and then in\ncombination, and we'd\nsort of get all that data\nand figure out what's\ngoing on, right?\nThat's sort of the dream, right?\nSo we're trying to build\ntowards that dream.\nDo you guys get it?\nRight."], "1630": [91, 130, "And then we're motivated\nby this kind of idea here.\nSo to build-- so we started--\nI'm just going to give\nyou a quick tour of we\nhave tools to start to do this.\nYou know, this is\nour recording, we\ncan localize what we're\nrecording two very fine grain\nusing x-rays.\nSo we know exactly where\nwe're recording the IT to like\nabout 300 micron resolution.\nSo that's why I'm\nputting this slide up.\nAnd what we're\ninterested in is going,\nif I silence this bit of\nIT, or that bit of IT,\nor that bit of IT, so actually\ndo this experiment, what\nhappens behaviorally?\nAnd Arash Afraz is a\npost-doc in the lab, started\nthese actual experiments.\nAnd one of the things\nArash did was to first say,\nlet's see if we can get this\nsilencing of optogenetics tool\nto work in our hands.\nAnd the reason we were\nso excited about that\nis because we think\nlesions, if we\ncan make temporary\nbrief silencing,\nthat that will give it much more\nreliable disruption of behavior\nthat then, if we started\nto try to inject signals,\nwhich would be our dream, but\nthat seems too risky to us.\nWe just want to say, what\nis a temporary lesion\nof each bit of IT do?\nAnd optogenetics is\ncool, because there's\nno other technique that\ncan briefly silence--\ntemporarily silence activity.\nYou can do pharmacological\nmanipulations,\nbut those last for hours.\nSo this could briefly\nsilence bits of IT.\nAnd that's why we\nwere excited about it.\nWe also did pharmacological\nmanipulation as a reference\nto get started.\nBut what we're doing is trying\nto silence 1 millimeter regions\nof IT using light delivered\nthrough optical fibers\nas the recording electrode.\nAnd to silence bits\nof neurons here.\nAnd so what Arash\ndid was first show\nthat you can actually\nsilence neurons in this way.\nSo if you guys haven't\nseen optogenetics plots,\nthis is data from our lab.\nWhat's quite cool\nabout this, again,\nis you have the same\nimages are being presented.\nSo this green line\nshould be up here.\nBut Arash turns a laser on right\nhere, shines light on there.\nAnd there's some opsins\nexpressed in the neurons\nin that local area.\nAnd you can see it just sort\nof shuts the thing down, and it\nsort of deletes or blocks this.\nYou have the same\ninput coming in.\nBut you can sort\nof delete it here.\nAnd this is another example.\nThese are some pretty\nstrong examples.\nIt's not always this strong.\nBut this is, again, you\ncan see we can return back\nto normal right away, right?\nSo this is a 200\nmillisecond silencing."], "1760": [92, 40, "But so this is what\nwe had done so far.\nAnd again, what we\ndid was say, look.\nThis is a risky tool.\nThis is it not going\nto work at all.\nSo Arash just wanted\nto test something\nthat was likely to work.\nAnd so we picked a\nface task because there\nwas a lot of evidence of\nspatial clustering of faces\nthat you'll hear from\nWinrich and you also\nknown in the literature.\nSo what Arash did\nwas to say, we picked\na task of discriminating\nmales from females.\nWe put in our notion\nof invariance.\nIt's not just do\nthis image access.\nBut you have to do it across\na bunch of transformations.\nIn this case, its identity\nas a transformation.\nSo you're saying, all of these\nare supposed to be called male,\nand all these are called female.\nAnd he wanted you to\ndistinguish this from this."], "1800": [93, 50, "And just to give you the upshot,\nis that, we do all this work,\nwe silence the bits of cortex.\nAnd here's the big take home.\nYou get a 2% deficit of\nsingle one millimeter\nsilencing of bits of IT cortex.\nParts of IT cortex,\nnot all of IT cortex,\nproduce a 2% deficit.\nHere's the animal running\nat 80%, 6% correct.\nThese are interleaved\ntrials where we\nsilence some local bit of IT.\nYou get a 2% deficit.\nThat's true only in the\ncontralateral field,\nnot that ipsilateral\nfield, for the aficionados.\nYou might look at this 2%\nand go, well, that's tiny.\nBut we looked at it,\nthis is exactly what's\npredicted by the models\nthat we were talking about.\nIt's right in the range\nof what should happen.\nAnd so this, to us,\nis really quite cool.\nThis is highly significant.\nAnd now we sort of are in\nposition to start to say,\nOK these tools work.\nThey do what\nthey're supposed to.\nAnd now we can start to\nexpand that task space.\nSo this result has been\npublished recently,"], "1850": [94, 40, "And here is one of the\nways we're going forward\nis that Rish Rajaingham, the one\ndoing those tasks in the monkey\nI showed you earlier.\nSilencing different parts of IT.\nThis is now with muscimol,\ndifferent bits of IT--\nthese are different tasks,\nlead to different patterns.\nThat's what these\ndots are here--\ndifferent patterns of deficits.\nAnd if you go back\nto the same location,\nyou get the same\npattern of deficits.\nSo this is only 10 tasks.\nBut I think it\nhopefully gives you\nthe spirit of what\nwe're trying to do.\nAnd again, this\nis only muscimol,\nwhich doesn't have all the\nadvantages of optogenetics.\nBut this is what we're\nwere building towards here.\nSo I'm just giving you the\nsort of state of the art.\nSo our aim is to measure\nthe specific pattern"], "1890": [95, 10, "region, ideally\ntesting many of them,\nand then compare with\nthe model predictions.\nI'm saying there's\nthis domain, and I\nwant to sort of sample\nthe whole domain."], "1900": [96, 40, "But we're really trying\nto define the domain.\nAnd I'm just--\nI'm going to skip through this\njust to give you the punchline,\nis that we do a whole bunch\nof behavioral measurements.\nWe presented this work before.\nIt's like, this is now up to\nthree million Mechanical Turk\ntrials.\nIt seems to us that we can\nembed all objects, even\nsubordinate objects,\nof the type of task\nthat I've been telling\nyou, in roughly,\nin essentially a 20\ndimensional space.\nSo there's 20 dimensions.\nWe think we infer that\nhumans are projecting\nto about 20 dimensions to\ndo these kind of, the tasks\nthat we've shown here.\nWhich is sort of\nsmaller, but eerily\nclose to that in the\norder of magnitude\nto that 100 or so features\nthat I've been talking about.\nSo that's where-- regardless\nof whether-- these"], "1940": [97, 10, "Again, I won't take\nyou through this,\nbecause I think we've\nalready used up enough time\nand I want to get\non to this part.\nBut we're trying to define\na domain of all tasks\nwhere we can sort\nof predict what"], "1950": [98, 20, "And that raises questions of the\ndimensionality of that domain.\nAnd there were behavioral\nmethods to do that.\nAnd we've been doing\nsome work on that.\nSo I'll just leave it at that.\nAnd if you guys\nhave questions, we\ncan talk about that some more.\nI want to sort of\nin the time I really\nhave left is to talk about\nthe encoding side of things,\nbecause I promised you\nguys I would get to this."], "1970": [99, 30, "questions on this decoding side.\nSo far I've been\ntalking about the link\nbetween IT and perception.\nNow I'm going to switch gears\nand talk about this other side.\nWhich is, so I\ntalked about this.\nAnd that tells us that\nthe mean rates in IT\nare something that seem\nto be highly predictive.\nI showed you at\nleast one model that\nhas the laws of RAD IT model.\nBut now, it's like now, we\ncan turn to the encoding side\nand say, we need to predict\nthe mean rates of IT.\nAnd that should be our goal\nif we want to explain images\nto IT activity.\nSo, these would be called\npredictive encoding mechanisms."], "2000": [100, 140, "deep convolutional networks.\nIf not, you've heard\nabout them already,\nyou'll probably hear\nabout them some more.\nSo we started messing\naround in 2008.\nThis is a model inspired--\nI mentioned this family\nof models before.\nHubel-Wiesel, Fukushima, and\nthere's a whole HMAX family\nof models, that really was the\ninspiration of this larger--\nthis large family\nof models, that\nhave this repeating\nstructure that are now\nreally the sort of modern\nday deep convolution networks\nreally grew out of all\nof this earlier work.\nAnd so we started exploring\nthe family in 2008.\nAnd just, this is a slide that\nyou've already sort of seen\na version of this from\nGabriel where you know,\nfor when you take an\nimage, you pass it\nthrough a set of operators.\nSo you have filters.\nSo these are dot products\nover some restricted spatial\nrestricted region,\nlike receptive fields.\nYou have a non linear area, like\na threshold and a saturation.\nYou have pooling operation.\nThen you have a normalization.\nSo you have all these\noperations happen here.\nAnd that produces a stack.\nSo think of like, if there\nare four filters here,\nlike four orientations,\nyou get four images,\nyou have one image in,\nyou have four images out.\nBut if you had 10 of these,\nyou'd get 10 of these out.\nThen you repeat\nthis here, right?\nAnd so as you keep\nadding more filters,\nthis stack just keeps\ngetting bigger and bigger.\nAnd it keeps, because\nyou're spatially pooling,\nit keeps getting narrower\nand narrower, right?\nSo you go from this\nimage to this sort\nof deep stack of features\nthat has less retinatopy.\nIt still has a little\nbit of retinotopy.\nAnd that, you can see, has\nbeen exactly a very good model\nwhy people liked\nit of how people\nthink about the ventral stream.\nSo these models\ntypically have thousands\nof feat-- visual neurons or\nfeatures at the top level.\nJust to give you a sense of\nscale of how they're run.\nAnd just to take\nyou through, you\nknow, I guess maybe\nyou'll hear about this,\nif you haven't already.\nEach element has like, a\nfilter, has a large fan in.\nLike these are like\nneuroscience related things.\nThey have non-linearities,\nlike thresholds of neurons.\nEach layer is\nconvolutional, which\nmeans you apply the same\nfilters across visual space.\nWhich is like retinotopy,\nthat is a view on cell that\nis oriented here.\nThere'll be another\nview on cell that's\nin another spatial\nposition, same orientation,\ndifferent spatial position.\nThat's what the\nconvolutional models are just\nan implementation of that idea\nof copying the same filter\ntype across the retina.\nAnd there's a deep\nstack of layers.\nThese are all\nthings that I think\nare commensurate with\nthe ventral stream\nanatomy and physiology.\nSo, but one of the\nkey things that those"], "2140": [101, 60, "that, they have lots\nof unknown parameters\nthat are not determined\nfrom the neurobiology.\nEven though the family of\nmodels is well described,\nwhat are the exact\nfilter weights?\nWhat are the\nthreshold parameters?\nHow exactly do you pool?\nHow do you normalize?\nThere's lots of parameters\nwhen you build these things,\nessentially thousands of\nparameters, most of them hidden\nin the weight structure here.\nWhich, if you think about,\nthe first layer, that\nwould be like, should\nI choose Gabor filters?\nOr should I do some other--\nyou know Haim was talking\nabout random weights, right?\nSo there's choices there.\nThere are lots of parameters.\nSo the upshot is, there's\na big-- that's why\nI call it a family of models.\nAnd how do you choose which one\nis the right one, so to speak?\nOr is there a right one?\nOr maybe the whole\nfamily is wrong, right?\nThese are the\ninteresting discussions.\nSo, what I like about it is,\nat least when you set it,\nit's a model.\nIt makes predictions.\nAnd then you can test it.\nSo it's at least a model.\nAnd it predicts the\nentire-- you know,\nif you start to map these, you\nsay this is V1, this is V2,\nthis is V4.\nIt predicts the full\nneural population response\nto any image across these areas."], "2200": [102, 50, "So that's nice.\nBut now you have to determine\nhow am I going to build it?\nHow do I set the parameters?\nSo how do we do that?\nWell, there's lots of\nways you could do it.\nAnd I'll tell you the\nway we chose to do it.\nWhich was to just not\nuse any neural data.\nIt was just to use\noptimization methods\nto find specific models\nto set the parameters\ninside this model class.\nAnd we chose an\noptimization target.\nThis is a little bit, again,\ninspired from a top down view\nof what the system's doing.\nWhat are the visual\ntasks that we\nsuppose the ventral stream\nwas supposed to solve?\nWhich I already told you, we\nthink it's invariant object\nrecognition.\nThat's what makes\nthe problem hard.\nSo we tried to optimize\nmodels to solve that.\nAnd essentially when\nwe're doing that,\nwe're kind of doing the same\nthing that computer vision is\ntrying to do, except we're doing\nit in our own domain of images\nand tasks that we set up.\nBut we essentially, there's a\nmeeting between computer vision"], "2250": [103, 20, "And when I say we, this\nis work by Dan Yamins,\na post-doc in the lab, and\nHa Hong, a graduate student.\nAnd what we did was to\njust try to simulate again,\nas I did earlier.\nWe took these\nsimple 3-D objects.\nWe could render\nthem, just as before,\nplace them on\nnaturalistic background.\nAnd then we just\nbuilt models that"], "2270": [104, 60, "from flowers from guns.\nSo they would have\ngood feature sets\nthat would discriminate\nbetween these things.\nAnd these were essentially\ntrained by various forms\nof supervision.\nNow there's lots of ways\nyou can train these models.\nI could tell you\nabout how we did it\nand how others have done it.\nI think those details\nare beyond what\nI want to talk about today.\nBut just, it's a\nsupervised class\nthat's probably not\nlearned in the same way\nthat the brain has learned.\nMost people don't think so.\nBut the interesting\nthing is the end state\nof these models might look very\nmuch like the current adult\nstate of the brain.\nAnd that's what I want\nto try to tell you next.\nSo first, let me show you that\nwhen we built these models,\nthis was in 2012.\nWe had a particular\noptimization approach\nthat we called HMO\nthat was trying\nto solve these kind of problems\nthat I showed you earlier\non these kind of images.\nAnd I showed you IT was\npretty good with humans.\nI showed you its performance\nwas almost up to humans, even\nwith just 168 samples.\nAnd when we first\nbuilt a model here,\nwe were able to do\nmuch better than some\nof our previous models that--\non these same kind of tasks.\nSo I told you we\nconstructed, because we\nknew it made these things--"], "2330": [105, 110, "So we built these\nhigh invariance tasks\nto push these models down.\nAnd then we had space\nto build a model\nthat we could do better on.\nAnd we called it HMO 1.0.\nAnd then we started\nto say, now we\nhave this model that has been\noptimized for performance.\nLet's see how well it does\non comparing with neurons.\nLet's see if its internals\nlook like the neural data.\nSo here's the model\nwe built, HMO 1.0.\nIt's a deep\nconvolutional network.\nIt has two different levels.\nIt had four levels.\nIt had a bunch of parameters\nthat we set by optimization,\nthat I'm just telling you\nkind of what we optimized.\nI didn't tell you--\nI'm not telling you\nany of the parameters.\nAnd now, we come back\nto say, well look.\nWe can show the same\nimages to the model\nthat we showed to the neurons.\nAnd then we can compare how\nwell these populations look\nlike that population, or this\npopulation looks like that.\nAnd so what we did was, we asked\nhow well can layer four predict\nIT first?\nThat was the first\nthing we wanted\nto do, take the top\nlayer of this model,\nthe last layer before the\nlinear readout of this model.\nAnd to do that, you might sort\nof say, well, wait a minute.\nThe model doesn't have mappings.\nIt has sort of neurons simulated\nhere, neuron 12 or something.\nAnd there's some\nneuron we recorded.\nBut there's no linkage between\nthat neuron and that neuron,\nright?\nYou have to make that map.\nSo what we do is we\ntake each IT neuron\nand treat this as sort\nof a generative space.\nYou can generate as many\nsimulated IT neurons\nas you want.\nYou would just ask,\nlet's take this neuron,\ntake some of its data, and try\nto build a linear regression\nto this neuron.\nTreat this as a basis\nto explain that neuron.\nAnd then test the predictive\npower on the held out IT data.\nAnd that's what\nI'm writing here.\nThat's cross-validation\nlinear regression.\nSo I'm going to show you\npredictions on held out data\nwhere some of the data were\nused to make the mapping.\nAnd there's lots\nof ways we chose--\nwe could make the mapping.\nAnd we did essentially\nall of them.\nAnd I could talk about\nthat if you want.\nBut that's this central idea."], "2440": [106, 70, "spanned by this basis set?\nSo I can I fit that well\nwith this linear basis here?\nAs a linear map from this basis?\nAnd here's what we actually--\nhere's what it looks like.\nHere's the IT neural response\nof one simulated-- one actual IT\nneuron in black.\nThis is not time.\nThese are images.\nI think there's like\n1,600 images here.\nSo each black going up and\ndown, you can barely see,\nis the response, the mean\nresponse, to different images.\nAnd you see we grouped them\nby categories, just so,\njust to help you kind\nof understand the data.\nOtherwise, it'd\njust be a big mess.\nBecause IT neurons\ndo-- you can kind of\nsee they have a bit of\ncategory selectivity.\nAnd again, this was known.\nThis neuron seems to like\nchair images, but not all chair\nimages.\nIt sometimes likes boats and\nsome planes a little bit.\nAnd the red line is the\nprediction of the model,\nonce fit to part of\nthe-- to this neuron.\nThis is the prediction on the\nheld out data for the neuron.\nYou can see the R\nsquared is 0.48.\nSo half the explainable\nresponse variance\nis explained by this model.\nAnd again, these\nare predictions.\nThe images were never seen--\nthe objects even were\nnever seen by this model\nbefore it makes these\npredictions here."], "2510": [107, 20, "It's actually quite well\ncaptured by the top level,\nin this case, of this\nfirst HMO model we built.\nI'll show you some other\nmodels in a minute.\nHere's another neuron\nthat you might call a face\nneuron because it tends to like\nfaces over other categories.\nSo it might-- it\nwould pass the test\nof the operational\ndefinition of a face neuron.\nThis model, this neuron\nwas well predicted, again,"], "2530": [108, 20, "by this HMO model.\nAgain, a slightly--\nan R squared near 0.5.\nHere's a neuron that you would\nlook at the category structure.\nAnd you don't even--\nyou can't really see\nthe categories here.\nThey're still here.\nBut you don't see\nthese sort of blocks.\nYou just see there's sort of\nsome images it likes and some\nit doesn't.\nIt's hard to even know\nwhat's driving this neuron."], "2550": [109, 80, "You don't have the R squared.\nBut it's similar.\nIt's about half the\nexplainable variance.\nJust another example.\nAnd here is a sort\nof summary here.\nIf you take-- this\nis a distribution\nof the explainable variance\nfor the top level of the model\nfitting about, I think\nthis is 168 IT sites.\nSome sites are fit\nreally well, near 100%.\nSome are fit not as well.\nThe average is about\n50%, which is shown here.\nSo this is the median of\nthat distribution here.\nSo the summary take\nhome is about 50%\nof singularly response\nvariance predicted.\nAnd this is a big improvement\nover previous models\nI'll show you in a minute.\nThe other levels of the model\ndon't predict nearly well.\nSo the first level\ndoesn't predict well.\nSecond level better,\nthird level better,\nthe fourth level the best.\nIf you take other models-- these\nare some of the models I showed\nyou earlier--\nthey don't fit nearly as well.\nHere's their distributions\nand here's their average,\ntheir median explained variance.\nAnd just to fix-- to just\nfix ideas, you might think,\nwell look, we built a model\nthat's a good categorizer.\nSo of course it fits\nIT neurons well.\nBecause IT neurons\nare categorizers.\nWell, here's a model that\nactually has explicit knowledge\nof the category.\nIt's not an image\ncomputable model,\nand it's not an easy one.\nBut it's just given\nthat sort of an oracle\nthat's given the category,\nand how well it explains IT.\nAnd you can see, it\nexplains IT much worse\nthan the actual model.\nSo this implies a model\nis limited by the real--"], "2630": [110, 20, "and how it adds variance\nthat the sustained IT\nneurons are categories\ndoes not easily capture.\nSo that kind of--\nthat sort of inspired\nus to say, OK.\nWhat about if we go down\nand say not just IT,\nbut let's go to V4.\nBecause we had a\nbunch of V4 data.\nAnd so we play the\nsame game in V4."], "2650": [111, 90, "And here's the IT data I\njust showed you a minute ago.\nAnd here's the V4 data.\nSo the V4 neurons are highly\npredicted in the middle layer.\nLayer three is the\nbest predictor of V4.\nThe top layer is actually not\nso predictive, less predictive\nof V4 neurons than\nthe middle layers.\nAnd the first layer is\nnot so well predictive.\nAnd again, the other\nmodels are actually,\nnow you can see they're\ngetting on relatively better.\nYou can think of them as\nsort of lower level models.\nAnd they're getting better,\nwhich is what you'd expect.\nBut interestingly, this\nis really exciting to us.\nBecause look, this\nmodel was not optimized\nto fit any neural data other\nthan that last mapping step.\nAll it is is a bio\ninspired algorithm class,\nwhich is the\nneuroscience sort of view\nof the feed-forward\nclass of the field.\nAnd tasks that we and\nothers hypothesize\nare important, that\nthe ventral stream\nmight be optimized to solve,\nand an actual optimization\nprocedure that we applied.\nAnd that leads to neural like\nencoding functions at the top\nand in the middle layer.\nSo you don't-- so this sort\nof leads to funny things like\nsaying, what does V4 do?\nThe answer here\nwould be, well, it's\nan intermediate layer\nin a network built\nto optimize these things.\nThat's the way to\ndescribe what V4\ndoes, according to this\nkind of modeling approach.\nNow I want to point\nout, this is only half\nof the explainable variance.\nSo it's far from perfect.\nThere's room to improve here.\nBut it's really dramatic how\nmuch improvement we got out\nof these kind of models.\nAnd so if you take\nthis sort of--\nwell, I'll skip this.\nIf you take this back to\nyou know, big picture,"], "2740": [112, 10, "of a model on high end\nvariance recognition tasks.\nWe're saying, this is what\nwe've been trying to optimize.\nAnd what we noticed\nis that if you"], "2750": [113, 40, "These black dots are\nother models I showed you.\nSo they're control models that\nwere in the field at the time.\nAnd this is the\nability of the top--\nthe model-- the top level\nof any of the models\nto predict IT responses.\nSo, you know, how good\nthey are predicting--\nthis is sort of the median\nvariance explained of single IT\nresponses.\nAnd you see there's\na correlation here.\nIf you're better at this, you're\nbetter at predicting that.\nAnd all we did was\noptimize this way,\nwhich we think of as like,\nevolution or development.\nSo we're not\nfitting neural data.\nWe're just optimizing\nfor task performance.\nAnd that led in 2012 to a\nmodel that I just showed you,\nexplained about half of\nthe IT response variance.\nOK, so it's like, well,\nthis looks like it's\ncontinuing up this way."], "2790": [114, 70, "if we can optimize further\non these kind of tasks,\nmaybe we can explain\nmore variance.\nAnd it turned out,\nwe didn't actually\nneed to do that,\nbecause again, I said,\ncomputer vision was\nalready working on this.\nAnd they got a lot\nmore resources.\nThey're already doing it.\nThey're already better\nthan us on this.\nSo here's our HMO model.\nThis is now Charles Cadieu,\na post-doc in the lab.\nThese were models that\ncame out at the time.\nThis is Krizhevski\net al. supervision.\nIt's ICLR 2013.\nThey were better than the\nmodel that we had built.\nYou know, we were in this\nrestricted image domain,\nyou know, there's\nlots of reasons why\nwe could say they're better.\nRegardless, they were better at\nour own tasks than the models\nthat we had built, right?\nSo they were already\nahead of us on the task\nthat we had designed.\nAnd so they were up here,\nand then they were up here.\nAnd so, if you follow\nthat prediction,\nthat means these models\nmight be better predictors\nof our neural data, right?\nThese guys don't\nhave our neural data.\nAll they're doing is building\nmodels to optimize performance\non tasks.\nAnd but we could take their\nfeatures from the neural data,\nplay the same game.\nAnd we actually explained\nour response-- data\nbetter than our model\nexplained our own data.\nSo this is a nice statement\nthat is not even in our own lab.\nJust a continued optimization\nfor those kinds of tasks"], "2860": [115, 10, "And that's what's shown here.\nSo I think that's what\nI just said there.\nSo, Charles took this\nfurther and analyzed"], "2870": [116, 10, "This is a summary of what I\npresented in the second half\nnow, showing that IT firing\nrates are feature based,\nlearned object judgments\nnaturally predict human monkey\nperformance."], "2880": [117, 10, "I picked a particular\nmodel, which\nis 100 millisecond read on this\ntime window, 50,000 neurons.\n100 training examples."], "2890": [118, 70, "is a current set of decode model\nthat fits a lot of our data,\nbut not all of our data.\nAnd we also want to\nget finer grain data.\nThe inference is, this might\nbe the specific neural code\nand decoding mechanism\nthat the brain uses\nto support these tasks.\nThat's what we'd like to think.\nBut now, we're trying to\ndo systematic causal tests.\nAnd we talked a lot about\ntrying to silence bits of IT\nas one example of that.\nAnd the tools are still not\nwhere we'd like them to be.\nBut you see we're\nmaking progress there.\nSo the second was I showed the\noptimization of deep CNN models\nfor invariant object\nrecognition tasks led\nto dramatic improvements\nin our ability\nto predict IT and V4 responses.\nI showed you our model HMO.\nBut then the convolutional\nneural networks in the field\nhave already surpassed\nour predictive ability\non our own data.\nAnd so the inference is that\nthese encoding mechanisms\nin these models might\nbe similar to those that\nwork in the ventral stream.\nAnd now, you know, there's\na whole sort of area\nwhere you can start to\nthink about doing physiology\non the models, so to speak.\nAnd that problem's\nalmost as hard\nas doing physiology\nexcept on the animal,\nexcept that you can\ngain a lot more data.\nAnd so, and this is\nallowing the field\nto design experiments\nto explore what remains,"], "2960": [119, 120, "So within core\nobject recognition\nor perhaps having to\nextend out of that,\nI think is now what\npeople are trying to do.\nSo big picture in terms\nof us for the future,\nI've talked about\nthis law's of RAD IT.\nCan we perturb here\nand get effects here\nthat are predictable?\nCan we predict for each\nimage, coding model,\nand for the optical\nmanipulations?\nWe talked about that.\nDynamics and feedback\nare something\nthat we're interested in.\nBut I haven't talked\nmuch at all about.\nI think that's a good\npoint, a discussion topic.\nI can tell you how\nwe're thinking about it.\nWe have some efforts\nin that regard.\nI talked on the encoding\nside about these kind\nof deep convolutional\nnetworks that map from images.\nBut the dash lines mean\nthey're only 50% predicted.\nBoth of these cases,\nthey're not perfect, right?\nSo there's work\nto be done there.\nAnd one of the really\nexciting things\nis here is how\nthese models learn.\nThis supervised way of\nlearning these models\nis almost surely not what's\ngoing on in the brain.\nSo finding more-- less\nsupervised, biologically\nmotivated learning\nof these models\nis a good-- is the next step,\nI think, for much of the field.\nBut what's nice is to\nhave an end state that\nis much better than any previous\nend state we'd had before.\nSo that sets a target of\nwhat success might look like.\nAnd you know, maybe we\ncan think about expanding\nbeyond core recognition.\nWe can talk in the\nquestion period about that.\nWhen is the right\ntime to kind of keep\nworking within the domain\nof core recognition that\nis set up, versus\nexpanding beyond that?\nBecause there's lots\nof aspects of object\nrecognition that I\ndidn't touch on here.\nAnd that comes up\nin the questions.\nI think, there's lots of work\nto be done within the domain,\nbut there's also\ninteresting directions that\nextend outside of that domain."]}