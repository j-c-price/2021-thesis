Algorithm Configuration:

Learning in the Space of Algorithm Designs

 

Kevin Leyton-Brown Frank Hutter

University of Freiburg and

University of British Columbia
Bosch Center for Artificial Intelligence

Canada CIFAR Al Chair, Amii

    

 

4)
seers sig (MBER LUDO
UNIVERSITAT FREIBURG.

@BOSCH
ENDELEMENT 

This Tutorial

High-Level Outline

Introduction, Technical Preliminaries, and a Case Study (Kevin)
Practical Methods for Algorithm Configuration (Frank)
Algorithm Configuration Methods with Theoretical Guarantees (Kevin)

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)
ENDELEMENT   

This Tutorial

Section Out

Introduction, Technical Preliminaries, and a Case Study (Kevin)
Learning in the Space of Algorithm Designs
Defining the Algorithm Configuration Problem
Algorithm Runtime Prediction

Applications and a Case Study
ENDELEMENTAlgorithm Configurati

+ Algorithm configuration is a powerful technique at the interface of ML and
optimization
+ It makes it possible to approach algorithm design as a machine learning problem

— stop imagining that we have good intuitions about how to approach combinatorial
optimization in practice!

— instead, expose heuristic design choices as parameters, use automatic methods to search for
good configurations

+ Many research challenges in the development of methods
+ Enormous scope for applications to practical problems
ENDELEMENT 

Machine learning
Classical approach

* Features based on expert insight
* Model family selected by hand
+ Manual tuning of hyperparameters
ENDELEMENT  

ut algorithm designs as a hypothesis space

Machine learning
Classical approach

+ Features based on expert insight
* Model family selected by hand
+ Manual tuning of hyperparameters

Deep learning

+ Very highly parameterized models, using
expert knowledge to identify appropriate
invariances and model biases (e.g,
convolutional structure)

“deep”: many layers of nodes, each
depending on the last

Use lots of data (plus e.g. dropout
regularization) to avoid overfitting
Computationally intensive search replaces
human design
ENDELEMENT  

Machine learning
Classical approach

+ Features based on expert insight
* Model family selected by hand
+ Manual tuning of hyperparameters

Deep learning

+ Very highly parameterized models, using
expert knowledge to identify appropriate
invariances and model biases (e.g,
convolutional structure)

“deep”: many layers of nodes, each
depending on the last

Use lots of data (plus e.g. dropout
regularization) to avoid overfitting
Computationally intensive search replaces
human design

ut algorithm designs as a hypothesis space

Discrete Optimization
Classical approach

+ Expert designs a heuristic algorithm
+ Iteratively conducts small experiments to
improve the design
ENDELEMENT 

Introduction

We should think

 

Machine learning
Classical approach

+ Features based on expert insight
* Model family selected by hand
+ Manual tuning of hyperparameters

Deep learning

+ Very highly parameterized models, using
expert knowledge to identify appropriate
invariances and model biases (e.g,
convolutional structure)

“deep”: many layers of nodes, each
depending on the last

Use lots of data (plus e.g. dropout
regularization) to avoid overfitting
Computationally intensive search replaces
human design

ut algorithm designs as a hypothesis space

Discrete Optimization
Classical approach

+ Expert designs a heuristic algorithm
* Iteratively conducts small experiments to
improve the design

Learning in the space of algorithm designs

+ Very highly parameterized algorithms

express a combinatorial space of heuristic

design choices that make sense to an

expert

“deep”: many layers of parameters, each

depending on the last

Use lots of data to characterize the

distribution of interest

+ Computationally intensive search
replaces human design
ENDELEMENTIntroduction

Approaches that seemed crazy in 2000 make a lot of sense today...

 

Moore's Law, 1971-2018
100,000,000,000

10,000,000,000
7,000,000,000
100,000,000
10,000,000

1,000,000 °

100,000

 

10,000

© Transistors

 

4,000
© Clock Speed (kHz)

100
1970 197519801985. 1990 1995-2000» 2005S 2010» 20152020
ENDELEMENTAlgorithm design in a world of learnable algorithms

Designers should:

+ Shift from choosing heuristics they think will work to exposing a wide variety of
design elements that might be sensible

— This can be integrated into software engineering workflows; see Hoos (20721.

+ get out of the business of manual experimentation, leaving this to automated
procedures

— this tutorial focuses mainly on how these automated procedures work
+ Reoptimize their designs for new use cases rather than trying to identify a single
algorithm to rule them all
ENDELEMENTIntroduction
An example of how this can look: SATenstein

+ Frankenstein's goal:
— Create “perfect” human being from scavenged body parts
+ SATenstein’s goal: Create high-performance SAT solvers
using components scavenged from existing solvers
— Components drawn from or inspired by existing local search
algorithms for SAT parameters determine which components are
selected and how they behave (41 parameters total)
— designed for use with algorithm configuration (3 levels of conditional
params)
+ SATenstein can instantiate:
— at least 29 distinct, high-performance local-search solvers from the
literature
— trillions of novel solver strategies

 

 
ENDELEMENT 

[khudabukhsh, Xu, Hoos, L-B, 2016]

Configured SATenstein vs 11 "Challengers" on 6 SAT Benchmarks

10000

‘sW-GCP R3SAT. HGEN CBMC(SE)

3
8
s

r=
8

Penalized Average Runtime (s)
3

©

o

8

mSATenstein = AG20 mAG2p =AG2+ mANOV =G2 =GNOV =PAWS =RANOV © RSAPS = SAPS VW
ENDELEMENT 

This Tutorial

Section Out

Introduction, Technical Preliminaries, and a Case Study (Kevin)
Learning in the Space of Algorithm Designs
Defining the Algorithm Configuration Problem
Algorithm Runtime Prediction

Applications and a Case Study
ENDELEMENTIntroduction
Algorithm Configuration Visualized

Parameter domains
& starting values

 

 

 

 

 

 

 

 

Calls with Configuration scenario | Problem
different instances
Configurator | parameter” Target Solves
A algorithm
settings

 

 

 

 

 

 

Returns solution cost

 

 
ENDELEMENT 

+ Continuous, integer, ordinal

+ Categorical: finite domain, unordered, e.g., {apple, tomato, pepper}
+ Conditional
— allowed values of some child parameter depend on the values taken by parent parameter(s)

Parameters give rise to a structured space of configurations

+ These spaces are often huge
— eg, SAT solver lingeling has 10°47 configurations
+ Changing one parameter can yield qualitatively different behaviour
+ Overall, that’s why we call it algorithm configuration (vs “parameter tuning”)
ENDELEMENTAlgorithm Configuration: General Definition

An algorithm configuration problem is a 5-tuple (A, ©, D, &,m) where:

+ Ais a parameterized algorithm,

+ @ is the parameter configuration space of A;

+ Dis a distribution over problem instances with domain I;

+ & < co is a cutoff time, after which each run of A will be terminated

+ m:®xII— Risa function that measures the cost incurred by A(@) on an
instance a € II

Optimal configuration * € arg ming-@ E,~p(m(@,7)) minimizes expected cost
ENDELEMENTIntroduction

Algorithm Configuration: Definition with Runtime Objective

 

An algorithm configuration problem is a 5-tuple (A, ©, D, &, Ri.) where:

+ Ais a parameterized algorithm;

+ @ is the parameter configuration space of A;

+ Dis a distribution over problem instances with domain I;

+ & < co is a cutoff time, after which each run of A will be terminated

+ Rz: © x Il Risa function that measures the time it takes to run A(@) with
cutoff time & on instance m € II

Optimal configuration 6* € arg ming-@ E,~p(Rx(@,7)) minimizes expected runtime
ENDELEMENTIntroduction

Beyond Runtime Optimization

Algorithm configuration methods can also be applied to objectives other than
runtime optimization (though not the focus of this tutorial).

Black-Box Optimization
Optimize a function to which the algorithm only has query access.

Hyperparameter Optimization
Find hyperparameters of a model that minimize validation set loss.

     
ENDELEMENT 

Introduction, Technical Preliminaries, and a Case Study (Kevin)

Learning in the Space of Algorithm Designs
Defining the Algorithm Configuration Problem
Algorithm Runtime Prediction

Applications and a Case Study
ENDELEMENT    

Algi m Ru ie Pre

 

n

A key enabling technology will be the ability to solve the following problem.
A pretty vanilla appli in of regression?
Predict how long an algorithm will take to run, given:

  

+ Aset of instances D
+ For each instance i € D, a vector x; of feature values

+ For each instance i € D a runtime observation y; We want a mapping f(x) + y
that accurately predicts y; given 2;

In other words, find a mapping f(x) — y that accurately predicts y; given aj.
ENDELEMENT    

Algi m Ru ie Pre

 

n

A key enabling technology will be the ability to solve the following problem.
A pretty vanilla appli in of regression?
Predict how long an algorithm will take to run, given:

  

+ Aset of instances D

+ For each instance i € D, a vector x; of feature values

+ For each instance i € D a runtime observation y; We want a mapping f(x) + y
that accurately predicts y; given 2;

In other words, find a mapping f(x) — y that accurately predicts y; given aj.

But, is it really possible to use supervised learning to predict the empirical behavior
of an exponential-time algorithm on held-out problem inputs?
ENDELEMENT 

Algorithm Runtime is Surprisingly Predictable

‘SAT Competition Random + Handmade + industrial) dts, MINISAT solver ‘SAT: 18M hardware verification data, PEAR solver
‘Random Forest (RMSE°0.7) ‘Random Forest (RNSE>0.38)

 

10710*10"'10° 10" 10° 10° 10° 101010"'10" 10° 10° 10° 10°

‘Actual Runtime ‘Actual Runtime
‘MIPLIB data, CPLEX 12.1 solver ‘Red Crested Woodpecker habitat data, CPLEX 12:1 solver
Random Forest (RNSE=0.53) ‘Random Forest (RNSE>0.02)

 

 

 

2 107
s
3 107
2 ho!
aa 10° 10° 10"
10°10"10"'10° 10' 10° 10° 10°
‘Actual Runtime ‘Actua Runtime

[H, Xu, L-B, Hoos, 2014]
ENDELEMENT 

That’s Not All, Folks

[H, Xu, L-B, Hoos, 2014]

We've found that that algorithm runtime is consistently predictable, across:
+ Four problem domains:
— Satisfiability (SAT)
— Mixed Integer Programming (MIP)
— Travelling Salesman Problem (TSP)
— Combinatorial Auctions
+ Dozens of solvers, including:
— state of the art solvers in each domain
— black-box, commercial solvers.
+ Dozens of instance distributions, including:
= major benchmarks (SAT competitions; MIPLIB; ...)
— real-world data (hardware verification, computational sustainability, ...)
ENDELEMENTt Modeling Algorithm Pa

 

+ So far we've considered the runtime of single, black box
algorithms

+ Our goal in this tutorial is understanding algorithm
performance as a function of an algorithm’s parameters
— with the ultimate aim of optimizing this function

+ Can we predict the performance of parameterized
algorithm families?
ENDELEMENT 

Introduction

at About Modeling Algorithm P%

 

meters, Too?

 

+ So far we've considered the runtime of single, black box
algorithms

+ Our goal in this tutorial is understanding algorithm
performance as a function of an algorithm’s parameters
— with the ultimate aim of optimizing this function

+ Can we predict the performance of parameterized
algorithm families?

— Performance is worse than before, but we're generalizing
simultaneously to unseen problem instances and unseen
parameter configurations

* On average, correct within roughly half an order of magnitude
— Despite discontinuities, an algorithm’s performance is well
approximated by a relatively simple function of its parameters

SAT: IBM hw verification data, SPEAR
Random Forest (AMSE=0 43)

10°
Eo
210"
E10”
Fao"

107]

 

10710710"'10" 10' 10° 10° 10°

‘MIP: MIPLIB data, CPLEX 12.1 solver
Random Forest (AMSE=055)

 

  

10°
10°10710''10" 10' 10° 10° 10°

‘etal Runtime
ENDELEMENT 

So, how does it work?

In fact, it’s a somewhat trickier regression problem than initially suggested
+ mixed continuous/discrete

+ high-dimensional, though often with low effective dimensionality
+ very noisy response variable (e.g., exponential runtime distribution)

Plus there are some extra features that will be nice to have

+ compatibility with censored observations
+ ability to offer uncertainty estimates at test time

We've tried a lot of different approaches
+ linear/ridge/lasso/polynomial; SVM; MARS; Gaussian processes; deep nets; ...

..to date, we've had the most success with random forests of regression trees
ENDELEMENT 

It’s most important to get features right. For example, in S.

 

* Problem Size (clauses, variables, clauses/variables, ...)

Syntactic properties (e.g, positive/negative clause ratio)
Statistics of various constraint graphs

+ factor graph

+ clause-clause graph

* variable-variable graph

+ Knuth’s search space size estimate Hr
* Cumulative # of unit propagations
at different depths

 

 

 

 

 

 

 

 

 

 

 

* Local search probing

 

 

 

 

 

 

 

* Linear programming relaxation

 
ENDELEMENT   

  

oltid

Introduction, Technical Preliminaries, and a Case Study (Kevin)
Learning in the Space of Algorithm Designs
Defining the Algorithm Configuration Problem
Algorithm Runtime Prediction

Applications and a Case Study
ENDELEMENT  

Intro

Algorithm Configura’

 

any Applications

 

FCC spectrum auction | Applications by Colleagues Applications by Others

 

 

[Auction lomics * Exam timetabling * Kidney exchange
* Motion, person * Linear algebra
tracking

subroutines

 

Mixed integer

ang programming + RNA sequence-

 

 

 

 

 

 

structure alignment + Java garbage
Analytics & Optimization * Protein Folding collection
ORTEC * Computer GO
ev Algorithm Competitions * Linear algebra
Social gaming + SAT, MIP, TSP, Al subroutines
planning, ASP, SMT, * Evolutionary
timetabling, protein Algorithms
source Albocat folding, ... * ML: Classification

Resource Allocation

fipacterum

 

 

 
ENDELEMENTA Case Study

[L-B, Milgrom & S

  

an, Fréchette & L-B, 2017]

Over 13 months in 2016-17 the FCC held an “incentive auction” to

repurpose radio spectrum from broadcast television to wireless internet

In total, the auction yielded $19.8 billion

+ over $10 billion was paid to 175 broadcasters for voluntarily relinquishing their
licenses across 14 UHF channels (84 MHz)

+ Stations that continued broadcasting were assigned potentially new channels to
fit as densely as possible into the channels that remained

+ The government netted over $7 billion (used to pay down the national debt) after
covering costs
ENDELEMENTIntroduction
Feasibility Testi

+ A key subproblem in the auction:

— asking “could station x leave the auction and
go back on-air into the reduced band of
spectrum, alongside all other stations X who
have already done the same?

= about 100K such problems arise per auction

— about 20K are nontrivial

+ A hard graph-colouring problem

— 2990 stations (nodes)

— 27 million interference constraints (channel-specific interference)

— Initial skepticism about whether this problem could be solved exactly at a national scale
+ What happens when we can’t solve an instance:

— Needed a minimum of two price decrements per 8h business day

— each feasibility check was allowed a maximum of one minute

— Treat unsolved problems as infeasible, raising the amount they're paid

 
ENDELEMENTIntroduction
First, We Need Some Data

We wrote a full reverse auction simulator (open source)

Generated valuations by sampling from a model due to Doraszelski et al. |

Assumptions:

— 84 MHz clearing target

— stations participated when their private value for continuing to broadcast was smaller than
their opening offer for going off-air

— 1 min timeout given to SATFC

20 simulated auctions + 60,057 instances

— 2,711-3,285 instances per auction

— all not solvable by directly augmenting the previous solution

— about 3% of the problems encountered in full simulations

Our goal: solve problems within a one-minute cutoff

 
ENDELEMENT 

 

 

The Incumbent Solution: MIP Eni

 

1.0

~ Gurobi

CPLEX

0.9 ~~~

2
3

yn © nH +O
sco oOo Go

‘s@0ue}sul Jo uoNoes 4

°

Runtime (s)
ENDELEMENTIntroduction

What about trying SAT solvers?

 

   

1.0
covey Pot
cesccsa014
0.9 — bccn
Minit NACK S296 _c88C
Peosat
08 Solver43
Rt39
807 RissSgEt
So ce
Resta?
$06 Sty
2 Yala
= 0.5 — Grovetyscone
Oo” Gnovetty+GCa
Fs sau
0.4 — Sperowtoriss
3 Ferteosrip
fy Crptoniist
tr 0.3 Spear
== carob
0.2 — Sateen
Serpsat
== che
0.1 — prossar
0.0
107

Runtime (s)
ENDELEMENT 

+ Choice of complete or local-search solver
— with which solver parameters
* and, depending on solver, conditional subparameters?
+ Various problem-specific speedups
(each of which furthermore had parameters of its own)
— reusing previous solutions
— problem decomposition
— caching similar solutions
— removing underconstrained stations
+ And further problem-independent heuristics
= constraint propagation preprocessor
— different SAT encodings

wr

~
AN
e

=e
Av

=e
Av
ENDELEMENT 

1.0

=~ + Configured SATenstein
0.9 — Gnovety+PoL

— Satenstein

   

0.8

oo
oy

Fraction of Instances
a)
Ba

2
@

102 107 10° 10! 10?
Runtime (s)
ENDELEMENT 

m Portfolios

-B, Nudelman, Shoham, 20

 

Xu, Hutter, Hoos, L-B, 2007-12
Often different solvers perform well on different instances

Idea: build an algorithm portfolio, consisting of different
algorithms that can work together to solve a problem
SATzilla: state-of-the-art portfolio developed by my group
= machine learning to choose algorithm on a per-instance basis

+ Or, just run all the algorithms together in parallel

 

 
ENDELEMENT 

Intro

Algorithm Portfolios

 

-B, Nudelman, Shoham, 2

Often

+ Idea: build an , consisting of different
algorithms that can work together to solve a problem

. : state-of-the-art portfolio developed by my group
— machine learning to choose algorithm on a per-instance basis

+ Or, just run all the algorithms together in parallel

Hydra: use algorithm configuration to of
complementary algorithms

* augment an additional portfolio P by targeting instances on
which P performs poorly

+ Give the algorithm configuration method a dynamic
performance metric:
— nerformance of als s when s autnerfarms P: nerformance of P

 

 
ENDELEMENTIntroduction

Performance of the Algorithm Portfolio

 

 
 
 
 
 
 
 
 
 
 
 
  

 

1.0 ;
— satro234 i
0.9 ~-- "SATenstein 1
“=< §:SATenstein 1
4:8ATenstein i
0.8 zcasp 4
T:sATenstein 1
B07 — beaten !
2 — &Clasp '
80.6 — 3Casp H
@ =m SAT f
= 0.5 mm UNSAT :
i TIMEOUT '
Fs :
S04 :
8 H
£03 :
0.2 ‘
o4 a
0.0 et
107 10" 10° 10! 10?

Runtime (s)
ENDELEMENT 

 

 

 

Economic Impact of a Stronger Solver
4.0. saTFC .
* — gnoveltytpcl
12.0 » — PicoSAT
10.0 *  Gurobi ot
e + Greedy “s
& go ° CPLEX
a
@ 6.0
" st
4.0 ™
2.0
40 1.0 2.0 3.0 4.0 5.0

Value Loss (Billions)
ENDELEMENTThis Tutorial
High-Level Outline

Introduction, Technical Preliminaries, and a Case Study (Kevin)
Practical Methods for Algorithm Configuration (Frank)
Algorithm Configuration Methods with Theoretical Guarantees (Kevin)

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)
ENDELEMENTPractical W

This Tuto!

Section Outli

Practical Methods for Algorithm Configuration (Frank)

 

Sequential Model-Based Algorithm Configuration (SMAC)
Details on the Bayesian Optimization in SMAC
Other Algorithm Configuration Methods

Case Studies and Evaluation
ENDELEMENTThe basic components of algorithm configuration methods

Recall the core of the algorithm configuration definition

Find: 6” € arg mingce@ E,~p(m(8,7)).

The two components of algorithm configuration methods

+ How to select a new configuration to evaluate?
+ How to compare this configuration to the best so far?

     
ENDELEMENT 

Sequential lel-based AC (SMAC): high-level overview

 

Algorithm 1: SMAC (high-level overview)

 

Learn a model 7 from performance data so far: 7: O x I> R
Use model 7 to select promising configurations Onew
ENDELEMENT 

Sequential lel-based AC (SMAC): high-level overview

 

Algorithm 1: SMAC (high-level overview)

 

Learn a model 7 from performance data so far: 7: O x I> R
Use model 7 to select promising configurations Onew
Compare ©, against best configuration so far by executing new algorithm runs
ENDELEMENTSequential lel-based AC (SMAC): high-level overview

 

 

Algorithm 1: SMAC (high-level overview)

Initialize by executing some runs and collecting their performance data
repeat

 

Learn a model 7 from performance data so far: 7: O x I> R
Use model 7 to select promising configurations Onew

Compare ©, against best configuration so far by executing new algorithm runs
until time budget exhausted

 
ENDELEMENTSequential lel-based AC (SMAC): high-level overview

 

 

Algorithm 1: SMAC (high-level overview)

Initialize by executing some runs and collecting their performance data
repeat

 

Learn a model 7 from performance data so far: 7: O x I> R
Use model sh to select promising configurations @,.w ~» Bayesian optimization

Compare ©, against best configuration so far by executing new algorithm runs
until time budget exhausted

 
ENDELEMENT 

General approach

+ Fit a probabilistic model to the collected function
samples (0, f(8))

+ Use the model to guide optimization, trading off
exploration vs exploitation

 

 

 

 

acqusion (tty function

 
ENDELEMENT 

General approach

+ Fit a probabilistic model to the collected function
samples (0, f(8))

+ Use the model to guide optimization, trading off
exploration vs exploitation

 

 

 

 

 

 

 
ENDELEMENTGeneral approach

+ Fit a probabilistic model to the collected function
samples (0, f(8))

+ Use the model to guide optimization, trading off
exploration vs exploitation

 

 

 

 

 

 

 

 

 

 
ENDELEMENT 

 

+ Fit a probabilistic model to the collected function
samples (0, f(8))

+ Use the model to guide optimization, trading off
exploration vs exploitation

Popular in the statistics literature [since Mockus, 1978]

+ Efficient in # function evaluations

+ Works when objective is nonconvex, noisy, has
unknown derivatives, etc

+ Recent convergence results [srinivas et al, 2010; Bull 2011

de Freitas et al, 2012; Kawaguchi et al, 2015]

 

 
   

posted? moah
posterior mash
8
“acqusiion max
acqusion (tty function

t=3

 

 

 

 

 

 

 

 

 
ENDELEMENT 

Sequential Model-based AC (SMAC): high-level overview

 

Algorithm 1: SMAC (high-level overview)

Initialize by executing some runs and collecting their performance data
repeat

 

Learn a model 7 from performance data so far: 7: O x I> R
Use model 7 to select promising configurations Onew

~+ Bayesian optimization with random forests
Compare ©,,.y against best configuration so far by executing new algorithm runs

~» How many instances to evaluate for 6 € ©,...,?
until time budget exhausted

 
ENDELEMENT 

 

y instances to evaluate per configuration?

 

Performance on individual instances often does not generalize

+ Instance hardness varies (from milliseconds to hours)
+ Aim to minimize cost in expectation over instances: c(@) = E,~p(m(8, 7))
ENDELEMENT 

rile) es to evaluate per configuration?

 

y inst

 

Performance on individual instances often does not generalize

+ Instance hardness varies (from milliseconds to hours)
+ Aim to minimize cost in expectation over instances: c(@) = E,~p(m(8, 7))

Simplest, suboptimal solution: use NV instances for each evaluation

+ Treats the problem as a blackbox function optimization problem
+ Issue: how large to choose N?

- too small: overtuning (equivalent to over-fitting)
~ too large: every function evaluation is slow

 
ENDELEMENT 

+ Race new configurations against the best known incumbent configuration 6

- Use same instances (and seeds) as previously used for 6 .
- Aggressively discard new configuration @ if it performs worse than @ on shared runs
ENDELEMENT 

SMAC’s racing approac

 

focus on configurations that might beat the incum

 

+ Race new configurations against the best known incumbent configuration 0
- Use same instances (and seeds) as previously used for 6
- Aggressively discard new configuration @ if it performs worse than 6 on shared runs
* No requirement for statistical domination
(this would be inefficient since there are exponentially many bad configurations)
* Search component allows to return to @ even if it is discarded based on current runs
ENDELEMENT 

SMAC’s racing approac

 

focus on configurations that might beat the incum

 

+ Race new configurations against the best known incumbent configuration 6
- Use same instances (and seeds) as previously used for 6 .
- Aggressively discard new configuration @ if it performs worse than @ on shared runs

* No requirement for statistical domination
(this would be inefficient since there are exponentially many bad configurations)
* Search component allows to return to @ even if it is discarded based on current runs

- Add more runs for 6 over time ~» build up confidence in 6
ENDELEMENTPractical
SMAC’s racing approach: focus on configurations that might beat the incumbent

+ Race new configurations against the best known incumbent configuration 6

- Use same instances (and seeds) as previously used for 6 .
- Aggressively discard new configuration @ if it performs worse than @ on shared runs

* No requirement for statistical domination
(this would be inefficient since there are exponentially many bad configurations)
* Search component allows to return to @ even if it is discarded based on current runs

- Add more runs for 6 over time ~» build up confidence in 6

Let © be finite. Then, the probability that SMAC finds the true optimal parameter
configuration 6* € © approaches 1 as the number of executed runs goes to infinity.
ENDELEMENT 

Pra

 

Saving More Time: Adaptive Capping

When minimizing algorithm runtime,
we can terminate runs for poor configurations 6’ early:

+ Is 0’ better than 6?
~ Example:

RT(8)=20 —- RT(0’)>20
ENDELEMENTSaving More Time: Adaptive Cappi

When minimizing algorithm runtime,
we can terminate runs for poor configurations 6’ early:

+ Is 0’ better than 6?
~ Example:

RT(8)=20 —- RT(0’)>20

+ Can terminate evaluation of 6’ once it is guaranteed to be worse than @
ENDELEMENTSaving More

 

When minimizing algorithm runtime,
we can terminate runs for poor configurations 6’ early:

+ Is 6’ better than 6?
~ Example:

RT(8)=20 —- RT(0’)>20

+ Can terminate evaluation of 6’ once it is guaranteed to be worse than @
Observation

 

Let © be finite. Then, the probability that SMAC with adaptive capping finds the
true optimal parameter configuration 6* < © approaches 1 the number of
executed runs goes to infinity.
ENDELEMENT|

Sequential Model-based AC (SMAC): sum

 

Algorithm 1: SMAC

Initialize by executing some runs and collecting their performance data
repeat

 

Learn a model 7m from performance data so far: nm: Ox IR
Use model 7h to select promising configurations Onew

~» Bayesian optimization with random forests
Compare ©,,-w against best configuration so far by executing new algorithm runs

~» Aggressive racing and adaptive capping
until time budget exhausted

 
ENDELEMENT 

  

 

 

  
    
  

S speedup

 

200x speedup

Loss (runtime of optimized solver

—e—-—+|

 

 

 

10? 10? 104 10°
Configuration Budget [sec]

%4|Random search

Random search
+ racing

ROAR:
Random search

+ racing

}+ adaptive capping

SMAC

Example: Optimizing CPLEX on combinatorial auctions (Regions 100)
ENDELEMENT 

Practical

This Tuto!

Section Outli

Practical Methods for Algorithm Configuration (Frank)

 

Sequential Model-Based Algorithm Configuration (SMAC)
Details on the Bayesian Optimization in SMAC
Other Algorithm Configuration Methods

Case Studies and Evaluation
ENDELEMENTPractical
AC poses many non-standard challenges to Bayesian optimization

Complex parameter space

+ High dimensionality (low effective dimensionality) {wang et al, 2013; Garnett et al, 2013]
+ Mixed continuous/discrete parameters (11, 2009; Hi. et al, 2014]
+ Conditional parameters [swersky et al, 2013; H. & Osborne, 2013; Levesque et al, 2017]
ENDELEMENTPractical
AC poses many non-standard challenges to Bayesian optimization

Complex parameter space

+ High dimensionality (low effective dimensionality) {wang et al, 2013; Garnett et al, 2013]
+ Mixed continuous/discrete parameters (11, 2009; Hi. et al, 2014]
+ Conditional parameters [swersky et al, 2013; H. & Osborne, 2013; Levesque et al, 2017]

Non-standard noise

+ Non-Gaussian noise [williams et al, 2000; Shah et al, 2018; Martinez-Cantinet al, 2018]
+ Heteroscedastic noise [Le et al, Wang & Neal, 2012]

    
ENDELEMENTPractical

 

Complex parameter space

+ High dimensionality (low effective dimensionality) {wang et al, 20 tt et al, 20
+ Mixed continuous/discrete parameters (11, 2009; 4. et a ]
+ Conditional parameters [s 1. & Osborne evesque et al ]

Non-standard noise

+ Non-Gaussian noise [williams et al, 2000; Sha 8; Martinez-Cantin 018)
+ Heteroscedastic noise {Le et al, Wang & Neal

Efficient use in off-the-shelf Bayesian optimization

+ Robustness of the model {malkow 1 Garnett, 2018]
+ Model overhead {quiron jela & Rasmu! 05; Bu noek
ENDELEMENTPractical

AC poses many non-standard challenges to Bayesian optimization

Complex parameter space

+ High dimensionality (low effective dimensionality) [wang et al, 2013; Garnett et al, 20
+ Mixed continuous/discrete parameters (x, 2009; 4. et al, 2014)
+ Conditional parameters [swersky 2013; H. & Osborne, 2013; Levesque et al., 2017]

 

Non-standard noise

+ Non-Gaussian noise [williams et al, 2000; Shah 2018; Martinez-Cantinet al, 2018]
+ Heteroscedastic noise [Le et al, Wang & Neal, 201

Efficient use in off-the-shelf Bayesian optimization

+ Robustness of the model {malkow 1 Garnett, 2018]
+ Model overhead [auifionero-candela & Rasmu! 005; Bu 2 2010; Snoek 2015

We'll use random forests to address all these; but we need
ENDELEMENTPractical

 

ptation of regression trees: storing empirical variance in every leaf

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

   

 

 

 

 

 

 

 

 

Feature 2 runtime
z 37
20
12
19
4 2
35 fa
param, ram; < {blue, green}
param 1 [Teature > Tuntime| param T Daram 3 [ runtime’
ae | 2 37 false blue | 20
tue | 55 21 false blue | 25,
false | 5 12 true green | 19
feature, < 3. feature, > 3.5 true bie | 12
true green | 17
[param 1 [Feature 2] param 3 [param T [feature 2 [param 3 runtime
Take [| 2 Ted true red || 21 n= 186
false | 5 | red || 12 -
o? =4.72
7 = 1.65

   

20
ENDELEMENTPr
dom Forests with Uncertainty Predictions

+ Random forest as a mixture model of T trees (Hi. et al, 2014]
+ Predict with each of the forest's trees: j and a? for tree t
+ Predictive distribution: M(u, 0?) with

iz
sy

t=1

12 rm
2 2 2 2
o -(#3 at) +70 My) —

t=1 t=1
mean of the variance of
variances the means

Another recent variant for uncertainty in random forests: Mondrian forests

[Lakshminare

 

nan, Roy & Teh, 2015; Lakshminarayanan, Roy & Teh, 2016]
ENDELEMENT 

A key modification of random forests: sampling spli

 

 

param 1] param 2] param 3 ]/runtime
false 2 red 37
true 5.5 red 2.1
false 5 red 12

 

 

 

 

 

 

 

param, s 35-——~ vara >3.5

param 1] param 2] param 3 | runtime: param | [param 2 [param 3][ rw
false 2 red 37 true 35 red
false 5 red

 

 

 

 

 

 

 

 

ae
be

 

 

 

 

 

 

 

 

+ To obtain this split, the split point should be somewhere between L=2, U=5
+ Standard: split at mid-point $(Z + U) = 3.5
+ Now instead: sample split point from Uniform [L,U]
ENDELEMENTAkey modification of random forests: sampling split

 

 

 

param 1] pi 2] param 3 |[runtime
false red 37 |
true red 2.1
false red 1.2

 

 

 

 

 

 

 

param, s 35 vara >3.5

param 1] param 2] param 3 ||runtime param | [param 2 [param 3] runtime
false 2 red true 35 red pal
false 5 red 1.2

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

+ To obtain this split, the split point should be somewhere between L=2, U=5
+ Standard: split at mid-point 3(L +U) = 3.5
+ Now instead: sample split point from Uniform [L,U]
ENDELEMENTindom forests with better uncertainty estimates

+ Sampling split points is crucial to obtain smooth uncertainty estimates

vo without randomized spit points with randomized spit points

 

 

   
  

“True func
standard

   

 

 

 

 

 

Split at mid point 0.5 (L+U) Split at sample from Uniform(L, U]

1000 trees, min. number of points per leaf = 1; with bootstrapping
ENDELEMENTical
indom forests with better uncertainty estimates

+ Sampling split points is crucial to obtain smooth uncertainty estimates

Without randomized sp

 

With randomized spit points

 

 

‘True fonction
te Observations

 

 

 

 

 

 

 

 

 

Split at mid point 0.5 (L+U) Split at sample from Uniform[L, U]

1000 trees, min. number of points per leaf = 1; without bootstrapping
ENDELEMENTAggregating Model Predictions Across Multiple Instances

+ Model rm : © x II > R predicts for one instance at a time
+ We want a model that marginalizes over instances: /(@) = B.~p(rm(0,7))

+ Intuition: predict for each instance and then average
+ More efficient implementation in random forests

— Keep track of fraction of instances compatible with each leaf
— Weight the predictions of the leaves accordingly

     
ENDELEMENTPr

optimizati

   

  

 

‘=22+= RF mean prediction
RF mean +/~ 2°stddev|
— True function
O Function evaluations
= =El (scaled)

 
   
   
 

 

   

response y

ae A

 

 

 

5!
) 02 04 06 08 1

parameter x

 
ENDELEMENTical
Bayesian optimization with censored data

+ Terminating poor runs early yields censored data points
~~» we only know a lower bound for some data points
+ Use an EM-style approach to fill in censored values (schinee & Hahn, 1979; 4. et al, 2013

 

RF mean prediction
RF mean +/~ 2'stddev
— True funetion
‘© Function evaluations
x  Right-censored fun. evals.
— = Exp. improvement (scaled)

response y

 

“to oi 02 03 o« 05 06 o7 os o9
parameter x
ENDELEMENT 

+ Only split on a parameter if it’s guaranteed to be active in the current node
— Splits higher up in the tree must guarantee parent parameters to have right values
ENDELEMENT 

EU Uitemermeolirel ta

 

+ Only split on a parameter if it’s guaranteed to be active in the current node
— Splits higher up in the tree must guarantee parent parameters to have right values

+ Empirically, both GPs and RFs have their advantages [«

 

(, 2073]

Low-dimensional, continuous

SMAC |
Valid. loss

0.65540.27

 

   
  

         

Experiment #evals

branin (0.398) 200

 

 

   

 

 

 

 

hhar6 (-3.322) 200 | -2.977-40.11 |

100 | 86+09 | |
LDA ongrid 50 | 1269.6£2.9 | 127264103 | 1271,543.5
SVM ongrid 100 | 2410.1 24.6409 24.240.0
ENDELEMENT 

EU Uitemermeolirel ta

  

+ Only split on a parameter if it’s guaranteed to be active in the current node
— Splits higher up in the tree must guarantee parent parameters to have right values

al, 2013]

+ Empirically, both GPs and RFs have their advantages

 

Low-dimensional, continuous

| SMAC | Spearmint
Valid. loss a

 

TPE

      
 

    

Experiment #evals

 

  

 

 

branin (0.398) 200 655+0.27
har6 (-3.322) 200 77 +0.11
Log Ree

 

 

 

 

ssion 100 | 860.9
|

 

LDA ongrid 50 | 1269.6-+2.9 1271,543.5
SVM ongrid 100 | "2410.1 24.6409 24.2400
HP-NNET convex 200 20.0409 185414
HP-NNET MRBL 200 S14£3.2 48.914
HP-DBNET convex. 200 1745$5.6 16.140.5
‘Auto-WEKA 30h 40.6447.2 35.5429

 

High-dimensional, conditional
ENDELEMENT 

Random forests

Standard GPs

 

Training | O(T'N log? N)

O(N)

 

Prediction | O(T log N)

 

 

O(N?)

Empirical scaling of runtime with the number of data points:

180 overhead on Hartmann 6

 

po ‘magnitude
i
& wo’

 

 

 
 

‘Spearmint, 5000s
DNGO, 700s

—t —{| Random Forest, 0.7s
ENDELEMENT 

imensions (low effective dimensionality)

 

10°
© Branin2d

ms Branin250
4-4 Branin10000

   

8
i
3
3
5

 

 

 

200 700 500 800
‘function evaluations

2 important dimensions (Branin test function)
+ additional unimportant dimensions, following Wang et al [2013]
ENDELEMENTThis Tutorial

Section Outli

Practical Methods for Algorithm Configuration (Frank)

 

Sequential Model-Based Algorithm Configuration (SMAC)
Details on the Bayesian Optimization in SMAC
Other Algorithm Configuration Methods

Case Studies and Evaluation
ENDELEMENT 

There are many continuous blackbox optimization methods

  

1; Hansen, 2016]

gel, 2004], eSpecially with

 

Evolutionary strategies, e.g., CMA-ES [Ha

— Strong results for continuous hyperparameter opti
parallel resources [Loshchilov & 4, 2016]

— Also strong results for optimizing NN parameters, especially when only approximate gradients
are available (RL) (salimans et =

en & Ostermeier

   
  

ation

drichs

   

 

7; Conti et al, 2

 

Differential evolution (storm and Price, 1997]

 

Particle swarm optimization {kennedy & Eberhart

~» For continuous parameter spaces, these could be used instead of Bayesian
optimization
ENDELEMENTThere are many approaches for model selection
ENDELEMENTThere are many approaches for model selection

 

+ Eg., Hoeffding races [Maron & Moore
+ To compare a set of configurations (or algorithms):

— Use Hoeffding’s bound to compute a confidence band for each configuration

— Stop evaluating configuration when its lower bound is above another's upper bound

 
ENDELEMENTIntroduction Practical Tt

F-race and Iterated F-race

 

F-race [Birattari et al, 2002]

+ Similar idea as Hoeffding races

+ But uses a statistical test instead to check whether @ is inferior
- Namely, the F-test, followed by pairwise t-tests

Iterated F-Race [Lépez-Ibafiez et al, 2016]
+ Maintain a probability distribution over which configurations are good

+ Sample k configurations from that distribution & race them with F-race
+ Update distributions with the results of the race

~» Focus on solution quality optimization
ENDELEMENT 

Iterated local search in parameter configuration space [1 H. et al, 2

   

Animation credit: Holger Hoos
ENDELEMENT 

Iterated local search in parameter configuration space [1 H. et al, 2

   

Initialisation

Animation credit: Holger Hoos
ENDELEMENT 

Iterated local search in parameter configuration space [1 H. et al, 2

   

Local Search

Animation credit: Holger Hoos
ENDELEMENTThe ParamiLS Framework

Iterated local search in parameter configuration space [1 e

 

Local Search

Animation credit: Holger Hoos
ENDELEMENTThe ParamiLS Framework

Iterated local search in parameter configuration space [1 e

 

Perturbation

Animation credit: Holger Hoos
ENDELEMENTThe ParamiLS Framework

Iterated local search in parameter configuration space [1 e

 

“wif

Local Search

Animation credit: Holger Hoos
ENDELEMENTThe ParamiLS Framework

Iterated local search in parameter configuration space [1 e

 

Local Search

Animation credit: Holger Hoos
ENDELEMENTThe ParamiLS Framework

Iterated local search in parameter configuration space [1 e

 

f

Local Search

Animation credit: Holger Hoos
ENDELEMENTThe ParamiLS Framework

Iterated local search in parameter configuration space [1 e

[

Selection (using Acceptance Criterion)

 

Animation credit: Holger Hoos
ENDELEMENT 

Iterated local search in parameter configuration space [1

’

Het al, 2

   

 

Perturbation

Animation credit: Holger Hoos
ENDELEMENTThe ParamiLS Framework

Iterated local search in parameter configuration space [1

’

 

7; Het al, 2

 

 

Perturbation
Animation credit: Holger Hoos

ParamlLS predates SMAC; aggressive racing & adaptive capping originate here
ENDELEMENTGender-based Genetic Algorithm (GGA) [ansoteguiet al, 2009]

Genetic algorithm:

+ Population of individuals as genomes (i.e., solution candidates)
+ Modify population by

— Mutations (i.e., random changes)
= Crossover (i.e, combination of 2 parents to form an offspring )
ENDELEMENTGender-based Genetic Algorithm (GGA) [ansoteguiet al, 2009]

Genetic algorithm:

+ Population of individuals as genomes (i.e., solution candidates)
+ Modify population by

— Mutations (i.e., random changes)
= Crossover (i.e, combination of 2 parents to form an offspring )
Genetic algorithm for algorithm configuration
+ Genome = parameter configuration

+ Crossover: Combine 2 configurations to form a new configuration
ENDELEMENTGender-based Genetic Algorithm (GGA) [ansotegui et al, 2009]

Genetic algorithm:

+ Population of individuals as genomes (i.e., solution candidates)
+ Modify population by

— Mutations (i.e., random changes)
= Crossover (i.e, combination of 2 parents to form an offspring )
Genetic algorithm for algorithm configuration
+ Genome = parameter configuration

+ Crossover: Combine 2 configurations to form a new configuration

Two genders in the population (competitive and non-competitive)

+ Selection pressure only on one gender
+ Preserves diversity of the population
ENDELEMENT 

Can exploit parallel resources

+ Evaluate population members in parallel
+ Adaptive capping: can stop when the first k succeed
ENDELEMENTGGA: Racing and Cappi

Can exploit parallel resources

+ Evaluate population members in parallel

+ Adaptive capping: can stop when the first k succeed
Use N instances to evaluate configurations

+ Increase N in each generation
+ Linear increase from Netart to Neng
ENDELEMENTPr ie
This Tutorial

Section Outli

Practical Methods for Algorithm Configuration (Frank)
Sequential Model-Based Algorithm Configuration (SMAC)
Details on the Bayesian Optimization in SMAC
Other Algorithm Configuration Methods

Case Studies and Evaluation
ENDELEMENTConfiguration of a SAT Solver for Verification [u. et al, FMCAD 2007]

SAT-encoded instances from formal verification

+ Software verification (Babi & Hu; Cav '07]

+ IBM bounded model checking (zarpas; sar '05]

State-of-the-art tree search solver for SAT-based verification

+ Spear, developed by Domagoj Babi¢ at UBC
+ 26 parameters, 8.34 x 10!” configurations

     
ENDELEMENT   

Configuration of a SAT Solver for Verifica’

 

al, FMCAD 2007]

+ Ran ParamILs, 2 days x 10 machines
— Ona training set from each of hardware and software verification
ENDELEMENT 

Configuration of a SAT Solver for Verification (1

 

et al, FMCAD 2007]

+ Ran ParamILs, 2 days x 10 machines

— Ona training set from each of hardware and software verification
+ Compared to manually-engineered default

— 1 week of performance tuning

— Competitive with the state of the art

— Comparison on unseen test instances
ENDELEMENT 

Configuration of a SAT Solver for Verification (1

 

et al, FMCAD 2007]

+ Ran ParamILs, 2 days x 10 machines

— Ona training set from each of hardware and software verification
+ Compared to manually-engineered default

— 1 week of performance tuning

— Competitive with the state of the art

— Comparison on unseen test instances

‘SPEAR, optimized for IBM-BMC (s)

 

 

107197 10" 10° 10° 10° 4
SPEAR, original default (s}

IBM Hardware verification:
4.5-fold speedup on average
ENDELEMENT 

Configuration of a SAT Solver for Verification [

 

et al, FMCAD 2007]

+ Ran ParamILs, 2 days x 10 machines

— Ona training set from each of hardware and software verification
+ Compared to manually-engineered default

— 1 week of performance tuning

— Competitive with the state of the art

— Comparison on unseen test instances

‘SPEAR, optimized for IBM-BMC (s)

 

 

 

 

10° 40" 10° 10' 10° 1 10719" 10° 10' 10° 10° 10°
‘SPEAR, original default (s) SPEAR, original default (s)
IBM Hardware verification: Software verification: 500-fold speedup

4.5-fold speedup on average ~» won QF_BV category in 2007 SMT competi

 
ENDELEMENTConfiguration of a Commercial MIP Solver [u. et al, cpaior 2010]

Mixed integer programming (MIP)

min da

st. Ax <b
a, €Zforiel

Combines efficiency of solving linear programs
with representational power of integer variables
ENDELEMENTConfiguration of a Commercial MIP Solver [u. et al, cpaior 2010]

Mixed integer programming (MIP)
min cla
Combines efficiency of solving linear programs
st. Ax <b

with representational power of integer variables
a,€Zforiel

Commercial MIP Solver CPLEX
+ Leading solver for 15 years (at the time)

+ Licensed by over 1000 universities and 1300 corporations
+ 76 parameters, 10” configurations
ENDELEMENTConfiguration of a Commercial MIP Solver [x. et al, cPAIOR 2010]

Mixed integer programming (MIP)
min cla . oo.
Combines efficiency of solving linear programs
st. Ax <b

with representational power of integer variables
a,€Zforiel

Commercial MIP Solver CPLEX
+ Leading solver for 15 years (at the time)

+ Licensed by over 1000 universities and 1300 corporations
+ 76 parameters, 10” configurations

 

 

 

Improvements by configuration with ParamILS

+ Between 2x and 50x speedups to solve optimally
+ Later work with CPLEX team: up to 10000 speedups 1010" 10) 301 305 10" 10° 10°
+ Reduction of optimality gap: 1.3x to 8.6 x Wildlife corridor instances

 

 
ENDELEMENT 

 

 

Comparison to CPLEX Tuni

   

Tool

  

t al, CPAIOR 2010]

+ CPLEX tuning tool

— Introduced in version 11 (late 2007, after ParamiLS)
— Evaluates predefined good configurations, returns best one
— Required runtime varies (from < 1h to weeks)

 

 

 

 

 

 

 

 

 

@® |{/—betaut
B al] X CPLEX tuning tool
9,
@6
8
Fa
El x
S
52
a
n 5 10

10" 0
Configuration budget [CPU s]
CPLEX on MIK instances
ENDELEMENT 

 

 

Com

 

ison to CPLEX Tunii

 

Tool

  

t al, CPAIOR 2010]

+ CPLEX tuning tool

— Introduced in version 11 (late 2007, after ParamiLS)
— Evaluates predefined good configurations, returns best one
— Required runtime varies (from < 1h to weeks)
+ ParamiLs: anytime algorithm
— At each time step, keeps track of its incumbent

 

 

 

 

 

@ |/—Ddetaut

S gl| X CPLEX tuning too!
Olle ParamiLs
@

@ 6)

8

S

e4 x
5

2

52

a

 

 

 

 

10° 10° *
Configuration budget [CPU s]
CPLEX on MIK instances
ENDELEMENT 

 

Comp:

 

ison to CPLEX Tuning Tool [x. et

 

CPAIOR 2010]

+ CPLEX tuning tool
— Introduced in version 11 (late 2007, after ParamiLs)
— Evaluates predefined good configurations, returns best one
— Required runtime varies (from < 1h to weeks)
+ ParamiLs: anytime algorithm
— At each time step, keeps track of its incumbent

 

 

 

 

 

 

 

 

    

 

@ |/—bdetaut ow

B al] X CPLEX tuning too 2

G [tszParamus &

@ 6 Q

8 8

S S

e4 x £ ;
3 S  [—Derauit

eal = CPLEX tuning tool
o o

a © jo'(ezzParamius

 

 

 

 

 

 

 

 

10 7 =
Configuration budget [CPU s]
CPLEX on MIK instances

10° 0° 0°
Configuration budget [CPU s]
CPLEX on SUST instances

 

Note: lower is better
ENDELEMENTSMAC further improved p ‘mance for both of

 

 

 

 

AC scenario GGA ParamILS = SMAC
CPLEX on CLS 5.36 2.12 L.77
CPLEX on CORLAT 20.47 9.57 5.38
CPLEX on RCW2 63.65 54.09 49.69
CPLEX on Regions200 | 7.09 3.04 3.09
SPEAR on IBM -- 801.32 775.15
SPEAR on SWV -- 1.26 0.87

 
ENDELEMENTConfigurable SAT Solver Competition (CSSC) [1. et al, al) 2015]

Annual SAT competition

+ Scores SAT solvers by their performance across instances
+ Medals for best average performance with solver defaults
+ Implicitly highlights solvers with good defaults

Configurable SAT Solver Challenge (CSSC)

+ Better reflects an application setting: homogeneous instances

+ Can automatically optimize parameters
+ Medals for best performance after configuration
— Based on configuration by all of SMAC, ParamILS and GGA
ENDELEMENTCSSC result proved a lot

 

  

 

 

 

 

 

8 35a om aor os oT

 

 

Lingeling on CircuitFuzz: Clasp on n-queens: probSAT on unif rnd 5-SAT:
Timeouts: 119 — 107 Timeouts: 211 — 102 Timeouts: 250 > 0
ENDELEMENTCSSC result #2: Automated configura’

 

Example: random SAT+UNSAT category in 2013

[sover | cssCranking | Default ranking

Clasp

Lingeling

Riss3g

Solver43
Simpsat

Sat4j
For1-nodrup
gNovelty+GCwa
gNovelty+Gca
gNovelty+PCL 10 10

O©OnrI FH b
©oN WRN
ENDELEMENT   

 

1 10 100 10? x
ParamILS gn”

ILS and GGA

100x

 

10x 2x °

   

 

 

 

10 100
GGA

 

To
0’
o®

te

Each dot: performance achieved by the two configurators being compared

for one solver on one benchmar

 

distribution
ENDELEMENTTheory
This Tutorial
High-Level Outline

Introduction, Technical Preliminaries, and a Case Study (Kevin)
Practical Methods for Algorithm Configuration (Frank)
Algorithm Configuration Methods with Theoretical Guarantees (Kevin)

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)
ENDELEMENTAlgorithm Configurati

+ It's trivial to achieve optimality in the limit
— what makes an algorithm configurator good is finding good configurations quickly

+ So far our focus, like most of the literature, has been on empirical performance

+ Let's now consider obtaining meaningful theoretical guarantees about
worst-case running time

— This section follows Kleinberg, L-B & Lucier (2017)
* but uses notation consistent with the rest of this tutorial
ENDELEMENTTheory
This Tutorial

Algorithm Configuration Methods with Theoretical Guarantees (Kevin)
Technical Setup

Structured Procrastination (the c

 

ew configurations)

Extensions to Structured Procrastination (r

 

any configuratio

 

LeapsAndBounds
CapsAndRuns
Structured Procrastination with Confidence

Related Work and Further Reading
ENDELEMENT 

Problem Definition

 

(Notation you'll need for this section, slide 1/2)

An algorithm configuration problem is defined by (A, 0, D, &, R):
+ Ais a parameterized algorithm
+ @ is the parameter configuration space of A

— We use @ to identify particular configurations

+ Dis a probability distribution over input instances with domain II; typically the
uniform distribution over a benchmark set

— We use x to identify (input instance, random seed) pairs, which we call instances

* K < oo iS a max cutoff time, after which each run of A will be terminated

+ R,(0,7) is the runtime of configuration @ € © on instance z, with cutoff time «
- R,(0) = E,.p[R,.(0,7)] denotes expected «-capped running time of 0
— R(#) = Rx(0) denotes expected running time of 6

+ Ko > Ois the minimum runtime: R(O,7) > Ko for all @ and +
ENDELEMENTTheory

Approximately Optimal Configurations

(Notation you'll need for this section, slide 2/2)
Let OPT = ming{R(4)}.
Definition (c-Optimality)

Given € > 0, find 6* € © such that R(6*) < (1+ )OPT.

+ If #'s average running time is driven by a small set of exceedingly bad inputs that
occur very rarely, then we'd need to run 6 on many inputs

+ Implies worst-case bounds scaling linearly with & even when OPT <k&
ENDELEMENTTheory

Approximately Optimal Configurations

Let = ming{ R(A)}.
Definition (c-Optimality)

Given € > 0, find 6* € © such that R(6*) < (1+ )OPT.

(Notation you'll need for this section, slide 2/2)

+ If #’s average running time is driven by
, then we'd need to run @ on many inputs
+ Implies worst-case bounds scaling even when OPT <k

by allowing the running time of 6* to be capped at some

We
threshold value « for a 6 fraction of (instance, seed) pairs

Definition ((c, 5)-Optimality)

A configuration 6* is (e,6)-optimal if there exists some threshold «

for which and
ENDELEMENT 

Theory

ng Approaches

  

Ex

Definition (incumbent-driven)

An algorithm configuration procedure is if, whenever an
algorithm run is performed, the captime is either & or (an amount proportional to)

the runtime of a previously performed algorithm run.

F-race [Birattari et al , ParamILs [Hutter et al 009], GGA [Ansétegui et al,, 2009; 2015], irace
pez-Ibafiez ¢ 6], ROAR and SMAC [Hutter et al, 2011]

Theorem (running time lower bound)

Any (e, 5)-optimal incumbent-driven search procedure has worst-case expected
runtime that scales at least
ENDELEMENTTheory P
This Tutorial

Section Out!

Algorithm Configuration Methods with Theoretical Guarantees (Kevin)
Technical Setup
Structured Procrastination (the case of few configurations)

Extensions to Structured Procrastination (many configura’

 

LeapsAndBounds
CapsAndRuns
Structured Procrastination with Confidence

Related Work and Further Reading
ENDELEMENTThee
Structured Procrastinati

+ Atime management scheme due to Stanford philosopher John Perry

[Perry, 1996; 2011 Ig Nobel Prize in Literature]

— Keep a set of daunting tasks that you procrastinate to avoid, thereby accomplishing other tasks

— Eventually, replace each daunting task with a new task that is even more daunting, and so
complete the first task

 
ENDELEMENTThee
Structured Procrastinati

+ Atime management scheme due to Stanford philosopher John Perry

[Perry, 1996; 2011 Ig Nobel ze in Literature]
— Keep a set of daunting tasks that you procrastinate to avoid, thereby accomplishing other tasks
— Eventually, replace each daunting task with a new task that is even more daunting, and so
complete the first task

+ Similarly, the Structured Procrastination algorithm configuration procedure
[Kleinberg, Lucier & L-B, 2017]:
— maintains sets of tasks (for each «
= starts with the easiest tasks (shortest captimes);
— procrastinates when these tasks prove daunting (puts them back on the queue),

 

 

figuration 0, a queue of runs to perform);

 
ENDELEMENTThee
Structured Procrastinati

+ Atime management scheme due to Stanford philosopher John Perry
[Perry, 1996; 2011 Ig Nobel Prize in Literature]

— Keep a set of daunting tasks that you procrastinate to avoid, thereby accomplishing other tasks
— Eventually, replace each daunting task with a new task that is even more daunting, and so
complete the first task
+ Similarly, the Structured Procrastination algorithm configuration procedure
[Kleinberg, Lucier & L-B, 2017]:
— maintains sets of tasks (for « figuration 0, a queue o}
= starts with the easiest tasks (shortest captimes);

    

— procrastinates when these tasks prove daunting (puts them back on the queue).
Key insight

Only spend a long time running a given configuration on a given instance after
having failed to find any other (configuration, instance) pair that could be
evaluated more quickly.
ENDELEMENTaL
Structured Procrastinatio!

For now we consider the case of few configurations; let |O| =n
ENDELEMENTStructured Procrastina’

For now we consider the case of few configurations; let |O| =n

1

Initialize a bounded-length queue Qo of (instance, captime) pairs for each
configuration @

— instances randomly sampled from D with randomly sampled seeds
— initial captimes of Ko

. Calculate approximate expected runtime for each 0

— zero for configurations on which no runs have yet been performed
— else average runtimes, treating capped runs as though they finished
ENDELEMENTaL
Structured Procrastinatio!

For now we consider the case of few configurations; let |O| =n

1. Initialize a bounded-length queue Qo of (instance, captime) pairs for each
configuration @

2. Calculate approximate expected runtime for each 0
— zero for configurations on which no runs have yet been performed
— else average runtimes, treating capped runs as though they finished

3. Choose the task optimistically predicted to be easiest: the (instance, captime)
pair at the head of the queue corresponding to the @ with smallest approximate
expected runtime
ENDELEMENTStructured Procrastination

For now we consider the case of few configurations; let |O| =n

1. Initialize a bounded-length queue Qo of (instance, captime) pairs for each
configuration @

2. Calculate approximate expected runtime for each 0

3. Choose the task optimistically predicted to be easiest: the (instance, captime)
pair at the head of the queue corresponding to the @ with smallest approximate
expected runtime

4. If the task does not complete within its captime, procrastinate:
double the captime and put the task at the tail of Qo
— We'll do many other runs before we'll forecast this to be the easiest task
ENDELEMENTStructured Procrastination

For now we consider the case of few configurations; let |O| =n

1. Initialize a bounded-length queue Qo of (instance, captime) pairs for each
configuration @

2. Calculate approximate expected runtime for each 0

3. Choose the task optimistically predicted to be easiest: the (instance, captime)
pair at the head of the queue corresponding to the @ with smallest approximate
expected runtime

4. If the task does not complete within its captime, procrastinate:
double the captime and put the task at the tail of Qo
— We'll do many other runs before we'll forecast this to be the easiest task

5. If execution has not yet been interrupted, goto 2
ENDELEMENT 

For now we consider the case of few configurations; let |O| =n

1.

Initialize a bounded-length queue Qo of (instance, captime) pairs for each
configuration @

2. Calculate approximate expected runtime for each 0
3. Choose the task optimistically predicted to be easiest: the (instance, captime)

pair at the head of the queue corresponding to the @ with smallest approximate
expected runtime

. If the task does not complete within its captime, procrastinate:

double the captime and put the task at the tail of Qo

. If execution has not yet been interrupted, goto 2
. Return the configuration that we spent the most total time running

— it might seem more intuitive to return the configuration with best approximate expected
runtime, but this isn’t statistically stable
ENDELEMENTTheory

Running Structured Procrastin

The user must specify

+ an algorithm configuration problem (A, 9, D, «, R, Ko);

+ a precision ¢ (how far solutions can be from optimal);

+ a failure probability ¢ (max probability with which guarantees can fail to hold).

The user does not need to specify 6 (the fraction of “outlying” instances on which
running times may be capped)
+ this parameter is gradually reduced as the algorithm runs

+ when the algorithm is stopped, it returns the 6 for which it is guaranteed
to have found an (e,6)-optimal configuration
ENDELEMENT 

ime

 

unni

 

Structured Procrastinati

Theorem (w e running time, few configurations)

For any 5 > 0, an execution of the Structured Procrastination algorithm identifies
an (c,6)-optimal configuration with probability at least 1 — ¢ within worst-case

expected time
nink
o (ja n( = opr).

 
ENDELEMENT 

ime

 

unni

 

Structured Procrastinati

Theorem (w e running time, few configurations)

For any 5 > 0, an execution of the Structured Procrastination algorithm identifies
an (c,6)-optimal configuration with probability at least 1 — ¢ within worst-case

expected time
nink
o(zain( — opr).

 
ENDELEMENT 

ime

 

unni

 

Structured Procrastinati

Theorem (worst-case running time, few configurations)

For any 5 > 0, an execution of the Structured Procrastination algorithm identifies
an (c,6)-optimal configuration with probability at least 1 — ¢ within worst-case

expected time
nink
o(zain( = opr).

Theorem (running time lower bound for few configurations)

Suppose an algorithm configuration procedure is guaranteed to select an
(e,6)-optimal configuration with probability at least 4. Its worst-case expected
running time must be at least Og opr).

 
ENDELEMENTTheory a
This Tutorial

Section Out!

Algorithm Configuration Methods with Theoretical Guarantees (Kevin)
Technical Setup

Structured Procrastination (the c

 

onfigurations)

Extensions to Structured Procrastination (many configurations and more)
LeapsAndBounds

CapsAndRuns

Structured Procrastination with Confidence

  

Related Work and Further Readi

 
ENDELEMENTThe Case of Many Configurations

+ We need a different approach if we want to handle infinitely many
configurations—our current guarantees are superlinear in n
— Relax the requirement that we find performance close to that of OPT
— Instead, seek a configuration with performance close to the best that remains after we exclude
the 4 fraction of fastest configurations from © (call this OPT,)

* in other words, seek a configuration within the top-performing |1/+|-quantile

 

A configuration 6* is (€, 6, y)-optimal if there exists some threshold x
for which Ry(@*) < (1+ ¢) OPT, and Prep (R(6*, 7) >) <6.
ENDELEMENTTheory

Extending Structured Procrastination to Many Configurations

We extend the Structured Procrastination algorithm to seek the best among a
random sample of 1/7 configurations

+ It gradually reduces both 6 and + to tighten guarantees

— reduces y by sampling more configurations
— setsd ="

For any 4, w and with 5 = y*, an execution of the Structured Procrastination

algorithm identifies an (e,6, y)-optimal configuration with probability at least
1—¢ in worst-case expected time

1 Ink
go (<a te (Gaz) OPT:) ,

 
ENDELEMENTTheory P

Extending Structured Procrastination to Many Configurations

Theorem

For any 7, w and with 6 = 7, an execution of the Structured Procrastination
algorithm identifies an (e,6,7)-optimal configuration with probability at least
1 —(¢ in worst-case expected time

1 Ink
oO (ea™ (a )orr, ) :
Theorem (running time lower bound for many configurations)

Suppose an algorithm configuration procedure is guaranteed to select an

(e, 6, y)-optimal configuration with probability at least 4. Its worst-case expected

running time must be at least 2(;t20PT, )h

 

 

 
ENDELEMENT 

Theory

extensions

 

Theorem (compatibility with Bayesian optimization & local search)

Suppose that half of the configurations sampled in Structured Procrastination are
. Then
worst-case runtime is increased by at most a factor of 2.

Theorem (linear speedups when parallelized)

Suppose that Structured Procrastination is
. Then, provided it is run for a sufficiently long time (linear in p), worst-case
runtime decreases by at least a factor of p — 1.
ENDELEMENTTheory
This Tutorial

Section Out!

Algorithm Configuration Methods with Theoretical Guarantees (Kevin)
Technical Setup

Structured Procrastination (the c

 

onfigurations)

Extensions to Structured Procrastination (many configura’

 

LeapsAndBounds
CapsAndRuns
Structured Procrastination with Confidence

Related Work and Further Reading
ENDELEMENTLeapsAndBounds

+ A second, approximately optimal algorithm configuration technique due to Weisz,
Gyorgy & Szepesvari (2018)
+ Improves on SP's worst-case performance by:
— removing dependence on & (replaced with OPT, usually much smaller)
— tightening the worst-case performance bound by a log factor
+ Empirically outperforms SP
— based on very limited experiments, but likely true overall

+ But is not anytime: requires both e,d as inputs
ENDELEMENTLeapsAndBounds: How it Works

The algorithm at a glance:

1. Attempt to guess an (initially) low value of OPT
2. Try to find a configuration whose mean is smaller than this guess
= Discard configurations whose mean is large relative to the current guess
— Use fewer samples to estimate mean runtime of configurations with low runtime variance
across instances

3. If none, double the guess and repeat
ENDELEMENT 

ime

 

LeapsAndBounds: Runni.

Theorem (worst-case running time)

For any « € (0, 1/3), 6 € (0, 1), an execution of LeapsAndBounds identifies an
(c, 6)-optimal configuration with probability at least 1 — ¢ within worst-case

expected time
n In OPT
o(75 ( - )orr).

Structured Procrastination

Compare to Structured Procrastination:

(gain Gz) opr).

 
ENDELEMENT 

972 minisat configurations running on 20,118 nontrivial CNFuzzDD SAT problems
Time to prove (¢ = 0.2, 6 = 0.2)-optimality: SP 1,169 CPU days; L&B 369 CPU days
ENDELEMENT 

LeapsAndBounds: Empirical Perform

 

e

972 minisat configurations running on 20,118 nontrivial CNFuzzDD SAT problems
Time to prove (¢ = 0.2, 6 = 0.2)-optimality: SP 1,169 CPU days; L&B 369 CPU days

— LeapsAndBounds
— Structured Procrastination

 

10°

Total time spent running configuration (s)

0 200 400 600 800 1000
Configurations (sorted according to mean below 0.2 quantile)

(e = 0.2,5 = 0.2 sz, Gyorgy & Szepesvai

 
ENDELEMENTTheory FR
This Tutorial

Section Out!

Algorithm Configuration Methods with Theoretical Guarantees (Kevin)
Technical Setup

Structured Procrastination (the c

 

onfigurations)

Extensions to Structured Procrastination (many configura’

 

LeapsAndBounds
CapsAndRuns
Structured Procrastination with Confidence

Related Work and Further Reading
ENDELEMENTCapsAndRuns

+ Recent extension to LeapsAndBounds |
— Tue Jun 11th 04:20-04:25 PM Room 103

 

pesvari, ICML 2019]
ENDELEMENTCapsAndRuns

+ Recent extension to LeapsAndBounds
— Tue Jun 11th 04:20-04:25 PM Room 103

+ Adapts to easy problem instances by eliminating configurations that are
dominated by other configurations

+ Also provides an improved bound for non-worst-case instances

— scales with suboptimality gap, sai, instead of e~!
— dependence on « and 6 individually, rather than product «6

  

vari, ICML 2019]

+ Bounds are also improved by defining (¢, 5)-optimality w.rt. OPT5/2, the optimal
configuration when capping runs at the 6/2-quantile, rather than OPT

+ Still not anytime
ENDELEMENTCapsAndRuns: How it Works

Proceeds in two phases:

+ Phase 1: Estimate (1 — 5)-quantile of each configuration’s runtime over D

+ Phase 2: Estimate mean runtime of each configuration using the quantile from
Phase 1 as captime

+ Return configuration with minimum estimated mean
ENDELEMENTCapsAndRuns: Empirical Results

972 minisat configurations running on 20,118 nontrivial CNFuzzDD SAT problems
Time to prove (¢ = 0.05, 6 = 0.2)-optimality (CPU days): SP 20,643; L&B 1,451; C&R: 586

 

§ 107 — CapsAndRuns
2 LeapsAndBounds
5 Structured Procrastinati
=
€
8 10°
2
€
€
2
€ 10°
Fi
&
Fa
£
B 10°
S
2
o 200 400 600 800 1000

Configurations (sorted according to mean below R*? quantile)
(c = 0.05, 6 = 0.2) (Weisz, Gyorgy & Szepes ML 2019

   
ENDELEMENTTheory
This Tutorial

Section Out!

Algorithm Configuration Methods with Theoretical Guarantees (Kevin)
Technical Setup

Structured Procrastination (the c

 

onfigurations)

Extensions to Structured Procrastination (many configura’

 

LeapsAndBounds
CapsAndRuns
Structured Procrastination with Confidence

Related Work and Further Reading
ENDELEMENT 

Structured Procrastina’

 

Confidence

+ Recent extension to Structured Procrastination [kicinberg, -8, Lucier & Graham, arxiv 2019]

+ Adapts to easy problem instances by maintaining confidence bounds on each
configuration’s runtime

+ Anytime algorithm: 6 is gradually refined during the search process

— helpful when user can't predict the relationship between these parameters and runtime
= also improves performance: by starting with large values of 5, SPC eliminates bad
configurations early on

SPC’s running time matches (up to log factors) the running time of a hypothetical

“optimality verification procedure” that knows the identity of the optimal
configuration

— ie, SPC takes about as long to prove (e, 6)-optimality as our hypothetical verification procedure
would need to demonstrate that fact to a skeptic

— When verification is easy, SPC is fast
ENDELEMENTRecall: Structured Procrastination

1. Initialize a bounded-length queue Qo of (instance, captime) pairs for each
configuration @

2. Calculate approximate expected runtime for each @

3. Choose the task optimistically predicted to be easiest: the (instance, captime)
pair at the head of the queue corresponding to the i with smallest approximate
expected runtime

4. If the task does not complete within its captime, procrastinate:
double the captime and put the task at the tail of Qo

5. If execution has not yet been interrupted, goto 2
6. Return the configuration that we spent the most total time running
ENDELEMENTStructured Procrastination with Confidence

1. Initialize a bounded-length queue Qo of (instance, captime) pairs for each
configuration @

lower confidence bound on expected runtime
2. Calculate appreximate-expeetedtruntime for each 0

3. Choose the task optimistically predicted to be easiest: the (instance, captime)
pair at the head of the queue corresponding to the i with smallest approximate
expected runtime

4. If the task does not complete within its captime, procrastinate:
double the captime and put the task at the tail of Qg

5. If execution has not yet been interrupted, goto 2
6. Return the configuration that we spent the most total time running
ENDELEMENTaL
Structured Procrastination with Confidence

1. Initialize a bounded-length queue Qs of (instance, captime) pairs for each

configuration @
lower confidence bound on expected runtime
2. Calculate appreximate-expectecruntime for each 0

3. Choose the task optimistically predicted to be easiest: the (instance, captime)
pair at the head of the queue corresponding to the i with smallest approximate
expected runtime

4. If the task does not complete within its captime, procrastinate:
double the captime and put the task at the tail of Qo

5. If execution has not yet been interrupted, goto 2

6.

Return the configuration that either solved or attempted the greatest number of instances
ENDELEMENTStructured Procrastination with Confidence: Empirical Perform:

 

972 minisat configurations running on 20,118 nontrivial CNFuzzDD SAT problems
Time to prove (¢ = 0.1,6 = 0.2)-optimality: SP 5,150; L&B 680; SPC 150 (CPU days)

19 <— Structured Procrastination
+ LeapsAndBounds

Structured Procrastination
with Confidence

   

0.2|

00 + F =
10 10 10 10 10"
Time to find («.4)-optimal solution (CPU days)
(c=

 

  

[Kleinberg, L-B, Lucier & Graham, 2019]
ENDELEMENTTheory
This Tutorial

Section Out!

Algorithm Configuration Methods with Theoretical Guarantees (Kevin)
Technical Setup

Structured Procrastination (the c

 

onfigurations)

Extensions to Structured Procrastination (many configura’

 

LeapsAndBounds
CapsAndRuns
Structured Procrastination with Confidence

Related Work and Further Reading
ENDELEMENTThee

 

   

 

lated Work: Bandits
+ Bandits:
— Optimism in the face of uncertainty [auer, cesa-Bianchi & Fis: 2, Bubeck & Cesa-Bianchi 2012]
— bandits with correlated arms that scale to large experimental design settings {Kcinverg 2006

Kleinberg, Slivkins & Upfal 2008; Chaudhuri, Freund & Hsu 2009, Bubeck, Munos, Stoltz & Szepesvari 20

 

 

. However, our runtime minimization objective i is crucially different from more
general objective functions targeted in most bandits literature:
— cost of pulling an arm measured in the same units as the minimization objective function
— freedom to set a maximum amount « we are willing to pay in pulling an arm; if true cost
exceeds «, we pay only « but learn only that true cost was higher
+ Beyond the assumption that all arms involve the same, fixed cost:
— Variable costs and a fixed overall budget but no capping [Guna & munagala 200:

  

s & Jennings

 

fanidiyuru, Kleinberg, & Slivkins 2013]
— The algorithm can specify a maximum cost to be paid when pulling an arm, but never pays less
than that amount [kandasamy, Dasarathy, Poczos & Schneider 2016)

— Observations are censored they exceed a given budget [anc

 

 

Nevmyvaka, Kearns & Vaughan 2010]
ENDELEMENT 

+ Hyperparameter optimization
— Key initial work (sergstra, sardenet, Bengio & Kégl 2011,Thornton, H, Hoo!
— Hyperband: uses similar theoretical tools (i,

 

& L-B 20131

mieson, DeSalvo, Rostamizadeh, & Talwalkar 2016,

+ Learning-theoretic foundations

— Gupta & Roughgarden (2017): framed configuration and selection in terms of learning theory
— Sample-efficient, special-purpose algorithms for particular classes of problems
* combinatorial partitioning problems (clustering, max-cut, etc) {velcan, vs
* branching strategies in tree search [oaica, oi
* various algorithm selection problems (sa

sarajan, Vitercik & White 20:
ENDELEMENTProblems
This Tutorial
High-Level Outline

Introduction, Technical Preliminaries, and a Case Study (Kevin)
Practical Methods for Algorithm Configuration (Frank)
Algorithm Configuration Methods with Theoretical Guarantees (Kevin)

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)
ENDELEMENTThis Tutorial

Section Out

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)

 

Parameter Importance
Algorithm Selection
End-to-End Learning of Combinatorial Solvers

Integrating ML and Combinatorial Optimization
ENDELEMENT 

+ To quantify the global effect of one or more parameters, we can marginalize
predicted performance across all settings of all other parameters (4, Hoos & L-8, 2014]

Hyperparam.  Hyperparam. 7) Hyperparam, $

 

 

Avg. perplexity

 

 

 

 
ENDELEMENT 

+ To quantify the glo
predicted performance across all settings of all other parameters (4, Hoos & L-8, 2014]

Hyperparam. « Hyperparam. 7)

 

 

Avg. perplexity

 

 

al effect of one or more parameters, we can marginalize

Hyperparam. S

02602 EO,

 

 

 

03602 04E0
1 1

= ial I((v, 2.

Be 5,

= Sv 1) - ef B)

 

1 1
we ys see ys Tea Pg lee

In regression trees, we can do this efficiently:

1 1
ave(v) = we ——... Ff (v,0,...
wed = oD Tea eal”

On)

On) € Pi) (Pi

Linear time
‘computation
ENDELEMENTce (fANOVA) [H., Hoos & L-B, 2014]

 

+ By definition, the variance of predictor f
across its domain @ is:

1 . .
V = Tq | AO) - faye

 

Hyperparam. « Hyperparam. 7) Hyperparam. $
= vo + Functional ANOVA (sobol, 1993] decomposes this

— , variance into components due to each subset
io a ewe §§— Of the parameters N:

1 y
V= Vu, where Vy = —— | f?(6,)d©,
a = To | H(GeMev

   

 

 

 

 
ENDELEMENT 

“Main effect” S explains 65% of variance

 

Computing this took milliseconds

Interaction effect” of S&« explains another 18%

Hyperparam. 6 Hyperparam. 7) Hyperparam. $

 

 

 

 

 

ww

 

co

+ By definition, the variance of predictor f
across its domain @ is:

1 . .
V = Tq | AO) - faye

+ Functional ANOVA (sobol, 1993] decomposes this
variance into components due to each subset
of the parameters N:

1 y
V= Vu, where Vy = —— | f?(6,)d©,
vw v= Tong | HOvv

UCN
ENDELEMENTRelated Problems
Functional analysis of variance (fANOVA) [x,, Hoos & L-8, 2014]

+ By definition, the variance of predictor f
across its domain @ is:

1 . .
V = Tq | AO) - faye

“Main effect” S explains 65% of variance
“Interaction effect” of S&« explains another 18%
Computing this took milliseconds

 

+ Functional ANOVA (sobol, 1993] decomposes this
variance into components due to each subset
of the parameters N:

 

 

 

 

1 n
V= Vu, where Vy = —— | f7-(07)d©
vw v= Teng | Ove

UCN

In regression trees, main effects can be
computed in linear time.

 
ENDELEMENT&

SAT solver Spear:
26 parameters

3

Posthoc analysis of data
gathered from optimization

average runtime [s]

 

"athe cclacin Rereae”

93% of variation in runtimes is
due to a single parameter: the
variable selection heuristic.

 

Analysis took seconds eee eaten cennee

variable selection heuristic

 

Data set:
bounded
model
checking

Data set:
software
verification
ENDELEMENTound the incumbent

 

+ What is the local effect of varying one parameter of the incumbent?
— Use relative changes to quantify local parameter importance

 

 

 

 

 

 

 

— Can also be done based on the predictive model of algorithm performance [siedenkapp et al, 2018)
2
vs HCOG GAGE apted,, (tu
J 1250 o
Fe 8 102
1000 a
© ra
E 750 £
€ f 10
5 500 fl 5
250
fl w
012345676 SinuasiMsie7619 012345676 SinuasiMs1eI 7619
sp-var-dec-heur sp-var-dec-heur

Results for Spear on SWV
ENDELEMENT 

Probler

lation between default and i

 

    

imbent configuration

+ Greedily change the parameter that improves performance most | et al. 2013]
— Can also be done based on the predictive model of algorithm performance [siedenkapp et al, 207

 

 

 

 

 

@ 1000 gs T T T T
S
3
100 F
&
vo L
10
<
6
— if
£ Default to configured
oO Configured to default - - -- -
2 orl . n . iL

 

0 5 10 15 20 25
Number of parameters modified
Results for Spear on SWV
ENDELEMENTProblems 9
This Tutorial

Section Out

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)
Parameter Importance
Algorithm Selection
End-to-End Learning of Combinatorial Solvers

Integrating ML and Combinatorial Optimization
ENDELEMENTd Prot 5
Algorithm selectio:

+ In this tutorial, we focussed on finding a single configuration that performs well
on average: arg ming-@ E,~p(m(@, 7))

+ We can also learn a function that picks the best configuration 6 € © or algorithm
a €P per instance m with features F,: arg miny.j_,6 Ex~p(m(f(Fx),7))

 

Algorithm
Portfolio P.

Compute Select Algorithm
Instance 7 ~ Run @ on
S(Fx) > @

Features F,

 

 

 

 

 

 

 

 

 

 

 

+ There is a rich literature on this algorithm selection problem (ve ct al

thoff, 2014; Malitsky et al, 2013; Lindauer et al, 2015; Lorreg

   

Smith-Miles,
ENDELEMENTRelated Problems

Example SAT Challenge 2012

 

  

 

Rank RIG Solver ‘#solved
- = Virtual Best Solver (VBS) 568
1 1. SATzilla2012 APP 531

   

2 © 2 SaTzillaz012 ALL 515
3 1 Industrial SAT Solver 499
= = lingeling (SAT Competition 2011 Bronze) ass
4 2 interactsaT 480
5 1 glucose 475
6 2 SINN 472
7 3 ZENN 468
8 4 Lingeling 467
9 5 linge_dyphase 458
10 6 simpsat 453

The VBS (virtual best solver) is an oracle algorithm selector of competition entries.
(pink: algorithm selectors, blue: portfolios, green: single-engine solvers)
ENDELEMENT 

Automated construction of portfolios fi a single algorithm

  

+ Algorithm Configuration
— Strength: find a single configuration with strong performance for a given cost metric

— Weakness: for heterogeneous instance sets, there is often no configuration that performs great
for all instances

+ Algorithm Selection
— Strength: works well for heterogeneous instance sets due to per-instance selection
— Weakness: in standard algorithm selection, the set of algorithms P to choose from typically
only contains a few algorithms

+ Putting the two together [Kaa
— Use algorithm configuration to determine useful configurations
— Use algorithm selection to select from them based on instance characteristics

2010; Xu et al, 2010]

 
ENDELEMENTProbler
Warmstarting of algorithm configuration [Lindauer & H., 2018]

+ Humans often don’t start from scratch when tuning an algorithm's parameters

— They use their previous experience
— Eg, tuning CPLEX for a few applications tells you which parameters tend to be important
ENDELEMENTProbler
Warmstarting of algorithm configuration [Lindauer & H., 2018]

+ Humans often don’t start from scratch when tuning an algorithm's parameters
— They use their previous experience

— Eg, tuning CPLEX for a few applications tells you which parameters tend to be important

+ We would also like to make use of previous AC runs on other distributions
— Option 1: initialize from strong previous configurations

— Option 2: reuse the previous models (weighted by how useful they are)
— Combination of 1+2 often works best
ENDELEMENTProbler
Warmstarting of algorithm configuration [Lindauer & H., 2018]

+ Humans often don’t start from scratch when tuning an algorithm's parameters
— They use their previous experience
— Eg, tuning CPLEX for a few applications tells you which parameters tend to be important

+ We would also like to make use of previous AC runs on other distributions
— Option 1: initialize from strong previous configurations
— Option 2: reuse the previous models (weighted by how useful they are)
— Combination of 1+2 often works best

+ Results

— Can yield large speedups (> 100x) when similar configurations work well
— Does not substantially slow down the search if misleading
— On average: 4x speedups over running SMAC from scratch
ENDELEMENTThis Tutorial

Section Out

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)

 

Parameter Importance
Algorithm Selection
End-to-End Learning of Combinatorial Solvers

Integrating ML and Combinatorial Optimization
ENDELEMENTCategorization of ML fo!

+ Recent survey article [yoshua Be

  

— Define three categories of combinining ML and OR

 

 

Problem
definition

 

 

ML acts alone to solve the problem

 

 

Problem
definition

 

 

 

 

“<a> ~ best oR

ML augments OR with valuable

information

 

 

 

 

 

 

Integrating ML into OR; OR algorithm
repeatedly calls the same model to make
decisions
ENDELEMENTroblems

End-to-end learning of algorithms (in general)

 

Learn a neural network with parameters ¢ that defines an algorithm

+ The network's parameters ¢ are trained to optimize the true objective (or a proxy)
+ The network is queried for each action of the algorithm
ENDELEMENT 

+ The network's parameters ¢ are trained to optimize the true objective (or a proxy)
+ The network is queried for each action of the algorithm

+ Learning to learn with gradient descent [andrychowicz et al, 2016) / learning to optimize
{Li & Malik, 2017 parameterize an update rule for base-level NN parameters w:

Wei = wi + o(V fF (we), 0)
ENDELEMENTRelated Problems

 

+ The network's parameters ¢ are trained to optimize the true objective (or a proxy)
+ The network is queried for each action of the algorithm

+ Learning to learn with gradient descent landrychowicz et al, 2016) / learning to optimize
Li & Malik, 2017: parameterize an update rule for base-level NN parameters w:
Wi = we + 9(V (wi), >)

+ Learning a gradient-free optimizer’s update rule [chen et al, 201
+ Learning unsupervised learning rules {1 2019
+ AlphaZero (silver 3], etc
ENDELEMENTRelated Problems
End-to-end learning of combinatorial problems

Learning to solve Euclidean TSP

+ Pointer networks (vinyals et al, 2015]

— RNN to encode TSP instance
— Another RNN with attention-like mechanism to predict probability distribution over next node

— Trained with supervised learning, using optimal solutions to TSP instances

+ Reinforcement learning avoids need for optimal solutions
— Train an RNN (ello etal, 2017) or a graph neural network {kool et al, 2019)

+ Directly predict the permutation [Emami & Ranka, 2018; Nowak et al, 2017]

+ Learn a greedy heuristic to choose next node [pai et al, 2018]
ENDELEMENTRelated Problems
End-to-end learning of combinatorial problems

Learning to solve SAT

+ NeuroSAT [selsam et al, 2019]

— Use permutation invariant graph neural network
— Learn a message passing algorithm for solving new instances

« SATNet [wang et al, 2019]
— Differentiable approximate MaxSAT solver
= Can be integrated as a component of a deep learning system (e.g, “visual Sudoku”)

+ Learning to predict satisfiability (cameron et al, 2019

— Even at the phase transition, with 80% accuracy
— Using exchangeable deep networks
ENDELEMENTThis Tutorial

Section Outline

Beyond Static Configuration: Related Problems and Emerging Directions (Frank)

 

Parameter Importance
Algorithm Selection
End-to-End Learning of Combinatorial Solvers

Integrating ML and Combinatorial Optimization
ENDELEMENTCategorization of ML fo!

+ Recent survey article [yoshua Be

  

— Defines three categories of combinining ML and OR

 

 

Problem
definition

 

 

ML acts alone to solve the problem

 

 

Problem
definition

 

 

 

 

“<a> ~ best oR

ML augments OR with valuable

information

 

 

 

 

 

 

Integrating ML into OR; OR algorithm
repeatedly calls the same model to make
decisions
ENDELEMENTrobler

Learning to make simple decisions online

Dynamic restart policies

+ For a randomized algorithm

+ Based on an initial observation window of a run, predict whether this run is good
or bad (and thus whether to restart) (Kautz et al, 2002; Horvitz et al, 2001)

 
ENDELEMENTRelated Probler

 

Learning to make simple decisions online

Dynamic restart policies

+ For a randomized algorithm

+ Based on an initial observation window of a run, predict whether this run is good
or bad (and thus whether to restart) (Kautz et al, 2002; Horvitz et al, 2001)

Dynamic algorithm portfolios

+ Run several algorithms in parallel

+ Decide time shares adaptively based on algorithms’ progress
[Carchrae & Beck, 2014; Gagliolo & Schmidhuber, 2006]

     
ENDELEMENT 

Dynamic restart policies

+ For a randomized algorithm

+ Based on an initial observation window of a run, predict whether this run is good
or bad (and thus whether to restart) {xa svi ]

Dynamic algorithm portfolios

+ Run several algorithms in parallel
+ Decide time shares adaptively based on algorithms’ progress

chr k Dlo & Schmidhub

Learning in which search nodes to apply primal heuristics

+ Primal heuristics can find feasible solutions in branch-and-bound
+ Too expensive to apply in every node ~ learn when to apply [khal
ENDELEMENT    

Learning to select/switch between alg

Learning to select a sorting algorithm at each node

+ Keep track of a state (e.g, length of sequence left to be sorted recursively)
+ Choose algorithm to use for subtree based on state using RL {Lagoudakis & Littmann, 2000]
— Eg. QuickSort for long sequences, InsertionSort for short ones

Learning to select branching rules for DPLL in SAT solving

+ Keep track of a backtracking state
+ Choose branching rule based on state using RL (Lagoudakis & Littmann, 2001

   
ENDELEMENT 

meter control

 

Adapting algorithm parameters online

+ Astrict generalization of algorithm configuration
— just pick a fixed setting and never change it
ENDELEMENT 

meter control

 

Adapting algorithm parameters online

+ Astrict generalization of algorithm configuration
— just pick a fixed setting and never change it

+ A strict generalization of per-instance algorithm configuration (PIAC)
— just select configuration once in the beginning per instance, never change
ENDELEMENT 

meter control

 

Adapting algorithm parameters online

+ Astrict generalization of algorithm configuration
— just pick a fixed setting and never change it

+ A strict generalization of per-instance algorithm configuration (PIAC)
— just select configuration once in the beginning per instance, never change

+ A strict generalization of algorithm selection (finite set of algorithms P)
— special case of PIAC with one categorical parameter with domain P
ENDELEMENT 

+ Formulation of the single-instance case as an MDP [Acriaensen & Nowe, 20
— Buta strong policy for a single instance may not generalize

+ Formulation of the general problem as a contextual MDP to learn to generalize
across instances [siedenkapp et al, 2019]

Sigmoid TEST

apply action a

 

 

 

 

pio
r ) 8 Bnew (Atgoritim) B5
Controller Algorithm
{_Gonte state se = ol
4
control of 6 reward rey instance i & — esreedy = — smacys
2 — urs 4 DON
0
10° 10! 10° 10 108 10°
Shared state & action spaces #Episodes

Different transition and reward functions
First promising results on toy functions
ENDELEMENTConclusions

 

+ Algorithm configuration:

are very mature; often able to speed up state-of-the-art
algorithms by orders of magnitude

+ Much recent progress on ; likely to impact
practice soon

parameter importance; algorithm selection; end-to-end
learning; other ways of integrating ML with combinatorial optimization

Further resources

available for SMAC, CAVE (parameter importance), Auto-WEKA, Auto-sklearn
+ See http://automLorg for ; also, we're hiring: http://automLorg/jobs
ENDELEMENT