{"0": [0, 70, "The following content is\nprovided under a Creative\nCommons license.\nYour support will help\nMIT OpenCourseWare\ncontinue to offer high quality\neducational resources for free.\nTo make a donation, or\nview additional materials\nfrom hundreds of MIT courses,\nvisit MIT OpenCourseWare\nat ocw.mit.edu.\nGABRIEL KREIMAN:\nWhat I'd like to do\ntoday is give a very\nbrief introduction\nto neural circuits, why we\nstudy them, how we study them,\nand the possibilities\nthat come out\nof understanding\nbiological codes,\nand trying to translate those\nideas into computational codes.\nThen I will be a\nbit more specific,\nand discuss some\ninitial attempts\nat studying the computational\nrole of feedback signals.\nAnd then I'll switch\ngears and talk\nfor a few minutes about a\ncouple of things that are not\nnecessarily related\nto things that we've\nmade any real work on,\nbut I'm particularly\nexcited about in the context\nof open question challenges,\nand opportunities,\nand what I think\nwill happen over the next\nseveral years in the field.\nIn the hope of\ninspiring several of you\nto actually solve some of these\nopen questions in the field."], "70": [1, 60, "biology and studying\nbrains is that our brains\nare the product of millions\nof years of evolution.\nAnd through evolution,\nwe have discovered\nhow to do things that are\ninteresting, fast, efficient.\nAnd so if we can understand\nthe biological cause,\nif we can understand\nthe machinery\nby which we do all of\nthese amazing feats, that\nin principle, we\nshould be able to take\nsome of these biological\ncodes, and write\ncomputer code that will\ndo all of those things\nin similar ways.\nIn similar ways that we can\nwrite algorithms to compute\nthe square root\nof 2, there could\nbe algorithms that\ndictate how we\nsee, how we can\nrecognize objects,\nhow we can recognize\nauditory events.\nIn short, the answer to all\nof these Turing questions,\nin some sense, is\nhidden somewhere here\ninside our brain.\nSo the question is, how can we\nlisten to neurons and circuits,\ndecode their activity,\nand maybe even write\nin information in\nthe brain, and then\ntrying to translate all of these\nideas into computational codes.\nSo there's a lot of\nfascinating properties\nthat biological codes cover."], "130": [2, 90, "yet in terms of\ncomputers and robots.\nSo our hardware and software\nworked for many decades.\nI think it's very unlikely\nthat your amazing iPhone 6 or 5\nor 7 whatever it is, will last\nfour, five, six, seven, eight,\nnine decades.\nNone of our computers\nwill last that long.\nOur hardware does.\nThere's amazing\nparallel computation\ngoing on in our brains.\nThis is quite\ndistinct from the way\nwe think about algorithms and\ncomputation in other domains\nnow.\nOur brains have a\nreprogrammable architecture.\nThe same chunk of\ntissue can be used\nfor several different purposes.\nThrough learning and\nthrough our experiences,\nwe can modify those\narchitectures.\nA thing that has been\nquite interesting,\nand that maybe\nwe'll come back to,\nis the notion of being able\nto do single shot learning, as\nopposed to some machine\nlearning algorithms that require\nlots and lots of data to train.\nWe can easily discover\na structure in data.\nThe notion of fault\ntolerance and robustness\nto transformations\nis an essential one.\nRobustness is arguably\na fundamental property\nof biology and one that has been\nvery, very hard to implement\nin computational circuitry.\nAnd for engineers,\nthe whole issue\nabout how to have different\nsystems integrate information,\nand interact with each\nother, has been and continues\nto be a fundamental challenge.\nAnd our brains do\nthat all the time.\nWe're walking down\nthe street, we\ncan integrate\nvisual information,\nwith auditory information with\nour targets, our plans, what\nwe're interested in doing, on\nsocial interactions, and so on."], "220": [3, 60, "So I think we are\nin the golden era\nright now, because we can begin\nto explore the answers to some\nof these Turing questions in\nbrains at the biological level.\nSo we can study high\nlevel cognitive phenomena\nat the level of neurons,\nand circuits of neurons.\nAnd I'll give you a few\nexamples of that later on.\nMore recently, and I'll come\nback to this towards the end,\nwe've had the\nopportunity to begin\nto manipulate, and disrupt, and\ninteract with neural circuits\nat unprecedented resolution.\nSo we can begin\nto turn on and off\nspecific subsets of neurons.\nAnd that has tremendously\naccelerated our possibility\nto test theories at\nthe neural level.\nAnd then again, the notion being\nthat empirical findings can\nbe translated into\ncomputational algorithms-- that\nis, if we really understand\nhow biology solves the problem,\nin principle, we\nshould be able to write\nmathematical equations, and\nthen write code that mimics\nsome of those computations.\nAnd some of the\nexamples of that, we"], "280": [4, 30, "but also in Jim\nDiCarlo's presentation.\nThese are just advertising\nfor a couple of books\nthat I find interesting\nand relevant\nin computational neuroscience.\nI'm not going to have\ntime to do any justice\nto the entire field of\ncomputation neuroscience\nat all.\nSo all these slides\nwill be in Dropbox,\nso if anyone wants\nto learn more about\ncomputational neuroscience.\nThese are lot of\ntremendous books.\nLarry Abbott is the\nauthor of this one,\nand he'll be talking tonight.\nSo how do we study\nbiological circuitry.\nAnd I realize that this is\ndeja vu and very well known"], "310": [5, 50, "But in general,\nwe have a variety\nof techniques to probe the\nfunction of brain circuits.\nAnd this is showing\nthe temporal resolution\nof different techniques,\nand the spatial resolution\nof different techniques used\nto study neural circuits.\nAll the way from\ntechniques that have\nlimited spatial and\ntemporal resolution,\nsuch as PET and fMRI--\ntechniques that have very\nhigh temporal resolution,\nbut relatively poor\nspatial resolution--\nall the way to\ntechniques that allow\nus to interrogate the function\nof individual channels\nwith neurons.\nSo most of what I'm\ngoing to talk about today\nis what we refer to as the\nneural circuit level, somewhere\nin between single neurons\nand then ensembles of neurons\nrecording the local\nfield potential,\nwhich give us the resolution\nof milliseconds, where we think\na lot of the computations\nin the cortex are happening,\nand where we think we can\nbegin to elucidate how neurons"], "360": [6, 110, "So to start from\nthe very beginning,\nwe need to understand\nwhat a neuron does.\nAnd again, many of you are\nquite familiar with this.\nBut the basic\nfundamental understanding\nof what a neuron does is to\nintegrate information-- receive\ninformation through\nits dendrites,\nintegrates that\ninformation, and decides\nwhether to fire a spike or not.\nInterestingly, some of the\nbasic intuitions of our neuron\nfunction were essentially\nconceived by a Spaniard,\nRam\u00c3\u00b3n y Cajal.\nHe wanted to be an artist.\nHis parents told him that he\ncould not become an artist,\nhe had to become a\nclinician, a medical doctor.\nSo he followed the tradition.\nHe became a medical doctor.\nBut then he said, well, what I\nreally like doing is drawing.\nAnd so he bought a microscope,\nhe put it in his kitchen,\nand he spent a good chunk of\nhis life drawing, essentially.\nSo he would look at neurons,\nand he would draw their shapes.\nAnd that's essentially\nhow neuroscience started.\nJust from these beautiful\nand amazing array\nof drawings of neurons, he\nconjectured the basic flow\nof information.\nThis notion that this\nintegration of information\nthrough dendrites, all\nof this integration\nhappens in the soma.\nAnd from there, neurons decide\nwhether to fire a spike or not.\nNothing more, nothing less.\nThat's essentially\nthe fundamental unit\nof computation in our brains.\nHow do we think about and\nmodel those processes?\nThere's a family of\ndifferent types of models\nthat people have used to\ndescribe what a neuron does.\nThese models differ in terms\nof their biological accuracy,\nand their computational\ncomplexity.\nOne of the most used ones is\nperhaps an integrate and fire\nneuron.\nThis is a very\nsimple RC circuit.\nIt basically integrates current,\nand then through a threshold,\nthe neuron decides when to\nfire or not to fire a spike."], "470": [7, 60, "There are people out there\nwho have argued that you\nneed more and more detail.\nYou need to know exactly\nhow many dendrites you have,\nand the position\nof each dendrite,\nand on and on and on and on.\nWhat's the exact\nresolution at which we\nshould study neuron systems is\na fundamental open question.\nWe don't know what's the\nright level of abstraction.\nThere are people who think about\nbrains in the context of blood\nflow, and millions and millions\nof neurons averaged together.\nThere are people who\nthink that we actually\nneed to pay attention to\nthe exact details of how\nevery single dendrite integrates\ninformation, and so on.\nFor many of us, this\nis a sufficient level\nof abstraction.\nThe notion that there's a neuron\nthat can integrate information.\nSo we would like\nto push this notion\nthat we can think about\nmodels with single neurons,\nand see how far we can go,\nunderstanding that we are\nignoring a lot of the inner\ncomplexity of what's happening\ninside a neuron itself.\nSo very, very\nbriefly just to push"], "530": [8, 20, "It's very, very easy to build\nthese integrate-and-fire model\nsimulations.\nI know many of you do\nthis on a daily basis.\nThis is the equation\nof the RC circuit.\nThere's current that flows\nthrough a capacitance.\nThere's current that flows\nthrough the resistance, which,"], "550": [9, 40, "in the membranes of the neurons.\nAnd this is all there\nis to it in terms\nof a lot of the simulation\nthat we use to understand\nthe function of neurons.\nAnd again, just to\ntell you that there's\nnothing scary or fundamentally\ndifficult about this,\nhere's just a couple\nof lines in MATLAB\nthat you can take a\nlook at if you've never\ndone these kind of simulations.\nThis is a very simple and\nperhaps even somewhat wrong\nsimulation of an\nintegrate-and-fire neuron.\nBut just to tell you that it's\nrelatively simple to build\nmodels of individual\nneurons that\nhave these\nfundamental properties\nof being able to integrate\ninformation, and decide\nwhen to fire a spike."], "590": [10, 60, "want to tackle in\nCBMM have to do\nwith putting together\nlots of neurons,\nand understanding the\nfunction of circuits.\nIt's not enough to understand\nindividual neurons.\nWe need to understand how\nthey interact together.\nWe want to understand\nwhat is there,\nwho's there, what are they doing\nto whom, and when, and why.\nWe really need to understand\nthe activity of multiple neurons\ntogether in the\nform of circuitry.\nSo just a handful of\nbasic definitions.\nIf we have a\ncircuitry like this,\nwhere we start connecting\nmultiple neurons together,\ninformation flows here in this\ncircuitry in this direction.\nWe refer to the\nconnections between neurons\nthat go in this direction\nas feed forward.\nWe refer to the connections that\nflow in the opposite direction\nas feedback and I use the\nword recurrent connections\nfor the horizontal connections\nwithin a particular layer.\nSo this is just to\nfix the nomenclature\nfor the discussion that\nwill come next, and also\ntoday in the afternoon with\nJim DiCarlo's presentation."], "650": [11, 230, "we have begun to elucidate\nsome of the basic connectivity\nbetween neurons in the cortex.\nAnd this is the\nprimary example that\nhas been cited extremely\noften of what we understand\nabout the connectivity\nbetween different areas\nin the macaque monkey.\nWe don't have a diagram like\nthis for the human brain.\nMost of the detailed\nanatomical work\nhas been done in\nmacaque monkeys.\nSo each of these boxes here\nrepresents a brain area,\nand this encapsulates\nour understanding\nof who talks to\nwhom, or which area\ntalks to which other area\nin terms of visual cortex.\nThere's a lot of\ndifferent parts of cortex\nthat represent\nvisual information.\nHere at the bottom,\nwe have the retina.\nInformation from the retina\nflows through to the LGN.\nFrom the LGN, information\ngoes to primary visual cortex,\nsitting right here.\nAnd from there,\nthere's a cascade\nthat is largely parallel, and\nat the same time, hierarchical,\nof a conglomerate\nof multiple areas\nthat are fundamental in\nprocessing visual information.\nWe'll talk about some\nof these areas next.\nAnd we'll also talk\nabout some of these areas\ntoday in the afternoon\nwhen Jim discusses\nwhat are the fundamental\ncomputations involved\nin visual object recognition.\nOne of the fundamental\nclues as to how\ndo we understand, how\ndo we know that this\nis a particular\nvisual area, how do\nwe know that this is\nimportant for our vision,\nhas come from\nanatomical lesions.\nMostly in monkeys, but in\nsome cases, in humans as well.\nSo if you make lesions\nin some of these areas,\ndepending on exactly where\nyou make that lesion,\npeople either become\ncompletely blind,\nor they have a\nparticular scotoma,\na particular chunk of the visual\nfield where they cannot see.\nOr they have more\nhigh order types\nof deficits in terms\nof visual recognition.\nAs an example, the\nprimary visual cortex\nwas discovered by people who\nwere of the [INAUDIBLE] they\nwere studying, the trajectory\nof bullets in soldiers\nduring World War I.\nAnd by discovering\nthat some of those\npeoples had a blind part\nto their visual field, and that\nwas a topographically organized\ndepending on the particular\ntrajectory of the bullet\nthrough their occipital cortex.\nAnd that's how we became to\nthink about V1 as fundamental\nin visual processing.\nIt is not a perfect hierarchy.\nIt's not there is\nA, B, C, D. Right?\nFor a number of reasons.\nOne is that there are lots\nof parallel connections.\nThere are lots of\ndifferent stages\nthat are connected\nto each other.\nAnd one of the ways\nto define a hierarchy\nis by looking at the\ntiming of the responses\nin different areas.\nSo if you look at the average\nlatency of the response in each\nof these areas, you'll\nfind that there's\nan approximate hierarchy.\nInformation gets out of the\nretina approximately at 50\nmilliseconds.\nAbout 60 or so milliseconds\nin LGN, and so on.\nSo it's approximately\na 10 millisecond cost\nper step in terms of\nthe average latency.\nHowever, if you start\nlooking at the distribution,\nyou'll see that it's\nnot a strict hierarchy.\nFor example, there are\nneurons in area V4 that\nare the early neurons\nin V4 may fire\nbefore the late neurons in V1.\nAnd that shows you that the\ncircuitry is far more complex\nthan just a simple hierarchy.\nOne way to put some\norder into this seemingly\ncomplex and chaotic\ncircuitry, one simplification\nis that there are\ntwo main pathways.\nOne is the so-called\nwhat pathway.\nThe other one is the\nso-called where pathway.\nThe what pathway essentially\nis the ventral pathway.\nIt's mostly involved\nin object recognition,\ntrying to understand\nwhat is there.\nThe dorsal pathway,\nthe where pathway,\nis most involved in\nmotion, and being\nable to detect where objects\nare, stereo, and so on.\nAgain, this is not\na strict division,\nbut it's a pretty good\napproximation that many of us\nhave used in terms of\nthinking about the fundamental\ncomputations in these areas.\nNow we often think\nabout these boxes,"], "880": [12, 60, "within each of these boxes.\nSo if we zoom in\none of these areas,\nwe discover that there's\na complex hierarchy\nof computations.\nThere are multiple\ndifferent layers.\nThe cortex is essentially\na six layer structure.\nAnd there are specific rules.\nPeople have referred to this\nas a canonical micro circuitry.\nThere's a specific set of rules\nin terms of how information\nflows from one layer to\nanother in terms of each\nof these cortical structures.\nTo a first approximation,\nthis canonical circuitry\nis common to most\nof these areas.\nThere are these\nrules about which\nlayer receives information\nfirst, and sends\ninformation to areas\nare more or less\nconstant throughout\nthe cortical circuitry.\nThis doesn't mean that we\nunderstand this circuitry well,\nor what each of these\nconnections is doing.\nWe certainly don't.\nBut these are initial steps\nto sort of decipher some\nof these basic biological\nconnectivity that\nhas fundamental computational\nproperties for vision\nprocessing."], "940": [13, 100, "we call the first\norder approximation\nor immediate approximation\nto visual object recognition.\nThe notion that we can\nrecognize objects very fast,\nand that this can be\nexplained, essentially,\nas the bottom-up\nhierarchical process.\nJim DiCarlo is going to\ntalk about this extensively\nthis afternoon, so I'm going to\nessentially skip that, and jump\ninto more recent work that\nwe've done trying to think\nabout top-down connections.\nBut just let me\nbriefly say why we\nthink that the first pass\nof visual information\ncan be semi-seriously\napproximated by these purely\nbottom-up processing.\nOne is that at the\nbehavioral level,\nwe can recognize\nobjects very, very fast.\nThere's a series of\npsychophysical experiments\nthat demonstrate that\nif I show you an object,\nrecognition can happen within\nabout 150 milliseconds or so.\nWe know that the\nphysiological signals\nunderlying visual\nobject recognition also\nhappen very fast.\nWithin about 100 to\n150 milliseconds,\nwe can find neurons that\nshow very selective responses\nto complex objects, and again,\nyou'll see examples of that\nthis afternoon.\nThe behavior and the physiology\nhave inspired generations\nof computational models that are\npurely bottom-up, where there\nis no recurrency, and that can\nbe quite successful in terms\nof visual recognition.\nTo our first approximation,\nthe recent excitement\nwith deep convolutional\nnetworks can be traced back\nto some of these ideas, and\nsome of these basic biologically\ninspired computations\nthat are purely bottom-up.\nSo to summarize--\nand I'm not going\nto give any more details--\nwe think that the first 100\nmilliseconds or so\nof visual processing\ncan be approximated by\nthese purely bottom-up,\nsemi hierarchical\nsequence of computations."], "1040": [14, 70, "which is, why we have all these\nmassive feedback connections?\nWe know that in cortex,\nthere are actually\nmore recurrent and\nfeedback connections\nthan feed-forward ones.\nAnd what I'd like\nto talk about today\nis a couple of ideas of what all\nof those feedback connections\nmay be doing.\nSo this is an anatomical study\nlooking at a lot of the boxes\nthat I showed you\nbefore, and showing\nhow many of the connections\nto any given area\ncome from one of\nthese other variants.\nFor example, if we take\njust primary visual cortex,\nthis is saying that\na good fraction\nof the connections to\nprimary visual cortex\nactually come from V2.\nThat's from the next\nstage of processing,\nrather than from V1 itself.\nAll in all, if you quantify\nfor a given neuron in V1,\nhow many signals are coming\nfrom a bottom-up source that\nis for LGN versus how\nmany signals are coming\nfrom other V1 neurons or\nfrom higher visual areas,\nit turns out that there are\nmore horizontal and top-down\nprojections than bottom-up ones.\nSo what are they doing?\nIf we can approximate the\nfirst 100 milliseconds or so\nof vision so well with\nbottom-up hierarchies,\nwhat are all these\nfeedback signals doing?\nSo this brings me\nto three examples"], "1110": [15, 50, "that we've done to take some\ninitial principles in thinking\nabout what this feedback\nconnections could be doing\nin terms of visual recognition.\nSo I'll start by\ngiving you an example\nof trying to understand\nthe basic fundamental unit\nof feedback.\nThat is these\ncanonical computations,\nand by looking at the feedback\nthat happens from V2 to V1\nin the visual system.\nNext, I'm going to give\nyou an example of what\nhappens during a visual\nsearch, where we also\nthink that feedback signals may\nbe playing a fundamental role,\nif you have to do or Where's\nWaldo kind of task, where\nyou have to search for objects\nand in the environment.\nAnd finally, I will talk\nabout pattern completion, how\nyou can recognize objects that\nare heavily occluded, where\nwe also think that\nfeedback signals may\nbe playing an important role.\nSo before I go on\nto describe what"], "1160": [16, 70, "let me describe very\nquickly classical work\nthat Hubel and Wiesel did\nthat got them the Nobel Prize\nby recording the\nactivity of neurons\nin primary visual cortex.\nThey started working\nin kittens, and then\nsubsequently in\nmonkeys, and discovered\nthat there are neurons that\nshow orientation tuning, meaning\nthat they respond\nvery vigorously.\nThese are spikes,\neach of these marks\ncorresponds to an\naction potential,\nthe fundamental language\nof computation in cortex.\nAnd this neuron responds\nquite vigorously\nwhen the cat was seeing a\nbar of this orientation.\nAnd essentially,\nthere's no firing\nat all with this\ntype of stumulus\nin the receptive field.\nThis was fundamental because it\ntransformed our understanding\nof the essential computations\nin primary visual cortex\nin terms of filtering\nthe initial stimulus.\nThis is what we now\ndescribe by Gabor functions.\nAnd if you look at deep\nconvolutional networks,\nmany of them, if not\nperhaps all of them,\nstart with some sort\nof filtering operation\nthat is either Gabor filters\nor resembles this type\nof orientation that we think\nis a fundamental aspect of how\nwe start to process information\nin the visual field."], "1230": [17, 80, "is not only to make\nthese discoveries,\nbut also to come up with very\nsimple graphical models of how\nthey thought this\ncould come about.\nAnd this remains today one\nof the fundamental ways\nin which we think about how\nour orientation tuning may\ncome about.\nIf you recall the activity\nof neurons in the retina\nor in the LGN,\nyou'll find what's\ncalled center surround\nreceptive fields.\nThese are circularly\nsymmetric receptive fields,\nwith an area in the center\nthat excites the neuron,\nand an area in the surround\nthat inhibits the neuron.\nWhat they conjecture is that if\nyou put together multiple LGN\ncells, whose receptive\nfields are aligned\nalong a certain orientation, and\nyou simply combine all of them,\nyou simply add the responses\nof all of those neurons,\nyou can get a neuron in\nthe primary visual cortex\nthat has orientation tuning.\nThis\nis a problem that's far from\nsolved, despite the fact\nthat we have four\nor five decades.\nThere are many,\nmany models of how\norientation tuning comes about.\nBut this remains one of the\nbasic bottom-up feed-forward\nideas of how you\ncan actually build\norientation tuning from very\nsimple receptive fields.\nThis has informed a\nlot of our thinking\nabout how basic\ncomputations can give rise\nto orientation tuning in a\npurely bottom-up fashion.\nIn primary visual\ncortex, in addition"], "1310": [18, 50, "that show invariance\nto the exact position\nor the exact phase\nof the oriented bar\nwithin the receptive field.\nAnd that's illustrated here.\nSo this is a simple cell.\nSo this simple cell\nhas orientation tuning,\nmeaning that it responds more\nvigorously to this orientation\nthan to this orientation.\nHowever, if you change the phase\nor the position of the oriented\nbar within the receptive\nfield, the response\ndecreases significantly.\nIn contrast to this\ncomplex cell that not only\nhas orientation\ntuning, meaning that it\nfires more vigorously to this\norientation than to this one,\nbut also has phase invariance,\nmeaning that the response is\nmore or less the same way,\nregardless of the exact phase\nor the exact position\nof the stimulus\nwithin the receptive field.\nAnd again, the notion\nthat they postulated"], "1360": [19, 40, "by a summation of activity\nor multiple simple cells.\nSo again, if you\nimagine now that you\nhave multiple simple cells\nwith different receptive fields\nthat are centered at\nthese different positions,\nyou can add them up, and\ncreate complex cells.\nThese fundamental operations\nof simple and complex cells\nand primary visual cortex\ncan be somehow traced\nto the root of a lot of the\nbottom-up hierarchical models.\nA lot of the deep\nconvolutional networks today\nessentially have variations\non these kind of themes,\nof filtering steps,\nnonlinear computations\nthat give you invariance,\nand a concatenation\nof these filtering\nand invariance steps"], "1400": [20, 110, "So in following\nup with this idea,\nI would like to understand\nthe basics of what's\nthe kind of information that's\nprovided when you have signals\nfrom V2 to V1.\nTo do that, we have\nbeen collaborating\nwith Richard Born at Harvard\nMedical School, who has\na way of implanting cryo loops.\nThis is a device that can be\nimplanted in monkeys in areas\nV2, and V3, lower\nthe temperature,\nand thus reduce or\nessentially eliminate activity\nfrom areas V2 and V3.\nSo that means that we can\nstudy V1 without activity\nin area V2 and V3.\nWe can study V1 sans feedback.\nSo this is an\nexample of recordings\nof a neuron in this area.\nThis is the normal activity\nthat you get from the neuron.\nHere is when they present\na visual stimulus.\nThis is a spontaneous activity.\nEach of these dots\ncorresponds to a spike.\nEach of these lines\ncorrespond to a repetition\nof the stimulus.\nThis is a traditional way\nof showing raster plots\nfor neuron responses.\nSo you see that this is\na spontaneous activity.\nYou present the stimulus.\nThere's an increase in the\nresponse of this neuron,\nas you might expect.\nActually, I'm sorry.\nThis actually starts here.\nSo this is the spontaneous\nactivity, this is the response.\nNow here, they\nturn on their pump.\nThey start lowering\nthe temperature.\nAnd you see within\na couple of minutes,\nthey essentially significantly\nreduce the responses.\nThe largely silence--\nnot completely--\nbut largely silence\nactivity in areas V2 and V3.\nAnd these are reversible, so\nwhen they turn the pumps off,\nactivity comes back in.\nSo the question is, what\nhappens in primary visual cortex\nwhen you don't have\nfeedback from V2 and V3."], "1510": [21, 70, "is that some of the basic\nproperties of V1 do not change.\nIt's consistent with\nthe simple models\nthat I just told you, where\nthe orientation tuning\nin the primary visual\ncortex is largely\ndictated by the\nbottom-up inputs,\nby the signals from the LGN.\nThe conjecture\nfrom that would be\nthat if you silence\nV2 and V3, nothing\nwould happen with\norientation tuning\nin primary visual cortex.\nAnd that's essentially\nwhat they're showing here.\nThese are example neurons.\nThis is showing\norientation selectivity.\nThis is showing\ndirection selectivity,\nwhat happens when\nyou move an oriented\nbar within the receptive field.\nSo this is showing\nthe direction.\nThis is showing the\nmean normalized response\nof a neuron.\nThis is the preferred direction,\nand direction orientation\nthat gives a maximum response.\nThe blue curve corresponds\nto when you don't\nhave activity in V2 and V3.\nRed corresponds to\ntheir control data.\nAnd essentially, the tuning\nof the neuron was not altered.\nThe orientation preferred by\nthis neuron was not altered.\nThe same thing goes for\ndirection selectivity.\nSo the basic problems\nof orientation tuning"], "1580": [22, 70, "Let me say a few words about\nthe dynamics of the responses.\nSo here, what I'm showing you\nis the mean normalized responses\nas a function of time.\nTime 0 is when the\nstimulus is turned on.\nAs I told you already, by\nabout 50 milliseconds or so,\nyou get a vigorous response\nin primary visual cortex.\nAnd if we compare the\norange and the blue curves,\nwe see that this initial\nresponse is largely identical.\nSo the initial response\nof these V1 neurons\nis not affected by the\nabsence of feedback from V2.\nWe start to see\neffects, we start\nto see a change in\nthe firing rate here.\nLargely at about 60 milliseconds\nor so after presentation.\nSo in a highly\noversimplified cartoon,\nI think of this as a bottom-up\nHubel and Wiesel like response,\ndriven by LGN.\nAnd signals from V2\nto V1 coming back\nabout 10 milliseconds later.\nAnd that's when we started\nseeing some of these feedback\nrelated effects.\nI told you that some of the\nbasic properties do not change.\nWe interpret this as\nbeing dictated largely\nby bottom-up signals.\nThe dynamics do change.\nThe initial response\nis unaffected."], "1650": [23, 40, "I want to say one\nthing that does change.\nAnd for that, I need to\nexplain what an area summation\ncurve is.\nSo if you present the\nstimulus within the receptive\nfield of a neuron of this size,\nyou get a certain response.\nAs you start increasing\nthe size of this stimulus,\nyou get a more\nvigorous response.\nSize matters.\nThe larger, the better--\nto a point.\nThere comes a point\nwhere it turns out\nthat the response of the\nneurons starts decreasing again.\nSo larger is not always better.\nA little bit larger is better.\nThis size has an\ninhibitory effect\noverall on the\nresponse of the neuron.\nThis is called\nsurround suppression."], "1690": [24, 50, "like primary visual cortex.\nAlso in earlier areas\nfor a very long time.\nIt turns out that when you\ndo these type of experiments\nin the absence of feedback, the\neffect of surround suppression\ndoes not disappear.\nThat is, you still have a peak\nin the response as a function\nof a stimulus size.\nBut there is a reduced amount\nof surround suppression.\nThat is, when you\ndon't have feedback,\nthere's less suppression.\nYou have a larger response\nfor bigger stimulus.\nSo we think that one of the\nfundamental computations\nthat feedback is\nproviding here is\nthis integration from\nmultiple neurons in V1\nthat happens in V2.\nAnd then inhibition to\nactivity of neurons in area V1\nto provide some of\nthe suppression.\nThis is partly the reason\nwhy our neurons are not\nvery excited about a uniform\nstimulus, like a blank wall."], "1740": [25, 70, "we think, is dictated by\nthis feedback from V2 to V1.\nWe can model these center\nsurround interactions\nas a ratio of two Gaussian\ncurves, two forces.\nOne is the one that\nincreases the response.\nThe other one is a\nnormalization term\nthat suppresses the response\nwhen the stimulus is too large.\nThere's a number\nof parameters here.\nEssentially, you\ncan think of this\nas a ratio of Gaussians, ROGs.\nThere's a ratio of\ntwo Gaussian curves.\nOne dictating the\ncenter that responds.\nThe other one, the\nsurround response.\nAnd to make a long\nstory short, we\ncan feed the data\nfrom the monkey\nwith this extremely simple\nratio of Gaussian's model.\nAnd we can show that\nthe main parameter\nthat feedback seems to be\nacting upon is what we call Wn--\nthat is this\nnormalization factor here.\nSo that the tuning\nfactor that dictates\nthe strength of the surrounding\ndivision from V2 to V1--\nwe think that's one of the\nfundamental things that's\nbeing affected by feedback.\nSo we would think\nof this as the gain.\nWe think of this as\nthe spatial extent\nover which the V2\ncan exert its action\non primary visual cortex."], "1810": [26, 20, "This type of spatial\neffect may be\nimportant in other role that\nhas been ascribed to feedback,\nwhich is the ability\nto direct attention\nto specific locations\nin the environment.\nI want to come back\nto this question\nhere, and ask, under\nwhat conditions,\nand how can a feedback also\nprovide important features"], "1830": [27, 60, "And for that, I'm\ngoing to switch\nto another task, another\ncompletely different prep,\nwhich is the\nWhere's Waldo task--\nthe task of visual search.\nHow do we search for particular\nobjects in the environment.\nAnd here, it's not sufficient\nto focus on a specific location,\nbut we need to be able to\nsearch for specific features.\nWe need to be able to\nbias our visual responses\nfor specific features\nof the stimulus\nthat we're searching for.\nSo this is a famous sort\nof Where's Waldo task.\nYou need to be able to\nsearch for specific features.\nIt's not enough to be able to\nsend feedback from V2 to V1,\nand direct attention, or change\nthe sizes of the receptive\nfields, or the direct attention\nto a specific location.\nAnother version\nthat I'm not going\nto talk about of visual that\nhas a related theme that relates\nto visual search is\nfeature based attention,\nwhen you're actually paying\nattention to a particular face,\nto a particular color, to a\nparticular feature that is not\nnecessarily located, and to\nspace, as our friend here has\nstudied quite significantly.\nPeople always like to know\nthe answer of where he is at.\nOK."], "1890": [28, 290, "and some behavioral data\nthat we have collected\nto try to get at this question\nof how feedback signals can\nbe relevant for visual search.\nThis initial part of\nthis computational model\nis essentially the HMAX\ntype of architecture\nthat has been pioneered by\nTommy Poggio and several people\nin his lab, most notably,\npeople like Max Riesenhuber\nand Thomas Serre.\nI was thinking\nthat by this time,\npeople would have described\nthis in more detail.\nI'm going to go through\nthese very quickly.\nAgain, today in the\nafternoon, we'll\nhave more discussion about\nthis family of models.\nSo these family of\nmodels essentially\ngoes through a series of linear\nand non-linear computations\nin a hierarchical way, inspired\nby the basic definition\nof simple and\ncomplex cells that I\ndescribed in the work\nof Hubel and Wiesel.\nSo basically, what these models\ndo is they take an image.\nThese are pixels.\nThere's a filtering step.\nThis filtering step involves\nGabor filtering of the image.\nIn this particular\ncase, there are\nfour different orientations.\nAnd what do you\nget here is a map\nof the visual input after\nthis linear filtering process.\nThe next step in this model\nis a local max operation.\nThis is pooling neurons that\nhave similar identical feature\npreferences, but\nslightly different scale\nin the receptive fields.\nOr slightly different positions\nin their receptive fields.\nAnd this max operation,\nthis non-linear operation\nis giving you invariance\nto the specific feature.\nSo now you can get a response to\nthe same feature, irrespective\nof the exact scale\nor the exact position\nwithin the receptive field.\nThese were labeled\nS1 and C1, initially\nin models by Fukushima.\nAnd this type of nomenclature\nwas carried on later\nby Tommy and many others.\nAnd this is directly inspired\nby the simple and complex cells\nthat I very briefly\nshowed you previously\nin the recordings\nof Hubel and Wiesel.\nThese filtering\nand max operations\nare repeated throughout the\nhierarchy again and again.\nSo here's another layer\nthat has a filtering\nstep and a nonlinear max step.\nIn this case, this filtering\nhere is not a Gabor filter.\nWe don't really understand very\nwell what neurons in V2 and V4\nare doing.\nOne of the types of\nfilters that have been used\nand that we are using here\nis a radial basis function,\nwhere the properties of\na neuron in this case\nare dictated by patches taking\nrandomly from natural images.\nAll of this is\npurely feed-forward.\nAll of this is essentially\nthe basic ingredient\nof the type of\nconvolutional networks\nthat had been used for\nobject recognition.\nYou can have more layers.\nYou can have different\ntypes of computations.\nThe basic properties\nare essentially\nthe ones that are\ndescribed briefly here.\nWhat I really want to talk\nabout is not the former part,\nbut this part of the model.\nNow I ask you, where's Waldo,\nyou need to do something,\nyou need be able to somehow\nlook at this information,\nand be able to\nbias your responses\nor bias the model towards\nregions of the visual space\nthat have features that resemble\nwhat you're looking for.\nYour car, your keys, Waldo.\nSo the way we do that\nis first, in this case,\nI'm going to show you\nwhat happens if you're\nlooking for the top hat here.\nSo first, we have\na representation\nin the model of the top hat.\nThis is the hat here.\nAnd we have a representation\nin our vocabulary\nof how units in the highest\nechelons of this model\nrepresent this hat.\nSo we have a representation\nof the features\nthat compose this object at\na high level in this model.\nWe use that representation\nto modulate,\nin a multiplicative\nfashion, the entire image.\nEssentially, we\nbias the responses\nin the entire image based\non the particular features\nthat we are searching for.\nThis is inspired by many\nphysiological experiments that\nhave shown that to a\ngood approximation,\nthis type of\nmodulation in feature\nbased attention has been\nobserved across different parts\nof the visual field.\nThat is, if you're\nsearching for red objects,\nneurons that like red will\nenhance their response\nthroughout the\nentire visual field.\nSo have the entire\nvisual field modulated\nby the pattern of features that\nwe're searching for in here.\nAfter that, we have\na normalization step.\nThis normalization\nstep is critical\nin order to discount\npurely bottom-up effects.\nWe don't want the competition\nbetween different objects\nto be purely dictated by\nwhich object is brighter,\nfor example.\nSo we normalize that\nafter modulating that\nwith the features\nthat we are searching.\nThat gives us a map of the\nimage, where each area has been\nessentially compared\nto this feature set\nthat we're looking for.\nAnd then we have a winner\ntake all mechanism that"], "2180": [29, 110, "or where the model\nwill fixate on first.\nWhere the model thinks that a\nparticular object is located.\nOK so what happens when we\nhave this feedback that's\nfeature specific, and that\nmodulates the responses based\non the targets object\nthat we're searching for.\nIn these two images,\neither in objects\narrays or when objects are\nembedded in complex scenes,\nwe're searching for\nthis top object.\nAnd the largest\nresponse in the model\nis indeed in the location\nof where the object is.\nIn these other two\nimages, the model\nis searching for\nthis accordion here.\nAnd again, the model\nwas able to find that\nby this comparison of the\nfeatures with the stimulus.\nMore generally, these\nare object array images.\nThis is the number\nof fixations required\nto find the object in\nthis object array images.\nSo one would correspond\nto the first fixation.\nIf the model does not find the\nobject in the first location,\nthere's what's called\ninhibition of return.\nSo we make sure the\nmodel does not come back\nto the same location,\nand the model will\nlook at the second best\npossible location in the image.\nAnd it will keep on searching\nuntil it finds the object.\nSo the model performs in the\nfirst fixation at 60% correct.\nAnd eventually,\nafter five fixations,\nit can find the object\nalmost always right in here.\nThis is what you would\nexpect by random search.\nIf you were to randomly\nfixate on different objects,\nso the model is doing\nmuch better than that.\nAnd then for the\naficionados, there's\na whole plethora of purely\nbottom-up models that\ndon't have feedback whatsoever.\nThis is a family\nof models that was\npioneered by people like\nLaurent Itti and Christof Koch.\nThese are saliency based models.\nAlthough you cannot see, there\nare a couple of other points\nin here.\nAll of those models cannot\nfind the object either.\nIt's not that these objects\nthat we're searching for\nare more salient,\nand therefore, that's\nwhy the model is finding them."], "2290": [30, 30, "bottom-up pure saliency.\nWe did a psychophysical\nexperiment.\nWe asked, well, this is how\nthe model searches for Waldo.\nHow will humans\nsearch for objects\nunder the same conditions.\nSo we had multiple objects.\nSubjects have to make a\nsaccade to a target object.\nTo make a long story short, this\nis the cumulative performance\nof the model and the\nnumber of fixations\nunder these conditions,\nand the model\nthat's reasonable in terms\nof how well humans do.\nThis is data from every single\nindividual subject in the task."], "2320": [31, 40, "You can compare the errors\nthat the model is making.\nHow consistent people are\nwith themselves with respect\nto other subjects.\nHow good it is with\nrespect to humans.\nThe long story is the\nmodel is far from perfect.\nWe don't think that\nwe have captured\neverything we need to\nunderstand about visual search.\nSome people alluded to before,\nfor example, the notion\nthat the model doesn't\nhave these major changes\nwith eccentricity, and\nthe fovea, and so on.\nA long way to go, but\nwe think that we've\ncaptured some of the\nessential initial ingredients\nof visual search.\nAnd that this is one example of\nhow visual feedback signals can\ninfluence this bottom-up\nhierarchy for recognition."], "2360": [32, 20, "that I wanted to give you\nof how feedback can help\nin terms of visual recognition.\nWhat are other functions that\nfeedback could be playing.\nAnd for that, I'd like to\ndiscuss the work that Hanlin\ndid here, and also,\nBill Lotter in the lab,\nin terms of how we\ncan recognize objects\nthat are partially occluded.\nThis happens all the time."], "2380": [33, 30, "You can also encounter\nobjects where you can only\nfind partial\ninformation, and you have\nto make pattern completion.\nPattern completion is\na fundamental aspect\nof intelligence.\nWe do that in all\nsorts of scenarios.\nIt's not just\nrestricted to vision.\nAll of you can probably\ncomplete all of these patterns.\nWe use pattern completion\nin social scenarios\nas well, right?\nYou make inferences\nfrom partial knowledge\nabout their intentions,\nand what they're doing,\nand what they're\ntrying to do, OK?\nSo we want to study this problem\nof how you complete pattern,"], "2410": [34, 30, "in the context of\nvisual recognition.\nThere are a lot\nof different ways\nin which one can present\npartially occluded objects.\nHere are just a few of them.\nWhat Hanlin did was\nuse a paradigm called\nbubbles that's shown here.\nEssentially, it's like looking\nat the world like these.\nYou only have small\nwindows through which\nyou can see the object.\nPerformance can be titrated to\nmake the task harder or easier.\nSo if you have a\nlot of bubbles, it's\nrelatively easy to recognize\nthat this is a toy school bus."], "2440": [35, 40, "actually pretty challenging.\nSo we can titrate performance\non the difficulty of this task.\nVery quickly, let me start\nby showing you psychophysics\nperformance here.\nThis is how subjects\nperform as a function\nof the amount of occlusion in\nthe image as a function of how\nmany pixels you're\nshowing for these images.\nAnd what you see here is\nthat with 60% occlusion,\nperformance is extremely high.\nPerformance essentially\ndrops to chance level\nwhen the object is\nmore and more occluded.\nThere is a significant\namount of robustness\nin human performance.\nFor example, you have\na little bit more\nthan 10% of the\npixels in the object,"], "2480": [36, 140, "So this is all behavioral data.\nLet me show you very quickly\nwhat Hanlin discovered\nby doing invasive\nrecordings in human patients\nwhile the subjects\nwere performing\nthis recognition of objects\nthat are partially occluded.\nIt's illegal to put\nelectrodes in the human brain\nin normal people, so\nwe work with subjects\nthat have pharmacological\nintractable epilepsy.\nSo inside of subjects\nthat have seizures,\nthe neurosurgeons need to\nimplant electrodes in order\nto localize the seizures.\nAnd B, in order to ensure\nthat when they do a resection,\nand they take out the\npart of the brain that's\nresponsible for seizures,\nthat they're not\ngoing to interfere with other\nfunctions, such as language.\nThese patients stay in the\nhospital for about one week.\nAnd during this one week,\nwe have a unique opportunity\nto go inside a human brain,\nand record physiological data.\nDepending on the\ntype of patient,\nwe've used the different\ntypes of electrodes.\nThis is what some people\nrefer to as ECoG electrodes.\nElectrocorticographic signals.\nThese are field\npotential signals,\nvery different from the\nones that I was showing you\nin the little spikes before.\nThese are aggregate measures,\nprobably of tens of thousands,\nif not millions of neurons,\nwhere we have very, very\nhigh temporal resolution at\nthe millisecond level, but very\npoor spatial resolution, only\nbeing able to localize things\nat the millimeter level or so.\nWith these, we can\npinpoint specific locations\nwithin about approximately\none millimeter,\nbut have very high signal to\nnoise ratio signals that are\ndictated by the visual input.\nAn example of those\nsignals is shown here.\nThese are intracranial\nfield potentials\nas a function of time.\nThis is the onset\nof the stimulus.\nAnd these 39\ndifferent repetitions,\nwhen Hanlin is showing\nthis unoccluded face,\nwe see a very vigorous\nchange, quite systematic\nfrom one trial to another.\nAll of those gray traces\nare single trials,\nsimilar to the raster plot\nthat I was showing you before.\nSo now I'm going to show you\na couple of single trials.\nWe're showing\nindividual images where\nobjects are partially occluded.\nIn this case, there's\nonly about 15%\nof the pixels of the face\nthat are being shown.\nAnd we see that despite\nthe fact that we're\ncovering 85%, more or\nless, of that image,\nwe still see a pretty\nconsistent physiological signal.\nThe signals are\nclearly not identical.\nFor example, this one\nlooks somewhat different."], "2620": [37, 80, "But again, these are just single\ntrials showing that there still\nis selectivity for these\nshape, despite the fact\nthat we are only showing a\nsmall fraction of this thing.\nThese are all the trials\nin which these five\ndifferent faces were presented.\nEach line corresponds to trial.\nThese are raster plots.\nAs you can see, the data\nare extremely clear.\nThere's no processing here.\nThis is raw data single trials.\nThese are single trials\nwith the partial images.\nYou again can see there's\na vigorous response here.\nThe responses are not\nas nicely and neatly\naligned here, in part\nbecause all of these images\nare different.\nAll of the locations on\nthe models are different.\nAs I just showed you, there's\na lot of variability here.\nIf you actually fix the\nbubble locations-- that\nis, you repeatedly present the\nsame image multiple times still\nin pseudorandom order,\nbut the same image,\nyou see that the signals\nare more consistent.\nNot as consistent as this one,\nbut certainly more consistent.\nAgain, very clear\nselective response\ntolerant to a tremendous amount\nof occlusion in the image.\nInterestingly, the\nlatency of the response\nis significantly later\ncompared to the whole images.\nSo if you look at, for\nexample, 200 milliseconds,\nyou see that the responses\nstarted significantly\nbefore 200 milliseconds\nfor the whole images.\nAll of the responses here\nstart after 200 milliseconds.\nWe spent a significant\namount of time\ntrying to characterize\nthis and showing\nthat pattern\ncompletion, the ability\nto recognize objects\nthat are occluded,"], "2700": [38, 10, "If you use the purely bottom-up\narchitecture and tried to do\nthis in silico--"], "2710": [39, 10, "The performance\ndeteriorates quite rapidly\nwhen you start having\nsignificant occlusion.\nI'm going to skip this and just\nvery quickly argue about some"], "2720": [40, 40, "been doing, trying to add\nrecurrency to the models.\nTrying to have both\nfeedback connections as well\nas recurrent connections\nwithin each layer\nto try to get a model that\nwill be able to perform pattern\ncompletion, and therefore,\nuse these feedback\nsignals to allow\nus to extrapolate\nfrom previous information\nabout these objects.\nBill will be here Friday\nor Monday, I'm not sure.\nSo you should talk to him\nmore about these models.\nEssentially, they belong\nto the family of HMAX.\nThey belong to a family\nof convolutional networks,\nwhere you have filter\noperations, threshold,\nand saturation pooling\non normalization.\nJim will say about\nthis family of models"], "2760": [41, 20, "These are purely\nbottom-up models.\nAnd what Bill has been doing\nis other than recurrent\nand feedback connections,\nretraining these models based\non these recurrent and\nfeedback connections,\nand then comparing\ntheir performance\nwith human psychophysics.\nSo this is the behavioral\ndata that I showed you before.\nThis is the performance\nof the feedforward model."], "2780": [42, 30, "Another way to try to\nget out whether feedback\nis relevant for\npattern completion\nis to use with backward masking.\nBackward masking means\nthat you present an image,\nand immediately\nafter that image,\nwithin a few milliseconds,\nyou present noise.\nYou present a mask.\nAnd people have argued\nthat masking essentially\ninterrupts feedback processing.\nEssentially, it\nallows you to have\na bottom-up flow of\ninformation-- stops feedback.\nI don't think this is\nquite extremely rigorous.\nI think that the story is\nprobably far more complicated\nthan that."], "2810": [43, 10, "you have a bottom-up\nstream, you put a mask,\nand you interrupt all the\nsubsequent feedback processing."], "2820": [44, 20, "you can show that when stimuli\nare masked, particularly\nif the interval is very short,\nyou can significantly impair\npattern completion performance.\nSo if the mask comes\nwithin 25 milliseconds\nof the actual\nstimulus performance\nin recognizing these\nheavily occluded objects\nis significantly impaired.\nWe interpreted this to\nindicate that feedback may be"], "2840": [45, 20, "This is Bill's instantiation\nof that recurrent model.\nBecause he has\nrecurrency now, he also\nhas time in this models.\nSo he can also\npresent the image,\npresent the mask to the model,\nand compare the performance\nof the computational\nmodel as a function"], "2860": [46, 50, "So to summarize this-- and\nthere's still two or three more\nslides that I want to show--\nI've given you three\nexamples of potential ways\nin which feedback\nsignals can be important.\nThe first one has to do\nwith the effects of feedback\non surround suppression,\ngoing from V2 to V1.\nWe think that by doing this\ntype of experiments combined\nwith the computational\nmodels to understand what\nare the fundamental\ncomputations,\nwe can begin to elucidate\nsome of the steps\nby which feedback\ncan exert its role.\nWe hoped to come up with\nthe essential alphabet\nof computations similar to the\nfiltering and normalization\noperations that are\nimplemented by feedback.\nThe second example\nwas feedback as being\nable to have features\nthat dictate what\nwe do in visual search\ntasks and the last example,\nin both our preliminary\nwork, trying to use feedback,\nas well as recurrent connections\nto perform pattern completion"], "2910": [47, 10, "So the last thing I\nwanted to do is just\nflash a few more\nslides about a couple\nof things that are happening in\nneuroscience and computational"], "2920": [48, 80, "exciting for people.\nIf I were young again,\nthese are some of the things\nthat I would definitely be very,\nvery excited to follow up on.\nSo the notion that we'll\nbe able to go inside brains\nand read our biological code,\nand eventually write down\ncomputer code, and build\namazing machines is, I think,\nvery appealing and sexy.\nBut at the same time,\nit's a far cry, right?\nWe're a long way from being\nable to take biological codes\nand translate that into\ncomputational codes.\nIt's really extremely tragic.\nSo here are three\nreasons why I think\nthere's optimism that this may\nnot be as crazy as it sounds.\nWe're beginning to have\ntremendous information\nabout wiring diagrams\nat exquisite resolution.\nThere are a lot of\npeople who are seriously\nthinking about providing\nus with maps about which\nneuron talks to\nwhich other neuron.\nAnd this was not\npresent ever before.\nSo we are now beginning to\nhave detailed information\nthat it's much higher resolution\nconnectivity than ever before.\nThe second one is the\nstrength in numbers.\nFor decades, we've\nbeen recording\nthe activity of one\nneuron at a time,\nmaybe a few neurons at a time.\nNow there are many different\nideas and techniques out there\nby which we can\nlisten to and monitor\nthe activity of multiple\nneurons simultaneously.\nAnd I think this is\ngoing to be game changing\nfor neurophysiology, but\nalso for the possibility\nof reputational models that\nare inspired by biology.\nAnd the third one is a series\nof techniques mostly developed"], "3000": [49, 60, "to do optogenetics, and to\nmanipulate these circuits\nwith unprecedented resolution.\nSo let me expand on\nthat for one second.\nThis is the C. elegans.\nThis is an intramicroscopy\nimage of how one\ncan categorize the circuitry.\nSo it turns out that this\npioneering work of Sydney\nBrenner a couple\nof decades ago has\nled to mapping the connectivity\nof each one of the 302 neurons.\nHow exactly for each neuron,\nwho it's connected with.\nAnd this is represented\nin that rather complex way\nin this diagram here.\nWell, it turns out\nthat people are\nbeginning to do these type\nof heroic type of experiments\nin cortex.\nSo we're beginning to\nhave initial insights\nabout connectivity\nabout how neurons\nare wired with each other at\nthis resolution in cortex.\nWe're nowhere near being able\nto have these for humans.\nNot even other species,\nmice, and so on.\nNot even Drosophila yet.\nThere's a huge amount of\n[INAUDIBLE] and interest\nin the community of having\na very detailed map.\nSo the question for you for\nthe young and next generation,\nwhat are we going to\ndo with these maps."], "3060": [50, 110, "of a chunk of\ncortex, how is that\ngoing to transform our ability\nto make inferences, and build\nnew computational models.\nThe second one has to\ndo with our ability\nto start the recording\nfor more and more neurons.\nThis is that other I didn't\nhave time to talk about.\nThis is work also that Hanlin\ndid with Matias Ison and Itzhak\nFried.\nThese are recordings of\nspikes from human cortex,\nagain, in patients\nthat have epilepsy.\nI'm just flashing this slide\nbecause I had it handy.\nThese are 300 neurons.\nThis is not a simultaneously\nrecorded population.\nThese are cases where we can\nrecord from a few neurons\nat a time using micro wires now.\nThis is different from\nthe type of recording\nthat I showed you before.\nThese are actual spikes\nthat we can record.\nAnd these 380 neurons\nis in a different task.\nSo recording from\nthese 318 neurons\ntook us about three\nto four years of time.\nThere are more and\nmore people that\nare using either\ntwo photon imaging\nand/or massive multielectrode\narrays that are beginning\nto be able to record the\nactivity of hundreds of neurons\nsimultaneously.\nMy good friend and crazy\ninventor, Ed Boyden,\nbelieves that we will be able\nto recover from 100,000 neurons\nsimultaneously.\nOf course, he is far\nmore grandiose than I am,\nand he can think big\nat this kind of scale.\nBut even to think about the\npossibility of recording\nfrom 1,000 or 5,000\nneurons simultaneously so\nthat in a week or\na month, one may\nbe able to have a\ntremendous amount\nfrom a very large population.\nThis is going to\nbe transformative.\nThree decades ago in the\nfield of molecular biology,\npeople would sequence\na single gene,\nand they would publish\nthe entire sequence--\nACCGG-- and so on.\nThat was the whole paper.\nA grad student would\nspend five years just\nsequencing a single gene.\nNow we have the possibility\nof downloading genome\nby advances in technology.\nI suspect that a lot\nof our recordings\nwill become obsolete.\nWe'll be able to listen to\nthe activity of thousands\nof neurons simultaneously.\nAnd again, it's\nfor your generation"], "3170": [51, 10, "our understanding of how quick\nwe can read biological codes.\nIn the unlikely event that you\nthink that that's not enough,\nhere's one more\nthing that I think"], "3180": [52, 70, "And that's again, Ed\nBoyden using techniques\nthat are referred to\nas optogenetics, where\nyou can manipulate the activity\nof specific types of neurons.\nI flashed a lot of\ncomputational models today.\nA lot of hypotheses about\nwhat different connections may\nbe doing.\nAt some point, we\nwill be able to test\nsome of those hypotheses with\nunprecedented resolution.\nSo if somebody wanted to\nknow what is this neuron V2,\nwhat kind of feedback\nits providing,\nwe may be able to silence\nonly neurons in V2 that\nprovide feedback to\nV1 in a clean manner\nwithout affecting,\nfor example, all\nof the other feed-forward\nprocesses, and so on.\nSo the amount of\nspecificity that\ncan be derived from these type\nof techniques is enormous.\nSo that's all I wanted to say.\nSo because we have very high\nspecificity in our ability\nto manipulate\ncircuits, because we'll\nbe able to record the activity\nof many, many more neurons\nsimultaneously,\nand because we'll\nhave more and more\ndetailed diagrams,\nI think that the dream of being\nable to read out and decode\nbiological codes, and translate\nthose into competition codes\nis less crazy than it may sound.\nWe think that in the next\nseveral years and decades,\nsmart people like you\nwill be able to make\nthis tremendous\ntransformation and discover\nspecific algorithms\nabout intelligence"], "3250": [53, 20, "So that's what's\nillustrated here.\nWe'll be happy to\nkeep on fighting.\nAndrei and I will fight.\nWe will be happy to keep on\nfighting about Eva and how\namazing she is and she isn't.\nWhat I try to describe is\nthat by really understanding\nbiological codes,\nwe'll be able to write\namazing computational code.\nI put a lot of arrows here."], "3270": [54, 50, "I'm not saying that\nwe solve the problem.\nThere's a huge amount of\nwork that we need in here."]}