1
00:00:37,170 --> 00:00:40,279
[Music]

2
00:00:55,400 --> 00:00:55,400
 

3
00:00:55,410 --> 00:00:58,790
[Music]

4
00:01:04,090 --> 00:01:04,090
 

5
00:01:04,100 --> 00:01:14,790
 are you feeling heavy do you have weight

6
00:01:09,320 --> 00:01:14,790
 

7
00:01:09,330 --> 00:01:18,540
 on your back it's too hard to carry the

8
00:01:14,780 --> 00:01:18,540
 

9
00:01:14,790 --> 00:01:21,170
 renders you still in your trucks tell me

10
00:01:18,530 --> 00:01:21,170
 

11
00:01:18,540 --> 00:01:21,170
 are you hurt

12
00:01:36,510 --> 00:01:36,510
 

13
00:01:36,520 --> 00:01:42,829
[Music]

14
00:01:49,720 --> 00:01:49,720
 

15
00:01:49,730 --> 00:01:52,849
[Music]

16
00:02:13,200 --> 00:02:13,200
 

17
00:02:13,210 --> 00:02:16,330
[Music]

18
00:02:32,170 --> 00:02:32,170
 

19
00:02:32,180 --> 00:02:35,389
[Music]

20
00:02:46,120 --> 00:02:46,120
 

21
00:02:46,130 --> 00:02:48,190
 you

22
00:05:03,760 --> 00:05:03,760
 

23
00:05:03,770 --> 00:05:06,770
 yeah

24
00:06:17,060 --> 00:06:17,060
 

25
00:06:17,070 --> 00:06:24,100
 hello everybody please come on in and

26
00:06:22,470 --> 00:06:24,100
 

27
00:06:22,480 --> 00:06:27,370
 take a seat we're ready to start the

28
00:06:24,090 --> 00:06:27,370
 

29
00:06:24,100 --> 00:06:29,820
 next tutorial the next tutorial will be

30
00:06:27,360 --> 00:06:29,820
 

31
00:06:27,370 --> 00:06:33,100
 on metal learning always a popular topic

32
00:06:29,810 --> 00:06:33,100
 

33
00:06:29,820 --> 00:06:36,490
 from fuschia learning to rap it RL and

34
00:06:33,090 --> 00:06:36,490
 

35
00:06:33,100 --> 00:06:41,410
 who better to give that tutorial than

36
00:06:36,480 --> 00:06:41,410
 

37
00:06:36,490 --> 00:06:45,330
 Sergei Lavigne and Chelsea Finn Sergei

38
00:06:41,400 --> 00:06:45,330
 

39
00:06:41,410 --> 00:06:48,190
 has been at UC Berkeley for the last

40
00:06:45,320 --> 00:06:48,190
 

41
00:06:45,330 --> 00:06:51,910
 five years three years three years a

42
00:06:48,180 --> 00:06:51,910
 

43
00:06:48,190 --> 00:06:54,430
 very productive three years and as a

44
00:06:51,900 --> 00:06:54,430
 

45
00:06:51,910 --> 00:06:57,460
 professor and Chelsea has just finished

46
00:06:54,420 --> 00:06:57,460
 

47
00:06:54,430 --> 00:06:59,740
 her PhD at Berkeley is currently

48
00:06:57,450 --> 00:06:59,740
 

49
00:06:57,460 --> 00:07:02,650
 spending some time at Google brain and

50
00:06:59,730 --> 00:07:02,650
 

51
00:06:59,740 --> 00:07:06,270
 will be starting her own lab at Stanford

52
00:07:02,640 --> 00:07:06,270
 

53
00:07:02,650 --> 00:07:06,270
 University as a professor in the fall

54
00:07:07,070 --> 00:07:07,070
 

55
00:07:07,080 --> 00:07:12,669
 together they have six papers at ICML

56
00:07:10,230 --> 00:07:12,669
 

57
00:07:10,240 --> 00:07:15,190
 this year so and and joy is seeing those

58
00:07:12,659 --> 00:07:15,190
 

59
00:07:12,669 --> 00:07:18,750
 in the days to come and without further

60
00:07:15,180 --> 00:07:18,750
 

61
00:07:15,190 --> 00:07:18,750
 ado Chelsea and Sergei

62
00:07:23,230 --> 00:07:23,230
 

63
00:07:23,240 --> 00:07:27,180
 thank you for the introduction Riya

64
00:07:25,129 --> 00:07:27,180
 

65
00:07:25,139 --> 00:07:28,919
 we're excited to be giving this tutorial

66
00:07:27,170 --> 00:07:28,919
 

67
00:07:27,180 --> 00:07:30,389
 on meta learning throughout the tutorial

68
00:07:28,909 --> 00:07:30,389
 

69
00:07:28,919 --> 00:07:32,430
 we encourage you to be thinking about

70
00:07:30,379 --> 00:07:32,430
 

71
00:07:30,389 --> 00:07:34,740
 questions that you might have about the

72
00:07:32,420 --> 00:07:34,740
 

73
00:07:32,430 --> 00:07:36,180
 content that we're presenting and as you

74
00:07:34,730 --> 00:07:36,180
 

75
00:07:34,740 --> 00:07:37,470
 go through if you have a question you

76
00:07:36,170 --> 00:07:37,470
 

77
00:07:36,180 --> 00:07:40,470
 may come up to one of the four

78
00:07:37,460 --> 00:07:40,470
 

79
00:07:37,470 --> 00:07:42,150
 microphones but also if you want to also

80
00:07:40,460 --> 00:07:42,150
 

81
00:07:40,470 --> 00:07:44,430
 ask a question from from your own seat

82
00:07:42,140 --> 00:07:44,430
 

83
00:07:42,150 --> 00:07:46,470
 without having to get up we have a link

84
00:07:44,420 --> 00:07:46,470
 

85
00:07:44,430 --> 00:07:49,050
 for that will allow you to post

86
00:07:46,460 --> 00:07:49,050
 

87
00:07:46,470 --> 00:07:50,970
 questions and to also upload other

88
00:07:49,040 --> 00:07:50,970
 

89
00:07:49,050 --> 00:07:52,290
 questions and we'll be monitoring that

90
00:07:50,960 --> 00:07:52,290
 

91
00:07:50,970 --> 00:07:54,630
 and also asking questions throughout

92
00:07:52,280 --> 00:07:54,630
 

93
00:07:52,290 --> 00:07:56,669
 from that link all right so that's lado

94
00:07:54,620 --> 00:07:56,669
 

95
00:07:54,630 --> 00:08:00,240
 comm / meta but we'll also have the link

96
00:07:56,659 --> 00:08:00,240
 

97
00:07:56,669 --> 00:08:02,210
 on future slides also we posted all of a

98
00:08:00,230 --> 00:08:02,210
 

99
00:08:00,240 --> 00:08:04,860
 pdf version of these slides at

100
00:08:02,200 --> 00:08:04,860
 

101
00:08:02,210 --> 00:08:07,919
 tinyurl.com Sasha I see my old - meta

102
00:08:04,850 --> 00:08:07,919
 

103
00:08:04,860 --> 00:08:09,270
 slides and so if you don't want to be

104
00:08:07,909 --> 00:08:09,270
 

105
00:08:07,919 --> 00:08:10,860
 putting your phone up to take pictures

106
00:08:09,260 --> 00:08:10,860
 

107
00:08:09,270 --> 00:08:14,220
 of the slides and you can look at the

108
00:08:10,850 --> 00:08:14,220
 

109
00:08:10,860 --> 00:08:15,180
 slides there as well great okay I mean

110
00:08:14,210 --> 00:08:15,180
 

111
00:08:14,220 --> 00:08:16,800
 additionally we'll also be taking

112
00:08:15,170 --> 00:08:16,800
 

113
00:08:15,180 --> 00:08:21,150
 questions at the break and at the end of

114
00:08:16,790 --> 00:08:21,150
 

115
00:08:16,800 --> 00:08:22,590
 this tutorial so let's get started so a

116
00:08:21,140 --> 00:08:22,590
 

117
00:08:21,150 --> 00:08:25,949
 lot of the motivation for medal learning

118
00:08:22,580 --> 00:08:25,949
 

119
00:08:22,590 --> 00:08:28,020
 comes from being able to learn from

120
00:08:25,939 --> 00:08:28,020
 

121
00:08:25,949 --> 00:08:30,300
 small amounts of data and in particular

122
00:08:28,010 --> 00:08:30,300
 

123
00:08:28,020 --> 00:08:32,940
 what we've seen is that meta learning

124
00:08:30,290 --> 00:08:32,940
 

125
00:08:30,300 --> 00:08:34,589
 thrives with large datasets if there's

126
00:08:32,930 --> 00:08:34,589
 

127
00:08:32,940 --> 00:08:36,360
 one thing to take away from the last few

128
00:08:34,579 --> 00:08:36,360
 

129
00:08:34,589 --> 00:08:38,159
 years of machine learning research I

130
00:08:36,350 --> 00:08:38,159
 

131
00:08:36,360 --> 00:08:40,860
 think it's that large diverse datasets

132
00:08:38,149 --> 00:08:40,860
 

133
00:08:38,159 --> 00:08:43,200
 plus large models leads to broad

134
00:08:40,850 --> 00:08:43,200
 

135
00:08:40,860 --> 00:08:45,029
 generalization we've seen this time and

136
00:08:43,190 --> 00:08:45,029
 

137
00:08:43,200 --> 00:08:47,040
 time again from systems trained on

138
00:08:45,019 --> 00:08:47,040
 

139
00:08:45,029 --> 00:08:49,740
 imagenet to transform our models trained

140
00:08:47,030 --> 00:08:49,740
 

141
00:08:47,040 --> 00:08:52,890
 on large machine translation systems to

142
00:08:49,730 --> 00:08:52,890
 

143
00:08:49,740 --> 00:08:55,740
 GPT to trained for large-scale language

144
00:08:52,880 --> 00:08:55,740
 

145
00:08:52,890 --> 00:08:59,730
 modeling and all this falls under the

146
00:08:55,730 --> 00:08:59,730
 

147
00:08:55,740 --> 00:09:01,380
 paradigm of deep supervised learning but

148
00:08:59,720 --> 00:09:01,380
 

149
00:08:59,730 --> 00:09:02,700
 what if you don't have a large data set

150
00:09:01,370 --> 00:09:02,700
 

151
00:09:01,380 --> 00:09:04,350
 what if you're in domains such as

152
00:09:02,690 --> 00:09:04,350
 

153
00:09:02,700 --> 00:09:05,940
 medical imaging or robotics or

154
00:09:04,340 --> 00:09:05,940
 

155
00:09:04,350 --> 00:09:07,800
 translation of rare languages or

156
00:09:05,930 --> 00:09:07,800
 

157
00:09:05,940 --> 00:09:09,330
 recommendation systems in each of these

158
00:09:07,790 --> 00:09:09,330
 

159
00:09:07,800 --> 00:09:11,790
 situations we don't have a large data

160
00:09:09,320 --> 00:09:11,790
 

161
00:09:09,330 --> 00:09:13,050
 set for every possible task every

162
00:09:11,780 --> 00:09:13,050
 

163
00:09:11,790 --> 00:09:14,400
 possible switch situation or every

164
00:09:13,040 --> 00:09:14,400
 

165
00:09:13,050 --> 00:09:17,400
 possible person we want to personalize

166
00:09:14,390 --> 00:09:17,400
 

167
00:09:14,400 --> 00:09:18,810
 our machine learning system to or what

168
00:09:17,390 --> 00:09:18,810
 

169
00:09:17,400 --> 00:09:20,760
 if you want a more general-purpose AI

170
00:09:18,800 --> 00:09:20,760
 

171
00:09:18,810 --> 00:09:22,589
 system that can do many different things

172
00:09:20,750 --> 00:09:22,589
 

173
00:09:20,760 --> 00:09:25,320
 that you want to be able to continuously

174
00:09:22,579 --> 00:09:25,320
 

175
00:09:22,589 --> 00:09:27,150
 adapt and learn on the job if so it's

176
00:09:25,310 --> 00:09:27,150
 

177
00:09:25,320 --> 00:09:30,120
 impractical to learn each and everything

178
00:09:27,140 --> 00:09:30,120
 

179
00:09:27,150 --> 00:09:31,350
 from scratch for doing the task and so

180
00:09:30,110 --> 00:09:31,350
 

181
00:09:30,120 --> 00:09:32,670
 instead we want to be able to very

182
00:09:31,340 --> 00:09:32,670
 

183
00:09:31,350 --> 00:09:34,220
 quickly learn new things based on our

184
00:09:32,660 --> 00:09:34,220
 

185
00:09:32,670 --> 00:09:36,810
 previous

186
00:09:34,210 --> 00:09:36,810
 

187
00:09:34,220 --> 00:09:39,090
 and finally what if your data has a long

188
00:09:36,800 --> 00:09:39,090
 

189
00:09:36,810 --> 00:09:40,680
 tail for example what if the number of

190
00:09:39,080 --> 00:09:40,680
 

191
00:09:39,090 --> 00:09:42,660
 data points starts going down

192
00:09:40,670 --> 00:09:42,660
 

193
00:09:40,680 --> 00:09:44,100
 significantly as you encounter more

194
00:09:42,650 --> 00:09:44,100
 

195
00:09:42,660 --> 00:09:46,980
 objects or as you interact with new

196
00:09:44,090 --> 00:09:46,980
 

197
00:09:44,100 --> 00:09:49,620
 people here new words and encountering

198
00:09:46,970 --> 00:09:49,620
 

199
00:09:46,980 --> 00:09:51,450
 new driving situations in these

200
00:09:49,610 --> 00:09:51,450
 

201
00:09:49,620 --> 00:09:53,100
 situations your standard machine

202
00:09:51,440 --> 00:09:53,100
 

203
00:09:51,450 --> 00:09:55,800
 learning systems will do well in the

204
00:09:53,090 --> 00:09:55,800
 

205
00:09:53,100 --> 00:09:57,450
 kind of the big data regime but as as

206
00:09:55,790 --> 00:09:57,450
 

207
00:09:55,800 --> 00:09:59,930
 you move towards having fewer examples

208
00:09:57,440 --> 00:09:59,930
 

209
00:09:57,450 --> 00:10:02,250
 these systems will start to break down

210
00:09:59,920 --> 00:10:02,250
 

211
00:09:59,930 --> 00:10:03,720
 then particularly it not only in the

212
00:10:02,240 --> 00:10:03,720
 

213
00:10:02,250 --> 00:10:06,540
 long tail setting but in all 3d

214
00:10:03,710 --> 00:10:06,540
 

215
00:10:03,720 --> 00:10:07,710
 situations these settings start to break

216
00:10:06,530 --> 00:10:07,710
 

217
00:10:06,540 --> 00:10:12,720
 the standard supervised learning

218
00:10:07,700 --> 00:10:12,720
 

219
00:10:07,710 --> 00:10:14,430
 paradigm ok so what I'd like to try out

220
00:10:12,710 --> 00:10:14,430
 

221
00:10:12,720 --> 00:10:17,730
 next is is it actually give you guys a

222
00:10:14,420 --> 00:10:17,730
 

223
00:10:14,430 --> 00:10:18,930
 test so supervised learning breaks down

224
00:10:17,720 --> 00:10:18,930
 

225
00:10:17,730 --> 00:10:20,790
 your production humans are pretty good

226
00:10:18,920 --> 00:10:20,790
 

227
00:10:18,930 --> 00:10:22,980
 at these situations and I want to give

228
00:10:20,780 --> 00:10:22,980
 

229
00:10:20,790 --> 00:10:24,870
 you a future learning test and your goal

230
00:10:22,970 --> 00:10:24,870
 

231
00:10:22,980 --> 00:10:27,330
 is I'll give you 6 training data points

232
00:10:24,860 --> 00:10:27,330
 

233
00:10:24,870 --> 00:10:28,860
 which are shown on the left the three on

234
00:10:27,320 --> 00:10:28,860
 

235
00:10:27,330 --> 00:10:31,350
 the the first column are from the

236
00:10:28,850 --> 00:10:31,350
 

237
00:10:28,860 --> 00:10:33,360
 painter Brock and the the middle three

238
00:10:31,340 --> 00:10:33,360
 

239
00:10:31,350 --> 00:10:37,320
 are from Cezanne and your goal is to be

240
00:10:33,350 --> 00:10:37,320
 

241
00:10:33,360 --> 00:10:38,640
 able to classify the painter for the for

242
00:10:37,310 --> 00:10:38,640
 

243
00:10:37,320 --> 00:10:42,120
 the paintings shown on the right who

244
00:10:38,630 --> 00:10:42,120
 

245
00:10:38,640 --> 00:10:43,260
 painted that painting and this is a

246
00:10:42,110 --> 00:10:43,260
 

247
00:10:42,120 --> 00:10:44,760
 future learning problem because you only

248
00:10:43,250 --> 00:10:44,760
 

249
00:10:43,260 --> 00:10:46,650
 get six data points in order to do this

250
00:10:44,750 --> 00:10:46,650
 

251
00:10:44,760 --> 00:10:48,200
 house you get six label data points for

252
00:10:46,640 --> 00:10:48,200
 

253
00:10:46,650 --> 00:10:50,970
 this binary classification problem

254
00:10:48,190 --> 00:10:50,970
 

255
00:10:48,200 --> 00:10:53,070
 okay so raise your hand if you think

256
00:10:50,960 --> 00:10:53,070
 

257
00:10:50,970 --> 00:10:58,320
 that the painting on the right was

258
00:10:53,060 --> 00:10:58,320
 

259
00:10:53,070 --> 00:10:59,640
 painted by Cezanne okay and raise your

260
00:10:58,310 --> 00:10:59,640
 

261
00:10:58,320 --> 00:11:03,660
 hand if you think that the painter the

262
00:10:59,630 --> 00:11:03,660
 

263
00:10:59,640 --> 00:11:05,670
 painting was drawn by Brock okay great

264
00:11:03,650 --> 00:11:05,670
 

265
00:11:03,660 --> 00:11:08,880
 so most of you got the right answer so

266
00:11:05,660 --> 00:11:08,880
 

267
00:11:05,670 --> 00:11:10,170
 this is indeed by Brock and so and and

268
00:11:08,870 --> 00:11:10,170
 

269
00:11:08,880 --> 00:11:11,430
 the way that you can recognize this is

270
00:11:10,160 --> 00:11:11,430
 

271
00:11:10,170 --> 00:11:14,160
 that there's kind of some more straight

272
00:11:11,420 --> 00:11:14,160
 

273
00:11:11,430 --> 00:11:17,820
 lines and more kind of high contrast

274
00:11:14,150 --> 00:11:17,820
 

275
00:11:14,160 --> 00:11:19,620
 lines in the painting and so how do you

276
00:11:17,810 --> 00:11:19,620
 

277
00:11:17,820 --> 00:11:21,120
 accomplish this so this sort of thing

278
00:11:19,610 --> 00:11:21,120
 

279
00:11:19,620 --> 00:11:23,280
 trading from learning from only six

280
00:11:21,110 --> 00:11:23,280
 

281
00:11:21,120 --> 00:11:25,050
 examples but it would be extremely hard

282
00:11:23,270 --> 00:11:25,050
 

283
00:11:23,280 --> 00:11:26,760
 for a lot of modern machine learning

284
00:11:25,040 --> 00:11:26,760
 

285
00:11:25,050 --> 00:11:28,200
 systems and yet all of you eight guys

286
00:11:26,750 --> 00:11:28,200
 

287
00:11:26,760 --> 00:11:30,750
 were able to do it or most of you guys

288
00:11:28,190 --> 00:11:30,750
 

289
00:11:28,200 --> 00:11:32,400
 were able to do it quite well

290
00:11:30,740 --> 00:11:32,400
 

291
00:11:30,750 --> 00:11:33,839
 so the way that you were able to

292
00:11:32,390 --> 00:11:33,839
 

293
00:11:32,400 --> 00:11:35,580
 accomplish this was because you have

294
00:11:33,829 --> 00:11:35,580
 

295
00:11:33,839 --> 00:11:37,050
 previous experience you weren't trying

296
00:11:35,570 --> 00:11:37,050
 

297
00:11:35,580 --> 00:11:40,230
 to learn from these six examples from

298
00:11:37,040 --> 00:11:40,230
 

299
00:11:37,050 --> 00:11:41,460
 scratch and many of you probably haven't

300
00:11:40,220 --> 00:11:41,460
 

301
00:11:40,230 --> 00:11:43,410
 seen these particular paintings before

302
00:11:41,450 --> 00:11:43,410
 

303
00:11:41,460 --> 00:11:45,180
 or maybe you haven't even seen paintings

304
00:11:43,400 --> 00:11:45,180
 

305
00:11:43,410 --> 00:11:46,640
 from these particular artists before but

306
00:11:45,170 --> 00:11:46,640
 

307
00:11:45,180 --> 00:11:48,380
 you have

308
00:11:46,630 --> 00:11:48,380
 

309
00:11:46,640 --> 00:11:49,700
 experience different shapes different

310
00:11:48,370 --> 00:11:49,700
 

311
00:11:48,380 --> 00:11:51,649
 textures you've probably seen other

312
00:11:49,690 --> 00:11:51,649
 

313
00:11:49,700 --> 00:11:52,940
 paintings before and do that previous

314
00:11:51,639 --> 00:11:52,940
 

315
00:11:51,649 --> 00:11:54,320
 experience you're able to figure out how

316
00:11:52,930 --> 00:11:54,320
 

317
00:11:52,940 --> 00:11:58,220
 to solve this task from only six

318
00:11:54,310 --> 00:11:58,220
 

319
00:11:54,320 --> 00:11:59,360
 examples okay so now how might we get a

320
00:11:58,210 --> 00:11:59,360
 

321
00:11:58,220 --> 00:12:02,390
 machine learning system to solve this

322
00:11:59,350 --> 00:12:02,390
 

323
00:11:59,360 --> 00:12:04,070
 task depending on what era you're in you

324
00:12:02,380 --> 00:12:04,070
 

325
00:12:02,390 --> 00:12:05,750
 would probably answer differently you

326
00:12:04,060 --> 00:12:05,750
 

327
00:12:04,070 --> 00:12:07,730
 might try to model the image formation

328
00:12:05,740 --> 00:12:07,730
 

329
00:12:05,750 --> 00:12:09,800
 process you might try them all the

330
00:12:07,720 --> 00:12:09,800
 

331
00:12:07,730 --> 00:12:11,930
 geometry of different objects in the

332
00:12:09,790 --> 00:12:11,930
 

333
00:12:09,800 --> 00:12:13,490
 image if you were using slightly more

334
00:12:11,920 --> 00:12:13,490
 

335
00:12:11,930 --> 00:12:14,839
 sophisticated techniques you might use

336
00:12:13,480 --> 00:12:14,839
 

337
00:12:13,490 --> 00:12:16,690
 something like isset features or Haga

338
00:12:14,829 --> 00:12:16,690
 

339
00:12:14,839 --> 00:12:18,800
 features with a support vector machine

340
00:12:16,680 --> 00:12:18,800
 

341
00:12:16,690 --> 00:12:20,240
 or more recently maybe you try to

342
00:12:18,790 --> 00:12:20,240
 

343
00:12:18,800 --> 00:12:22,100
 fine-tune from image that features or

344
00:12:20,230 --> 00:12:22,100
 

345
00:12:20,240 --> 00:12:25,700
 try to do domain adaptation from other

346
00:12:22,090 --> 00:12:25,700
 

347
00:12:22,100 --> 00:12:27,410
 painters for example um and and maybe in

348
00:12:25,690 --> 00:12:27,410
 

349
00:12:25,700 --> 00:12:29,750
 the future we'll be doing something even

350
00:12:27,400 --> 00:12:29,750
 

351
00:12:27,410 --> 00:12:31,670
 more sophisticated so these different

352
00:12:29,740 --> 00:12:31,670
 

353
00:12:29,750 --> 00:12:33,200
 approaches may seem very distinct in

354
00:12:31,660 --> 00:12:33,200
 

355
00:12:31,670 --> 00:12:35,600
 that kind of the approach that they're

356
00:12:33,190 --> 00:12:35,600
 

357
00:12:33,200 --> 00:12:36,920
 taking but they all share one thing in

358
00:12:35,590 --> 00:12:36,920
 

359
00:12:35,600 --> 00:12:38,480
 common which is all of them are

360
00:12:36,910 --> 00:12:38,480
 

361
00:12:36,920 --> 00:12:40,070
 different ways to inject previous

362
00:12:38,470 --> 00:12:40,070
 

363
00:12:38,480 --> 00:12:43,610
 knowledge or previous experience into

364
00:12:40,060 --> 00:12:43,610
 

365
00:12:40,070 --> 00:12:45,170
 the system and as you move down these

366
00:12:43,600 --> 00:12:45,170
 

367
00:12:43,610 --> 00:12:47,180
 these prior knowledge you get few

368
00:12:45,160 --> 00:12:47,180
 

369
00:12:45,170 --> 00:12:48,950
 engineered human engineered priors and

370
00:12:47,170 --> 00:12:48,950
 

371
00:12:47,180 --> 00:12:51,140
 more data-driven priors

372
00:12:48,940 --> 00:12:51,140
 

373
00:12:48,950 --> 00:12:54,829
 and also as you move down you get

374
00:12:51,130 --> 00:12:54,829
 

375
00:12:51,140 --> 00:12:57,380
 systems that work undoubtedly better and

376
00:12:54,819 --> 00:12:57,380
 

377
00:12:54,829 --> 00:12:59,180
 so in this tutorial we want to try to

378
00:12:57,370 --> 00:12:59,180
 

379
00:12:57,380 --> 00:13:00,829
 take this one step further and in

380
00:12:59,170 --> 00:13:00,829
 

381
00:12:59,180 --> 00:13:03,350
 particular we want to be able to learn

382
00:13:00,819 --> 00:13:03,350
 

383
00:13:00,829 --> 00:13:05,690
 priors explicitly from previous

384
00:13:03,340 --> 00:13:05,690
 

385
00:13:03,350 --> 00:13:07,490
 experience that lead to efficient

386
00:13:05,680 --> 00:13:07,490
 

387
00:13:05,690 --> 00:13:10,070
 downstream learning an entirely data

388
00:13:07,480 --> 00:13:10,070
 

389
00:13:07,490 --> 00:13:13,160
 Duren approach to acquiring these priors

390
00:13:10,060 --> 00:13:13,160
 

391
00:13:10,070 --> 00:13:15,260
 that is can we how these systems learn

392
00:13:13,150 --> 00:13:15,260
 

393
00:13:13,160 --> 00:13:18,110
 how to learn to solve tasks and this is

394
00:13:15,250 --> 00:13:18,110
 

395
00:13:15,260 --> 00:13:19,010
 what is known as meta learning in the

396
00:13:18,100 --> 00:13:19,010
 

397
00:13:18,110 --> 00:13:20,959
 rest of this tutorial

398
00:13:19,000 --> 00:13:20,959
 

399
00:13:19,010 --> 00:13:22,399
 Sergey will first talk about the problem

400
00:13:20,949 --> 00:13:22,399
 

401
00:13:20,959 --> 00:13:24,470
 statement and overview the general meta

402
00:13:22,389 --> 00:13:24,470
 

403
00:13:22,399 --> 00:13:25,760
 learning problem then we'll be talking

404
00:13:24,460 --> 00:13:25,760
 

405
00:13:24,470 --> 00:13:27,260
 about different meta learning algorithms

406
00:13:25,750 --> 00:13:27,260
 

407
00:13:25,760 --> 00:13:29,449
 ranging from black box out updation

408
00:13:27,250 --> 00:13:29,449
 

409
00:13:27,260 --> 00:13:30,860
 approaches to autumn tape optimization

410
00:13:29,439 --> 00:13:30,860
 

411
00:13:29,449 --> 00:13:33,290
 based approaches to nonparametric

412
00:13:30,850 --> 00:13:33,290
 

413
00:13:30,860 --> 00:13:34,670
 methods then we'll discuss how we can

414
00:13:33,280 --> 00:13:34,670
 

415
00:13:33,290 --> 00:13:37,790
 develop bayesian variants of each of

416
00:13:34,660 --> 00:13:37,790
 

417
00:13:34,670 --> 00:13:39,350
 these methods then we'll talk about how

418
00:13:37,780 --> 00:13:39,350
 

419
00:13:37,790 --> 00:13:41,750
 meta learning has a plot been applied to

420
00:13:39,340 --> 00:13:41,750
 

421
00:13:39,350 --> 00:13:43,640
 different application areas we'll take a

422
00:13:41,740 --> 00:13:43,640
 

423
00:13:41,750 --> 00:13:45,829
 short five-minute break and also allows

424
00:13:43,630 --> 00:13:45,829
 

425
00:13:43,640 --> 00:13:47,120
 additional questions Sergey will then

426
00:13:45,819 --> 00:13:47,120
 

427
00:13:45,829 --> 00:13:48,740
 talk about meta reinforcement learning

428
00:13:47,110 --> 00:13:48,740
 

429
00:13:47,120 --> 00:13:51,860
 and we'll conclude by discussing

430
00:13:48,730 --> 00:13:51,860
 

431
00:13:48,740 --> 00:13:52,850
 challenges on frontiers ok next circuit

432
00:13:51,850 --> 00:13:52,850
 

433
00:13:51,860 --> 00:13:55,970
 will be talking about the problem

434
00:13:52,840 --> 00:13:55,970
 

435
00:13:52,850 --> 00:13:58,550
 statement and overview Thank You shell C

436
00:13:55,960 --> 00:13:58,550
 

437
00:13:55,970 --> 00:14:00,120
 and those of you that were trying to

438
00:13:58,540 --> 00:14:00,120
 

439
00:13:58,550 --> 00:14:01,680
 find the slides

440
00:14:00,110 --> 00:14:01,680
 

441
00:14:00,120 --> 00:14:03,690
 we did actually there was somebody who

442
00:14:01,670 --> 00:14:03,690
 

443
00:14:01,680 --> 00:14:05,250
 actually posted the link again if you go

444
00:14:03,680 --> 00:14:05,250
 

445
00:14:03,690 --> 00:14:06,990
 to the thing on the slide here the slide

446
00:14:05,240 --> 00:14:06,990
 

447
00:14:05,250 --> 00:14:09,000
 Oh / meta the first question is actually

448
00:14:06,980 --> 00:14:09,000
 

449
00:14:06,990 --> 00:14:10,290
 a link to the slide deck so if you want

450
00:14:08,990 --> 00:14:10,290
 

451
00:14:09,000 --> 00:14:12,810
 the slide deck please check that out

452
00:14:10,280 --> 00:14:12,810
 

453
00:14:10,290 --> 00:14:14,790
 there alright so let's start with a

454
00:14:12,800 --> 00:14:14,790
 

455
00:14:12,810 --> 00:14:16,530
 discussion of how we can actually

456
00:14:14,780 --> 00:14:16,530
 

457
00:14:14,790 --> 00:14:19,200
 formulate the meta learning problem and

458
00:14:16,520 --> 00:14:19,200
 

459
00:14:16,530 --> 00:14:21,420
 there are really kind of two distinct

460
00:14:19,190 --> 00:14:21,420
 

461
00:14:19,200 --> 00:14:23,100
 viewpoints on meta learning there's kind

462
00:14:21,410 --> 00:14:23,100
 

463
00:14:21,420 --> 00:14:24,810
 of a mechanistic view and a

464
00:14:23,090 --> 00:14:24,810
 

465
00:14:23,100 --> 00:14:27,810
 probabilistic view let me explain what I

466
00:14:24,800 --> 00:14:27,810
 

467
00:14:24,810 --> 00:14:29,670
 mean by these the mechanistic view looks

468
00:14:27,800 --> 00:14:29,670
 

469
00:14:27,810 --> 00:14:31,770
 at meta learning as a setting where

470
00:14:29,660 --> 00:14:31,770
 

471
00:14:29,670 --> 00:14:34,110
 there's a deep neural network model that

472
00:14:31,760 --> 00:14:34,110
 

473
00:14:31,770 --> 00:14:36,290
 can read in an entire data set and then

474
00:14:34,100 --> 00:14:36,290
 

475
00:14:34,110 --> 00:14:39,390
 make predictions for new data points

476
00:14:36,280 --> 00:14:39,390
 

477
00:14:36,290 --> 00:14:41,700
 training this network use a metadata set

478
00:14:39,380 --> 00:14:41,700
 

479
00:14:39,390 --> 00:14:44,340
 which itself consists of many data sets

480
00:14:41,690 --> 00:14:44,340
 

481
00:14:41,700 --> 00:14:46,110
 each for a different task and this view

482
00:14:44,330 --> 00:14:46,110
 

483
00:14:44,340 --> 00:14:47,850
 of meta learning makes it easier to

484
00:14:46,100 --> 00:14:47,850
 

485
00:14:46,110 --> 00:14:49,140
 implement meta learning algorithms so if

486
00:14:47,840 --> 00:14:49,140
 

487
00:14:47,850 --> 00:14:51,540
 you're actually coding something up in

488
00:14:49,130 --> 00:14:51,540
 

489
00:14:49,140 --> 00:14:52,860
 tensorflow or PI torch the mechanistic

490
00:14:51,530 --> 00:14:52,860
 

491
00:14:51,540 --> 00:14:55,950
 view is probably the one that makes it

492
00:14:52,850 --> 00:14:55,950
 

493
00:14:52,860 --> 00:14:57,930
 clearest the probabilistic view treats

494
00:14:55,940 --> 00:14:57,930
 

495
00:14:55,950 --> 00:15:00,360
 meta learning as the problem of

496
00:14:57,920 --> 00:15:00,360
 

497
00:14:57,930 --> 00:15:02,580
 extracting prior information from a set

498
00:15:00,350 --> 00:15:02,580
 

499
00:15:00,360 --> 00:15:05,310
 of meta training tasks that allows for

500
00:15:02,570 --> 00:15:05,310
 

501
00:15:02,580 --> 00:15:07,560
 efficient learning of new tasks this

502
00:15:05,300 --> 00:15:07,560
 

503
00:15:05,310 --> 00:15:10,230
 view says that learning a new task

504
00:15:07,550 --> 00:15:10,230
 

505
00:15:07,560 --> 00:15:12,030
 basically used this prior plus a small

506
00:15:10,220 --> 00:15:12,030
 

507
00:15:10,230 --> 00:15:13,890
 amount of training data to infer the

508
00:15:12,020 --> 00:15:13,890
 

509
00:15:12,030 --> 00:15:15,420
 most likely posterior parameters that

510
00:15:13,880 --> 00:15:15,420
 

511
00:15:13,890 --> 00:15:17,370
 will allow you to solve this task and

512
00:15:15,410 --> 00:15:17,370
 

513
00:15:15,420 --> 00:15:18,690
 this view of meta learning makes it

514
00:15:17,360 --> 00:15:18,690
 

515
00:15:17,370 --> 00:15:21,060
 easier to understand meta learning

516
00:15:18,680 --> 00:15:21,060
 

517
00:15:18,690 --> 00:15:22,350
 algorithms these are not two views that

518
00:15:21,050 --> 00:15:22,350
 

519
00:15:21,060 --> 00:15:23,640
 result in different methods they're

520
00:15:22,340 --> 00:15:23,640
 

521
00:15:22,350 --> 00:15:26,280
 actually two viewpoints that can be

522
00:15:23,630 --> 00:15:26,280
 

523
00:15:23,640 --> 00:15:29,100
 taken to understand the same methods so

524
00:15:26,270 --> 00:15:29,100
 

525
00:15:26,280 --> 00:15:30,450
 in this part of the tutorial I'll

526
00:15:29,090 --> 00:15:30,450
 

527
00:15:29,100 --> 00:15:32,160
 actually focus on the second view on the

528
00:15:30,440 --> 00:15:32,160
 

529
00:15:30,450 --> 00:15:33,900
 probabilistic view because our aim is

530
00:15:32,150 --> 00:15:33,900
 

531
00:15:32,160 --> 00:15:35,700
 really to help everybody to understand

532
00:15:33,890 --> 00:15:35,700
 

533
00:15:33,900 --> 00:15:37,020
 meta learning algorithms but we'll see

534
00:15:35,690 --> 00:15:37,020
 

535
00:15:35,700 --> 00:15:38,430
 the more mechanistic view emerged when

536
00:15:37,010 --> 00:15:38,430
 

537
00:15:37,020 --> 00:15:42,000
 we talk about particular practical

538
00:15:38,420 --> 00:15:42,000
 

539
00:15:38,430 --> 00:15:43,890
 instantiation of these methods okay so

540
00:15:41,990 --> 00:15:43,890
 

541
00:15:42,000 --> 00:15:45,720
 just to work towards a problem

542
00:15:43,880 --> 00:15:45,720
 

543
00:15:43,890 --> 00:15:47,220
 definition for meta learning let's first

544
00:15:45,710 --> 00:15:47,220
 

545
00:15:45,720 --> 00:15:49,290
 start with a problem definition for

546
00:15:47,210 --> 00:15:49,290
 

547
00:15:47,220 --> 00:15:51,600
 regular supervised learning and cast it

548
00:15:49,280 --> 00:15:51,600
 

549
00:15:49,290 --> 00:15:52,650
 in a probabilistic framework so a lot of

550
00:15:51,590 --> 00:15:52,650
 

551
00:15:51,600 --> 00:15:54,360
 what I'm gonna say some of you might

552
00:15:52,640 --> 00:15:54,360
 

553
00:15:52,650 --> 00:15:56,220
 have already seen may be in a course on

554
00:15:54,350 --> 00:15:56,220
 

555
00:15:54,360 --> 00:15:57,600
 machine learning or a textbook but I

556
00:15:56,210 --> 00:15:57,600
 

557
00:15:56,220 --> 00:15:59,430
 just want to walk through it step by

558
00:15:57,590 --> 00:15:59,430
 

559
00:15:57,600 --> 00:16:01,410
 step because the meta learning problem

560
00:15:59,420 --> 00:16:01,410
 

561
00:15:59,430 --> 00:16:03,330
 definition will build on this so if

562
00:16:01,400 --> 00:16:03,330
 

563
00:16:01,410 --> 00:16:04,860
 we're doing supervised learning what

564
00:16:03,320 --> 00:16:04,860
 

565
00:16:03,330 --> 00:16:07,560
 we're really doing is we're finding the

566
00:16:04,850 --> 00:16:07,560
 

567
00:16:04,860 --> 00:16:11,400
 most likely parameters Phi given our

568
00:16:07,550 --> 00:16:11,400
 

569
00:16:07,560 --> 00:16:12,180
 data D so Phi denotes the parameters of

570
00:16:11,390 --> 00:16:12,180
 

571
00:16:11,400 --> 00:16:13,230
 your model so if

572
00:16:12,170 --> 00:16:13,230
 

573
00:16:12,180 --> 00:16:15,360
 training for example a deep neural

574
00:16:13,220 --> 00:16:15,360
 

575
00:16:13,230 --> 00:16:18,029
 network model 5 literally refers to the

576
00:16:15,350 --> 00:16:18,029
 

577
00:16:15,360 --> 00:16:21,660
 weights D refers to your training data

578
00:16:18,019 --> 00:16:21,660
 

579
00:16:18,029 --> 00:16:23,339
 so it's a set of tuples of input-output

580
00:16:21,650 --> 00:16:23,339
 

581
00:16:21,660 --> 00:16:25,050
 pairs where the input might be something

582
00:16:23,329 --> 00:16:25,050
 

583
00:16:23,339 --> 00:16:26,940
 like an image and the output is maybe

584
00:16:25,040 --> 00:16:26,940
 

585
00:16:25,050 --> 00:16:30,570
 the label corresponding to the class of

586
00:16:26,930 --> 00:16:30,570
 

587
00:16:26,940 --> 00:16:33,570
 the object in that image now when we

588
00:16:30,560 --> 00:16:33,570
 

589
00:16:30,570 --> 00:16:34,680
 actually want to do this kind of maximum

590
00:16:33,560 --> 00:16:34,680
 

591
00:16:33,570 --> 00:16:36,450
 likelihood estimation problem we

592
00:16:34,670 --> 00:16:36,450
 

593
00:16:34,680 --> 00:16:39,089
 typically apply Bayes rule and we

594
00:16:36,440 --> 00:16:39,089
 

595
00:16:36,450 --> 00:16:41,520
 rewrite it as the sum of log P of D

596
00:16:39,079 --> 00:16:41,520
 

597
00:16:39,089 --> 00:16:42,959
 given Phi plus log P of Phi and the

598
00:16:41,510 --> 00:16:42,959
 

599
00:16:41,520 --> 00:16:44,610
 first term is typically referred to as

600
00:16:42,949 --> 00:16:44,610
 

601
00:16:42,959 --> 00:16:46,920
 the likelihood of your data and the

602
00:16:44,600 --> 00:16:46,920
 

603
00:16:44,610 --> 00:16:48,300
 second term is the prior or the

604
00:16:46,910 --> 00:16:48,300
 

605
00:16:46,920 --> 00:16:49,620
 regularizer so if you're using weight

606
00:16:48,290 --> 00:16:49,620
 

607
00:16:48,300 --> 00:16:53,700
 decay that corresponds to a Gaussian

608
00:16:49,610 --> 00:16:53,700
 

609
00:16:49,620 --> 00:16:56,040
 prior for example and if we factorize

610
00:16:53,690 --> 00:16:56,040
 

611
00:16:53,700 --> 00:16:57,930
 the likelihood if we assume independent

612
00:16:56,030 --> 00:16:57,930
 

613
00:16:56,040 --> 00:16:59,820
 and identically distributed data points

614
00:16:57,920 --> 00:16:59,820
 

615
00:16:57,930 --> 00:17:01,260
 then we get the familiar form shown here

616
00:16:59,810 --> 00:17:01,260
 

617
00:16:59,820 --> 00:17:03,570
 it's a sum over all of your data points

618
00:17:01,250 --> 00:17:03,570
 

619
00:17:01,260 --> 00:17:05,880
 of the log probability of the label Y I

620
00:17:03,560 --> 00:17:05,880
 

621
00:17:03,570 --> 00:17:08,490
 given the input X I and your parameters

622
00:17:05,870 --> 00:17:08,490
 

623
00:17:05,880 --> 00:17:11,309
 Phi so this is essentially supervised

624
00:17:08,480 --> 00:17:11,309
 

625
00:17:08,490 --> 00:17:12,510
 learning now of course there are some

626
00:17:11,299 --> 00:17:12,510
 

627
00:17:11,309 --> 00:17:14,280
 things that are a little bit problematic

628
00:17:12,500 --> 00:17:14,280
 

629
00:17:12,510 --> 00:17:16,920
 about this as Chelsea alluded to in the

630
00:17:14,270 --> 00:17:16,920
 

631
00:17:14,280 --> 00:17:18,630
 motivation the models that will work

632
00:17:16,910 --> 00:17:18,630
 

633
00:17:16,920 --> 00:17:20,250
 best the most powerful models will

634
00:17:18,620 --> 00:17:20,250
 

635
00:17:18,630 --> 00:17:22,980
 typically require a large amounts of

636
00:17:20,240 --> 00:17:22,980
 

637
00:17:20,250 --> 00:17:24,300
 data so if your data is very limited it

638
00:17:22,970 --> 00:17:24,300
 

639
00:17:22,980 --> 00:17:26,220
 might be very difficult to get a very

640
00:17:24,290 --> 00:17:26,220
 

641
00:17:24,300 --> 00:17:29,370
 accurate posterior or very accurate

642
00:17:26,210 --> 00:17:29,370
 

643
00:17:26,220 --> 00:17:30,720
 estimate of Phi so the problem at its

644
00:17:29,360 --> 00:17:30,720
 

645
00:17:29,370 --> 00:17:32,309
 core that we're going to be dealing with

646
00:17:30,710 --> 00:17:32,309
 

647
00:17:30,720 --> 00:17:34,770
 a meta learning is how do you do a good

648
00:17:32,299 --> 00:17:34,770
 

649
00:17:32,309 --> 00:17:36,330
 job of estimating Phi when your data is

650
00:17:34,760 --> 00:17:36,330
 

651
00:17:34,770 --> 00:17:38,100
 limited and the way you're going to do

652
00:17:36,320 --> 00:17:38,100
 

653
00:17:36,330 --> 00:17:39,809
 that is by incorporating more data that

654
00:17:38,090 --> 00:17:39,809
 

655
00:17:38,100 --> 00:17:42,800
 is not exactly for the tasks that you

656
00:17:39,799 --> 00:17:42,800
 

657
00:17:39,809 --> 00:17:44,760
 want but somehow structurally related so

658
00:17:42,790 --> 00:17:44,760
 

659
00:17:42,800 --> 00:17:47,040
 the question is how can we incorporate

660
00:17:44,750 --> 00:17:47,040
 

661
00:17:44,760 --> 00:17:49,200
 additional data and I should say as an

662
00:17:47,030 --> 00:17:49,200
 

663
00:17:47,040 --> 00:17:50,610
 aside this is very much the same kind of

664
00:17:49,190 --> 00:17:50,610
 

665
00:17:49,200 --> 00:17:52,380
 challenge that things like semi

666
00:17:50,600 --> 00:17:52,380
 

667
00:17:50,610 --> 00:17:53,610
 supervised learning and unsupervised

668
00:17:52,370 --> 00:17:53,610
 

669
00:17:52,380 --> 00:17:55,440
 learning deal with so in semi-supervised

670
00:17:53,600 --> 00:17:55,440
 

671
00:17:53,610 --> 00:17:58,520
 learning you incorporate additional data

672
00:17:55,430 --> 00:17:58,520
 

673
00:17:55,440 --> 00:18:00,660
 that doesn't have wise and and so forth

674
00:17:58,510 --> 00:18:00,660
 

675
00:17:58,520 --> 00:18:01,830
 in metal learning you incorporate

676
00:18:00,650 --> 00:18:01,830
 

677
00:18:00,660 --> 00:18:05,610
 additional data that we're going to call

678
00:18:01,820 --> 00:18:05,610
 

679
00:18:01,830 --> 00:18:07,050
 d mehta train which is labeled data

680
00:18:05,600 --> 00:18:07,050
 

681
00:18:05,610 --> 00:18:10,140
 it's just labeled data for different

682
00:18:07,040 --> 00:18:10,140
 

683
00:18:07,050 --> 00:18:12,420
 tasks so d mehta train is actually a

684
00:18:10,130 --> 00:18:12,420
 

685
00:18:10,140 --> 00:18:15,420
 data set of data sets so it's a set of

686
00:18:12,410 --> 00:18:15,420
 

687
00:18:12,420 --> 00:18:17,730
 data sets D 1 through D n where each of

688
00:18:15,410 --> 00:18:17,730
 

689
00:18:15,420 --> 00:18:21,570
 those data sets di itself consists of a

690
00:18:17,720 --> 00:18:21,570
 

691
00:18:17,730 --> 00:18:24,480
 set of tuples X I and why I where those

692
00:18:21,560 --> 00:18:24,480
 

693
00:18:21,570 --> 00:18:25,380
 wise are labels for different tasks

694
00:18:24,470 --> 00:18:25,380
 

695
00:18:24,480 --> 00:18:27,030
 we assume the tasks are somehow

696
00:18:25,370 --> 00:18:27,030
 

697
00:18:25,380 --> 00:18:28,530
 structurally similar but not actually

698
00:18:27,020 --> 00:18:28,530
 

699
00:18:27,030 --> 00:18:30,270
 the same so you can't just directly

700
00:18:28,520 --> 00:18:30,270
 

701
00:18:28,530 --> 00:18:32,460
 incorporate Dee Mehta trained as trainee

702
00:18:30,260 --> 00:18:32,460
 

703
00:18:30,270 --> 00:18:34,590
 dated or supervised learning let me give

704
00:18:32,450 --> 00:18:34,590
 

705
00:18:32,460 --> 00:18:36,840
 a little example this is based on a

706
00:18:34,580 --> 00:18:36,840
 

707
00:18:34,590 --> 00:18:38,490
 popular benchmark for meta learning

708
00:18:36,830 --> 00:18:38,490
 

709
00:18:36,840 --> 00:18:40,620
 called a mini image in a data set let's

710
00:18:38,480 --> 00:18:40,620
 

711
00:18:38,490 --> 00:18:42,299
 say that your few shot classification

712
00:18:40,610 --> 00:18:42,299
 

713
00:18:40,620 --> 00:18:44,910
 tasks requires you to do five weigh

714
00:18:42,289 --> 00:18:44,910
 

715
00:18:42,299 --> 00:18:47,130
 classification between cats dogs lions

716
00:18:44,900 --> 00:18:47,130
 

717
00:18:44,910 --> 00:18:48,390
 worms and stacks of bowls I don't know

718
00:18:47,120 --> 00:18:48,390
 

719
00:18:47,130 --> 00:18:49,620
 why you would want to do this task but

720
00:18:48,380 --> 00:18:49,620
 

721
00:18:48,390 --> 00:18:50,760
 let's say there's a few shot tasks and

722
00:18:49,610 --> 00:18:50,760
 

723
00:18:49,620 --> 00:18:53,160
 you have only a few examples of each

724
00:18:50,750 --> 00:18:53,160
 

725
00:18:50,760 --> 00:18:54,630
 image now those few examples are not

726
00:18:53,150 --> 00:18:54,630
 

727
00:18:53,160 --> 00:18:57,660
 enough to solve the tasks by themselves

728
00:18:54,620 --> 00:18:57,660
 

729
00:18:54,630 --> 00:18:59,580
 so we're gonna use the meta train which

730
00:18:57,650 --> 00:18:59,580
 

731
00:18:57,660 --> 00:19:01,320
 is a collection of data sets for other

732
00:18:59,570 --> 00:19:01,320
 

733
00:18:59,580 --> 00:19:03,090
 five-way classification problems so

734
00:19:01,310 --> 00:19:03,090
 

735
00:19:01,320 --> 00:19:05,100
 maybe one of them classifies you know

736
00:19:03,080 --> 00:19:05,100
 

737
00:19:03,090 --> 00:19:07,620
 Birds mushrooms dogs

738
00:19:05,090 --> 00:19:07,620
 

739
00:19:05,100 --> 00:19:09,360
 singers and pianos for instance

740
00:19:07,610 --> 00:19:09,360
 

741
00:19:07,620 --> 00:19:10,650
 different tasks but some structural

742
00:19:09,350 --> 00:19:10,650
 

743
00:19:09,360 --> 00:19:13,500
 similarity because all the more visual

744
00:19:10,640 --> 00:19:13,500
 

745
00:19:10,650 --> 00:19:15,210
 recognition tasks another example may be

746
00:19:13,490 --> 00:19:15,210
 

747
00:19:13,500 --> 00:19:16,559
 the tasks you want to solve is a few

748
00:19:15,200 --> 00:19:16,559
 

749
00:19:15,210 --> 00:19:18,780
 shot regression problem you have a few

750
00:19:16,549 --> 00:19:18,780
 

751
00:19:16,559 --> 00:19:21,900
 examples of input-output pairs but then

752
00:19:18,770 --> 00:19:21,900
 

753
00:19:18,780 --> 00:19:23,790
 your meta training tasks consist of

754
00:19:21,890 --> 00:19:23,790
 

755
00:19:21,900 --> 00:19:25,679
 other curve fitting problems so other

756
00:19:23,780 --> 00:19:25,679
 

757
00:19:23,790 --> 00:19:28,620
 curves with a few sample input-output

758
00:19:25,669 --> 00:19:28,620
 

759
00:19:25,679 --> 00:19:30,299
 pairs or maybe you have some kind of

760
00:19:28,610 --> 00:19:30,299
 

761
00:19:28,620 --> 00:19:32,220
 speech recognition task or some kind of

762
00:19:30,289 --> 00:19:32,220
 

763
00:19:30,299 --> 00:19:33,990
 language translation task and so on so

764
00:19:32,210 --> 00:19:33,990
 

765
00:19:32,220 --> 00:19:36,030
 in all these cases you can formulate a

766
00:19:33,980 --> 00:19:36,030
 

767
00:19:33,990 --> 00:19:37,260
 set of meta training tasks that are not

768
00:19:36,020 --> 00:19:37,260
 

769
00:19:36,030 --> 00:19:38,549
 the same as the tasks you want to solve

770
00:19:37,250 --> 00:19:38,549
 

771
00:19:37,260 --> 00:19:39,870
 but somehow structurally similar and

772
00:19:38,539 --> 00:19:39,870
 

773
00:19:38,549 --> 00:19:44,730
 typically these would come from your

774
00:19:39,860 --> 00:19:44,730
 

775
00:19:39,870 --> 00:19:47,429
 prior data now we could simply stop

776
00:19:44,720 --> 00:19:47,429
 

777
00:19:44,730 --> 00:19:49,500
 right there and treat meta learning as a

778
00:19:47,419 --> 00:19:49,500
 

779
00:19:47,429 --> 00:19:51,570
 nonparametric learning problem so you

780
00:19:49,490 --> 00:19:51,570
 

781
00:19:49,500 --> 00:19:53,940
 want to basically use D&D meta train

782
00:19:51,560 --> 00:19:53,940
 

783
00:19:51,570 --> 00:19:55,650
 together but often times we want to use

784
00:19:53,930 --> 00:19:55,650
 

785
00:19:53,940 --> 00:19:57,059
 high capacity models we don't want to

786
00:19:55,640 --> 00:19:57,059
 

787
00:19:55,650 --> 00:19:58,679
 store all of our data and keep it around

788
00:19:57,049 --> 00:19:58,679
 

789
00:19:57,059 --> 00:20:00,660
 forever we'd like to somehow distill it

790
00:19:58,669 --> 00:20:00,660
 

791
00:19:58,679 --> 00:20:03,780
 into a model into a parametric model

792
00:20:00,650 --> 00:20:03,780
 

793
00:20:00,660 --> 00:20:05,370
 with learn model parameters so in meta

794
00:20:03,770 --> 00:20:05,370
 

795
00:20:03,780 --> 00:20:07,140
 learning we don't want to keep D meta

796
00:20:05,360 --> 00:20:07,140
 

797
00:20:05,370 --> 00:20:09,419
 train around forever what we'd like to

798
00:20:07,130 --> 00:20:09,419
 

799
00:20:07,140 --> 00:20:11,610
 do instead is learn some meta parameters

800
00:20:09,409 --> 00:20:11,610
 

801
00:20:09,419 --> 00:20:14,370
 theta so we're going to use D meta train

802
00:20:11,600 --> 00:20:14,370
 

803
00:20:11,610 --> 00:20:16,320
 to learn theta and theta will basically

804
00:20:14,360 --> 00:20:16,320
 

805
00:20:14,370 --> 00:20:17,940
 contain all the information that we need

806
00:20:16,310 --> 00:20:17,940
 

807
00:20:16,320 --> 00:20:19,650
 for solving new tasks that we've

808
00:20:17,930 --> 00:20:19,650
 

809
00:20:17,940 --> 00:20:21,120
 extracted from D meta train so whatever

810
00:20:19,640 --> 00:20:21,120
 

811
00:20:19,650 --> 00:20:23,549
 we need to know about D meta train is

812
00:20:21,110 --> 00:20:23,549
 

813
00:20:21,120 --> 00:20:25,710
 going to be baked into theta via a meta

814
00:20:23,539 --> 00:20:25,710
 

815
00:20:23,549 --> 00:20:27,059
 learning process and that's essentially

816
00:20:25,700 --> 00:20:27,059
 

817
00:20:25,710 --> 00:20:28,740
 the essence of the meta learning problem

818
00:20:27,049 --> 00:20:28,740
 

819
00:20:27,059 --> 00:20:30,840
 now if we want to treat this

820
00:20:28,730 --> 00:20:30,840
 

821
00:20:28,740 --> 00:20:33,390
 probabilistically what we can do now is

822
00:20:30,830 --> 00:20:33,390
 

823
00:20:30,840 --> 00:20:36,630
 we can say well we can write out our P

824
00:20:33,380 --> 00:20:36,630
 

825
00:20:33,390 --> 00:20:38,230
 of Phi given D comma D meta train as an

826
00:20:36,620 --> 00:20:38,230
 

827
00:20:36,630 --> 00:20:40,240
 equation where we're in degree

828
00:20:38,220 --> 00:20:40,240
 

829
00:20:38,230 --> 00:20:44,050
 the these sufficient statistics of our

830
00:20:40,230 --> 00:20:44,050
 

831
00:20:40,240 --> 00:20:46,090
 meta training data called theta and this

832
00:20:44,040 --> 00:20:46,090
 

833
00:20:44,050 --> 00:20:47,650
 implies the assumption that Phi is

834
00:20:46,080 --> 00:20:47,650
 

835
00:20:46,090 --> 00:20:49,390
 conditionally independent of D meta

836
00:20:47,640 --> 00:20:49,390
 

837
00:20:47,650 --> 00:20:50,590
 train given theta which is very

838
00:20:49,380 --> 00:20:50,590
 

839
00:20:49,390 --> 00:20:52,600
 reasonable because we just said the

840
00:20:50,580 --> 00:20:52,600
 

841
00:20:50,590 --> 00:20:54,460
 theta should be whatever extracts all

842
00:20:52,590 --> 00:20:54,460
 

843
00:20:52,600 --> 00:20:58,000
 the necessary sufficient statistics from

844
00:20:54,450 --> 00:20:58,000
 

845
00:20:54,460 --> 00:20:59,470
 D meta train now in reality integrating

846
00:20:57,990 --> 00:20:59,470
 

847
00:20:58,000 --> 00:21:00,970
 out theta is computationally very very

848
00:20:59,460 --> 00:21:00,970
 

849
00:20:59,470 --> 00:21:02,590
 expensive so we wouldn't want to do this

850
00:21:00,960 --> 00:21:02,590
 

851
00:21:00,970 --> 00:21:05,050
 what we would want to do in practice

852
00:21:02,580 --> 00:21:05,050
 

853
00:21:02,590 --> 00:21:08,530
 typically is use a maximum a posteriori

854
00:21:05,040 --> 00:21:08,530
 

855
00:21:05,050 --> 00:21:10,240
 estimate which is which means that we're

856
00:21:08,520 --> 00:21:10,240
 

857
00:21:08,530 --> 00:21:11,830
 going to approximate this integral with

858
00:21:10,230 --> 00:21:11,830
 

859
00:21:10,240 --> 00:21:14,080
 just a point estimate for theta star

860
00:21:11,820 --> 00:21:14,080
 

861
00:21:11,830 --> 00:21:16,870
 where theta star is whatever actually

862
00:21:14,070 --> 00:21:16,870
 

863
00:21:14,080 --> 00:21:18,670
 maximizes the log probability of theta

864
00:21:16,860 --> 00:21:18,670
 

865
00:21:16,870 --> 00:21:20,680
 given D meta train which again is a very

866
00:21:18,660 --> 00:21:20,680
 

867
00:21:18,670 --> 00:21:23,020
 standard thing to do in machine learning

868
00:21:20,670 --> 00:21:23,020
 

869
00:21:20,680 --> 00:21:24,940
 so this Arg max that I have written on

870
00:21:23,010 --> 00:21:24,940
 

871
00:21:23,020 --> 00:21:25,960
 the right side here this is the meta

872
00:21:24,930 --> 00:21:25,960
 

873
00:21:24,940 --> 00:21:28,420
 learning problem the meta learning

874
00:21:25,950 --> 00:21:28,420
 

875
00:21:25,960 --> 00:21:31,750
 problem is to pull out the right theta

876
00:21:28,410 --> 00:21:31,750
 

877
00:21:28,420 --> 00:21:33,550
 from your meta training data so that

878
00:21:31,740 --> 00:21:33,550
 

879
00:21:31,750 --> 00:21:36,100
 that theta contains everything you need

880
00:21:33,540 --> 00:21:36,100
 

881
00:21:33,550 --> 00:21:38,950
 to know to efficiently solve new tasks

882
00:21:36,090 --> 00:21:38,950
 

883
00:21:36,100 --> 00:21:41,650
 and efficiently solve new tasks means

884
00:21:38,940 --> 00:21:41,650
 

885
00:21:38,950 --> 00:21:43,570
 figure out Phi so once you have theta

886
00:21:41,640 --> 00:21:43,570
 

887
00:21:41,650 --> 00:21:45,700
 the problem of getting Phi can be

888
00:21:43,560 --> 00:21:45,700
 

889
00:21:43,570 --> 00:21:48,220
 written as the Arg max of log P Phi

890
00:21:45,690 --> 00:21:48,220
 

891
00:21:45,700 --> 00:21:49,300
 given D comma theta star because you

892
00:21:48,210 --> 00:21:49,300
 

893
00:21:48,220 --> 00:21:51,690
 don't need the dimelo train anymore

894
00:21:49,290 --> 00:21:51,690
 

895
00:21:49,300 --> 00:21:54,160
 that's all been baked into theta star

896
00:21:51,680 --> 00:21:54,160
 

897
00:21:51,690 --> 00:21:56,080
 okay so that's the basic problem

898
00:21:54,150 --> 00:21:56,080
 

899
00:21:54,160 --> 00:21:58,480
 formulation if anybody has any questions

900
00:21:56,070 --> 00:21:58,480
 

901
00:21:56,080 --> 00:21:59,950
 feel free to come up to the microphones

902
00:21:58,470 --> 00:21:59,950
 

903
00:21:58,480 --> 00:22:07,660
 and ask me otherwise I'm going to move

904
00:21:59,940 --> 00:22:07,660
 

905
00:21:59,950 --> 00:22:09,910
 on to a simple example yeah that's an

906
00:22:07,650 --> 00:22:09,910
 

907
00:22:07,660 --> 00:22:11,530
 excellent question so meta learning is

908
00:22:09,900 --> 00:22:11,530
 

909
00:22:09,910 --> 00:22:13,180
 conceptually quite related to a number

910
00:22:11,520 --> 00:22:13,180
 

911
00:22:11,530 --> 00:22:16,420
 of other problems settings including

912
00:22:13,170 --> 00:22:16,420
 

913
00:22:13,180 --> 00:22:17,470
 transfer learning multitask learning you

914
00:22:16,410 --> 00:22:17,470
 

915
00:22:16,420 --> 00:22:19,390
 know even things like semi-supervised

916
00:22:17,460 --> 00:22:19,390
 

917
00:22:17,470 --> 00:22:21,040
 learning and that all of these problem

918
00:22:19,380 --> 00:22:21,040
 

919
00:22:19,390 --> 00:22:23,110
 settings deal with incorporating

920
00:22:21,030 --> 00:22:23,110
 

921
00:22:21,040 --> 00:22:24,490
 additional data that is not quite from

922
00:22:23,100 --> 00:22:24,490
 

923
00:22:23,110 --> 00:22:26,530
 your task but it's going to help you

924
00:22:24,480 --> 00:22:26,530
 

925
00:22:24,490 --> 00:22:28,600
 solve your task more efficiently the

926
00:22:26,520 --> 00:22:28,600
 

927
00:22:26,530 --> 00:22:30,820
 main difference is that meta learning

928
00:22:28,590 --> 00:22:30,820
 

929
00:22:28,600 --> 00:22:32,470
 deals with a setting where you still

930
00:22:30,810 --> 00:22:32,470
 

931
00:22:30,820 --> 00:22:35,770
 have to do some amount of adaptation on

932
00:22:32,460 --> 00:22:35,770
 

933
00:22:32,470 --> 00:22:36,820
 your new task transfer learning formula

934
00:22:35,760 --> 00:22:36,820
 

935
00:22:35,770 --> 00:22:39,220
 in certain ways can actually be viewed

936
00:22:36,810 --> 00:22:39,220
 

937
00:22:36,820 --> 00:22:40,870
 as a type of metal learning as well I'll

938
00:22:39,210 --> 00:22:40,870
 

939
00:22:39,220 --> 00:22:41,740
 describe related problem settings a

940
00:22:40,860 --> 00:22:41,740
 

941
00:22:40,870 --> 00:22:42,940
 little bit more at the end of this

942
00:22:41,730 --> 00:22:42,940
 

943
00:22:41,740 --> 00:22:45,570
 section and maybe then things will be a

944
00:22:42,930 --> 00:22:45,570
 

945
00:22:42,940 --> 00:22:45,570
 little clearer okay

946
00:22:46,410 --> 00:22:46,410
 

947
00:22:46,420 --> 00:22:51,799
 let's continue

948
00:22:48,190 --> 00:22:51,799
 

949
00:22:48,200 --> 00:22:54,769
 so let's work through a little example

950
00:22:51,789 --> 00:22:54,769
 

951
00:22:51,799 --> 00:22:56,059
 of how we can design a cartoony version

952
00:22:54,759 --> 00:22:56,059
 

953
00:22:54,769 --> 00:22:56,960
 of a meta learning algorithm Chelsea

954
00:22:56,049 --> 00:22:56,960
 

955
00:22:56,059 --> 00:22:58,850
 will talk about much more practical

956
00:22:56,950 --> 00:22:58,850
 

957
00:22:56,960 --> 00:23:01,220
 algorithms this is just meant to be an

958
00:22:58,840 --> 00:23:01,220
 

959
00:22:58,850 --> 00:23:02,989
 illustration so first let's talk about

960
00:23:01,210 --> 00:23:02,989
 

961
00:23:01,220 --> 00:23:04,879
 the the adaptation let's say that we

962
00:23:02,979 --> 00:23:04,879
 

963
00:23:02,989 --> 00:23:06,019
 already have this theta star we don't

964
00:23:04,869 --> 00:23:06,019
 

965
00:23:04,879 --> 00:23:08,419
 care how we learn it and now we just

966
00:23:06,009 --> 00:23:08,419
 

967
00:23:06,019 --> 00:23:11,509
 want to classify new data points using a

968
00:23:08,409 --> 00:23:11,509
 

969
00:23:08,419 --> 00:23:13,549
 small data set D so classifying new data

970
00:23:11,499 --> 00:23:13,549
 

971
00:23:11,509 --> 00:23:15,859
 points means your test data point X test

972
00:23:13,539 --> 00:23:15,859
 

973
00:23:13,549 --> 00:23:19,639
 goes in your label Y test comes out and

974
00:23:15,849 --> 00:23:19,639
 

975
00:23:15,859 --> 00:23:21,799
 this function that does this is somehow

976
00:23:19,629 --> 00:23:21,799
 

977
00:23:19,639 --> 00:23:23,539
 parameterize by theta star so theta star

978
00:23:21,789 --> 00:23:23,539
 

979
00:23:21,799 --> 00:23:25,940
 determines the mapping between X test

980
00:23:23,529 --> 00:23:25,940
 

981
00:23:23,539 --> 00:23:28,669
 and Y test Waiters theta star come from

982
00:23:25,930 --> 00:23:28,669
 

983
00:23:25,940 --> 00:23:30,590
 well it comes from using your data set D

984
00:23:28,659 --> 00:23:30,590
 

985
00:23:28,669 --> 00:23:33,169
 which might be a small data set for your

986
00:23:30,580 --> 00:23:33,169
 

987
00:23:30,590 --> 00:23:36,230
 new task together with your theta star

988
00:23:33,159 --> 00:23:36,230
 

989
00:23:33,169 --> 00:23:38,269
 so D is going to be read in by some

990
00:23:36,220 --> 00:23:38,269
 

991
00:23:36,230 --> 00:23:40,909
 function and that function that reason D

992
00:23:38,259 --> 00:23:40,909
 

993
00:23:38,269 --> 00:23:42,950
 is parametrized by theta star sorry

994
00:23:40,899 --> 00:23:42,950
 

995
00:23:40,909 --> 00:23:47,450
 about that so that function is

996
00:23:42,940 --> 00:23:47,450
 

997
00:23:42,950 --> 00:23:51,409
 parametrized by theta star now you would

998
00:23:47,440 --> 00:23:51,409
 

999
00:23:47,450 --> 00:23:53,210
 also like to of course be able to learn

1000
00:23:51,399 --> 00:23:53,210
 

1001
00:23:51,409 --> 00:23:56,059
 this theta star using large amounts of

1002
00:23:53,200 --> 00:23:56,059
 

1003
00:23:53,210 --> 00:23:59,389
 meta training data which I'll come to in

1004
00:23:56,049 --> 00:23:59,389
 

1005
00:23:56,059 --> 00:24:01,190
 a second but if you can somehow use that

1006
00:23:59,379 --> 00:24:01,190
 

1007
00:23:59,389 --> 00:24:02,779
 Mediterranea to get theta star then that

1008
00:24:01,180 --> 00:24:02,779
 

1009
00:24:01,190 --> 00:24:04,940
 will process your data set up with Phi

1010
00:24:02,769 --> 00:24:04,940
 

1011
00:24:02,779 --> 00:24:06,739
 star and allow you to turn your test

1012
00:24:04,930 --> 00:24:06,739
 

1013
00:24:04,940 --> 00:24:09,080
 inputs into test labels so theta star is

1014
00:24:06,729 --> 00:24:09,080
 

1015
00:24:06,739 --> 00:24:10,970
 what parametrize is this function okay

1016
00:24:09,070 --> 00:24:10,970
 

1017
00:24:09,080 --> 00:24:13,879
 so now how do we actually train this

1018
00:24:10,960 --> 00:24:13,879
 

1019
00:24:10,970 --> 00:24:15,169
 thing well as I alluded to before it's

1020
00:24:13,869 --> 00:24:15,169
 

1021
00:24:13,879 --> 00:24:17,869
 going to involve this meta training data

1022
00:24:15,159 --> 00:24:17,869
 

1023
00:24:15,169 --> 00:24:19,340
 and the key idea behind setting up meta

1024
00:24:17,859 --> 00:24:19,340
 

1025
00:24:17,869 --> 00:24:20,929
 learning algorithms I think is best

1026
00:24:19,330 --> 00:24:20,929
 

1027
00:24:19,340 --> 00:24:22,609
 summarized by this sentence from a paper

1028
00:24:20,919 --> 00:24:22,609
 

1029
00:24:20,929 --> 00:24:24,440
 by venules at all called matching

1030
00:24:22,599 --> 00:24:24,440
 

1031
00:24:22,609 --> 00:24:26,480
 networks which says that our training

1032
00:24:24,430 --> 00:24:26,480
 

1033
00:24:24,440 --> 00:24:28,609
 procedure is based on a simple machine

1034
00:24:26,470 --> 00:24:28,609
 

1035
00:24:26,480 --> 00:24:31,669
 learning principle which is that tests

1036
00:24:28,599 --> 00:24:31,669
 

1037
00:24:28,609 --> 00:24:33,169
 and train conditions must match now

1038
00:24:31,659 --> 00:24:33,169
 

1039
00:24:31,669 --> 00:24:34,639
 let's unpack this a little bit what are

1040
00:24:33,159 --> 00:24:34,639
 

1041
00:24:33,169 --> 00:24:38,119
 the test conditions well test here

1042
00:24:34,629 --> 00:24:38,119
 

1043
00:24:34,639 --> 00:24:40,759
 refers to meta test right so meta test

1044
00:24:38,109 --> 00:24:40,759
 

1045
00:24:38,119 --> 00:24:43,429
 time is adaptation the test condition is

1046
00:24:40,749 --> 00:24:43,429
 

1047
00:24:40,759 --> 00:24:47,389
 that a model parametrized by theta star

1048
00:24:43,419 --> 00:24:47,389
 

1049
00:24:43,429 --> 00:24:49,700
 reads in d outputs Phi star and five

1050
00:24:47,379 --> 00:24:49,700
 

1051
00:24:47,389 --> 00:24:52,070
 stars then used to classify new data

1052
00:24:49,690 --> 00:24:52,070
 

1053
00:24:49,700 --> 00:24:53,840
 points for your task so the training

1054
00:24:52,060 --> 00:24:53,840
 

1055
00:24:52,070 --> 00:24:55,340
 time conditions need to match so it met

1056
00:24:53,830 --> 00:24:55,340
 

1057
00:24:53,840 --> 00:24:58,249
 a training time you also need to have a

1058
00:24:55,330 --> 00:24:58,249
 

1059
00:24:55,340 --> 00:25:00,799
 model that reads in a data set which

1060
00:24:58,239 --> 00:25:00,799
 

1061
00:24:58,249 --> 00:25:01,090
 data set well a data set di from your

1062
00:25:00,789 --> 00:25:01,090
 

1063
00:25:00,799 --> 00:25:03,880
 med

1064
00:25:01,080 --> 00:25:03,880
 

1065
00:25:01,090 --> 00:25:06,010
 training set it is going to be

1066
00:25:03,870 --> 00:25:06,010
 

1067
00:25:03,880 --> 00:25:08,350
 parametrized by theta it's gonna output

1068
00:25:06,000 --> 00:25:08,350
 

1069
00:25:06,010 --> 00:25:12,300
 Phi star and that Phi star needs to be

1070
00:25:08,340 --> 00:25:12,300
 

1071
00:25:08,350 --> 00:25:15,940
 good for classifying points which points

1072
00:25:12,290 --> 00:25:15,940
 

1073
00:25:12,300 --> 00:25:17,080
 well that's maybe the puzzle so what is

1074
00:25:15,930 --> 00:25:17,080
 

1075
00:25:15,940 --> 00:25:19,890
 it that we're actually going to classify

1076
00:25:17,070 --> 00:25:19,890
 

1077
00:25:17,080 --> 00:25:22,270
 here what we need to do in order to

1078
00:25:19,880 --> 00:25:22,270
 

1079
00:25:19,890 --> 00:25:23,770
 complete the meta learning problem

1080
00:25:22,260 --> 00:25:23,770
 

1081
00:25:22,270 --> 00:25:25,450
 definition is we need to reserve a

1082
00:25:23,760 --> 00:25:25,450
 

1083
00:25:23,770 --> 00:25:26,710
 little test set for each task so it's

1084
00:25:25,440 --> 00:25:26,710
 

1085
00:25:25,450 --> 00:25:28,120
 not enough to just have a training set

1086
00:25:26,700 --> 00:25:28,120
 

1087
00:25:26,710 --> 00:25:29,680
 the training set is what the model needs

1088
00:25:28,110 --> 00:25:29,680
 

1089
00:25:28,120 --> 00:25:31,120
 to read in but then it needs to be

1090
00:25:29,670 --> 00:25:31,120
 

1091
00:25:29,680 --> 00:25:32,530
 trained on something and what it's

1092
00:25:31,110 --> 00:25:32,530
 

1093
00:25:31,120 --> 00:25:35,740
 actually going to be trained on is a

1094
00:25:32,520 --> 00:25:35,740
 

1095
00:25:32,530 --> 00:25:37,330
 little test set for each task so for

1096
00:25:35,730 --> 00:25:37,330
 

1097
00:25:35,740 --> 00:25:39,280
 every one of our few shot tasks we're

1098
00:25:37,320 --> 00:25:39,280
 

1099
00:25:37,330 --> 00:25:41,830
 gonna assume that we have K training

1100
00:25:39,270 --> 00:25:41,830
 

1101
00:25:39,280 --> 00:25:44,560
 points but done also some number L of

1102
00:25:41,820 --> 00:25:44,560
 

1103
00:25:41,830 --> 00:25:45,850
 little test points and those test points

1104
00:25:44,550 --> 00:25:45,850
 

1105
00:25:44,560 --> 00:25:47,920
 are was gonna supervise the meta

1106
00:25:45,840 --> 00:25:47,920
 

1107
00:25:45,850 --> 00:25:52,000
 learning they're not used for adaptation

1108
00:25:47,910 --> 00:25:52,000
 

1109
00:25:47,920 --> 00:25:54,520
 they're just used for meta learning so D

1110
00:25:51,990 --> 00:25:54,520
 

1111
00:25:52,000 --> 00:25:56,320
 test is where X tests and Y tests will

1112
00:25:54,510 --> 00:25:56,320
 

1113
00:25:54,520 --> 00:25:58,990
 be sampled from so the game that you're

1114
00:25:56,310 --> 00:25:58,990
 

1115
00:25:56,320 --> 00:26:01,420
 playing then is read in D train output

1116
00:25:58,980 --> 00:26:01,420
 

1117
00:25:58,990 --> 00:26:03,610
 Phi and make sure that Phi is good for

1118
00:26:01,410 --> 00:26:03,610
 

1119
00:26:01,420 --> 00:26:05,280
 classifying points from D test for that

1120
00:26:03,600 --> 00:26:05,280
 

1121
00:26:03,610 --> 00:26:08,020
 same task

1122
00:26:05,270 --> 00:26:08,020
 

1123
00:26:05,280 --> 00:26:11,350
 so now we can actually complete the meta

1124
00:26:08,010 --> 00:26:11,350
 

1125
00:26:08,020 --> 00:26:13,000
 learning problem definition so the

1126
00:26:11,340 --> 00:26:13,000
 

1127
00:26:11,350 --> 00:26:15,910
 adaptation step we can write more

1128
00:26:12,990 --> 00:26:15,910
 

1129
00:26:13,000 --> 00:26:18,640
 compactly as some function f theta star

1130
00:26:15,900 --> 00:26:18,640
 

1131
00:26:15,910 --> 00:26:23,380
 of D train so f theta star reads in D

1132
00:26:18,630 --> 00:26:23,380
 

1133
00:26:18,640 --> 00:26:25,660
 train and outputs Phi star so now all we

1134
00:26:23,370 --> 00:26:25,660
 

1135
00:26:23,380 --> 00:26:28,480
 have to do is learn theta such that Phi

1136
00:26:25,650 --> 00:26:28,480
 

1137
00:26:25,660 --> 00:26:31,780
 equals F theta D train is good for D

1138
00:26:28,470 --> 00:26:31,780
 

1139
00:26:28,480 --> 00:26:33,760
 test so for every task I you want to

1140
00:26:31,770 --> 00:26:33,760
 

1141
00:26:31,780 --> 00:26:36,700
 read in D train eye and be good for D

1142
00:26:33,750 --> 00:26:36,700
 

1143
00:26:33,760 --> 00:26:38,680
 test eye which means that we can write

1144
00:26:36,690 --> 00:26:38,680
 

1145
00:26:36,700 --> 00:26:40,840
 down the meta learning problem

1146
00:26:38,670 --> 00:26:40,840
 

1147
00:26:38,680 --> 00:26:42,640
 formulation like this theta star is the

1148
00:26:40,830 --> 00:26:42,640
 

1149
00:26:40,840 --> 00:26:47,290
 Arg max for the sum over all of your

1150
00:26:42,630 --> 00:26:47,290
 

1151
00:26:42,640 --> 00:26:50,770
 tasks of log p phii given D test I where

1152
00:26:47,280 --> 00:26:50,770
 

1153
00:26:47,290 --> 00:26:53,410
 Phi I is equal to F theta apply to D

1154
00:26:50,760 --> 00:26:53,410
 

1155
00:26:50,770 --> 00:26:58,600
 train so notice that we get five from D

1156
00:26:53,400 --> 00:26:58,600
 

1157
00:26:53,410 --> 00:27:00,310
 train but we met a train on D test we

1158
00:26:58,590 --> 00:27:00,310
 

1159
00:26:58,600 --> 00:27:02,860
 can also represent this with a graphical

1160
00:27:00,300 --> 00:27:02,860
 

1161
00:27:00,310 --> 00:27:04,180
 model so if you're into graphical models

1162
00:27:02,850 --> 00:27:04,180
 

1163
00:27:02,860 --> 00:27:05,710
 here's a graphical model that represents

1164
00:27:04,170 --> 00:27:05,710
 

1165
00:27:04,180 --> 00:27:07,720
 this relationship so you have theta

1166
00:27:05,700 --> 00:27:07,720
 

1167
00:27:05,710 --> 00:27:10,060
 which are your global metal earned

1168
00:27:07,710 --> 00:27:10,060
 

1169
00:27:07,720 --> 00:27:10,870
 parameters for every task you have a Phi

1170
00:27:10,050 --> 00:27:10,870
 

1171
00:27:10,060 --> 00:27:12,610
 I

1172
00:27:10,860 --> 00:27:12,610
 

1173
00:27:10,870 --> 00:27:15,280
 and X train together with Phii

1174
00:27:12,600 --> 00:27:15,280
 

1175
00:27:12,610 --> 00:27:17,830
 determines y train and X tests together

1176
00:27:15,270 --> 00:27:17,830
 

1177
00:27:15,280 --> 00:27:20,290
 with Phii determines Y test and Y test

1178
00:27:17,820 --> 00:27:20,290
 

1179
00:27:17,830 --> 00:27:24,700
 is observed during meta training but not

1180
00:27:20,280 --> 00:27:24,700
 

1181
00:27:20,290 --> 00:27:27,670
 observed during that a testing that's

1182
00:27:24,690 --> 00:27:27,670
 

1183
00:27:24,700 --> 00:27:29,380
 why it's kind of half shaded there okay

1184
00:27:27,660 --> 00:27:29,380
 

1185
00:27:27,670 --> 00:27:31,960
 so this basically defines the meta

1186
00:27:29,370 --> 00:27:31,960
 

1187
00:27:29,380 --> 00:27:33,340
 learning problem but let's kind of round

1188
00:27:31,950 --> 00:27:33,340
 

1189
00:27:31,960 --> 00:27:34,780
 out this explanation with a little bit

1190
00:27:33,330 --> 00:27:34,780
 

1191
00:27:33,340 --> 00:27:35,950
 of an overview of terminology because

1192
00:27:34,770 --> 00:27:35,950
 

1193
00:27:34,780 --> 00:27:38,850
 we'll see a bunch of this terminology

1194
00:27:35,940 --> 00:27:38,850
 

1195
00:27:35,950 --> 00:27:43,059
 pop up again again during the tutorial

1196
00:27:38,840 --> 00:27:43,059
 

1197
00:27:38,850 --> 00:27:46,270
 so we make a distinction between meta

1198
00:27:43,049 --> 00:27:46,270
 

1199
00:27:43,059 --> 00:27:48,370
 training meta testing training and

1200
00:27:46,260 --> 00:27:48,370
 

1201
00:27:46,270 --> 00:27:50,470
 testing so you're learning the

1202
00:27:48,360 --> 00:27:50,470
 

1203
00:27:48,370 --> 00:27:53,530
 parameters theta during a meta training

1204
00:27:50,460 --> 00:27:53,530
 

1205
00:27:50,470 --> 00:27:55,510
 phase that meta training phase trains on

1206
00:27:53,520 --> 00:27:55,510
 

1207
00:27:53,530 --> 00:27:57,100
 a collection of data sets each of which

1208
00:27:55,500 --> 00:27:57,100
 

1209
00:27:55,510 --> 00:27:59,800
 is separate into a training set and a

1210
00:27:57,090 --> 00:27:59,800
 

1211
00:27:57,100 --> 00:28:01,780
 test set so when we say training set we

1212
00:27:59,790 --> 00:28:01,780
 

1213
00:27:59,800 --> 00:28:03,610
 mean that small few shots set for a

1214
00:28:01,770 --> 00:28:03,610
 

1215
00:28:01,780 --> 00:28:05,590
 particular task when we say test set we

1216
00:28:03,600 --> 00:28:05,590
 

1217
00:28:03,610 --> 00:28:07,390
 mean the corresponding test images when

1218
00:28:05,580 --> 00:28:07,390
 

1219
00:28:05,590 --> 00:28:09,880
 we say meta training we mean the whole

1220
00:28:07,380 --> 00:28:09,880
 

1221
00:28:07,390 --> 00:28:11,770
 set of tasks so Mediterranean consists

1222
00:28:09,870 --> 00:28:11,770
 

1223
00:28:09,880 --> 00:28:13,390
 of multiple tasks each one with a

1224
00:28:11,760 --> 00:28:13,390
 

1225
00:28:11,770 --> 00:28:15,250
 training set of a test set and meta

1226
00:28:13,380 --> 00:28:15,250
 

1227
00:28:13,390 --> 00:28:17,170
 testing is what happens once you're done

1228
00:28:15,240 --> 00:28:17,170
 

1229
00:28:15,250 --> 00:28:22,690
 meta training and you want to adapt to a

1230
00:28:17,160 --> 00:28:22,690
 

1231
00:28:17,170 --> 00:28:24,370
 training set for a new task so the set

1232
00:28:22,680 --> 00:28:24,370
 

1233
00:28:22,690 --> 00:28:27,070
 of day of data sets is called the meta

1234
00:28:24,360 --> 00:28:27,070
 

1235
00:28:24,370 --> 00:28:29,950
 train that's what we refer to as these

1236
00:28:27,060 --> 00:28:29,950
 

1237
00:28:27,070 --> 00:28:33,550
 are meta training tasks we're gonna use

1238
00:28:29,940 --> 00:28:33,550
 

1239
00:28:29,950 --> 00:28:36,760
 Ti to refer to meta training tasks so

1240
00:28:33,540 --> 00:28:36,760
 

1241
00:28:33,550 --> 00:28:44,650
 these are all of our key is this is our

1242
00:28:36,750 --> 00:28:44,650
 

1243
00:28:36,760 --> 00:28:46,720
 meta test tasks I'm sorry and then

1244
00:28:44,640 --> 00:28:46,720
 

1245
00:28:44,650 --> 00:28:50,050
 sometimes you hear people say support

1246
00:28:46,710 --> 00:28:50,050
 

1247
00:28:46,720 --> 00:28:51,580
 set and quarry set so support refer is

1248
00:28:50,040 --> 00:28:51,580
 

1249
00:28:50,050 --> 00:28:53,170
 basically synonymous with training set

1250
00:28:51,570 --> 00:28:53,170
 

1251
00:28:51,580 --> 00:28:54,610
 and sometimes people use supports that

1252
00:28:53,160 --> 00:28:54,610
 

1253
00:28:53,170 --> 00:28:56,260
 just to avoid the confusion between meta

1254
00:28:54,600 --> 00:28:56,260
 

1255
00:28:54,610 --> 00:28:58,210
 training and training so if someone says

1256
00:28:56,250 --> 00:28:58,210
 

1257
00:28:56,260 --> 00:29:00,220
 support they mean the inner training set

1258
00:28:58,200 --> 00:29:00,220
 

1259
00:28:58,210 --> 00:29:02,770
 and when someone says quarry they're

1260
00:29:00,210 --> 00:29:02,770
 

1261
00:29:00,220 --> 00:29:04,330
 referring to the test the the inner test

1262
00:29:02,760 --> 00:29:04,330
 

1263
00:29:02,770 --> 00:29:05,350
 set not the meta test just the test so

1264
00:29:04,320 --> 00:29:05,350
 

1265
00:29:04,330 --> 00:29:06,720
 the quarry is the thing that you

1266
00:29:05,340 --> 00:29:06,720
 

1267
00:29:05,350 --> 00:29:12,640
 actually want to classify correctly

1268
00:29:06,710 --> 00:29:12,640
 

1269
00:29:06,720 --> 00:29:15,490
 after reading in the support and if

1270
00:29:12,630 --> 00:29:15,490
 

1271
00:29:12,640 --> 00:29:17,500
 someone says like oh I have a case shot

1272
00:29:15,480 --> 00:29:17,500
 

1273
00:29:15,490 --> 00:29:19,350
 classification problem what they're

1274
00:29:17,490 --> 00:29:19,350
 

1275
00:29:17,500 --> 00:29:22,630
 referring to is the number of examples

1276
00:29:19,340 --> 00:29:22,630
 

1277
00:29:19,350 --> 00:29:24,040
 if someone says I have a five-way

1278
00:29:22,620 --> 00:29:24,040
 

1279
00:29:22,630 --> 00:29:24,340
 classification problem they're referring

1280
00:29:24,030 --> 00:29:24,340
 

1281
00:29:24,040 --> 00:29:26,080
 to the

1282
00:29:24,330 --> 00:29:26,080
 

1283
00:29:24,340 --> 00:29:27,640
 number of classes so if you say have a

1284
00:29:26,070 --> 00:29:27,640
 

1285
00:29:26,080 --> 00:29:29,440
 five-shot five-way classification

1286
00:29:27,630 --> 00:29:29,440
 

1287
00:29:27,640 --> 00:29:31,750
 problem that means I have five classes

1288
00:29:29,430 --> 00:29:31,750
 

1289
00:29:29,440 --> 00:29:32,980
 each of which has five examples there's

1290
00:29:31,740 --> 00:29:32,980
 

1291
00:29:31,750 --> 00:29:34,600
 a little bit of confusion about the word

1292
00:29:32,970 --> 00:29:34,600
 

1293
00:29:32,980 --> 00:29:36,760
 shot sometimes it means the number of

1294
00:29:34,590 --> 00:29:36,760
 

1295
00:29:34,600 --> 00:29:38,380
 images per class and sometimes it means

1296
00:29:36,750 --> 00:29:38,380
 

1297
00:29:36,760 --> 00:29:40,240
 the total number of images usually we'll

1298
00:29:38,370 --> 00:29:40,240
 

1299
00:29:38,380 --> 00:29:42,279
 use it to mean the number of images per

1300
00:29:40,230 --> 00:29:42,279
 

1301
00:29:40,240 --> 00:29:46,029
 class so five shot five way means

1302
00:29:42,269 --> 00:29:46,029
 

1303
00:29:42,279 --> 00:29:47,980
 twenty-five data points okay now just to

1304
00:29:46,019 --> 00:29:47,980
 

1305
00:29:46,029 --> 00:29:48,990
 wrap up a few closely related problems

1306
00:29:47,970 --> 00:29:48,990
 

1307
00:29:47,980 --> 00:29:51,130
 settings that are good to be aware of

1308
00:29:48,980 --> 00:29:51,130
 

1309
00:29:48,990 --> 00:29:53,100
 and this is coming back to that question

1310
00:29:51,120 --> 00:29:53,100
 

1311
00:29:51,130 --> 00:29:55,210
 about transfer learning from before so

1312
00:29:53,090 --> 00:29:55,210
 

1313
00:29:53,100 --> 00:29:56,620
 meta learning is closely relate to a few

1314
00:29:55,200 --> 00:29:56,620
 

1315
00:29:55,210 --> 00:29:59,169
 other things that we can actually cast

1316
00:29:56,610 --> 00:29:59,169
 

1317
00:29:56,620 --> 00:30:01,659
 as you know within the same terminology

1318
00:29:59,159 --> 00:30:01,659
 

1319
00:29:59,169 --> 00:30:03,100
 so multitask learning deals with the

1320
00:30:01,649 --> 00:30:03,100
 

1321
00:30:01,659 --> 00:30:05,049
 problem of learning a model with

1322
00:30:03,090 --> 00:30:05,049
 

1323
00:30:03,100 --> 00:30:06,909
 parameters theta star that immediately

1324
00:30:05,039 --> 00:30:06,909
 

1325
00:30:05,049 --> 00:30:08,740
 solve multiple tasks so you can think of

1326
00:30:06,899 --> 00:30:08,740
 

1327
00:30:06,909 --> 00:30:11,620
 multitask learning as sort of zero shot

1328
00:30:08,730 --> 00:30:11,620
 

1329
00:30:08,740 --> 00:30:12,880
 metal learning so that corresponds to

1330
00:30:11,610 --> 00:30:12,880
 

1331
00:30:11,620 --> 00:30:14,980
 just finding parameters that immediately

1332
00:30:12,870 --> 00:30:14,980
 

1333
00:30:12,880 --> 00:30:16,750
 solve all the tasks at the same time

1334
00:30:14,970 --> 00:30:16,750
 

1335
00:30:14,980 --> 00:30:18,010
 this is usually not possible at meta

1336
00:30:16,740 --> 00:30:18,010
 

1337
00:30:16,750 --> 00:30:20,110
 learning problems you can't have one

1338
00:30:18,000 --> 00:30:20,110
 

1339
00:30:18,010 --> 00:30:21,520
 model that classifies you know that does

1340
00:30:20,100 --> 00:30:21,520
 

1341
00:30:20,110 --> 00:30:24,130
 the five Way classification with dogs

1342
00:30:21,510 --> 00:30:24,130
 

1343
00:30:21,520 --> 00:30:27,700
 and lions and also with you know the

1344
00:30:24,120 --> 00:30:27,700
 

1345
00:30:24,130 --> 00:30:29,649
 pianos and the cats but you can view

1346
00:30:27,690 --> 00:30:29,649
 

1347
00:30:27,700 --> 00:30:32,010
 multi task learning as a special case

1348
00:30:29,639 --> 00:30:32,010
 

1349
00:30:29,649 --> 00:30:34,149
 where Phi is just equal to theta

1350
00:30:32,000 --> 00:30:34,149
 

1351
00:30:32,010 --> 00:30:35,080
 another very closely related problem

1352
00:30:34,139 --> 00:30:35,080
 

1353
00:30:34,149 --> 00:30:38,200
 setting is type of parameter

1354
00:30:35,070 --> 00:30:38,200
 

1355
00:30:35,080 --> 00:30:39,370
 optimization and auto ml these can be

1356
00:30:38,190 --> 00:30:39,370
 

1357
00:30:38,200 --> 00:30:40,600
 cast as metal learning they're actually

1358
00:30:39,360 --> 00:30:40,600
 

1359
00:30:39,370 --> 00:30:41,980
 they are essentially metal learning

1360
00:30:40,590 --> 00:30:41,980
 

1361
00:30:40,600 --> 00:30:43,539
 problems they're outside of the scope of

1362
00:30:41,970 --> 00:30:43,539
 

1363
00:30:41,980 --> 00:30:45,520
 this tutorial but I'll just mention

1364
00:30:43,529 --> 00:30:45,520
 

1365
00:30:43,539 --> 00:30:47,020
 briefly how they can be related so in

1366
00:30:45,510 --> 00:30:47,020
 

1367
00:30:45,520 --> 00:30:48,520
 hyper parameter optimization you can say

1368
00:30:47,010 --> 00:30:48,520
 

1369
00:30:47,020 --> 00:30:49,539
 that theta refers to your hyper

1370
00:30:48,510 --> 00:30:49,539
 

1371
00:30:48,520 --> 00:30:50,770
 parameters that's where you're going to

1372
00:30:49,529 --> 00:30:50,770
 

1373
00:30:49,539 --> 00:30:52,809
 get out of your meta training set and

1374
00:30:50,760 --> 00:30:52,809
 

1375
00:30:50,770 --> 00:30:54,700
 Phi is the network weights so you'll

1376
00:30:52,799 --> 00:30:54,700
 

1377
00:30:52,809 --> 00:30:56,350
 learn your hyper parameters from d mehta

1378
00:30:54,690 --> 00:30:56,350
 

1379
00:30:54,700 --> 00:30:59,200
 train and then you'll use them to get

1380
00:30:56,340 --> 00:30:59,200
 

1381
00:30:56,350 --> 00:31:00,610
 Phi architecture search for same deal

1382
00:30:59,190 --> 00:31:00,610
 

1383
00:30:59,200 --> 00:31:02,830
 theta refers to the parameters of your

1384
00:31:00,600 --> 00:31:02,830
 

1385
00:31:00,610 --> 00:31:05,020
 architecture and Phi is the actual

1386
00:31:02,820 --> 00:31:05,020
 

1387
00:31:02,830 --> 00:31:06,730
 weights in the model this is a very

1388
00:31:05,010 --> 00:31:06,730
 

1389
00:31:05,020 --> 00:31:08,020
 active area of research unfortunately

1390
00:31:06,720 --> 00:31:08,020
 

1391
00:31:06,730 --> 00:31:09,730
 outside the scope of this tutorial but

1392
00:31:08,010 --> 00:31:09,730
 

1393
00:31:08,020 --> 00:31:12,159
 hopefully this will tell you a little

1394
00:31:09,720 --> 00:31:12,159
 

1395
00:31:09,730 --> 00:31:14,140
 bit about how they relate okay and next

1396
00:31:12,149 --> 00:31:14,140
 

1397
00:31:12,159 --> 00:31:15,279
 Chelsea will discuss a number of actual

1398
00:31:14,130 --> 00:31:15,279
 

1399
00:31:14,140 --> 00:31:16,510
 metal learning algorithms that we can

1400
00:31:15,269 --> 00:31:16,510
 

1401
00:31:15,279 --> 00:31:20,080
 use based on this problem setting

1402
00:31:16,500 --> 00:31:20,080
 

1403
00:31:16,510 --> 00:31:22,000
 oh yes and we'd be happy to take any

1404
00:31:20,070 --> 00:31:22,000
 

1405
00:31:20,080 --> 00:31:25,210
 questions right now - yeah so one

1406
00:31:21,990 --> 00:31:25,210
 

1407
00:31:22,000 --> 00:31:26,890
 question from what can you elaborate

1408
00:31:25,200 --> 00:31:26,890
 

1409
00:31:25,210 --> 00:31:28,720
 more on the structural similarity that's

1410
00:31:26,880 --> 00:31:28,720
 

1411
00:31:26,890 --> 00:31:30,940
 required between the Mediterranean tasks

1412
00:31:28,710 --> 00:31:30,940
 

1413
00:31:28,720 --> 00:31:32,049
 yeah so in regard to the structural

1414
00:31:30,930 --> 00:31:32,049
 

1415
00:31:30,940 --> 00:31:34,840
 similarity between the metal training

1416
00:31:32,039 --> 00:31:34,840
 

1417
00:31:32,049 --> 00:31:36,610
 tasks this is we can actually make that

1418
00:31:34,830 --> 00:31:36,610
 

1419
00:31:34,840 --> 00:31:37,480
 notion formal the way we make it formal

1420
00:31:36,600 --> 00:31:37,480
 

1421
00:31:36,610 --> 00:31:39,340
 as we say there's a

1422
00:31:37,470 --> 00:31:39,340
 

1423
00:31:37,480 --> 00:31:41,410
 lucien over tasks there's a distribution

1424
00:31:39,330 --> 00:31:41,410
 

1425
00:31:39,340 --> 00:31:43,150
 p task and you assume that all your

1426
00:31:41,400 --> 00:31:43,150
 

1427
00:31:41,410 --> 00:31:44,860
 mediterranean tasks are drawn from that

1428
00:31:43,140 --> 00:31:44,860
 

1429
00:31:43,150 --> 00:31:46,540
 distribution and you assume that all of

1430
00:31:44,850 --> 00:31:46,540
 

1431
00:31:44,860 --> 00:31:48,280
 your meta test tasks are drawn from the

1432
00:31:46,530 --> 00:31:48,280
 

1433
00:31:46,540 --> 00:31:49,960
 same distribution so this is the meta

1434
00:31:48,270 --> 00:31:49,960
 

1435
00:31:48,280 --> 00:31:52,570
 learning analogue of the standard

1436
00:31:49,950 --> 00:31:52,570
 

1437
00:31:49,960 --> 00:31:54,160
 supervised learning assumption now what

1438
00:31:52,560 --> 00:31:54,160
 

1439
00:31:52,570 --> 00:31:56,350
 does a distribution over tasks really

1440
00:31:54,150 --> 00:31:56,350
 

1441
00:31:54,160 --> 00:31:58,000
 mean well that's sometimes ends up being

1442
00:31:56,340 --> 00:31:58,000
 

1443
00:31:56,350 --> 00:31:59,559
 a much more subjective notion if you

1444
00:31:57,990 --> 00:31:59,559
 

1445
00:31:58,000 --> 00:32:00,820
 have a piece of code that generates your

1446
00:31:59,549 --> 00:32:00,820
 

1447
00:31:59,559 --> 00:32:01,929
 tasks and you can say well these it

1448
00:32:00,810 --> 00:32:01,929
 

1449
00:32:00,820 --> 00:32:04,120
 needs to be generate about the same code

1450
00:32:01,919 --> 00:32:04,120
 

1451
00:32:01,929 --> 00:32:06,190
 but of course in reality those tasks are

1452
00:32:04,110 --> 00:32:06,190
 

1453
00:32:04,120 --> 00:32:08,350
 probably produced by nature and there it

1454
00:32:06,180 --> 00:32:08,350
 

1455
00:32:06,190 --> 00:32:09,760
 becomes a much fuzzier line so Chelsea

1456
00:32:08,340 --> 00:32:09,760
 

1457
00:32:08,350 --> 00:32:11,350
 will also discuss a little bit about

1458
00:32:09,750 --> 00:32:11,350
 

1459
00:32:09,760 --> 00:32:15,700
 extrapolation and generalization the

1460
00:32:11,340 --> 00:32:15,700
 

1461
00:32:11,350 --> 00:32:17,320
 prompts pertains to this great so before

1462
00:32:15,690 --> 00:32:17,320
 

1463
00:32:15,700 --> 00:32:19,419
 we actually want to start going about

1464
00:32:17,310 --> 00:32:19,419
 

1465
00:32:17,320 --> 00:32:20,950
 evaluating meta learning algorithms are

1466
00:32:19,409 --> 00:32:20,950
 

1467
00:32:19,419 --> 00:32:22,150
 going about designing my learning

1468
00:32:20,940 --> 00:32:22,150
 

1469
00:32:20,950 --> 00:32:23,500
 algorithms we need to figure out how do

1470
00:32:22,140 --> 00:32:23,500
 

1471
00:32:22,150 --> 00:32:25,840
 you actually evaluate a meta learning

1472
00:32:23,490 --> 00:32:25,840
 

1473
00:32:23,500 --> 00:32:27,730
 algorithm once we have one and so it's

1474
00:32:25,830 --> 00:32:27,730
 

1475
00:32:25,840 --> 00:32:31,000
 worth mentioning that there a lot of the

1476
00:32:27,720 --> 00:32:31,000
 

1477
00:32:27,730 --> 00:32:33,309
 the modern metal learning advances and

1478
00:32:30,990 --> 00:32:33,309
 

1479
00:32:31,000 --> 00:32:35,140
 and techniques were motivated by some

1480
00:32:33,299 --> 00:32:35,140
 

1481
00:32:33,309 --> 00:32:37,570
 some work done by Brendan Lake in 2015

1482
00:32:35,130 --> 00:32:37,570
 

1483
00:32:35,140 --> 00:32:41,169
 and Brendan introduced the Omniglot

1484
00:32:37,560 --> 00:32:41,169
 

1485
00:32:37,570 --> 00:32:42,910
 dataset which is a is much more simple

1486
00:32:41,159 --> 00:32:42,910
 

1487
00:32:41,169 --> 00:32:44,080
 than the mini imagenet dataset that the

1488
00:32:42,900 --> 00:32:44,080
 

1489
00:32:42,910 --> 00:32:46,510
 circuit was showing on the previous

1490
00:32:44,070 --> 00:32:46,510
 

1491
00:32:44,080 --> 00:32:48,549
 slides but allows us to really study

1492
00:32:46,500 --> 00:32:48,549
 

1493
00:32:46,510 --> 00:32:51,309
 some of the basics of metal earning so

1494
00:32:48,539 --> 00:32:51,309
 

1495
00:32:48,549 --> 00:32:52,690
 the Omniglot dataset it has six hundred

1496
00:32:51,299 --> 00:32:52,690
 

1497
00:32:51,309 --> 00:32:53,890
 twenty three sixteen hundred twenty

1498
00:32:52,680 --> 00:32:53,890
 

1499
00:32:52,690 --> 00:33:07,900
 three characters from 50 different

1500
00:32:53,880 --> 00:33:07,900
 

1501
00:32:53,890 --> 00:33:24,429
 alphabets that are had many classes and

1502
00:33:07,890 --> 00:33:24,429
 

1503
00:33:07,900 --> 00:33:25,990
 few examples per class and what I find

1504
00:33:24,419 --> 00:33:25,990
 

1505
00:33:24,429 --> 00:33:27,820
 really appealing about these kinds of

1506
00:33:25,980 --> 00:33:27,820
 

1507
00:33:25,990 --> 00:33:29,679
 datasets is that they're more reflective

1508
00:33:27,810 --> 00:33:29,679
 

1509
00:33:27,820 --> 00:33:31,600
 of the statistics of the real world in

1510
00:33:29,669 --> 00:33:31,600
 

1511
00:33:29,679 --> 00:33:33,370
 the real world we have tremendous

1512
00:33:31,590 --> 00:33:33,370
 

1513
00:33:31,600 --> 00:33:35,110
 diversity in terms of the number of

1514
00:33:33,360 --> 00:33:35,110
 

1515
00:33:33,370 --> 00:33:37,240
 objects and number of items and people

1516
00:33:35,100 --> 00:33:37,240
 

1517
00:33:35,110 --> 00:33:39,220
 that we encounter and we don't encounter

1518
00:33:37,230 --> 00:33:39,220
 

1519
00:33:37,240 --> 00:33:41,260
 them over and over again we often

1520
00:33:39,210 --> 00:33:41,260
 

1521
00:33:39,220 --> 00:33:44,650
 encounter many new things constantly

1522
00:33:41,250 --> 00:33:44,650
 

1523
00:33:41,260 --> 00:33:46,030
 throughout our lifetime ok so this data

1524
00:33:44,640 --> 00:33:46,030
 

1525
00:33:44,650 --> 00:33:51,360
 set proposes both you talk

1526
00:33:46,020 --> 00:33:51,360
 

1527
00:33:46,030 --> 00:33:51,360
 discriminative and generative problems

1528
00:33:52,580 --> 00:33:52,580
 

1529
00:33:52,590 --> 00:33:58,240
 for example an initial approaches for

1530
00:33:56,279 --> 00:33:58,240
 

1531
00:33:56,289 --> 00:33:59,440
 this data set and for other data sets

1532
00:33:58,230 --> 00:33:59,440
 

1533
00:33:58,240 --> 00:34:01,679
 for a few short learning we're based off

1534
00:33:59,430 --> 00:34:01,679
 

1535
00:33:59,440 --> 00:34:04,330
 of Bayesian models and nonparametric s--

1536
00:34:01,669 --> 00:34:04,330
 

1537
00:34:01,679 --> 00:34:06,490
 and similar to what Sergey was

1538
00:34:04,320 --> 00:34:06,490
 

1539
00:34:04,330 --> 00:34:07,900
 mentioning before in addition to this

1540
00:34:06,480 --> 00:34:07,900
 

1541
00:34:06,490 --> 00:34:10,000
 simple I'm gonna collect data set which

1542
00:34:07,890 --> 00:34:10,000
 

1543
00:34:07,900 --> 00:34:12,099
 in many ways actually methods are doing

1544
00:34:09,990 --> 00:34:12,099
 

1545
00:34:10,000 --> 00:34:13,869
 quite well on these days you've also

1546
00:34:12,089 --> 00:34:13,869
 

1547
00:34:12,099 --> 00:34:16,240
 been using things like mini image net C

1548
00:34:13,859 --> 00:34:16,240
 

1549
00:34:13,869 --> 00:34:18,609
 far cub celeb and other data sets for

1550
00:34:16,230 --> 00:34:18,609
 

1551
00:34:16,240 --> 00:34:19,990
 for evaluating meta training algorithms

1552
00:34:18,599 --> 00:34:19,990
 

1553
00:34:18,609 --> 00:34:21,669
 and many of these were not necessarily

1554
00:34:19,980 --> 00:34:21,669
 

1555
00:34:19,990 --> 00:34:23,590
 initially purposed for for med learning

1556
00:34:21,659 --> 00:34:23,590
 

1557
00:34:21,669 --> 00:34:26,980
 but we're able to kind of put them into

1558
00:34:23,580 --> 00:34:26,980
 

1559
00:34:23,590 --> 00:34:28,450
 the the purpose that we would like okay

1560
00:34:26,970 --> 00:34:28,450
 

1561
00:34:26,980 --> 00:34:29,859
 so how do we actually evaluate a meta

1562
00:34:28,440 --> 00:34:29,859
 

1563
00:34:28,450 --> 00:34:31,540
 learning algorithm this is similar to

1564
00:34:29,849 --> 00:34:31,540
 

1565
00:34:29,859 --> 00:34:34,210
 what Sergey was discussing earlier where

1566
00:34:31,530 --> 00:34:34,210
 

1567
00:34:31,540 --> 00:34:37,419
 we have some anyway ka-chow

1568
00:34:34,200 --> 00:34:37,419
 

1569
00:34:34,210 --> 00:34:40,330
 classification problem such as image

1570
00:34:37,409 --> 00:34:40,330
 

1571
00:34:37,419 --> 00:34:44,050
 where we wanted algorithm to be able to

1572
00:34:40,320 --> 00:34:44,050
 

1573
00:34:40,330 --> 00:34:45,730
 perform learning from very small data

1574
00:34:44,040 --> 00:34:45,730
 

1575
00:34:44,050 --> 00:34:47,169
 sets so we might want to be able to

1576
00:34:45,720 --> 00:34:47,169
 

1577
00:34:45,730 --> 00:34:49,899
 learn from one example of five different

1578
00:34:47,159 --> 00:34:49,899
 

1579
00:34:47,169 --> 00:34:52,240
 classes to classify new examples or new

1580
00:34:49,889 --> 00:34:52,240
 

1581
00:34:49,899 --> 00:34:54,849
 images as being among one of the five

1582
00:34:52,230 --> 00:34:54,849
 

1583
00:34:52,240 --> 00:34:56,470
 classes shown on the left and the way

1584
00:34:54,839 --> 00:34:56,470
 

1585
00:34:54,849 --> 00:34:58,810
 that we can do this is we can take data

1586
00:34:56,460 --> 00:34:58,810
 

1587
00:34:56,470 --> 00:34:59,830
 from other image classes structure in

1588
00:34:58,800 --> 00:34:59,830
 

1589
00:34:58,810 --> 00:35:01,780
 the same way is what we're gonna be

1590
00:34:59,820 --> 00:35:01,780
 

1591
00:34:59,830 --> 00:35:05,460
 seeing it met a test time for example

1592
00:35:01,770 --> 00:35:05,460
 

1593
00:35:01,780 --> 00:35:07,780
 taking images of mushrooms and dogs and

1594
00:35:05,450 --> 00:35:07,780
 

1595
00:35:05,460 --> 00:35:09,910
 singer is structuring it into the

1596
00:35:07,770 --> 00:35:09,910
 

1597
00:35:07,780 --> 00:35:11,619
 likewise these these five way Wangchuk

1598
00:35:09,900 --> 00:35:11,619
 

1599
00:35:09,910 --> 00:35:12,910
 classification problems doing this for

1600
00:35:11,609 --> 00:35:12,910
 

1601
00:35:11,619 --> 00:35:14,710
 many different other image classes

1602
00:35:12,900 --> 00:35:14,710
 

1603
00:35:12,910 --> 00:35:16,540
 training a training or networked in

1604
00:35:14,700 --> 00:35:16,540
 

1605
00:35:14,710 --> 00:35:18,430
 order to perform these types of things

1606
00:35:16,530 --> 00:35:18,430
 

1607
00:35:16,540 --> 00:35:20,680
 across these training classes such that

1608
00:35:18,420 --> 00:35:20,680
 

1609
00:35:18,430 --> 00:35:23,609
 an evaluation is able to solve the

1610
00:35:20,670 --> 00:35:23,609
 

1611
00:35:20,680 --> 00:35:26,109
 problem on the top with held out classes

1612
00:35:23,599 --> 00:35:26,109
 

1613
00:35:23,609 --> 00:35:29,200
 and this is an example that's specific

1614
00:35:26,099 --> 00:35:29,200
 

1615
00:35:26,109 --> 00:35:31,390
 to image classification and we're gonna

1616
00:35:29,190 --> 00:35:31,390
 

1617
00:35:29,200 --> 00:35:33,010
 be coming back to this example a number

1618
00:35:31,380 --> 00:35:33,010
 

1619
00:35:31,390 --> 00:35:35,500
 of times because it's useful for for

1620
00:35:33,000 --> 00:35:35,500
 

1621
00:35:33,010 --> 00:35:37,720
 comparing different approaches but the

1622
00:35:35,490 --> 00:35:37,720
 

1623
00:35:35,500 --> 00:35:40,180
 same sorts of ideas are also applicable

1624
00:35:37,710 --> 00:35:40,180
 

1625
00:35:37,720 --> 00:35:44,020
 to things like regression to language

1626
00:35:40,170 --> 00:35:44,020
 

1627
00:35:40,180 --> 00:35:46,330
 generation and and and prediction to

1628
00:35:44,010 --> 00:35:46,330
 

1629
00:35:44,020 --> 00:35:48,369
 skill learning really any machine

1630
00:35:46,320 --> 00:35:48,369
 

1631
00:35:46,330 --> 00:35:49,810
 learning problem you can construct in

1632
00:35:48,359 --> 00:35:49,810
 

1633
00:35:48,369 --> 00:35:52,030
 this way where you're training it on a

1634
00:35:49,800 --> 00:35:52,030
 

1635
00:35:49,810 --> 00:35:54,460
 number of machine learning problems and

1636
00:35:52,020 --> 00:35:54,460
 

1637
00:35:52,030 --> 00:35:55,810
 you want it to be able to generalize to

1638
00:35:54,450 --> 00:35:55,810
 

1639
00:35:54,460 --> 00:35:59,290
 learning a new problem with a small

1640
00:35:55,800 --> 00:35:59,290
 

1641
00:35:55,810 --> 00:36:00,490
 amount of data okay so now that we know

1642
00:35:59,280 --> 00:36:00,490
 

1643
00:35:59,290 --> 00:36:02,290
 how to evaluate a meta learning

1644
00:36:00,480 --> 00:36:02,290
 

1645
00:36:00,490 --> 00:36:03,220
 algorithm was actually dig into how we

1646
00:36:02,280 --> 00:36:03,220
 

1647
00:36:02,290 --> 00:36:04,260
 actually design these meta learning

1648
00:36:03,210 --> 00:36:04,260
 

1649
00:36:03,220 --> 00:36:06,660
 awkward

1650
00:36:04,250 --> 00:36:06,660
 

1651
00:36:04,260 --> 00:36:08,130
 so the general recipe and the general

1652
00:36:06,650 --> 00:36:08,130
 

1653
00:36:06,660 --> 00:36:09,750
 principle behind these algorithms is

1654
00:36:08,120 --> 00:36:09,750
 

1655
00:36:08,130 --> 00:36:12,570
 that we need to choose some form of

1656
00:36:09,740 --> 00:36:12,570
 

1657
00:36:09,750 --> 00:36:15,540
 inferring the parameters of a model Phi

1658
00:36:12,560 --> 00:36:15,540
 

1659
00:36:12,570 --> 00:36:18,119
 given our chain data set and our meta

1660
00:36:15,530 --> 00:36:18,119
 

1661
00:36:15,540 --> 00:36:19,770
 parameters theta and then once we choose

1662
00:36:18,109 --> 00:36:19,770
 

1663
00:36:18,119 --> 00:36:21,720
 choose the form of this we can then

1664
00:36:19,760 --> 00:36:21,720
 

1665
00:36:19,770 --> 00:36:23,369
 optimize it optimize the meta parameters

1666
00:36:21,710 --> 00:36:23,369
 

1667
00:36:21,720 --> 00:36:24,960
 fight theta with respect to a maximum

1668
00:36:23,359 --> 00:36:24,960
 

1669
00:36:23,369 --> 00:36:28,260
 likelihood objective using our meta

1670
00:36:24,950 --> 00:36:28,260
 

1671
00:36:24,960 --> 00:36:29,220
 training data okay and many of the

1672
00:36:28,250 --> 00:36:29,220
 

1673
00:36:28,260 --> 00:36:30,720
 different algorithms that we're gonna be

1674
00:36:29,210 --> 00:36:30,720
 

1675
00:36:29,220 --> 00:36:32,369
 looking at today really only differ in

1676
00:36:30,710 --> 00:36:32,369
 

1677
00:36:30,720 --> 00:36:34,530
 step one choosing how we want to

1678
00:36:32,359 --> 00:36:34,530
 

1679
00:36:32,369 --> 00:36:39,030
 represent this inference problem

1680
00:36:34,520 --> 00:36:39,030
 

1681
00:36:34,530 --> 00:36:40,440
 essentially and so I you can ask well

1682
00:36:39,020 --> 00:36:40,440
 

1683
00:36:39,030 --> 00:36:42,330
 can we just treat this as an inference

1684
00:36:40,430 --> 00:36:42,330
 

1685
00:36:40,440 --> 00:36:43,770
 problem and and pretty clear neural

1686
00:36:42,320 --> 00:36:43,770
 

1687
00:36:42,330 --> 00:36:45,300
 networks are actually are quite good at

1688
00:36:43,760 --> 00:36:45,300
 

1689
00:36:43,770 --> 00:36:48,000
 inference so maybe we can just use a

1690
00:36:45,290 --> 00:36:48,000
 

1691
00:36:45,300 --> 00:36:50,790
 neural network to to represent this

1692
00:36:47,990 --> 00:36:50,790
 

1693
00:36:48,000 --> 00:36:53,640
 function itself and that's exactly what

1694
00:36:50,780 --> 00:36:53,640
 

1695
00:36:50,790 --> 00:36:55,230
 the first approach will be so this is

1696
00:36:53,630 --> 00:36:55,230
 

1697
00:36:53,640 --> 00:36:57,000
 what we'll refer to as blackbox

1698
00:36:55,220 --> 00:36:57,000
 

1699
00:36:55,230 --> 00:36:58,800
 adaptation approaches and the key idea

1700
00:36:56,990 --> 00:36:58,800
 

1701
00:36:57,000 --> 00:37:00,540
 is for a neural network to represent

1702
00:36:58,790 --> 00:37:00,540
 

1703
00:36:58,800 --> 00:37:03,480
 this function that outputs a set of

1704
00:37:00,530 --> 00:37:03,480
 

1705
00:37:00,540 --> 00:37:06,359
 parameters given a data set and a set of

1706
00:37:03,470 --> 00:37:06,359
 

1707
00:37:03,480 --> 00:37:08,970
 meta parameters and for now we're going

1708
00:37:06,349 --> 00:37:08,970
 

1709
00:37:06,359 --> 00:37:11,250
 to be using deterministic or point

1710
00:37:08,960 --> 00:37:11,250
 

1711
00:37:08,970 --> 00:37:14,310
 estimate of this function key which I'll

1712
00:37:11,240 --> 00:37:14,310
 

1713
00:37:11,250 --> 00:37:15,900
 denote as F theta and of course we'll

1714
00:37:14,300 --> 00:37:15,900
 

1715
00:37:14,310 --> 00:37:19,520
 get back to Bayesian methods later and

1716
00:37:15,890 --> 00:37:19,520
 

1717
00:37:15,900 --> 00:37:22,140
 so we'll see Bayes a bit later okay so

1718
00:37:19,510 --> 00:37:22,140
 

1719
00:37:19,520 --> 00:37:24,900
 how do you actually try to design a

1720
00:37:22,130 --> 00:37:24,900
 

1721
00:37:22,140 --> 00:37:25,680
 neural network to do this well one thing

1722
00:37:24,890 --> 00:37:25,680
 

1723
00:37:24,900 --> 00:37:27,450
 you could do is you could use a

1724
00:37:25,670 --> 00:37:27,450
 

1725
00:37:25,680 --> 00:37:30,869
 recurrent neural network that takes in

1726
00:37:27,440 --> 00:37:30,869
 

1727
00:37:27,450 --> 00:37:33,390
 data points sequentially and produces a

1728
00:37:30,859 --> 00:37:33,390
 

1729
00:37:30,869 --> 00:37:35,040
 set of parameters v and so this

1730
00:37:33,380 --> 00:37:35,040
 

1731
00:37:33,390 --> 00:37:37,680
 recurrent neural network in this case

1732
00:37:35,030 --> 00:37:37,680
 

1733
00:37:35,040 --> 00:37:39,720
 will be representing F theta and then

1734
00:37:37,670 --> 00:37:39,720
 

1735
00:37:37,680 --> 00:37:41,040
 we'll take the outputted parameters use

1736
00:37:39,710 --> 00:37:41,040
 

1737
00:37:39,720 --> 00:37:42,210
 those parameters for another neural

1738
00:37:41,030 --> 00:37:42,210
 

1739
00:37:41,040 --> 00:37:45,780
 network that's gonna make predictions

1740
00:37:42,200 --> 00:37:45,780
 

1741
00:37:42,210 --> 00:37:48,930
 about test data points and so these are

1742
00:37:45,770 --> 00:37:48,930
 

1743
00:37:45,780 --> 00:37:50,040
 gonna be the data points from D test ok

1744
00:37:48,920 --> 00:37:50,040
 

1745
00:37:48,930 --> 00:37:51,180
 and then once we have this model we can

1746
00:37:50,030 --> 00:37:51,180
 

1747
00:37:50,040 --> 00:37:52,770
 train it with standard supervised

1748
00:37:51,170 --> 00:37:52,770
 

1749
00:37:51,180 --> 00:37:54,960
 learning this is just a standard

1750
00:37:52,760 --> 00:37:54,960
 

1751
00:37:52,770 --> 00:37:58,369
 recurrent neural network so we can train

1752
00:37:54,950 --> 00:37:58,369
 

1753
00:37:54,960 --> 00:38:01,349
 it to maximize a log probability of the

1754
00:37:58,359 --> 00:38:01,349
 

1755
00:37:58,369 --> 00:38:02,970
 labels of the test data points given the

1756
00:38:01,339 --> 00:38:02,970
 

1757
00:38:01,349 --> 00:38:05,190
 test inputs and we can do this

1758
00:38:02,960 --> 00:38:05,190
 

1759
00:38:02,970 --> 00:38:08,280
 optimization across all of the tasks and

1760
00:38:05,180 --> 00:38:08,280
 

1761
00:38:05,190 --> 00:38:11,570
 our meta training data set we can

1762
00:38:08,270 --> 00:38:11,570
 

1763
00:38:08,280 --> 00:38:13,560
 rewrite this loss function of performing

1764
00:38:11,560 --> 00:38:13,560
 

1765
00:38:11,570 --> 00:38:16,200
 evaluating predictions of a model as

1766
00:38:13,550 --> 00:38:16,200
 

1767
00:38:13,560 --> 00:38:17,200
 simply a loss function operating over

1768
00:38:16,190 --> 00:38:17,200
 

1769
00:38:16,200 --> 00:38:19,599
 the parameters Phi

1770
00:38:17,190 --> 00:38:19,599
 

1771
00:38:17,200 --> 00:38:20,980
 and the test data points so we're gonna

1772
00:38:19,589 --> 00:38:20,980
 

1773
00:38:19,599 --> 00:38:22,710
 write this right here and this this will

1774
00:38:20,970 --> 00:38:22,710
 

1775
00:38:20,980 --> 00:38:25,990
 be used mostly for convenience later on

1776
00:38:22,700 --> 00:38:25,990
 

1777
00:38:22,710 --> 00:38:28,180
 and then with this form we can write the

1778
00:38:25,980 --> 00:38:28,180
 

1779
00:38:25,990 --> 00:38:32,020
 full optimization problem as an

1780
00:38:28,170 --> 00:38:32,020
 

1781
00:38:28,180 --> 00:38:33,609
 optimization of the over the the

1782
00:38:32,010 --> 00:38:33,609
 

1783
00:38:32,020 --> 00:38:37,530
 parameters outputted by the neural

1784
00:38:33,599 --> 00:38:37,530
 

1785
00:38:33,609 --> 00:38:40,660
 network F theta and the test data set

1786
00:38:37,520 --> 00:38:40,660
 

1787
00:38:37,530 --> 00:38:42,670
 okay so now that we have this this

1788
00:38:40,650 --> 00:38:42,670
 

1789
00:38:40,660 --> 00:38:44,470
 optimization objective how do actually

1790
00:38:42,660 --> 00:38:44,470
 

1791
00:38:42,670 --> 00:38:47,109
 what is the algorithm that's used to

1792
00:38:44,460 --> 00:38:47,109
 

1793
00:38:44,470 --> 00:38:48,880
 optimize this so what the algorithm

1794
00:38:47,099 --> 00:38:48,880
 

1795
00:38:47,109 --> 00:38:51,730
 looks like is we first sample a task

1796
00:38:48,870 --> 00:38:51,730
 

1797
00:38:48,880 --> 00:38:54,819
 from our meta training data set or a

1798
00:38:51,720 --> 00:38:54,819
 

1799
00:38:51,730 --> 00:38:58,119
 mini batches of tasks then for that task

1800
00:38:54,809 --> 00:38:58,119
 

1801
00:38:54,819 --> 00:39:00,520
 we have a data set di and will sample

1802
00:38:58,109 --> 00:39:00,520
 

1803
00:38:58,119 --> 00:39:04,000
 disjoint data sets D train I and D test

1804
00:39:00,510 --> 00:39:04,000
 

1805
00:39:00,520 --> 00:39:05,799
 I from that data set and then once we

1806
00:39:03,990 --> 00:39:05,799
 

1807
00:39:04,000 --> 00:39:07,569
 have so I guess what this looks like I

1808
00:39:05,789 --> 00:39:07,569
 

1809
00:39:05,799 --> 00:39:09,160
 say these are the images corresponding

1810
00:39:07,559 --> 00:39:09,160
 

1811
00:39:07,569 --> 00:39:13,020
 to tasks I we want to be able to

1812
00:39:09,150 --> 00:39:13,020
 

1813
00:39:09,160 --> 00:39:15,250
 partition this or basically sample this

1814
00:39:13,010 --> 00:39:15,250
 

1815
00:39:13,020 --> 00:39:17,440
 sample D train and sample t-tests from

1816
00:39:15,240 --> 00:39:17,440
 

1817
00:39:15,250 --> 00:39:21,160
 from this data set and so we'll assign

1818
00:39:17,430 --> 00:39:21,160
 

1819
00:39:17,440 --> 00:39:22,960
 like so and then once we have D train D

1820
00:39:21,150 --> 00:39:22,960
 

1821
00:39:21,160 --> 00:39:25,690
 test will compute the parameters using D

1822
00:39:22,950 --> 00:39:25,690
 

1823
00:39:22,960 --> 00:39:27,690
 train and then evaluate those predicted

1824
00:39:25,680 --> 00:39:27,690
 

1825
00:39:25,690 --> 00:39:29,859
 parameters using the test data points

1826
00:39:27,680 --> 00:39:29,859
 

1827
00:39:27,690 --> 00:39:31,420
 and it's quite important that D train

1828
00:39:29,849 --> 00:39:31,420
 

1829
00:39:29,859 --> 00:39:32,950
 and D tests are disjoint so that we're

1830
00:39:31,410 --> 00:39:32,950
 

1831
00:39:31,420 --> 00:39:34,510
 not training for memorization of the

1832
00:39:32,940 --> 00:39:34,510
 

1833
00:39:32,950 --> 00:39:37,210
 labels but instead training for

1834
00:39:34,500 --> 00:39:37,210
 

1835
00:39:34,510 --> 00:39:39,520
 generalization and then once we update

1836
00:39:37,200 --> 00:39:39,520
 

1837
00:39:37,210 --> 00:39:40,960
 our meta parameters of them were of

1838
00:39:39,510 --> 00:39:40,960
 

1839
00:39:39,520 --> 00:39:43,240
 course going to repeat this process for

1840
00:39:40,950 --> 00:39:43,240
 

1841
00:39:40,960 --> 00:39:45,369
 new tasks and if we use a mini batch of

1842
00:39:43,230 --> 00:39:45,369
 

1843
00:39:43,240 --> 00:39:47,609
 tasks the gradient in step four is going

1844
00:39:45,359 --> 00:39:47,609
 

1845
00:39:45,369 --> 00:39:50,859
 to be averaged across that mini batch

1846
00:39:47,599 --> 00:39:50,859
 

1847
00:39:47,609 --> 00:39:52,450
 great okay so there's the algorithm now

1848
00:39:50,849 --> 00:39:52,450
 

1849
00:39:50,859 --> 00:39:54,579
 how to actually represent the form of F

1850
00:39:52,440 --> 00:39:54,579
 

1851
00:39:52,450 --> 00:39:55,869
 data so the form that I have written

1852
00:39:54,569 --> 00:39:55,869
 

1853
00:39:54,579 --> 00:39:58,059
 here is a recurrent neural network you

1854
00:39:55,859 --> 00:39:58,059
 

1855
00:39:55,869 --> 00:39:59,799
 could use something like an LCM you

1856
00:39:58,049 --> 00:39:59,799
 

1857
00:39:58,059 --> 00:40:01,089
 could also use something that another

1858
00:39:59,789 --> 00:40:01,089
 

1859
00:39:59,799 --> 00:40:02,470
 memory augmented neural network like a

1860
00:40:01,079 --> 00:40:02,470
 

1861
00:40:01,089 --> 00:40:04,270
 neural Turing machine which has done it

1862
00:40:02,460 --> 00:40:04,270
 

1863
00:40:02,470 --> 00:40:05,680
 has been done in past work you could

1864
00:40:04,260 --> 00:40:05,680
 

1865
00:40:04,270 --> 00:40:08,710
 also use something like self attention

1866
00:40:05,670 --> 00:40:08,710
 

1867
00:40:05,680 --> 00:40:10,390
 or 1d convolutions or even really just a

1868
00:40:08,700 --> 00:40:10,390
 

1869
00:40:08,710 --> 00:40:13,420
 feed-forward network that then averages

1870
00:40:10,380 --> 00:40:13,420
 

1871
00:40:10,390 --> 00:40:14,859
 in some embedding space the key thing is

1872
00:40:13,410 --> 00:40:14,859
 

1873
00:40:13,420 --> 00:40:17,410
 that you want this you want these

1874
00:40:14,849 --> 00:40:17,410
 

1875
00:40:14,859 --> 00:40:19,180
 networks to be able to take in sets of

1876
00:40:17,400 --> 00:40:19,180
 

1877
00:40:17,410 --> 00:40:20,380
 data points and often times you want it

1878
00:40:19,170 --> 00:40:20,380
 

1879
00:40:19,180 --> 00:40:22,240
 to be able to take in variable numbers

1880
00:40:20,370 --> 00:40:22,240
 

1881
00:40:20,380 --> 00:40:23,200
 of data points and so that's where I

1882
00:40:22,230 --> 00:40:23,200
 

1883
00:40:22,240 --> 00:40:24,250
 will be using these types of

1884
00:40:23,190 --> 00:40:24,250
 

1885
00:40:23,200 --> 00:40:27,280
 architectures that have the capability

1886
00:40:24,240 --> 00:40:27,280
 

1887
00:40:24,250 --> 00:40:30,360
 to take in take in sets and variable

1888
00:40:27,270 --> 00:40:30,360
 

1889
00:40:27,280 --> 00:40:33,430
 numbers of data points okay

1890
00:40:30,350 --> 00:40:33,430
 

1891
00:40:30,360 --> 00:40:35,440
 great so I know that we've gone over

1892
00:40:33,420 --> 00:40:35,440
 

1893
00:40:33,430 --> 00:40:36,520
 kind of this type of approach and how it

1894
00:40:35,430 --> 00:40:36,520
 

1895
00:40:35,440 --> 00:40:37,900
 works and and the different

1896
00:40:36,510 --> 00:40:37,900
 

1897
00:40:36,520 --> 00:40:40,390
 architectures what are some of the

1898
00:40:37,890 --> 00:40:40,390
 

1899
00:40:37,900 --> 00:40:42,490
 challenges that come up so one thing

1900
00:40:40,380 --> 00:40:42,490
 

1901
00:40:40,390 --> 00:40:43,960
 that you might ask is well if our neural

1902
00:40:42,480 --> 00:40:43,960
 

1903
00:40:42,490 --> 00:40:46,660
 network is outputting all of the

1904
00:40:43,950 --> 00:40:46,660
 

1905
00:40:43,960 --> 00:40:50,050
 parameters of of another neural network

1906
00:40:46,650 --> 00:40:50,050
 

1907
00:40:46,660 --> 00:40:52,120
 this doesn't really seem scalable but if

1908
00:40:50,040 --> 00:40:52,120
 

1909
00:40:50,050 --> 00:40:53,740
 the the the neural network that's making

1910
00:40:52,110 --> 00:40:53,740
 

1911
00:40:52,120 --> 00:40:55,120
 inferences about test data points has

1912
00:40:53,730 --> 00:40:55,120
 

1913
00:40:53,740 --> 00:40:57,010
 millions of parameters then we need a

1914
00:40:55,110 --> 00:40:57,010
 

1915
00:40:55,120 --> 00:40:58,390
 neural network that outputs millions

1916
00:40:57,000 --> 00:40:58,390
 

1917
00:40:57,010 --> 00:41:02,770
 about a million a million dimensional

1918
00:40:58,380 --> 00:41:02,770
 

1919
00:40:58,390 --> 00:41:04,420
 output and one idea that we can use to

1920
00:41:02,760 --> 00:41:04,420
 

1921
00:41:02,770 --> 00:41:06,430
 remedy this is we don't actually need to

1922
00:41:04,410 --> 00:41:06,430
 

1923
00:41:04,420 --> 00:41:08,140
 output all of the parameters of another

1924
00:41:06,420 --> 00:41:08,140
 

1925
00:41:06,430 --> 00:41:10,390
 neural network we really just need to

1926
00:41:08,130 --> 00:41:10,390
 

1927
00:41:08,140 --> 00:41:12,940
 output the sufficient statistics of that

1928
00:41:10,380 --> 00:41:12,940
 

1929
00:41:10,390 --> 00:41:14,320
 data set of the training tasks that

1930
00:41:12,930 --> 00:41:14,320
 

1931
00:41:12,940 --> 00:41:16,570
 allow us to make predictions about new

1932
00:41:14,310 --> 00:41:16,570
 

1933
00:41:14,320 --> 00:41:18,250
 data points and so what we can do is we

1934
00:41:16,560 --> 00:41:18,250
 

1935
00:41:16,570 --> 00:41:20,290
 can take this take this architecture

1936
00:41:18,240 --> 00:41:20,290
 

1937
00:41:18,250 --> 00:41:22,270
 instead of outputting Phi we can output

1938
00:41:20,280 --> 00:41:22,270
 

1939
00:41:20,290 --> 00:41:25,330
 something like H where H is representing

1940
00:41:22,260 --> 00:41:25,330
 

1941
00:41:22,270 --> 00:41:26,590
 a low dimensional vector and then and

1942
00:41:25,320 --> 00:41:26,590
 

1943
00:41:25,330 --> 00:41:28,060
 this will be essentially representing

1944
00:41:26,580 --> 00:41:28,060
 

1945
00:41:26,590 --> 00:41:29,740
 information about the task everything

1946
00:41:28,050 --> 00:41:29,740
 

1947
00:41:28,060 --> 00:41:32,350
 that's needed about the task in order to

1948
00:41:29,730 --> 00:41:32,350
 

1949
00:41:29,740 --> 00:41:34,390
 make predictions and then you can

1950
00:41:32,340 --> 00:41:34,390
 

1951
00:41:32,350 --> 00:41:36,010
 combine this this division statistic H

1952
00:41:34,380 --> 00:41:36,010
 

1953
00:41:34,390 --> 00:41:37,930
 with another set of parameters theta G

1954
00:41:36,000 --> 00:41:37,930
 

1955
00:41:36,010 --> 00:41:41,170
 that are also metal earned along with

1956
00:41:37,920 --> 00:41:41,170
 

1957
00:41:37,930 --> 00:41:43,360
 the parameters of F such that with these

1958
00:41:41,160 --> 00:41:43,360
 

1959
00:41:41,170 --> 00:41:44,920
 with both H and theta we can make

1960
00:41:43,350 --> 00:41:44,920
 

1961
00:41:43,360 --> 00:41:46,960
 predictions about new data points and

1962
00:41:44,910 --> 00:41:46,960
 

1963
00:41:44,920 --> 00:41:48,820
 then theta G can be something very high

1964
00:41:46,950 --> 00:41:48,820
 

1965
00:41:46,960 --> 00:41:50,380
 dimensional and within the combination

1966
00:41:48,810 --> 00:41:50,380
 

1967
00:41:48,820 --> 00:41:55,060
 of the two we'll be able to make

1968
00:41:50,370 --> 00:41:55,060
 

1969
00:41:50,380 --> 00:41:56,500
 predictions for a new task okay so the

1970
00:41:55,050 --> 00:41:56,500
 

1971
00:41:55,060 --> 00:41:58,900
 general form of what this looks like is

1972
00:41:56,490 --> 00:41:58,900
 

1973
00:41:56,500 --> 00:42:01,090
 you can kind of abstract oh wait is this

1974
00:41:58,890 --> 00:42:01,090
 

1975
00:41:58,900 --> 00:42:02,410
 notion of H and just write out the

1976
00:42:01,080 --> 00:42:02,410
 

1977
00:42:01,090 --> 00:42:04,320
 ability to make predictions given a

1978
00:42:02,400 --> 00:42:04,320
 

1979
00:42:02,410 --> 00:42:08,430
 training data set and a new test input

1980
00:42:04,310 --> 00:42:08,430
 

1981
00:42:04,320 --> 00:42:10,600
 outputting the corresponding label okay

1982
00:42:08,420 --> 00:42:10,600
 

1983
00:42:08,430 --> 00:42:14,140
 before I move on to the next type of

1984
00:42:10,590 --> 00:42:14,140
 

1985
00:42:10,600 --> 00:42:16,060
 approach are there any questions so the

1986
00:42:14,130 --> 00:42:16,060
 

1987
00:42:14,140 --> 00:42:17,920
 one question from the audience was does

1988
00:42:16,050 --> 00:42:17,920
 

1989
00:42:16,060 --> 00:42:18,910
 the network need to be recurrent or are

1990
00:42:17,910 --> 00:42:18,910
 

1991
00:42:17,920 --> 00:42:21,430
 there other architectures that could

1992
00:42:18,900 --> 00:42:21,430
 

1993
00:42:18,910 --> 00:42:24,730
 work well absolutely so as I mentioned

1994
00:42:21,420 --> 00:42:24,730
 

1995
00:42:21,430 --> 00:42:26,530
 on the previous slide this could be

1996
00:42:24,720 --> 00:42:26,530
 

1997
00:42:24,730 --> 00:42:28,000
 something that's recurrent like LOC M's

1998
00:42:26,520 --> 00:42:28,000
 

1999
00:42:26,530 --> 00:42:29,230
 and and neural Turing machines you could

2000
00:42:27,990 --> 00:42:29,230
 

2001
00:42:28,000 --> 00:42:33,070
 use something like self attention or

2002
00:42:29,220 --> 00:42:33,070
 

2003
00:42:29,230 --> 00:42:34,420
 recursive models but you also don't need

2004
00:42:33,060 --> 00:42:34,420
 

2005
00:42:33,070 --> 00:42:35,650
 to have something that actually is like

2006
00:42:34,410 --> 00:42:35,650
 

2007
00:42:34,420 --> 00:42:37,390
 a sequence the sequence model could also

2008
00:42:35,640 --> 00:42:37,390
 

2009
00:42:35,650 --> 00:42:39,570
 simply being be something that has a

2010
00:42:37,380 --> 00:42:39,570
 

2011
00:42:37,390 --> 00:42:41,040
 feed-forward model and then averages

2012
00:42:39,560 --> 00:42:41,040
 

2013
00:42:39,570 --> 00:42:43,110
 if you assume that you're trained is

2014
00:42:41,030 --> 00:42:43,110
 

2015
00:42:41,040 --> 00:42:44,640
 that has fixed-length than you could of

2016
00:42:43,100 --> 00:42:44,640
 

2017
00:42:43,110 --> 00:42:45,360
 course also concatenate and use a fully

2018
00:42:44,630 --> 00:42:45,360
 

2019
00:42:44,640 --> 00:42:47,220
 connected Network

2020
00:42:45,350 --> 00:42:47,220
 

2021
00:42:45,360 --> 00:42:50,790
 although these approaches on this slide

2022
00:42:47,210 --> 00:42:50,790
 

2023
00:42:47,220 --> 00:42:57,090
 tend to be a bit more scalable there I

2024
00:42:50,780 --> 00:42:57,090
 

2025
00:42:50,790 --> 00:43:01,140
 think it's on hello hi so this might be

2026
00:42:57,080 --> 00:43:01,140
 

2027
00:42:57,090 --> 00:43:03,750
 related to the few previous slide so I'm

2028
00:43:01,130 --> 00:43:03,750
 

2029
00:43:01,140 --> 00:43:06,540
 trying to understand the difference

2030
00:43:03,740 --> 00:43:06,540
 

2031
00:43:03,750 --> 00:43:10,250
 between meta learning and the learning

2032
00:43:06,530 --> 00:43:10,250
 

2033
00:43:06,540 --> 00:43:14,250
 to learn framework by Jonathan vector of

2034
00:43:10,240 --> 00:43:14,250
 

2035
00:43:10,250 --> 00:43:17,040
 the tipo posts around 1990s so I think

2036
00:43:14,240 --> 00:43:17,040
 

2037
00:43:14,250 --> 00:43:19,020
 if I understand correctly the the basic

2038
00:43:17,030 --> 00:43:19,020
 

2039
00:43:17,040 --> 00:43:21,570
 idea of learning to learn is that you're

2040
00:43:19,010 --> 00:43:21,570
 

2041
00:43:19,020 --> 00:43:24,120
 trying to learn the inductive bias of

2042
00:43:21,560 --> 00:43:24,120
 

2043
00:43:21,570 --> 00:43:29,070
 the learning problem which in this case

2044
00:43:24,110 --> 00:43:29,070
 

2045
00:43:24,120 --> 00:43:31,650
 is better a star in your notation could

2046
00:43:29,060 --> 00:43:31,650
 

2047
00:43:29,070 --> 00:43:34,620
 you elaborate a little bit more on this

2048
00:43:31,640 --> 00:43:34,620
 

2049
00:43:31,650 --> 00:43:37,410
 similarity or the difference

2050
00:43:34,610 --> 00:43:37,410
 

2051
00:43:34,620 --> 00:43:38,970
 Thanks yeah so I'm not sure if I'm

2052
00:43:37,400 --> 00:43:38,970
 

2053
00:43:37,410 --> 00:43:40,770
 familiar with the particular work that

2054
00:43:38,960 --> 00:43:40,770
 

2055
00:43:38,970 --> 00:43:42,360
 you mentioned but in general many of the

2056
00:43:40,760 --> 00:43:42,360
 

2057
00:43:40,770 --> 00:43:44,340
 ideas that we're presenting are inspired

2058
00:43:42,350 --> 00:43:44,340
 

2059
00:43:42,360 --> 00:43:48,420
 by work that was done initially in the

2060
00:43:44,330 --> 00:43:48,420
 

2061
00:43:44,340 --> 00:43:50,670
 late 80s and early 90s with with older

2062
00:43:48,410 --> 00:43:50,670
 

2063
00:43:48,420 --> 00:43:52,170
 types of neural network approaches many

2064
00:43:50,660 --> 00:43:52,170
 

2065
00:43:50,670 --> 00:43:53,340
 of those approaches didn't specifically

2066
00:43:52,160 --> 00:43:53,340
 

2067
00:43:52,170 --> 00:43:54,660
 look at the few shot learning setting

2068
00:43:53,330 --> 00:43:54,660
 

2069
00:43:53,340 --> 00:43:56,940
 which were focusing on this tutorial but

2070
00:43:54,650 --> 00:43:56,940
 

2071
00:43:54,660 --> 00:43:58,200
 looked at in general learning from from

2072
00:43:56,930 --> 00:43:58,200
 

2073
00:43:56,940 --> 00:43:59,130
 relatively large data sets and it's

2074
00:43:58,190 --> 00:43:59,130
 

2075
00:43:58,200 --> 00:44:01,290
 worth mentioning that actually this

2076
00:43:59,120 --> 00:44:01,290
 

2077
00:43:59,130 --> 00:44:03,630
 particular approach up here was actually

2078
00:44:01,280 --> 00:44:03,630
 

2079
00:44:01,290 --> 00:44:06,360
 done was was one of the approaches that

2080
00:44:03,620 --> 00:44:06,360
 

2081
00:44:03,630 --> 00:44:08,940
 was used in the 90s and and also by hawk

2082
00:44:06,350 --> 00:44:08,940
 

2083
00:44:06,360 --> 00:44:10,440
 radar at all in 2001 but some of the

2084
00:44:08,930 --> 00:44:10,440
 

2085
00:44:08,940 --> 00:44:14,750
 approaches that we'll be discussing in

2086
00:44:10,430 --> 00:44:14,750
 

2087
00:44:10,440 --> 00:44:16,770
 later parts of the tutorial are more new

2088
00:44:14,740 --> 00:44:16,770
 

2089
00:44:14,750 --> 00:44:18,570
 another question from the audience was

2090
00:44:16,760 --> 00:44:18,570
 

2091
00:44:16,770 --> 00:44:19,800
 can we use meta learning approaches to

2092
00:44:18,560 --> 00:44:19,800
 

2093
00:44:18,570 --> 00:44:21,720
 solve classical supervised learning

2094
00:44:19,790 --> 00:44:21,720
 

2095
00:44:19,800 --> 00:44:24,140
 problems and are there any benefits to

2096
00:44:21,710 --> 00:44:24,140
 

2097
00:44:21,720 --> 00:44:24,140
 doing so

2098
00:44:26,680 --> 00:44:26,680
 

2099
00:44:26,690 --> 00:44:31,580
 so I think that we'll get to this

2100
00:44:28,450 --> 00:44:31,580
 

2101
00:44:28,460 --> 00:44:34,160
 question a bit at the end there are

2102
00:44:31,570 --> 00:44:34,160
 

2103
00:44:31,580 --> 00:44:35,660
 situations where I guess the main thing

2104
00:44:34,150 --> 00:44:35,660
 

2105
00:44:34,160 --> 00:44:38,030
 the main type of problem that you want

2106
00:44:35,650 --> 00:44:38,030
 

2107
00:44:35,660 --> 00:44:39,860
 to be able to address with meta learning

2108
00:44:38,020 --> 00:44:39,860
 

2109
00:44:38,030 --> 00:44:41,870
 techniques is settings where you want to

2110
00:44:39,850 --> 00:44:41,870
 

2111
00:44:39,860 --> 00:44:45,290
 be able to take in information about a

2112
00:44:41,860 --> 00:44:45,290
 

2113
00:44:41,870 --> 00:44:47,060
 task that has that takes on the form of

2114
00:44:45,280 --> 00:44:47,060
 

2115
00:44:45,290 --> 00:44:50,450
 some data whether it be fully supervised

2116
00:44:47,050 --> 00:44:50,450
 

2117
00:44:47,060 --> 00:44:52,940
 or weekly supervised and so if your

2118
00:44:50,440 --> 00:44:52,940
 

2119
00:44:50,450 --> 00:44:54,800
 supervisor problems setting doesn't have

2120
00:44:52,930 --> 00:44:54,800
 

2121
00:44:52,940 --> 00:44:56,480
 that sort of structure where you want to

2122
00:44:54,790 --> 00:44:56,480
 

2123
00:44:54,800 --> 00:44:58,430
 learn from data to solve new problems

2124
00:44:56,470 --> 00:44:58,430
 

2125
00:44:56,480 --> 00:44:59,390
 then I think it would be quite

2126
00:44:58,420 --> 00:44:59,390
 

2127
00:44:58,430 --> 00:45:01,430
 challenging to apply some of these

2128
00:44:59,380 --> 00:45:01,430
 

2129
00:44:59,390 --> 00:45:02,720
 techniques but in other situations maybe

2130
00:45:01,420 --> 00:45:02,720
 

2131
00:45:01,430 --> 00:45:03,620
 your meta learning maybe your supervised

2132
00:45:02,710 --> 00:45:03,620
 

2133
00:45:02,720 --> 00:45:05,150
 learning problem does have that

2134
00:45:03,610 --> 00:45:05,150
 

2135
00:45:03,620 --> 00:45:09,350
 structure and these approaches would

2136
00:45:05,140 --> 00:45:09,350
 

2137
00:45:05,150 --> 00:45:11,090
 definitely but definitely do well ok so

2138
00:45:09,340 --> 00:45:11,090
 

2139
00:45:09,350 --> 00:45:14,300
 let's move on to the optimization based

2140
00:45:11,080 --> 00:45:14,300
 

2141
00:45:11,090 --> 00:45:15,890
 approaches so now that we just kind of

2142
00:45:14,290 --> 00:45:15,890
 

2143
00:45:14,300 --> 00:45:18,590
 talked about one way to kind of make

2144
00:45:15,880 --> 00:45:18,590
 

2145
00:45:15,890 --> 00:45:20,300
 this approach more scalable is there a

2146
00:45:18,580 --> 00:45:20,300
 

2147
00:45:18,590 --> 00:45:22,220
 way to infer all of the parameters of

2148
00:45:20,290 --> 00:45:22,220
 

2149
00:45:20,300 --> 00:45:23,630
 the neural network in a way that's

2150
00:45:22,210 --> 00:45:23,630
 

2151
00:45:22,220 --> 00:45:24,860
 scalable without having to train a

2152
00:45:23,620 --> 00:45:24,860
 

2153
00:45:23,630 --> 00:45:28,460
 neural network to output all of the

2154
00:45:24,850 --> 00:45:28,460
 

2155
00:45:24,860 --> 00:45:30,320
 parameters in a particular as Sergei

2156
00:45:28,450 --> 00:45:30,320
 

2157
00:45:28,460 --> 00:45:31,550
 mentioned before you can view the

2158
00:45:30,310 --> 00:45:31,550
 

2159
00:45:30,320 --> 00:45:33,020
 problem of supervised learning as an

2160
00:45:31,540 --> 00:45:33,020
 

2161
00:45:31,550 --> 00:45:35,390
 inference problem where I goes in for a

2162
00:45:33,010 --> 00:45:35,390
 

2163
00:45:33,020 --> 00:45:36,710
 set of parameters using data and the way

2164
00:45:35,380 --> 00:45:36,710
 

2165
00:45:35,390 --> 00:45:38,720
 that we we solve supervised learning

2166
00:45:36,700 --> 00:45:38,720
 

2167
00:45:36,710 --> 00:45:40,370
 problems is through optimization and

2168
00:45:38,710 --> 00:45:40,370
 

2169
00:45:38,720 --> 00:45:42,830
 these optimization perches are quite

2170
00:45:40,360 --> 00:45:42,830
 

2171
00:45:40,370 --> 00:45:44,240
 scalable so what if we treat the problem

2172
00:45:42,820 --> 00:45:44,240
 

2173
00:45:42,830 --> 00:45:46,130
 of inferring a set of parameters from

2174
00:45:44,230 --> 00:45:46,130
 

2175
00:45:44,240 --> 00:45:49,100
 data and using meta parameters as

2176
00:45:46,120 --> 00:45:49,100
 

2177
00:45:46,130 --> 00:45:50,330
 exactly an optimization problem and and

2178
00:45:49,090 --> 00:45:50,330
 

2179
00:45:49,100 --> 00:45:53,180
 this is what optimization based

2180
00:45:50,320 --> 00:45:53,180
 

2181
00:45:50,330 --> 00:45:55,100
 approaches do and so the key idea here

2182
00:45:53,170 --> 00:45:55,100
 

2183
00:45:53,180 --> 00:45:57,080
 is that we're going to acquire our TAS

2184
00:45:55,090 --> 00:45:57,080
 

2185
00:45:55,100 --> 00:45:58,730
 specific parameters Phii through an

2186
00:45:57,070 --> 00:45:58,730
 

2187
00:45:57,080 --> 00:46:00,530
 optimization procedure that depends both

2188
00:45:58,720 --> 00:46:00,530
 

2189
00:45:58,730 --> 00:46:03,290
 on the training data and our meta

2190
00:46:00,520 --> 00:46:03,290
 

2191
00:46:00,530 --> 00:46:04,400
 parameters theta and the optimization

2192
00:46:03,280 --> 00:46:04,400
 

2193
00:46:03,290 --> 00:46:06,470
 will look something like this

2194
00:46:04,390 --> 00:46:06,470
 

2195
00:46:04,400 --> 00:46:07,550
 where we are optimizing objective that

2196
00:46:06,460 --> 00:46:07,550
 

2197
00:46:06,470 --> 00:46:09,560
 looks like the likelihood of the data

2198
00:46:07,540 --> 00:46:09,560
 

2199
00:46:07,550 --> 00:46:11,660
 given our toss parameters as well as the

2200
00:46:09,550 --> 00:46:11,660
 

2201
00:46:09,560 --> 00:46:13,430
 likely of our task parameters given our

2202
00:46:11,650 --> 00:46:13,430
 

2203
00:46:11,660 --> 00:46:15,550
 meta parameters where essentially the

2204
00:46:13,420 --> 00:46:15,550
 

2205
00:46:13,430 --> 00:46:18,080
 meta parameters are serving as a prior

2206
00:46:15,540 --> 00:46:18,080
 

2207
00:46:15,550 --> 00:46:19,880
 now you might ask well what should the

2208
00:46:18,070 --> 00:46:19,880
 

2209
00:46:18,080 --> 00:46:22,670
 form of the prior be for I meta

2210
00:46:19,870 --> 00:46:22,670
 

2211
00:46:19,880 --> 00:46:23,990
 parameters well there's a lot of

2212
00:46:22,660 --> 00:46:23,990
 

2213
00:46:22,670 --> 00:46:25,190
 different approaches a lot of different

2214
00:46:23,980 --> 00:46:25,190
 

2215
00:46:23,990 --> 00:46:26,870
 things that we could do here but one

2216
00:46:25,180 --> 00:46:26,870
 

2217
00:46:25,190 --> 00:46:29,090
 very successful form of prior knowledge

2218
00:46:26,860 --> 00:46:29,090
 

2219
00:46:26,870 --> 00:46:33,410
 that we've seen in deep learning for

2220
00:46:29,080 --> 00:46:33,410
 

2221
00:46:29,090 --> 00:46:35,480
 example is using is training from an

2222
00:46:33,400 --> 00:46:35,480
 

2223
00:46:33,410 --> 00:46:39,020
 initialization provided by another data

2224
00:46:35,470 --> 00:46:39,020
 

2225
00:46:35,480 --> 00:46:40,339
 set and in particular what we seen is if

2226
00:46:39,010 --> 00:46:40,339
 

2227
00:46:39,020 --> 00:46:42,440
 we train on things like image

2228
00:46:40,329 --> 00:46:42,440
 

2229
00:46:40,339 --> 00:46:44,749
 and then fine-tune that model on other

2230
00:46:42,430 --> 00:46:44,749
 

2231
00:46:42,440 --> 00:46:46,279
 data sets were able to capture a lot of

2232
00:46:44,739 --> 00:46:46,279
 

2233
00:46:44,749 --> 00:46:48,349
 the rich information and supervision

2234
00:46:46,269 --> 00:46:48,349
 

2235
00:46:46,279 --> 00:46:51,499
 that's exists in the image net data set

2236
00:46:48,339 --> 00:46:51,499
 

2237
00:46:48,349 --> 00:46:52,789
 and use it for new tasks so this is a

2238
00:46:51,489 --> 00:46:52,789
 

2239
00:46:51,499 --> 00:46:54,559
 very successful form of prior knowledge

2240
00:46:52,779 --> 00:46:54,559
 

2241
00:46:52,789 --> 00:46:55,910
 and of course the way that it works is

2242
00:46:54,549 --> 00:46:55,910
 

2243
00:46:54,559 --> 00:46:58,519
 you have a set of pre trained parameters

2244
00:46:55,900 --> 00:46:58,519
 

2245
00:46:55,910 --> 00:47:01,940
 data and you run gradient descent using

2246
00:46:58,509 --> 00:47:01,940
 

2247
00:46:58,519 --> 00:47:03,739
 training data for your new tasks okay so

2248
00:47:01,930 --> 00:47:03,739
 

2249
00:47:01,940 --> 00:47:06,140
 this is this works really well in a

2250
00:47:03,729 --> 00:47:06,140
 

2251
00:47:03,739 --> 00:47:07,549
 number of different situations but what

2252
00:47:06,130 --> 00:47:07,549
 

2253
00:47:06,140 --> 00:47:09,859
 if your trained data for your new tasks

2254
00:47:07,539 --> 00:47:09,859
 

2255
00:47:07,549 --> 00:47:11,150
 has only a few data points like the six

2256
00:47:09,849 --> 00:47:11,150
 

2257
00:47:09,859 --> 00:47:13,460
 data points that I thought I showed in

2258
00:47:11,140 --> 00:47:13,460
 

2259
00:47:11,150 --> 00:47:14,809
 the example at the beginning well in

2260
00:47:13,450 --> 00:47:14,809
 

2261
00:47:13,460 --> 00:47:16,009
 this case things like fine tuning are

2262
00:47:14,799 --> 00:47:16,009
 

2263
00:47:14,809 --> 00:47:17,690
 gonna break down a bit because though

2264
00:47:15,999 --> 00:47:17,690
 

2265
00:47:16,009 --> 00:47:20,180
 they weren't actually trained for the

2266
00:47:17,680 --> 00:47:20,180
 

2267
00:47:17,690 --> 00:47:21,650
 ability to adopt very quickly and as a

2268
00:47:20,170 --> 00:47:21,650
 

2269
00:47:20,180 --> 00:47:23,359
 result you'll either over fit to those

2270
00:47:21,640 --> 00:47:23,359
 

2271
00:47:21,650 --> 00:47:25,460
 six examples or you won't be able to

2272
00:47:23,349 --> 00:47:25,460
 

2273
00:47:23,359 --> 00:47:28,880
 adapt quickly enough and move far enough

2274
00:47:25,450 --> 00:47:28,880
 

2275
00:47:25,460 --> 00:47:30,469
 from your initialization so this is what

2276
00:47:28,870 --> 00:47:30,469
 

2277
00:47:28,880 --> 00:47:31,700
 we want to be able to do a test I may be

2278
00:47:30,459 --> 00:47:31,700
 

2279
00:47:30,469 --> 00:47:34,519
 quite nice if we can just run fine

2280
00:47:31,690 --> 00:47:34,519
 

2281
00:47:31,700 --> 00:47:37,039
 tuning on on our six examples and get a

2282
00:47:34,509 --> 00:47:37,039
 

2283
00:47:34,519 --> 00:47:38,420
 get it up get some answer some function

2284
00:47:37,029 --> 00:47:38,420
 

2285
00:47:37,039 --> 00:47:40,940
 so this is what we wanna be able to do

2286
00:47:38,410 --> 00:47:40,940
 

2287
00:47:38,420 --> 00:47:42,950
 at test time the key idea behind this

2288
00:47:40,930 --> 00:47:42,950
 

2289
00:47:40,940 --> 00:47:44,989
 approach is to explicitly optimize for

2290
00:47:42,940 --> 00:47:44,989
 

2291
00:47:42,950 --> 00:47:46,549
 us at a pre trained parameters such that

2292
00:47:44,979 --> 00:47:46,549
 

2293
00:47:44,989 --> 00:47:49,339
 fine-tuning with a very small data set

2294
00:47:46,539 --> 00:47:49,339
 

2295
00:47:46,549 --> 00:47:51,079
 works very well and so what this looks

2296
00:47:49,329 --> 00:47:51,079
 

2297
00:47:49,339 --> 00:47:53,029
 like is we're going to take the fine

2298
00:47:51,069 --> 00:47:53,029
 

2299
00:47:51,079 --> 00:47:54,499
 tuning process written here this is just

2300
00:47:53,019 --> 00:47:54,499
 

2301
00:47:53,029 --> 00:47:57,109
 one step of gradient descent but you

2302
00:47:54,489 --> 00:47:57,109
 

2303
00:47:54,499 --> 00:47:58,900
 could also use a few steps or up to like

2304
00:47:57,099 --> 00:47:58,900
 

2305
00:47:57,109 --> 00:48:02,059
 ten steps of green descent for example

2306
00:47:58,890 --> 00:48:02,059
 

2307
00:47:58,900 --> 00:48:04,400
 then we are going to take where we were

2308
00:48:02,049 --> 00:48:04,400
 

2309
00:48:02,059 --> 00:48:06,769
 after got after fine-tuning this can be

2310
00:48:04,390 --> 00:48:06,769
 

2311
00:48:04,400 --> 00:48:08,450
 like Phii for example evaluate how well

2312
00:48:06,759 --> 00:48:08,450
 

2313
00:48:06,769 --> 00:48:10,160
 that generalizes to new data points for

2314
00:48:08,440 --> 00:48:10,160
 

2315
00:48:08,450 --> 00:48:12,529
 that task this is measuring how

2316
00:48:10,150 --> 00:48:12,529
 

2317
00:48:10,160 --> 00:48:14,749
 successful fine-tuning was and then we

2318
00:48:12,519 --> 00:48:14,749
 

2319
00:48:12,529 --> 00:48:17,150
 can optimize this objective with regard

2320
00:48:14,739 --> 00:48:17,150
 

2321
00:48:14,749 --> 00:48:18,529
 to the initial set of parameters so

2322
00:48:17,140 --> 00:48:18,529
 

2323
00:48:17,150 --> 00:48:20,420
 we're gonna optimize for set of pre

2324
00:48:18,519 --> 00:48:20,420
 

2325
00:48:18,529 --> 00:48:23,059
 trained parameters such that fine-tuning

2326
00:48:20,410 --> 00:48:23,059
 

2327
00:48:20,420 --> 00:48:25,430
 gives us a generalizable function for

2328
00:48:23,049 --> 00:48:25,430
 

2329
00:48:23,059 --> 00:48:26,809
 that task and of course we don't want to

2330
00:48:25,420 --> 00:48:26,809
 

2331
00:48:25,430 --> 00:48:28,940
 just do this over one task but we'll do

2332
00:48:26,799 --> 00:48:28,940
 

2333
00:48:26,809 --> 00:48:30,680
 this over all of the tasks that are meta

2334
00:48:28,930 --> 00:48:30,680
 

2335
00:48:28,940 --> 00:48:32,029
 training data set so that we can learn

2336
00:48:30,670 --> 00:48:32,029
 

2337
00:48:30,680 --> 00:48:33,890
 an initialization that's amenable to

2338
00:48:32,019 --> 00:48:33,890
 

2339
00:48:32,029 --> 00:48:37,640
 fine tuning for many different types of

2340
00:48:33,880 --> 00:48:37,640
 

2341
00:48:33,890 --> 00:48:38,960
 tasks okay so the key ideas is to learn

2342
00:48:37,630 --> 00:48:38,960
 

2343
00:48:37,640 --> 00:48:42,769
 this parameter vector that transfers

2344
00:48:38,950 --> 00:48:42,769
 

2345
00:48:38,960 --> 00:48:45,229
 effectively via fine tuning okay so what

2346
00:48:42,759 --> 00:48:45,229
 

2347
00:48:42,769 --> 00:48:48,140
 does this look like at a somewhat more

2348
00:48:45,219 --> 00:48:48,140
 

2349
00:48:45,229 --> 00:48:48,979
 intuitive level say theta is the the

2350
00:48:48,130 --> 00:48:48,979
 

2351
00:48:48,140 --> 00:48:49,489
 parameter vector that we're

2352
00:48:48,969 --> 00:48:49,489
 

2353
00:48:48,979 --> 00:48:52,009
 meta-learning

2354
00:48:49,479 --> 00:48:52,009
 

2355
00:48:49,489 --> 00:48:53,220
 and Phi I star is the optimal parameter

2356
00:48:51,999 --> 00:48:53,220
 

2357
00:48:52,009 --> 00:48:54,839
 vector for tasks I am

2358
00:48:53,210 --> 00:48:54,839
 

2359
00:48:53,220 --> 00:48:56,940
 then you can view the meta learning

2360
00:48:54,829 --> 00:48:56,940
 

2361
00:48:54,839 --> 00:48:58,050
 process as a thick black line where when

2362
00:48:56,930 --> 00:48:58,050
 

2363
00:48:56,940 --> 00:48:59,430
 we're at this point during méditerranée

2364
00:48:58,040 --> 00:48:59,430
 

2365
00:48:58,050 --> 00:49:01,380
 and we take a gradient step with respect

2366
00:48:59,420 --> 00:49:01,380
 

2367
00:48:59,430 --> 00:49:03,930
 to task 3 we're quite far from the

2368
00:49:01,370 --> 00:49:03,930
 

2369
00:49:01,380 --> 00:49:05,430
 optimum for task 3 whereas at the end of

2370
00:49:03,920 --> 00:49:05,430
 

2371
00:49:03,930 --> 00:49:07,020
 medal earning if we take a gradient step

2372
00:49:05,420 --> 00:49:07,020
 

2373
00:49:05,430 --> 00:49:09,000
 with respect to task three or quite

2374
00:49:07,010 --> 00:49:09,000
 

2375
00:49:07,020 --> 00:49:12,180
 close to the optimum and likewise for a

2376
00:49:08,990 --> 00:49:12,180
 

2377
00:49:09,000 --> 00:49:13,650
 number of other tasks we refer to this

2378
00:49:12,170 --> 00:49:13,650
 

2379
00:49:12,180 --> 00:49:15,480
 procedure as model agnostic medal

2380
00:49:13,640 --> 00:49:15,480
 

2381
00:49:13,650 --> 00:49:17,880
 earning in the sense that is agnostic to

2382
00:49:15,470 --> 00:49:17,880
 

2383
00:49:15,480 --> 00:49:19,619
 the model that you use and the loss

2384
00:49:17,870 --> 00:49:19,619
 

2385
00:49:17,880 --> 00:49:21,119
 function that you'll use as long as both

2386
00:49:19,609 --> 00:49:21,119
 

2387
00:49:19,619 --> 00:49:25,020
 of them are amenable to gradient based

2388
00:49:21,109 --> 00:49:25,020
 

2389
00:49:21,119 --> 00:49:26,250
 adaptation okay um so now that we've

2390
00:49:25,010 --> 00:49:26,250
 

2391
00:49:25,020 --> 00:49:27,420
 gone through the objective again let's

2392
00:49:26,240 --> 00:49:27,420
 

2393
00:49:26,250 --> 00:49:29,550
 let's go through what actually that

2394
00:49:27,410 --> 00:49:29,550
 

2395
00:49:27,420 --> 00:49:31,050
 algorithm looks like so we can take the

2396
00:49:29,540 --> 00:49:31,050
 

2397
00:49:29,550 --> 00:49:33,630
 algorithm that we showed before for the

2398
00:49:31,040 --> 00:49:33,630
 

2399
00:49:31,050 --> 00:49:37,170
 standard for the the the black box

2400
00:49:33,620 --> 00:49:37,170
 

2401
00:49:33,630 --> 00:49:38,280
 adaptation approach and instead we want

2402
00:49:37,160 --> 00:49:38,280
 

2403
00:49:37,170 --> 00:49:39,510
 to derive the algorithm for an

2404
00:49:38,270 --> 00:49:39,510
 

2405
00:49:38,280 --> 00:49:40,650
 optimization based approach and what we

2406
00:49:39,500 --> 00:49:40,650
 

2407
00:49:39,510 --> 00:49:41,339
 can do is we can simply just replace

2408
00:49:40,640 --> 00:49:41,339
 

2409
00:49:40,650 --> 00:49:42,869
 step 3

2410
00:49:41,329 --> 00:49:42,869
 

2411
00:49:41,339 --> 00:49:44,579
 that's inferring the parameters with the

2412
00:49:42,859 --> 00:49:44,579
 

2413
00:49:42,869 --> 00:49:46,079
 neural network with a step that's

2414
00:49:44,569 --> 00:49:46,079
 

2415
00:49:44,579 --> 00:49:48,510
 actually optimizing for the parameters

2416
00:49:46,069 --> 00:49:48,510
 

2417
00:49:46,079 --> 00:49:50,010
 in this case through gradient descent so

2418
00:49:48,500 --> 00:49:50,010
 

2419
00:49:48,510 --> 00:49:52,200
 we'll sample a task sample destroy its

2420
00:49:50,000 --> 00:49:52,200
 

2421
00:49:50,010 --> 00:49:53,339
 data sets for that in four parameters

2422
00:49:52,190 --> 00:49:53,339
 

2423
00:49:52,200 --> 00:49:55,260
 with gradient descent on the training

2424
00:49:53,329 --> 00:49:55,260
 

2425
00:49:53,339 --> 00:49:58,260
 data and then update our meta parameters

2426
00:49:55,250 --> 00:49:58,260
 

2427
00:49:55,260 --> 00:50:00,380
 using the test data points note that

2428
00:49:58,250 --> 00:50:00,380
 

2429
00:49:58,260 --> 00:50:02,670
 this does bring up a second-order

2430
00:50:00,370 --> 00:50:02,670
 

2431
00:50:00,380 --> 00:50:04,380
 optimization problem because you have to

2432
00:50:02,660 --> 00:50:04,380
 

2433
00:50:02,670 --> 00:50:05,670
 compute the gradient and in step four

2434
00:50:04,370 --> 00:50:05,670
 

2435
00:50:04,380 --> 00:50:07,980
 you have to differentiate through a

2436
00:50:05,660 --> 00:50:07,980
 

2437
00:50:05,670 --> 00:50:10,890
 gradient step in test in in step three

2438
00:50:07,970 --> 00:50:10,890
 

2439
00:50:07,980 --> 00:50:12,390
 in practice a number of standard auto

2440
00:50:10,880 --> 00:50:12,390
 

2441
00:50:10,890 --> 00:50:14,040
 differentiation libraries like tensor

2442
00:50:12,380 --> 00:50:14,040
 

2443
00:50:12,390 --> 00:50:15,810
 flow and PI torch can handle this quite

2444
00:50:14,030 --> 00:50:15,810
 

2445
00:50:14,040 --> 00:50:17,250
 gracefully and really you don't have to

2446
00:50:15,800 --> 00:50:17,250
 

2447
00:50:15,810 --> 00:50:19,500
 worry about it too much at all and it

2448
00:50:17,240 --> 00:50:19,500
 

2449
00:50:17,250 --> 00:50:21,329
 also isn't particularly computationally

2450
00:50:19,490 --> 00:50:21,329
 

2451
00:50:19,500 --> 00:50:23,849
 expensive but we will talk a bit more

2452
00:50:21,319 --> 00:50:23,849
 

2453
00:50:21,329 --> 00:50:27,540
 about a ways to mitigate this in a few

2454
00:50:23,839 --> 00:50:27,540
 

2455
00:50:23,849 --> 00:50:29,160
 slides okay so now how does this

2456
00:50:27,530 --> 00:50:29,160
 

2457
00:50:27,540 --> 00:50:30,510
 approach compared to the black box out

2458
00:50:29,150 --> 00:50:30,510
 

2459
00:50:29,160 --> 00:50:32,910
 of it Asian perches that we mentioned

2460
00:50:30,500 --> 00:50:32,910
 

2461
00:50:30,510 --> 00:50:34,260
 before so let's bring up the general

2462
00:50:32,900 --> 00:50:34,260
 

2463
00:50:32,910 --> 00:50:35,760
 form that we talked about before where

2464
00:50:34,250 --> 00:50:35,760
 

2465
00:50:34,260 --> 00:50:37,140
 you have some narrow that were just

2466
00:50:35,750 --> 00:50:37,140
 

2467
00:50:35,760 --> 00:50:38,849
 taking a training data point or a

2468
00:50:37,130 --> 00:50:38,849
 

2469
00:50:37,140 --> 00:50:40,470
 training data set and a test data point

2470
00:50:38,839 --> 00:50:40,470
 

2471
00:50:38,849 --> 00:50:42,960
 and is producing a prediction for the

2472
00:50:40,460 --> 00:50:42,960
 

2473
00:50:40,470 --> 00:50:45,390
 test data point it turns out that you

2474
00:50:42,950 --> 00:50:45,390
 

2475
00:50:42,960 --> 00:50:48,359
 can view the optimization based approach

2476
00:50:45,380 --> 00:50:48,359
 

2477
00:50:45,390 --> 00:50:49,349
 in the same general form so before we

2478
00:50:48,349 --> 00:50:49,349
 

2479
00:50:48,359 --> 00:50:51,390
 were using like a recurrent neural

2480
00:50:49,339 --> 00:50:51,390
 

2481
00:50:49,349 --> 00:50:53,760
 network to represent this function but

2482
00:50:51,380 --> 00:50:53,760
 

2483
00:50:51,390 --> 00:50:55,410
 now we're using what I'll denote as F

2484
00:50:53,750 --> 00:50:55,410
 

2485
00:50:53,760 --> 00:50:57,540
 mammal to represent this function and

2486
00:50:55,400 --> 00:50:57,540
 

2487
00:50:55,410 --> 00:50:59,880
 that is the function with parameters Phi

2488
00:50:57,530 --> 00:50:59,880
 

2489
00:50:57,540 --> 00:51:03,210
 that takes an X tests and produces a

2490
00:50:59,870 --> 00:51:03,210
 

2491
00:50:59,880 --> 00:51:05,609
 prediction where Phi is defined as the

2492
00:51:03,200 --> 00:51:05,609
 

2493
00:51:03,210 --> 00:51:06,360
 initial meta parameters plus gradient

2494
00:51:05,599 --> 00:51:06,360
 

2495
00:51:05,609 --> 00:51:09,180
 descent on the

2496
00:51:06,350 --> 00:51:09,180
 

2497
00:51:06,360 --> 00:51:11,940
 data point and so essentially you can

2498
00:51:09,170 --> 00:51:11,940
 

2499
00:51:09,180 --> 00:51:14,430
 view the mammal algorithm as a

2500
00:51:11,930 --> 00:51:14,430
 

2501
00:51:11,940 --> 00:51:18,480
 computation graph just with this funny

2502
00:51:14,420 --> 00:51:18,480
 

2503
00:51:14,430 --> 00:51:21,270
 embedded gradient operator within it and

2504
00:51:18,470 --> 00:51:21,270
 

2505
00:51:18,480 --> 00:51:23,370
 so I really you can just view it as a

2506
00:51:21,260 --> 00:51:23,370
 

2507
00:51:21,270 --> 00:51:24,810
 very similar approach for doing it but

2508
00:51:23,360 --> 00:51:24,810
 

2509
00:51:23,370 --> 00:51:26,610
 one that has a lot more structure the

2510
00:51:24,800 --> 00:51:26,610
 

2511
00:51:24,810 --> 00:51:28,680
 structure namely of optimization within

2512
00:51:26,600 --> 00:51:28,680
 

2513
00:51:26,610 --> 00:51:30,090
 it and also with this view that means

2514
00:51:28,670 --> 00:51:30,090
 

2515
00:51:28,680 --> 00:51:31,470
 that we can very naturally mix and match

2516
00:51:30,080 --> 00:51:31,470
 

2517
00:51:30,090 --> 00:51:33,600
 components of these different approaches

2518
00:51:31,460 --> 00:51:33,600
 

2519
00:51:31,470 --> 00:51:34,980
 so for example one of the things we

2520
00:51:33,590 --> 00:51:34,980
 

2521
00:51:33,600 --> 00:51:37,650
 could do is learn the initialization

2522
00:51:34,970 --> 00:51:37,650
 

2523
00:51:34,980 --> 00:51:40,230
 that mammal is doing but also learn

2524
00:51:37,640 --> 00:51:40,230
 

2525
00:51:37,650 --> 00:51:42,480
 learn how to make gradient updates the

2526
00:51:40,220 --> 00:51:42,480
 

2527
00:51:40,230 --> 00:51:44,880
 initialization and that's exactly what

2528
00:51:42,470 --> 00:51:44,880
 

2529
00:51:42,480 --> 00:51:48,530
 ravi and la rochelle did in 2017 which

2530
00:51:44,870 --> 00:51:48,530
 

2531
00:51:44,880 --> 00:51:52,740
 actually preceded the mammal work and

2532
00:51:48,520 --> 00:51:52,740
 

2533
00:51:48,530 --> 00:51:55,620
 the and this computation graph view will

2534
00:51:52,730 --> 00:51:55,620
 

2535
00:51:52,740 --> 00:51:58,920
 come back again in as we discussed the

2536
00:51:55,610 --> 00:51:58,920
 

2537
00:51:55,620 --> 00:52:05,370
 third type of approach great so

2538
00:51:58,910 --> 00:52:05,370
 

2539
00:51:58,920 --> 00:52:06,900
 questions how is theta G learned this is

2540
00:52:05,360 --> 00:52:06,900
 

2541
00:52:05,370 --> 00:52:10,260
 coming back to the black box adaptation

2542
00:52:06,890 --> 00:52:10,260
 

2543
00:52:06,900 --> 00:52:12,270
 yeah so great question so theta G this

2544
00:52:10,250 --> 00:52:12,270
 

2545
00:52:10,260 --> 00:52:14,750
 is going back to the black box and in

2546
00:52:12,260 --> 00:52:14,750
 

2547
00:52:12,270 --> 00:52:16,590
 that case we had a we had a very like

2548
00:52:14,740 --> 00:52:16,590
 

2549
00:52:14,750 --> 00:52:18,420
 sufficient statistics age that are

2550
00:52:16,580 --> 00:52:18,420
 

2551
00:52:16,590 --> 00:52:20,730
 produced by the neural network and we

2552
00:52:18,410 --> 00:52:20,730
 

2553
00:52:18,420 --> 00:52:22,260
 also had theta G that was using to use

2554
00:52:20,720 --> 00:52:22,260
 

2555
00:52:20,730 --> 00:52:24,120
 to make predictions about the test data

2556
00:52:22,250 --> 00:52:24,120
 

2557
00:52:22,260 --> 00:52:25,530
 points and in that case theta G is

2558
00:52:24,110 --> 00:52:25,530
 

2559
00:52:24,120 --> 00:52:28,410
 optimized with all the other meta

2560
00:52:25,520 --> 00:52:28,410
 

2561
00:52:25,530 --> 00:52:29,900
 parameters of F so it's optimized just

2562
00:52:28,400 --> 00:52:29,900
 

2563
00:52:28,410 --> 00:52:31,950
 like all the other meta parameters

2564
00:52:29,890 --> 00:52:31,950
 

2565
00:52:29,900 --> 00:52:36,180
 another question about the black box

2566
00:52:31,940 --> 00:52:36,180
 

2567
00:52:31,950 --> 00:52:37,680
 setup tation so will a chai be trivial

2568
00:52:36,170 --> 00:52:37,680
 

2569
00:52:36,180 --> 00:52:40,020
 meaning like why wouldn't a chai just

2570
00:52:37,670 --> 00:52:40,020
 

2571
00:52:37,680 --> 00:52:41,160
 learn to recognize which task is fed in

2572
00:52:40,010 --> 00:52:41,160
 

2573
00:52:40,020 --> 00:52:45,380
 and just stop what sort of like a one

2574
00:52:41,150 --> 00:52:45,380
 

2575
00:52:41,160 --> 00:52:45,380
 hot indicator for one of the end tasks

2576
00:52:45,730 --> 00:52:45,730
 

2577
00:52:45,740 --> 00:52:53,250
 that's a good question I think that in

2578
00:52:50,030 --> 00:52:53,250
 

2579
00:52:50,040 --> 00:52:54,990
 practice it doesn't do you have an

2580
00:52:53,240 --> 00:52:54,990
 

2581
00:52:53,250 --> 00:52:56,880
 answer for that yeah maybe one way to

2582
00:52:54,980 --> 00:52:56,880
 

2583
00:52:54,990 --> 00:52:59,220
 think about this is it's kind of the

2584
00:52:56,870 --> 00:52:59,220
 

2585
00:52:56,880 --> 00:53:00,600
 same problem as memorizing labels for

2586
00:52:59,210 --> 00:53:00,600
 

2587
00:52:59,220 --> 00:53:01,860
 regular supervised learning so in the

2588
00:53:00,590 --> 00:53:01,860
 

2589
00:53:00,600 --> 00:53:03,930
 same way that supervised learning can

2590
00:53:01,850 --> 00:53:03,930
 

2591
00:53:01,860 --> 00:53:06,570
 overfit meta learning can met over fit

2592
00:53:03,920 --> 00:53:06,570
 

2593
00:53:03,930 --> 00:53:08,340
 so if you start seeing that your that

2594
00:53:06,560 --> 00:53:08,340
 

2595
00:53:06,570 --> 00:53:09,690
 your model just outputs a task indicator

2596
00:53:08,330 --> 00:53:09,690
 

2597
00:53:08,340 --> 00:53:11,100
 that can happen if you have a very small

2598
00:53:09,680 --> 00:53:11,100
 

2599
00:53:09,690 --> 00:53:12,390
 number of meta training tasks that's

2600
00:53:11,090 --> 00:53:12,390
 

2601
00:53:11,100 --> 00:53:13,680
 just an instance of overfitting we'll

2602
00:53:12,380 --> 00:53:13,680
 

2603
00:53:12,390 --> 00:53:14,630
 talk about metamorphic towards the end

2604
00:53:13,670 --> 00:53:14,630
 

2605
00:53:13,680 --> 00:53:18,930
 of the tutorial

2606
00:53:14,620 --> 00:53:18,930
 

2607
00:53:14,630 --> 00:53:19,890
 great oh one more another one I don't

2608
00:53:18,920 --> 00:53:19,890
 

2609
00:53:18,930 --> 00:53:21,869
 know if you know this but what's the

2610
00:53:19,880 --> 00:53:21,869
 

2611
00:53:19,890 --> 00:53:23,700
 gee from memory augmented neural

2612
00:53:21,859 --> 00:53:23,700
 

2613
00:53:21,869 --> 00:53:25,920
 networks yes oh that's a great question

2614
00:53:23,690 --> 00:53:25,920
 

2615
00:53:23,700 --> 00:53:28,740
 so the memory augmented neural networks

2616
00:53:25,910 --> 00:53:28,740
 

2617
00:53:25,920 --> 00:53:31,440
 question by santoro at all basically

2618
00:53:28,730 --> 00:53:31,440
 

2619
00:53:28,740 --> 00:53:32,789
 that just used a standard RNN to take in

2620
00:53:31,430 --> 00:53:32,789
 

2621
00:53:31,440 --> 00:53:35,880
 data points including the test data

2622
00:53:32,779 --> 00:53:35,880
 

2623
00:53:32,789 --> 00:53:37,470
 point and so in that case theta G was

2624
00:53:35,870 --> 00:53:37,470
 

2625
00:53:35,880 --> 00:53:41,460
 actually the same exact parameters as

2626
00:53:37,460 --> 00:53:41,460
 

2627
00:53:37,470 --> 00:53:43,680
 theta in F so H the statistics is simply

2628
00:53:41,450 --> 00:53:43,680
 

2629
00:53:41,460 --> 00:53:46,349
 the hidden state of an RNN and there is

2630
00:53:43,670 --> 00:53:46,349
 

2631
00:53:43,680 --> 00:53:48,980
 weight sharing across across theta G and

2632
00:53:46,339 --> 00:53:48,980
 

2633
00:53:46,349 --> 00:53:50,940
 theta an F where it's represented by the

2634
00:53:48,970 --> 00:53:50,940
 

2635
00:53:48,980 --> 00:53:53,640
 weights that are shared across time in a

2636
00:53:50,930 --> 00:53:53,640
 

2637
00:53:50,940 --> 00:53:56,069
 recurrent neural network so this one is

2638
00:53:53,630 --> 00:53:56,069
 

2639
00:53:53,640 --> 00:53:57,690
 a crowd favorite apparently how is all

2640
00:53:56,059 --> 00:53:57,690
 

2641
00:53:56,069 --> 00:53:59,309
 this related to hyper networks where we

2642
00:53:57,680 --> 00:53:59,309
 

2643
00:53:57,690 --> 00:54:00,299
 were interested in giving parameters of

2644
00:53:59,299 --> 00:54:00,299
 

2645
00:53:59,309 --> 00:54:04,019
 a model as output

2646
00:54:00,289 --> 00:54:04,019
 

2647
00:54:00,299 --> 00:54:06,329
 great yeah so the the first black box

2648
00:54:04,009 --> 00:54:06,329
 

2649
00:54:04,019 --> 00:54:08,490
 allocation approach is also what is done

2650
00:54:06,319 --> 00:54:08,490
 

2651
00:54:06,329 --> 00:54:09,599
 in in hyper networks I think that I'm

2652
00:54:08,480 --> 00:54:09,599
 

2653
00:54:08,490 --> 00:54:10,589
 not completely sure about this but I

2654
00:54:09,589 --> 00:54:10,589
 

2655
00:54:09,599 --> 00:54:12,029
 think that the hyper networks paper

2656
00:54:10,579 --> 00:54:12,029
 

2657
00:54:10,589 --> 00:54:14,250
 wasn't particularly focused on metal

2658
00:54:12,019 --> 00:54:14,250
 

2659
00:54:12,029 --> 00:54:15,569
 learning problems and looking looking at

2660
00:54:14,240 --> 00:54:15,569
 

2661
00:54:14,250 --> 00:54:17,279
 other problems where may be outputting

2662
00:54:15,559 --> 00:54:17,279
 

2663
00:54:15,569 --> 00:54:18,630
 parameters of neural networks but the

2664
00:54:17,269 --> 00:54:18,630
 

2665
00:54:17,279 --> 00:54:19,890
 approach the algorithm used is is

2666
00:54:18,620 --> 00:54:19,890
 

2667
00:54:18,630 --> 00:54:21,150
 exactly the same as the blackbox

2668
00:54:19,880 --> 00:54:21,150
 

2669
00:54:19,890 --> 00:54:23,760
 annotation approaches that I mentioned

2670
00:54:21,140 --> 00:54:23,760
 

2671
00:54:21,150 --> 00:54:26,339
 before this question I can answer myself

2672
00:54:23,750 --> 00:54:26,339
 

2673
00:54:23,760 --> 00:54:27,900
 can we adopt mammal in a framework that

2674
00:54:26,329 --> 00:54:27,900
 

2675
00:54:26,339 --> 00:54:30,210
 we don't have a batch of tasks ahead um

2676
00:54:27,890 --> 00:54:30,210
 

2677
00:54:27,900 --> 00:54:31,230
 instead we get them sequentially to hear

2678
00:54:30,200 --> 00:54:31,230
 

2679
00:54:30,210 --> 00:54:32,670
 the answer this question you'll have to

2680
00:54:31,220 --> 00:54:32,670
 

2681
00:54:31,230 --> 00:54:34,319
 wait until the very end of the tutorial

2682
00:54:32,660 --> 00:54:34,319
 

2683
00:54:32,670 --> 00:54:37,410
 on the last slide and she'll see we'll

2684
00:54:34,309 --> 00:54:37,410
 

2685
00:54:34,319 --> 00:54:40,829
 answer it okay one quick audience

2686
00:54:37,400 --> 00:54:40,829
 

2687
00:54:37,410 --> 00:54:44,549
 question okay um so this is a question

2688
00:54:40,819 --> 00:54:44,549
 

2689
00:54:40,829 --> 00:54:46,829
 about FML so if I understand correctly

2690
00:54:44,539 --> 00:54:46,829
 

2691
00:54:44,549 --> 00:54:49,200
 that MMO essentially learns the base

2692
00:54:46,819 --> 00:54:49,200
 

2693
00:54:46,829 --> 00:54:51,240
 model which kind of in the middle of

2694
00:54:49,190 --> 00:54:51,240
 

2695
00:54:49,200 --> 00:54:54,869
 like a the optimal parameter for

2696
00:54:51,230 --> 00:54:54,869
 

2697
00:54:51,240 --> 00:54:57,210
 different tasks but it's it is working

2698
00:54:54,859 --> 00:54:57,210
 

2699
00:54:54,869 --> 00:54:59,069
 underlying assumption that those optimal

2700
00:54:57,200 --> 00:54:59,069
 

2701
00:54:57,210 --> 00:55:00,809
 parameter for those different tasks are

2702
00:54:59,059 --> 00:55:00,809
 

2703
00:54:59,069 --> 00:55:03,720
 now too far away

2704
00:55:00,799 --> 00:55:03,720
 

2705
00:55:00,809 --> 00:55:06,299
 so if depth in some kind of if you are

2706
00:55:03,710 --> 00:55:06,299
 

2707
00:55:03,720 --> 00:55:10,380
 choosing a model space that the optimal

2708
00:55:06,289 --> 00:55:10,380
 

2709
00:55:06,299 --> 00:55:12,809
 like parameter or far away so that maybe

2710
00:55:10,370 --> 00:55:12,809
 

2711
00:55:10,380 --> 00:55:16,349
 you're the middle point of the base

2712
00:55:12,799 --> 00:55:16,349
 

2713
00:55:12,809 --> 00:55:19,140
 below although it is not too far away

2714
00:55:16,339 --> 00:55:19,140
 

2715
00:55:16,349 --> 00:55:22,380
 from each of the optimal parameter it

2716
00:55:19,130 --> 00:55:22,380
 

2717
00:55:19,140 --> 00:55:24,829
 might be not optimal for any of those I

2718
00:55:22,370 --> 00:55:24,829
 

2719
00:55:22,380 --> 00:55:27,509
 noticed it in the original paper that

2720
00:55:24,819 --> 00:55:27,509
 

2721
00:55:24,829 --> 00:55:30,500
 you are basically using a pretty simple

2722
00:55:27,499 --> 00:55:30,500
 

2723
00:55:27,509 --> 00:55:32,080
 commercial neural net which have fewer

2724
00:55:30,490 --> 00:55:32,080
 

2725
00:55:30,500 --> 00:55:34,960
 neurons

2726
00:55:32,070 --> 00:55:34,960
 

2727
00:55:32,080 --> 00:55:37,930
 I'm just wondering will that mmm works

2728
00:55:34,950 --> 00:55:37,930
 

2729
00:55:34,960 --> 00:55:39,760
 keeps its performance like if you are

2730
00:55:37,920 --> 00:55:39,760
 

2731
00:55:37,930 --> 00:55:41,980
 using a more complex model where its

2732
00:55:39,750 --> 00:55:41,980
 

2733
00:55:39,760 --> 00:55:44,770
 parameter space is more complex and

2734
00:55:41,970 --> 00:55:44,770
 

2735
00:55:41,980 --> 00:55:47,050
 beaker yeah so first it's mentioning

2736
00:55:44,760 --> 00:55:47,050
 

2737
00:55:44,770 --> 00:55:48,310
 that this diagram we like to use for an

2738
00:55:47,040 --> 00:55:48,310
 

2739
00:55:47,050 --> 00:55:49,540
 illustration purposes in terms of

2740
00:55:48,300 --> 00:55:49,540
 

2741
00:55:48,310 --> 00:55:50,980
 understanding the algorithm but it can

2742
00:55:49,530 --> 00:55:50,980
 

2743
00:55:49,540 --> 00:55:53,140
 also be a bit misleading which is that

2744
00:55:50,970 --> 00:55:53,140
 

2745
00:55:50,980 --> 00:55:54,910
 in many cases particularly with heavily

2746
00:55:53,130 --> 00:55:54,910
 

2747
00:55:53,140 --> 00:55:56,620
 over parametrized neural networks there

2748
00:55:54,900 --> 00:55:56,620
 

2749
00:55:54,910 --> 00:55:57,730
 isn't just a single optimum for the

2750
00:55:56,610 --> 00:55:57,730
 

2751
00:55:56,620 --> 00:56:01,360
 correct solution there's actually an

2752
00:55:57,720 --> 00:56:01,360
 

2753
00:55:57,730 --> 00:56:03,250
 entire space of optimum and and with

2754
00:56:01,350 --> 00:56:03,250
 

2755
00:56:01,360 --> 00:56:05,170
 with those types of problems we see that

2756
00:56:03,240 --> 00:56:05,170
 

2757
00:56:03,250 --> 00:56:06,820
 the it is actually a little bit easier

2758
00:56:05,160 --> 00:56:06,820
 

2759
00:56:05,170 --> 00:56:09,100
 to find something where you're simply

2760
00:56:06,810 --> 00:56:09,100
 

2761
00:56:06,820 --> 00:56:10,660
 one or a few gradient steps away and in

2762
00:56:09,090 --> 00:56:10,660
 

2763
00:56:09,100 --> 00:56:13,090
 fact in a minute I'll talk about the

2764
00:56:10,650 --> 00:56:13,090
 

2765
00:56:10,660 --> 00:56:15,100
 expressive power of of the mammal

2766
00:56:13,080 --> 00:56:15,100
 

2767
00:56:13,090 --> 00:56:16,660
 algorithm and its ability to adapt even

2768
00:56:15,090 --> 00:56:16,660
 

2769
00:56:15,100 --> 00:56:18,670
 when your tasks are extremely different

2770
00:56:16,650 --> 00:56:18,670
 

2771
00:56:16,660 --> 00:56:20,740
 with regard to architectures I'll talk a

2772
00:56:18,660 --> 00:56:20,740
 

2773
00:56:18,670 --> 00:56:22,600
 bit about that but in practice we do

2774
00:56:20,730 --> 00:56:22,600
 

2775
00:56:20,740 --> 00:56:25,300
 find that it scales well to to larger

2776
00:56:22,590 --> 00:56:25,300
 

2777
00:56:22,600 --> 00:56:26,650
 larger architectures but it may require

2778
00:56:25,290 --> 00:56:26,650
 

2779
00:56:25,300 --> 00:56:29,040
 a bit more tuning than other meta

2780
00:56:26,640 --> 00:56:29,040
 

2781
00:56:26,650 --> 00:56:33,190
 learning methods okay thank you

2782
00:56:29,030 --> 00:56:33,190
 

2783
00:56:29,040 --> 00:56:36,370
 okay so great leading off where we left

2784
00:56:33,180 --> 00:56:36,370
 

2785
00:56:33,190 --> 00:56:38,290
 off so man will exhibits this kind of

2786
00:56:36,360 --> 00:56:38,290
 

2787
00:56:36,370 --> 00:56:39,790
 structure unlike the black box out

2788
00:56:38,280 --> 00:56:39,790
 

2789
00:56:38,290 --> 00:56:41,320
 updation approaches which it has this

2790
00:56:39,780 --> 00:56:41,320
 

2791
00:56:39,790 --> 00:56:43,870
 gradient operator inside of it and so

2792
00:56:41,310 --> 00:56:43,870
 

2793
00:56:41,320 --> 00:56:46,120
 it's actually performing an optimization

2794
00:56:43,860 --> 00:56:46,120
 

2795
00:56:43,870 --> 00:56:47,620
 problem both within the Mediterranean

2796
00:56:46,110 --> 00:56:47,620
 

2797
00:56:46,120 --> 00:56:50,020
 process as well as that meta test time

2798
00:56:47,610 --> 00:56:50,020
 

2799
00:56:47,620 --> 00:56:51,640
 and so one thing that might be quite

2800
00:56:50,010 --> 00:56:51,640
 

2801
00:56:50,020 --> 00:56:53,440
 natural to ask is using that structure

2802
00:56:51,630 --> 00:56:53,440
 

2803
00:56:51,640 --> 00:56:55,660
 does that mean that we can generalize

2804
00:56:53,430 --> 00:56:55,660
 

2805
00:56:53,440 --> 00:56:57,910
 better to two tasks that are slightly

2806
00:56:55,650 --> 00:56:57,910
 

2807
00:56:55,660 --> 00:57:00,280
 out of distribution and this is of

2808
00:56:57,900 --> 00:57:00,280
 

2809
00:56:57,910 --> 00:57:01,840
 course an empirical question that we're

2810
00:57:00,270 --> 00:57:01,840
 

2811
00:57:00,280 --> 00:57:03,760
 gonna study more so than a theoretical

2812
00:57:01,830 --> 00:57:03,760
 

2813
00:57:01,840 --> 00:57:06,520
 question and so what we're gonna do is

2814
00:57:03,750 --> 00:57:06,520
 

2815
00:57:03,760 --> 00:57:08,680
 we're gonna compare mammal with with

2816
00:57:06,510 --> 00:57:08,680
 

2817
00:57:06,520 --> 00:57:11,080
 black box adaptation approaches such as

2818
00:57:08,670 --> 00:57:11,080
 

2819
00:57:08,680 --> 00:57:12,670
 snail and meta networks and we looked at

2820
00:57:11,070 --> 00:57:12,670
 

2821
00:57:11,080 --> 00:57:14,020
 image the omnigul and image

2822
00:57:12,660 --> 00:57:14,020
 

2823
00:57:12,670 --> 00:57:15,880
 classification problem and we tried to

2824
00:57:14,010 --> 00:57:15,880
 

2825
00:57:14,020 --> 00:57:17,380
 plot the task variability versus

2826
00:57:15,870 --> 00:57:17,380
 

2827
00:57:15,880 --> 00:57:19,780
 performance and what we found

2828
00:57:17,370 --> 00:57:19,780
 

2829
00:57:17,380 --> 00:57:22,120
 consistently across the board is as we

2830
00:57:19,770 --> 00:57:22,120
 

2831
00:57:19,780 --> 00:57:25,060
 move away from the the Mediterranean

2832
00:57:22,110 --> 00:57:25,060
 

2833
00:57:22,120 --> 00:57:27,670
 asks with either zero zero shear or a

2834
00:57:25,050 --> 00:57:27,670
 

2835
00:57:25,060 --> 00:57:29,830
 scale of one we see that of course

2836
00:57:27,660 --> 00:57:29,830
 

2837
00:57:27,670 --> 00:57:31,270
 performance drops for all approaches but

2838
00:57:29,820 --> 00:57:31,270
 

2839
00:57:29,830 --> 00:57:33,250
 that mammal is able to perform better

2840
00:57:31,260 --> 00:57:33,250
 

2841
00:57:31,270 --> 00:57:34,840
 because it has the structure of being

2842
00:57:33,240 --> 00:57:34,840
 

2843
00:57:33,250 --> 00:57:38,110
 able to run in gradient descent at test

2844
00:57:34,830 --> 00:57:38,110
 

2845
00:57:34,840 --> 00:57:39,970
 time and at the very least you are still

2846
00:57:38,100 --> 00:57:39,970
 

2847
00:57:38,110 --> 00:57:41,400
 running gradient descent so you'll you

2848
00:57:39,960 --> 00:57:41,400
 

2849
00:57:39,970 --> 00:57:45,160
 won't be doing significantly worse than

2850
00:57:41,390 --> 00:57:45,160
 

2851
00:57:41,400 --> 00:57:45,430
 then what you might be doing with with

2852
00:57:45,150 --> 00:57:45,430
 

2853
00:57:45,160 --> 00:57:47,079
 an arrow

2854
00:57:45,420 --> 00:57:47,079
 

2855
00:57:45,430 --> 00:57:48,970
 that work that you really don't get

2856
00:57:47,069 --> 00:57:48,970
 

2857
00:57:47,079 --> 00:57:50,940
 can't really say anything about if it's

2858
00:57:48,960 --> 00:57:50,940
 

2859
00:57:48,970 --> 00:57:54,190
 just outputting parameters for example

2860
00:57:50,930 --> 00:57:54,190
 

2861
00:57:50,940 --> 00:57:56,290
 okay so you might say well we get this

2862
00:57:54,180 --> 00:57:56,290
 

2863
00:57:54,190 --> 00:57:58,599
 nice structure but does this come at a

2864
00:57:56,280 --> 00:57:58,599
 

2865
00:57:56,290 --> 00:58:00,670
 cost like as the question alluded to

2866
00:57:58,589 --> 00:58:00,670
 

2867
00:57:58,599 --> 00:58:02,170
 before do the did you need to assume

2868
00:58:00,660 --> 00:58:02,170
 

2869
00:58:00,670 --> 00:58:04,960
 that the task parameters are very close

2870
00:58:02,160 --> 00:58:04,960
 

2871
00:58:02,170 --> 00:58:06,220
 to each other for different tasks and so

2872
00:58:04,950 --> 00:58:06,220
 

2873
00:58:04,960 --> 00:58:08,290
 we studied this question by studying the

2874
00:58:06,210 --> 00:58:08,290
 

2875
00:58:06,220 --> 00:58:10,750
 expressive power of a single gradient

2876
00:58:08,280 --> 00:58:10,750
 

2877
00:58:08,290 --> 00:58:13,690
 step basically the update that's used in

2878
00:58:10,740 --> 00:58:13,690
 

2879
00:58:10,750 --> 00:58:15,700
 the mammal function and what we can say

2880
00:58:13,680 --> 00:58:15,700
 

2881
00:58:13,690 --> 00:58:17,619
 is that actually for sufficiently deep

2882
00:58:15,690 --> 00:58:17,619
 

2883
00:58:15,700 --> 00:58:19,180
 neural network function f the male

2884
00:58:17,609 --> 00:58:19,180
 

2885
00:58:17,619 --> 00:58:20,859
 function on the right can represent

2886
00:58:19,170 --> 00:58:20,859
 

2887
00:58:19,180 --> 00:58:22,390
 anything that the recurrent neural

2888
00:58:20,849 --> 00:58:22,390
 

2889
00:58:20,859 --> 00:58:24,099
 network can represent on the Left which

2890
00:58:22,380 --> 00:58:24,099
 

2891
00:58:22,390 --> 00:58:27,369
 is that it can represent any function of

2892
00:58:24,089 --> 00:58:27,369
 

2893
00:58:24,099 --> 00:58:28,569
 the training data and the test input and

2894
00:58:27,359 --> 00:58:28,569
 

2895
00:58:27,369 --> 00:58:30,490
 we can show this under a few assumptions

2896
00:58:28,559 --> 00:58:30,490
 

2897
00:58:28,569 --> 00:58:32,109
 that are relatively mild such as the non

2898
00:58:30,480 --> 00:58:32,109
 

2899
00:58:30,490 --> 00:58:34,420
 zero learning rate as well as unique

2900
00:58:32,099 --> 00:58:34,420
 

2901
00:58:32,109 --> 00:58:36,040
 data points in the training data set and

2902
00:58:34,410 --> 00:58:36,040
 

2903
00:58:34,420 --> 00:58:37,540
 the reason why this is interesting is

2904
00:58:36,030 --> 00:58:37,540
 

2905
00:58:36,040 --> 00:58:40,180
 that it means that mammal has the

2906
00:58:37,530 --> 00:58:40,180
 

2907
00:58:37,540 --> 00:58:42,280
 inductive bias of optimization for easy

2908
00:58:40,170 --> 00:58:42,280
 

2909
00:58:40,180 --> 00:58:44,020
 procedures being embedded within it but

2910
00:58:42,270 --> 00:58:44,020
 

2911
00:58:42,280 --> 00:58:46,270
 without losing the expressive power of

2912
00:58:44,010 --> 00:58:46,270
 

2913
00:58:44,020 --> 00:58:47,829
 gradient descent or without losing the

2914
00:58:46,260 --> 00:58:47,829
 

2915
00:58:46,270 --> 00:58:51,849
 expressive power of deep recurrent

2916
00:58:47,819 --> 00:58:51,849
 

2917
00:58:47,829 --> 00:58:54,220
 neural networks okay great so let's go

2918
00:58:51,839 --> 00:58:54,220
 

2919
00:58:51,849 --> 00:58:55,150
 back to some of the motivation that we

2920
00:58:54,210 --> 00:58:55,150
 

2921
00:58:54,220 --> 00:58:57,400
 talked about a little bit with

2922
00:58:55,140 --> 00:58:57,400
 

2923
00:58:55,150 --> 00:58:58,299
 optimization based approaches where

2924
00:58:57,390 --> 00:58:58,299
 

2925
00:58:57,400 --> 00:59:01,390
 we're saying that the meta parameters

2926
00:58:58,289 --> 00:59:01,390
 

2927
00:58:58,299 --> 00:59:03,099
 serve as a prior and we talked about how

2928
00:59:01,380 --> 00:59:03,099
 

2929
00:59:01,390 --> 00:59:05,740
 one form of prior knowledge is is

2930
00:59:03,089 --> 00:59:05,740
 

2931
00:59:03,099 --> 00:59:07,480
 initialization for fine-tuning can we

2932
00:59:05,730 --> 00:59:07,480
 

2933
00:59:05,740 --> 00:59:09,940
 make this a bit more formal and actually

2934
00:59:07,470 --> 00:59:09,940
 

2935
00:59:07,480 --> 00:59:12,849
 better characterize what's work sort of

2936
00:59:09,930 --> 00:59:12,849
 

2937
00:59:09,940 --> 00:59:15,609
 prior that is it mammal is imposing on

2938
00:59:12,839 --> 00:59:15,609
 

2939
00:59:12,849 --> 00:59:17,109
 the learning process and to do so we're

2940
00:59:15,599 --> 00:59:17,109
 

2941
00:59:15,609 --> 00:59:19,630
 going to look at Bayesian meta learning

2942
00:59:17,099 --> 00:59:19,630
 

2943
00:59:17,109 --> 00:59:21,819
 approaches that use graphical models so

2944
00:59:19,620 --> 00:59:21,819
 

2945
00:59:19,630 --> 00:59:22,960
 this is a kind of a graphical model

2946
00:59:21,809 --> 00:59:22,960
 

2947
00:59:21,819 --> 00:59:25,089
 similar to the one that Sergei showed

2948
00:59:22,950 --> 00:59:25,089
 

2949
00:59:22,960 --> 00:59:27,010
 before where Phi is the toss Pacific

2950
00:59:25,079 --> 00:59:27,010
 

2951
00:59:25,089 --> 00:59:30,670
 parameters and theta is the meta

2952
00:59:27,000 --> 00:59:30,670
 

2953
00:59:27,010 --> 00:59:32,680
 parameters so if you want to do do meta

2954
00:59:30,660 --> 00:59:32,680
 

2955
00:59:30,670 --> 00:59:34,390
 learning or learn a prior theta in this

2956
00:59:32,670 --> 00:59:34,390
 

2957
00:59:32,680 --> 00:59:35,559
 graphical model it's gonna look like the

2958
00:59:34,380 --> 00:59:35,559
 

2959
00:59:34,390 --> 00:59:36,880
 following optimization where we're

2960
00:59:35,549 --> 00:59:36,880
 

2961
00:59:35,559 --> 00:59:39,099
 optimizing the logs likelihood of the

2962
00:59:36,870 --> 00:59:39,099
 

2963
00:59:36,880 --> 00:59:40,990
 data given given the parameters you can

2964
00:59:39,089 --> 00:59:40,990
 

2965
00:59:39,099 --> 00:59:42,250
 write this out similar to the equations

2966
00:59:40,980 --> 00:59:42,250
 

2967
00:59:40,990 --> 00:59:43,809
 that survey showed earlier as an

2968
00:59:42,240 --> 00:59:43,809
 

2969
00:59:42,250 --> 00:59:45,599
 integration over the top specific

2970
00:59:43,799 --> 00:59:45,599
 

2971
00:59:43,809 --> 00:59:49,930
 parameters Phi which are not observed

2972
00:59:45,589 --> 00:59:49,930
 

2973
00:59:45,599 --> 00:59:55,119
 and the and this isn't simply empirical

2974
00:59:49,920 --> 00:59:55,119
 

2975
00:59:49,930 --> 00:59:55,700
 pains and the we can approximate is is

2976
00:59:55,109 --> 00:59:55,700
 

2977
00:59:55,119 --> 00:59:56,720
 it's quite

2978
00:59:55,690 --> 00:59:56,720
 

2979
00:59:55,700 --> 00:59:58,040
 tractable it's the one thing we could do

2980
00:59:56,710 --> 00:59:58,040
 

2981
00:59:56,720 --> 00:59:59,990
 is we could approximate this integral

2982
00:59:58,030 --> 00:59:59,990
 

2983
00:59:58,040 --> 01:00:02,570
 with the maximum an a posteriori

2984
00:59:59,980 --> 01:00:02,570
 

2985
00:59:59,990 --> 01:00:06,530
 estimate of this how specific parameters

2986
01:00:02,560 --> 01:00:06,530
 

2987
01:00:02,570 --> 01:00:09,740
 Phi I and this is a fairly crude

2988
01:00:06,520 --> 01:00:09,740
 

2989
01:00:06,530 --> 01:00:10,940
 approximation but it but one thing is

2990
01:00:09,730 --> 01:00:10,940
 

2991
01:00:09,740 --> 01:00:13,880
 interestingly that we can show is if you

2992
01:00:10,930 --> 01:00:13,880
 

2993
01:00:10,940 --> 01:00:15,730
 compute the map estimate basically

2994
01:00:13,870 --> 01:00:15,730
 

2995
01:00:13,880 --> 01:00:18,109
 gradient descent with early stopping

2996
01:00:15,720 --> 01:00:18,109
 

2997
01:00:15,730 --> 01:00:20,210
 corresponds to map inference under a

2998
01:00:18,099 --> 01:00:20,210
 

2999
01:00:18,109 --> 01:00:22,339
 Gaussian prior with mean that theta and

3000
01:00:20,200 --> 01:00:22,339
 

3001
01:00:20,210 --> 01:00:24,530
 a variance that's a function of the

3002
01:00:22,329 --> 01:00:24,530
 

3003
01:00:22,339 --> 01:00:26,990
 number of gradient descent steps and the

3004
01:00:24,520 --> 01:00:26,990
 

3005
01:00:24,530 --> 01:00:28,579
 learning rate and this is exact in the

3006
01:00:26,980 --> 01:00:28,579
 

3007
01:00:26,990 --> 01:00:30,440
 in the linear case and approximate

3008
01:00:28,569 --> 01:00:30,440
 

3009
01:00:28,579 --> 01:00:33,109
 approximate in the non-linear case and

3010
01:00:30,430 --> 01:00:33,109
 

3011
01:00:30,440 --> 01:00:35,119
 so what we can see is that through this

3012
01:00:33,099 --> 01:00:35,119
 

3013
01:00:33,109 --> 01:00:36,800
 approximate approximate equivalents as

3014
01:00:35,109 --> 01:00:36,800
 

3015
01:00:35,119 --> 01:00:38,960
 well as Thea prop the approximation of

3016
01:00:36,790 --> 01:00:38,960
 

3017
01:00:36,800 --> 01:00:41,420
 the integral with the map estimate is

3018
01:00:38,950 --> 01:00:41,420
 

3019
01:00:38,960 --> 01:00:43,089
 that mammal is approximating inference

3020
01:00:41,410 --> 01:00:43,089
 

3021
01:00:41,420 --> 01:00:45,020
 in this hierarchical bayesian model

3022
01:00:43,079 --> 01:00:45,020
 

3023
01:00:43,089 --> 01:00:46,910
 which I think is useful for providing

3024
01:00:45,010 --> 01:00:46,910
 

3025
01:00:45,020 --> 01:00:49,070
 some intuition for the types of priors

3026
01:00:46,900 --> 01:00:49,070
 

3027
01:00:46,910 --> 01:00:52,880
 that we're learning in the meta learning

3028
01:00:49,060 --> 01:00:52,880
 

3029
01:00:49,070 --> 01:00:54,980
 process okay so mammal is a form of

3030
01:00:52,870 --> 01:00:54,980
 

3031
01:00:52,880 --> 01:00:56,359
 implicit prior are there other forms of

3032
01:00:54,970 --> 01:00:56,359
 

3033
01:00:54,980 --> 01:00:58,849
 priors that we can impose on the

3034
01:00:56,349 --> 01:00:58,849
 

3035
01:00:56,359 --> 01:00:59,990
 optimization procedure one thing we

3036
01:00:58,839 --> 01:00:59,990
 

3037
01:00:58,849 --> 01:01:01,910
 could do is do grain descent with an

3038
01:00:59,980 --> 01:01:01,910
 

3039
01:00:59,990 --> 01:01:03,440
 explicit Gaussian prior the log

3040
01:01:01,900 --> 01:01:03,440
 

3041
01:01:01,910 --> 01:01:04,819
 likelihood of a Gaussian shown here and

3042
01:01:03,430 --> 01:01:04,819
 

3043
01:01:03,440 --> 01:01:07,760
 this is what was done by Reuters Warren

3044
01:01:04,809 --> 01:01:07,760
 

3045
01:01:04,819 --> 01:01:12,020
 at all in the implicit mammal paper we

3046
01:01:07,750 --> 01:01:12,020
 

3047
01:01:07,760 --> 01:01:14,030
 could also have a prior used in Bayesian

3048
01:01:12,010 --> 01:01:14,030
 

3049
01:01:12,020 --> 01:01:16,099
 linear regression in this case we can't

3050
01:01:14,020 --> 01:01:16,099
 

3051
01:01:14,030 --> 01:01:17,359
 and put impose a prior on all weights of

3052
01:01:16,089 --> 01:01:17,359
 

3053
01:01:16,099 --> 01:01:19,160
 the neural network that would be

3054
01:01:17,349 --> 01:01:19,160
 

3055
01:01:17,359 --> 01:01:20,900
 intractable but we can impose it on the

3056
01:01:19,150 --> 01:01:20,900
 

3057
01:01:19,160 --> 01:01:22,430
 last layer of the neural network and do

3058
01:01:20,890 --> 01:01:22,430
 

3059
01:01:20,900 --> 01:01:24,319
 that on top of metal earned features

3060
01:01:22,420 --> 01:01:24,319
 

3061
01:01:22,430 --> 01:01:27,829
 this was done in all pakka

3062
01:01:24,309 --> 01:01:27,829
 

3063
01:01:24,319 --> 01:01:29,930
 another type of mammal and we can also

3064
01:01:27,819 --> 01:01:29,930
 

3065
01:01:27,829 --> 01:01:31,790
 this is moving more away from Bayesian

3066
01:01:29,920 --> 01:01:31,790
 

3067
01:01:29,930 --> 01:01:34,010
 methods but we can also do other forms

3068
01:01:31,780 --> 01:01:34,010
 

3069
01:01:31,790 --> 01:01:35,510
 of optimization on the last layer of the

3070
01:01:34,000 --> 01:01:35,510
 

3071
01:01:34,010 --> 01:01:36,800
 neural networks such as doing Ridge

3072
01:01:35,500 --> 01:01:36,800
 

3073
01:01:35,510 --> 01:01:38,660
 regression logistic regression or

3074
01:01:36,790 --> 01:01:38,660
 

3075
01:01:36,800 --> 01:01:40,790
 support vector machines and this is uh

3076
01:01:38,650 --> 01:01:40,790
 

3077
01:01:38,660 --> 01:01:42,829
 this forward priors essentially that we

3078
01:01:40,780 --> 01:01:42,829
 

3079
01:01:40,790 --> 01:01:44,660
 want features that are useful for linear

3080
01:01:42,819 --> 01:01:44,660
 

3081
01:01:42,829 --> 01:01:46,790
 classification that can be performed

3082
01:01:44,650 --> 01:01:46,790
 

3083
01:01:44,660 --> 01:01:49,460
 with these methods and to my knowledge

3084
01:01:46,780 --> 01:01:49,460
 

3085
01:01:46,790 --> 01:01:51,319
 is this last approach meta objet as the

3086
01:01:49,450 --> 01:01:51,319
 

3087
01:01:49,460 --> 01:01:56,359
 current state of the art on tshat image

3088
01:01:51,309 --> 01:01:56,359
 

3089
01:01:51,319 --> 01:01:57,530
 recognition benchmarks okay so now that

3090
01:01:56,349 --> 01:01:57,530
 

3091
01:01:56,359 --> 01:01:58,849
 we've talked about optimization based

3092
01:01:57,520 --> 01:01:58,849
 

3093
01:01:57,530 --> 01:02:00,800
 approaches let's go through a couple

3094
01:01:58,839 --> 01:02:00,800
 

3095
01:01:58,849 --> 01:02:01,849
 challenges with them so one challenge is

3096
01:02:00,790 --> 01:02:01,849
 

3097
01:02:00,800 --> 01:02:04,579
 how do you choose an architecture that's

3098
01:02:01,839 --> 01:02:04,579
 

3099
01:02:01,849 --> 01:02:07,369
 effective for embedding this integrated

3100
01:02:04,569 --> 01:02:07,369
 

3101
01:02:04,579 --> 01:02:08,000
 into some procedure and one way to do

3102
01:02:07,359 --> 01:02:08,000
 

3103
01:02:07,369 --> 01:02:10,340
 this is

3104
01:02:07,990 --> 01:02:10,340
 

3105
01:02:08,000 --> 01:02:11,450
 to do architecture search and one of the

3106
01:02:10,330 --> 01:02:11,450
 

3107
01:02:10,340 --> 01:02:13,160
 interesting things that was found in

3108
01:02:11,440 --> 01:02:13,160
 

3109
01:02:11,450 --> 01:02:14,420
 this paper is it found that highly

3110
01:02:13,150 --> 01:02:14,420
 

3111
01:02:13,160 --> 01:02:16,490
 non-standard architectures that were

3112
01:02:14,410 --> 01:02:16,490
 

3113
01:02:14,420 --> 01:02:18,890
 very very deep and very narrow we're

3114
01:02:16,480 --> 01:02:18,890
 

3115
01:02:16,490 --> 01:02:20,330
 quite effective for use with mammal and

3116
01:02:18,880 --> 01:02:20,330
 

3117
01:02:18,890 --> 01:02:22,010
 this is a bit different from standard

3118
01:02:20,320 --> 01:02:22,010
 

3119
01:02:20,330 --> 01:02:23,510
 architectures that work well for

3120
01:02:22,000 --> 01:02:23,510
 

3121
01:02:22,010 --> 01:02:26,570
 standard supervised learning problems

3122
01:02:23,500 --> 01:02:26,570
 

3123
01:02:23,510 --> 01:02:28,130
 and I'm particular on the mini image

3124
01:02:26,560 --> 01:02:28,130
 

3125
01:02:26,570 --> 01:02:30,800
 that five-way five-shot classification

3126
01:02:28,120 --> 01:02:30,800
 

3127
01:02:28,130 --> 01:02:32,900
 proceed benchmark mammal with the basic

3128
01:02:30,790 --> 01:02:32,900
 

3129
01:02:30,800 --> 01:02:33,710
 architecture as he achieves around 63

3130
01:02:32,890 --> 01:02:33,710
 

3131
01:02:32,900 --> 01:02:36,140
 percent accuracy

3132
01:02:33,700 --> 01:02:36,140
 

3133
01:02:33,710 --> 01:02:38,300
 well mammal with the architecture search

3134
01:02:36,130 --> 01:02:38,300
 

3135
01:02:36,140 --> 01:02:40,460
 is able to achieve 74 percent accuracy a

3136
01:02:38,290 --> 01:02:40,460
 

3137
01:02:38,300 --> 01:02:41,900
 pretty substantial boost by actually

3138
01:02:40,450 --> 01:02:41,900
 

3139
01:02:40,460 --> 01:02:43,000
 tuning the architecture that works well

3140
01:02:41,890 --> 01:02:43,000
 

3141
01:02:41,900 --> 01:02:45,320
 for it

3142
01:02:42,990 --> 01:02:45,320
 

3143
01:02:43,000 --> 01:02:46,880
 lastly one other challenge that you come

3144
01:02:45,310 --> 01:02:46,880
 

3145
01:02:45,320 --> 01:02:49,190
 up with with the medal the mammal

3146
01:02:46,870 --> 01:02:49,190
 

3147
01:02:46,880 --> 01:02:51,230
 algorithm is that you run into

3148
01:02:49,180 --> 01:02:51,230
 

3149
01:02:49,190 --> 01:02:53,420
 second-order optimization procedure and

3150
01:02:51,220 --> 01:02:53,420
 

3151
01:02:51,230 --> 01:02:56,240
 this can exhibit different and different

3152
01:02:53,410 --> 01:02:56,240
 

3153
01:02:53,420 --> 01:02:58,760
 instabilities one idea for trying to

3154
01:02:56,230 --> 01:02:58,760
 

3155
01:02:56,240 --> 01:03:00,200
 mitigate this is really the the dumbest

3156
01:02:58,750 --> 01:03:00,200
 

3157
01:02:58,760 --> 01:03:02,930
 idea you can come up with is to assume

3158
01:03:00,190 --> 01:03:02,930
 

3159
01:03:00,200 --> 01:03:05,270
 that the Jacobian of v with respect to

3160
01:03:02,920 --> 01:03:05,270
 

3161
01:03:02,930 --> 01:03:06,890
 theta is identity and simply copy the

3162
01:03:05,260 --> 01:03:06,890
 

3163
01:03:05,270 --> 01:03:09,290
 gradient with respect to Phi to be the

3164
01:03:06,880 --> 01:03:09,290
 

3165
01:03:06,890 --> 01:03:10,790
 gradient with respect to theta and it

3166
01:03:09,280 --> 01:03:10,790
 

3167
01:03:09,290 --> 01:03:13,130
 actually works somewhat surprisingly

3168
01:03:10,780 --> 01:03:13,130
 

3169
01:03:10,790 --> 01:03:15,050
 well oddly enough on relatively simple

3170
01:03:13,120 --> 01:03:15,050
 

3171
01:03:13,130 --> 01:03:16,370
 problems although anecdotally we found

3172
01:03:15,040 --> 01:03:16,370
 

3173
01:03:15,050 --> 01:03:17,750
 it not to work well as you try to move

3174
01:03:16,360 --> 01:03:17,750
 

3175
01:03:16,370 --> 01:03:18,710
 towards more complex problems like

3176
01:03:17,740 --> 01:03:18,710
 

3177
01:03:17,750 --> 01:03:21,920
 imitation learning and reinforcement

3178
01:03:18,700 --> 01:03:21,920
 

3179
01:03:18,710 --> 01:03:23,060
 learning I another thing you can do is

3180
01:03:21,910 --> 01:03:23,060
 

3181
01:03:21,920 --> 01:03:26,360
 automatically learn the inner and outer

3182
01:03:23,050 --> 01:03:26,360
 

3183
01:03:23,060 --> 01:03:28,040
 learning rates you can also optimize

3184
01:03:26,350 --> 01:03:28,040
 

3185
01:03:26,360 --> 01:03:30,110
 only a subset of parameters in the inner

3186
01:03:28,030 --> 01:03:30,110
 

3187
01:03:28,040 --> 01:03:32,600
 loop such as the last layer or or affine

3188
01:03:30,100 --> 01:03:32,600
 

3189
01:03:30,110 --> 01:03:34,400
 transformations at each layer you could

3190
01:03:32,590 --> 01:03:34,400
 

3191
01:03:32,600 --> 01:03:35,600
 also try to decouple the the learning

3192
01:03:34,390 --> 01:03:35,600
 

3193
01:03:34,400 --> 01:03:37,310
 rate and the bat term statistics that

3194
01:03:35,590 --> 01:03:37,310
 

3195
01:03:35,600 --> 01:03:38,570
 each gradient step to have fewer

3196
01:03:37,300 --> 01:03:38,570
 

3197
01:03:37,310 --> 01:03:40,730
 decoupled parameters that might cause

3198
01:03:38,560 --> 01:03:40,730
 

3199
01:03:38,570 --> 01:03:42,380
 instabilities and finally you could also

3200
01:03:40,720 --> 01:03:42,380
 

3201
01:03:40,730 --> 01:03:44,720
 introduce introduce additional context

3202
01:03:42,370 --> 01:03:44,720
 

3203
01:03:42,380 --> 01:03:46,280
 variables into the architecture to allow

3204
01:03:44,710 --> 01:03:46,280
 

3205
01:03:44,720 --> 01:03:47,900
 for multiplicative interactions between

3206
01:03:46,270 --> 01:03:47,900
 

3207
01:03:46,280 --> 01:03:50,570
 parameters and allow for a more

3208
01:03:47,890 --> 01:03:50,570
 

3209
01:03:47,900 --> 01:03:52,040
 expressive gradient and so come my

3210
01:03:50,560 --> 01:03:52,040
 

3211
01:03:50,570 --> 01:03:53,180
 takeaway here is that there are a range

3212
01:03:52,030 --> 01:03:53,180
 

3213
01:03:52,040 --> 01:03:55,910
 of simple tricks that can help the

3214
01:03:53,170 --> 01:03:55,910
 

3215
01:03:53,180 --> 01:03:56,930
 optimization significantly great so

3216
01:03:55,900 --> 01:03:56,930
 

3217
01:03:55,910 --> 01:03:59,030
 before I move on to nonparametric

3218
01:03:56,920 --> 01:03:59,030
 

3219
01:03:56,930 --> 01:04:00,500
 methods let's take one question from the

3220
01:03:59,020 --> 01:04:00,500
 

3221
01:03:59,030 --> 01:04:04,430
 audience and potentially some some saito

3222
01:04:00,490 --> 01:04:04,430
 

3223
01:04:00,500 --> 01:04:06,470
 questions do you know how Mammon

3224
01:04:04,420 --> 01:04:06,470
 

3225
01:04:04,430 --> 01:04:08,890
 compares to other first-order meta

3226
01:04:06,460 --> 01:04:08,890
 

3227
01:04:06,470 --> 01:04:13,070
 learning algorithms particularly reptile

3228
01:04:08,880 --> 01:04:13,070
 

3229
01:04:08,890 --> 01:04:14,060
 by Jones shown monitor so you questions

3230
01:04:13,060 --> 01:04:14,060
 

3231
01:04:13,070 --> 01:04:15,890
 how does it compare to first-order

3232
01:04:14,050 --> 01:04:15,890
 

3233
01:04:14,060 --> 01:04:17,540
 algorithms like I mean can it get over

3234
01:04:15,880 --> 01:04:17,540
 

3235
01:04:15,890 --> 01:04:21,230
 some of these problems you just

3236
01:04:17,530 --> 01:04:21,230
 

3237
01:04:17,540 --> 01:04:24,710
 presented in this fight in your

3238
01:04:21,220 --> 01:04:24,710
 

3239
01:04:21,230 --> 01:04:26,270
 about the mammal algorithm no like if

3240
01:04:24,700 --> 01:04:26,270
 

3241
01:04:24,710 --> 01:04:27,920
 you the reptile which is a first-order

3242
01:04:26,260 --> 01:04:27,920
 

3243
01:04:26,270 --> 01:04:29,690
 method for meta learning instead of

3244
01:04:27,910 --> 01:04:29,690
 

3245
01:04:27,920 --> 01:04:32,210
 panel can it get over some of the

3246
01:04:29,680 --> 01:04:32,210
 

3247
01:04:29,690 --> 01:04:34,520
 second-order gradient problems you just

3248
01:04:32,200 --> 01:04:34,520
 

3249
01:04:32,210 --> 01:04:37,040
 presented here yeah so as I mentioned on

3250
01:04:34,510 --> 01:04:37,040
 

3251
01:04:34,520 --> 01:04:41,260
 the on the first idea listed here

3252
01:04:37,030 --> 01:04:41,260
 

3253
01:04:37,040 --> 01:04:43,520
 both first-order mammal and reptile do

3254
01:04:41,250 --> 01:04:43,520
 

3255
01:04:41,260 --> 01:04:46,670
 by using this crude approximation you

3256
01:04:43,510 --> 01:04:46,670
 

3257
01:04:43,520 --> 01:04:49,369
 can have a faster optimization procedure

3258
01:04:46,660 --> 01:04:49,369
 

3259
01:04:46,670 --> 01:04:50,990
 and and it potentially can be less

3260
01:04:49,359 --> 01:04:50,990
 

3261
01:04:49,369 --> 01:04:52,820
 stable but the the main benefit that you

3262
01:04:50,980 --> 01:04:52,820
 

3263
01:04:50,990 --> 01:04:55,040
 get from it is faster and lower in

3264
01:04:52,810 --> 01:04:55,040
 

3265
01:04:52,820 --> 01:04:56,480
 memory but we have found that there are

3266
01:04:55,030 --> 01:04:56,480
 

3267
01:04:55,040 --> 01:04:57,890
 a number of problems where these types

3268
01:04:56,470 --> 01:04:57,890
 

3269
01:04:56,480 --> 01:05:00,650
 of first-order methods don't work at all

3270
01:04:57,880 --> 01:05:00,650
 

3271
01:04:57,890 --> 01:05:02,150
 and you need to use the the second-order

3272
01:05:00,640 --> 01:05:02,150
 

3273
01:05:00,650 --> 01:05:07,220
 methods in order to optimization

3274
01:05:02,140 --> 01:05:07,220
 

3275
01:05:02,150 --> 01:05:10,300
 optimize them well thank you why is

3276
01:05:07,210 --> 01:05:10,300
 

3277
01:05:07,220 --> 01:05:28,070
 theta minus Phi in the Gaussian prior

3278
01:05:10,290 --> 01:05:28,070
 

3279
01:05:10,300 --> 01:05:31,220
 this is a few slides ago the answer to

3280
01:05:28,060 --> 01:05:31,220
 

3281
01:05:28,070 --> 01:05:33,800
 this question though is that the

3282
01:05:31,210 --> 01:05:33,800
 

3283
01:05:31,220 --> 01:05:36,920
 interpretation as a Gaussian prior it

3284
01:05:33,790 --> 01:05:36,920
 

3285
01:05:33,800 --> 01:05:39,020
 basically says that the prior is on Phi

3286
01:05:36,910 --> 01:05:39,020
 

3287
01:05:36,920 --> 01:05:41,270
 and Phi is normally distributed with a

3288
01:05:39,010 --> 01:05:41,270
 

3289
01:05:39,020 --> 01:05:42,740
 mean of theta the variance of that prior

3290
01:05:41,260 --> 01:05:42,740
 

3291
01:05:41,270 --> 01:05:43,940
 depends on the number of rating steps

3292
01:05:42,730 --> 01:05:43,940
 

3293
01:05:42,740 --> 01:05:45,290
 you take which is actually a very

3294
01:05:43,930 --> 01:05:45,290
 

3295
01:05:43,940 --> 01:05:46,520
 natural thing basically the more grading

3296
01:05:45,280 --> 01:05:46,520
 

3297
01:05:45,290 --> 01:05:48,290
 steps you take the further away you get

3298
01:05:46,510 --> 01:05:48,290
 

3299
01:05:46,520 --> 01:05:52,250
 from theta that corresponds to a prior

3300
01:05:48,280 --> 01:05:52,250
 

3301
01:05:48,290 --> 01:05:54,260
 with a wider variance next question is

3302
01:05:52,240 --> 01:05:54,260
 

3303
01:05:52,250 --> 01:05:56,660
 there any guarantee or test that Phi is

3304
01:05:54,250 --> 01:05:56,660
 

3305
01:05:54,260 --> 01:06:03,410
 not multimodal as map will assume

3306
01:05:56,650 --> 01:06:03,410
 

3307
01:05:56,660 --> 01:06:05,330
 unimodality yeah so it's certainly yeah

3308
01:06:03,400 --> 01:06:05,330
 

3309
01:06:03,410 --> 01:06:07,070
 this this tribution certainly could be

3310
01:06:05,320 --> 01:06:07,070
 

3311
01:06:05,330 --> 01:06:09,320
 multimodal and this approximation is

3312
01:06:07,060 --> 01:06:09,320
 

3313
01:06:07,070 --> 01:06:11,270
 ignoring that in practice we have found

3314
01:06:09,310 --> 01:06:11,270
 

3315
01:06:09,320 --> 01:06:13,820
 that that mammal can work quite well on

3316
01:06:11,260 --> 01:06:13,820
 

3317
01:06:11,270 --> 01:06:15,710
 multi modal problems where you have like

3318
01:06:13,810 --> 01:06:15,710
 

3319
01:06:13,820 --> 01:06:17,600
 Quitely quite different functions that

3320
01:06:15,700 --> 01:06:17,600
 

3321
01:06:15,710 --> 01:06:19,760
 your that are represented in the task

3322
01:06:17,590 --> 01:06:19,760
 

3323
01:06:17,600 --> 01:06:20,900
 variables although you do need a deeper

3324
01:06:19,750 --> 01:06:20,900
 

3325
01:06:19,760 --> 01:06:23,420
 neural network for that and there are

3326
01:06:20,890 --> 01:06:23,420
 

3327
01:06:20,900 --> 01:06:25,520
 also approaches such as multi modal

3328
01:06:23,410 --> 01:06:25,520
 

3329
01:06:23,420 --> 01:06:27,590
 mammal I believe that try to tackle this

3330
01:06:25,510 --> 01:06:27,590
 

3331
01:06:25,520 --> 01:06:30,890
 problem head-on and enable you to get

3332
01:06:27,580 --> 01:06:30,890
 

3333
01:06:27,590 --> 01:06:32,990
 more efficient use of of your neural

3334
01:06:30,880 --> 01:06:32,990
 

3335
01:06:30,890 --> 01:06:33,829
 network parameters and by by allowing it

3336
01:06:32,980 --> 01:06:33,829
 

3337
01:06:32,990 --> 01:06:36,519
 to represent multi

3338
01:06:33,819 --> 01:06:36,519
 

3339
01:06:33,829 --> 01:06:39,109
 or distributions over if I given theta a

3340
01:06:36,509 --> 01:06:39,109
 

3341
01:06:36,519 --> 01:06:40,969
 couple more quick ones in the case of

3342
01:06:39,099 --> 01:06:40,969
 

3343
01:06:39,109 --> 01:06:50,509
 orthogonal tasks what ma'am we'll just

3344
01:06:40,959 --> 01:06:50,509
 

3345
01:06:40,969 --> 01:06:53,930
 memorize so if the tasks are mph unction

3346
01:06:50,499 --> 01:06:53,930
 

3347
01:06:50,509 --> 01:06:55,579
 can represent both tasks without relying

3348
01:06:53,920 --> 01:06:55,579
 

3349
01:06:53,930 --> 01:06:58,249
 on the data then it will just memorize

3350
01:06:55,569 --> 01:06:58,249
 

3351
01:06:55,579 --> 01:07:01,999
 the function and ignore the data in many

3352
01:06:58,239 --> 01:07:01,999
 

3353
01:06:58,249 --> 01:07:05,749
 cases can we do gradient descent for

3354
01:07:01,989 --> 01:07:05,749
 

3355
01:07:01,999 --> 01:07:08,660
 multiple steps to get Phi in mammal yeah

3356
01:07:05,739 --> 01:07:08,660
 

3357
01:07:05,749 --> 01:07:10,130
 absolutely so you can use a variable

3358
01:07:08,650 --> 01:07:10,130
 

3359
01:07:08,660 --> 01:07:11,630
 number of gradient steps in practice we

3360
01:07:10,120 --> 01:07:11,630
 

3361
01:07:10,130 --> 01:07:14,660
 found that up to five gradient stops

3362
01:07:11,620 --> 01:07:14,660
 

3363
01:07:11,630 --> 01:07:16,940
 works well but in practice you can use

3364
01:07:14,650 --> 01:07:16,940
 

3365
01:07:14,660 --> 01:07:18,950
 you can use more than that if if you

3366
01:07:16,930 --> 01:07:18,950
 

3367
01:07:16,940 --> 01:07:20,839
 find it helpful for your algorithm I it

3368
01:07:18,940 --> 01:07:20,839
 

3369
01:07:18,950 --> 01:07:22,190
 does not introduce higher order terms

3370
01:07:20,829 --> 01:07:22,190
 

3371
01:07:20,839 --> 01:07:23,089
 than a second order optimization it

3372
01:07:22,180 --> 01:07:23,089
 

3373
01:07:22,190 --> 01:07:26,029
 still remains a second order

3374
01:07:23,079 --> 01:07:26,029
 

3375
01:07:23,089 --> 01:07:31,190
 optimization if you go through if you go

3376
01:07:26,019 --> 01:07:31,190
 

3377
01:07:26,029 --> 01:07:34,819
 through the mouth okay great so let's

3378
01:07:31,180 --> 01:07:34,819
 

3379
01:07:31,190 --> 01:07:37,670
 move on to nonparametric approaches and

3380
01:07:34,809 --> 01:07:37,670
 

3381
01:07:34,819 --> 01:07:39,140
 so far we talked about parametric

3382
01:07:37,660 --> 01:07:39,140
 

3383
01:07:37,670 --> 01:07:41,479
 methods and that where it could be

3384
01:07:39,130 --> 01:07:41,479
 

3385
01:07:39,140 --> 01:07:44,869
 learning a model that's parametrized by

3386
01:07:41,469 --> 01:07:44,869
 

3387
01:07:41,479 --> 01:07:46,309
 Phi and and what about using pet methods

3388
01:07:44,859 --> 01:07:46,309
 

3389
01:07:44,869 --> 01:07:49,369
 that don't have parameters fine

3390
01:07:46,299 --> 01:07:49,369
 

3391
01:07:46,309 --> 01:07:52,279
 and the motivation here is that in low

3392
01:07:49,359 --> 01:07:52,279
 

3393
01:07:49,369 --> 01:07:54,880
 danger regimes nonparametric methods are

3394
01:07:52,269 --> 01:07:54,880
 

3395
01:07:52,279 --> 01:07:57,589
 quite simple and work quite well and

3396
01:07:54,870 --> 01:07:57,589
 

3397
01:07:54,880 --> 01:07:59,630
 during meta test time we're in a future

3398
01:07:57,579 --> 01:07:59,630
 

3399
01:07:57,589 --> 01:08:01,999
 learning setting and so we are in Elite

3400
01:07:59,620 --> 01:08:01,999
 

3401
01:07:59,630 --> 01:08:03,589
 low data regime however during

3402
01:08:01,989 --> 01:08:03,589
 

3403
01:08:01,999 --> 01:08:06,049
 Mediterranean we still want to be

3404
01:08:03,579 --> 01:08:06,049
 

3405
01:08:03,589 --> 01:08:07,579
 parametric because we have large amounts

3406
01:08:06,039 --> 01:08:07,579
 

3407
01:08:06,049 --> 01:08:10,910
 of data across all of the meta training

3408
01:08:07,569 --> 01:08:10,910
 

3409
01:08:07,579 --> 01:08:12,920
 tasks so the key idea behind these

3410
01:08:10,900 --> 01:08:12,920
 

3411
01:08:10,910 --> 01:08:15,199
 approaches is can we use a parametric

3412
01:08:12,910 --> 01:08:15,199
 

3413
01:08:12,920 --> 01:08:20,659
 meta learner in order to produce a

3414
01:08:15,189 --> 01:08:20,659
 

3415
01:08:15,199 --> 01:08:21,619
 nonparametric learner okay and note that

3416
01:08:20,649 --> 01:08:21,619
 

3417
01:08:20,659 --> 01:08:23,330
 some of these methods that I'll be

3418
01:08:21,609 --> 01:08:23,330
 

3419
01:08:21,619 --> 01:08:25,369
 presenting do precede parametric

3420
01:08:23,320 --> 01:08:25,369
 

3421
01:08:23,330 --> 01:08:27,290
 approaches but we're presenting them in

3422
01:08:25,359 --> 01:08:27,290
 

3423
01:08:25,369 --> 01:08:31,219
 this in this group setting to eat an

3424
01:08:27,280 --> 01:08:31,219
 

3425
01:08:27,290 --> 01:08:32,750
 understanding okay so the key idea here

3426
01:08:31,209 --> 01:08:32,750
 

3427
01:08:31,219 --> 01:08:34,909
 is is here's a few shot learning problem

3428
01:08:32,740 --> 01:08:34,909
 

3429
01:08:32,750 --> 01:08:36,560
 and one of the things you might ask is

3430
01:08:34,899 --> 01:08:36,560
 

3431
01:08:34,909 --> 01:08:37,609
 well what one thing you could one it's

3432
01:08:36,550 --> 01:08:37,609
 

3433
01:08:36,560 --> 01:08:38,869
 very simple thing you could do in this

3434
01:08:37,599 --> 01:08:38,869
 

3435
01:08:37,609 --> 01:08:40,639
 approach is just take your test data

3436
01:08:38,859 --> 01:08:40,639
 

3437
01:08:38,869 --> 01:08:41,739
 point and compare it to each of the the

3438
01:08:40,629 --> 01:08:41,739
 

3439
01:08:40,639 --> 01:08:44,960
 data points in your training data

3440
01:08:41,729 --> 01:08:44,960
 

3441
01:08:41,739 --> 01:08:47,170
 basically do nearest neighbors by by

3442
01:08:44,950 --> 01:08:47,170
 

3443
01:08:44,960 --> 01:08:49,060
 comparing to each of the images

3444
01:08:47,160 --> 01:08:49,060
 

3445
01:08:47,170 --> 01:08:50,590
 this is this is a quite a simple

3446
01:08:49,050 --> 01:08:50,590
 

3447
01:08:49,060 --> 01:08:53,890
 opportunity quite valid for these types

3448
01:08:50,580 --> 01:08:53,890
 

3449
01:08:50,590 --> 01:08:55,600
 of problems the key question is in what

3450
01:08:53,880 --> 01:08:55,600
 

3451
01:08:53,890 --> 01:08:59,320
 space do you compare these images and

3452
01:08:55,590 --> 01:08:59,320
 

3453
01:08:55,600 --> 01:09:00,850
 with what distance metric I for example

3454
01:08:59,310 --> 01:09:00,850
 

3455
01:08:59,320 --> 01:09:02,050
 you could do pixel space or l2 distance

3456
01:09:00,840 --> 01:09:02,050
 

3457
01:09:00,850 --> 01:09:04,120
 but that probably wouldn't give you an

3458
01:09:02,040 --> 01:09:04,120
 

3459
01:09:02,050 --> 01:09:07,480
 effective metric over the similarity

3460
01:09:04,110 --> 01:09:07,480
 

3461
01:09:04,120 --> 01:09:09,610
 between these images and so the key idea

3462
01:09:07,470 --> 01:09:09,610
 

3463
01:09:07,480 --> 01:09:12,520
 behind these long parametric methods is

3464
01:09:09,600 --> 01:09:12,520
 

3465
01:09:09,610 --> 01:09:15,280
 to learn a metric space that leads to

3466
01:09:12,510 --> 01:09:15,280
 

3467
01:09:12,520 --> 01:09:17,560
 effective comparisons learn a more

3468
01:09:15,270 --> 01:09:17,560
 

3469
01:09:15,280 --> 01:09:18,970
 semantic metric space that leads to

3470
01:09:17,550 --> 01:09:18,970
 

3471
01:09:17,560 --> 01:09:20,710
 effective predictions on the test data

3472
01:09:18,960 --> 01:09:20,710
 

3473
01:09:18,970 --> 01:09:22,600
 points and then we learn how to compare

3474
01:09:20,700 --> 01:09:22,600
 

3475
01:09:20,710 --> 01:09:26,140
 these images to make effective

3476
01:09:22,590 --> 01:09:26,140
 

3477
01:09:22,600 --> 01:09:27,880
 predictions and so the first very simple

3478
01:09:26,130 --> 01:09:27,880
 

3479
01:09:26,140 --> 01:09:29,200
 approach for doing this is to train a

3480
01:09:27,870 --> 01:09:29,200
 

3481
01:09:27,880 --> 01:09:31,500
 Siamese network to predict whether or

3482
01:09:29,190 --> 01:09:31,500
 

3483
01:09:29,200 --> 01:09:33,580
 not two images are the same so we can

3484
01:09:31,490 --> 01:09:33,580
 

3485
01:09:31,500 --> 01:09:36,730
 train a neural network that takes in two

3486
01:09:33,570 --> 01:09:36,730
 

3487
01:09:33,580 --> 01:09:37,810
 images and is trained to output whether

3488
01:09:36,720 --> 01:09:37,810
 

3489
01:09:36,730 --> 01:09:39,370
 or not they're the same we're not so

3490
01:09:37,800 --> 01:09:39,370
 

3491
01:09:37,810 --> 01:09:41,290
 zero correspond so them not being the

3492
01:09:39,360 --> 01:09:41,290
 

3493
01:09:39,370 --> 01:09:44,380
 same one corresponds to them being the

3494
01:09:41,280 --> 01:09:44,380
 

3495
01:09:41,290 --> 01:09:46,150
 same class and repeat this through all

3496
01:09:44,370 --> 01:09:46,150
 

3497
01:09:44,380 --> 01:09:49,150
 of the data in your meta training data

3498
01:09:46,140 --> 01:09:49,150
 

3499
01:09:46,150 --> 01:09:50,320
 set and so once you've trained this

3500
01:09:49,140 --> 01:09:50,320
 

3501
01:09:49,150 --> 01:09:52,300
 neural network to be able to compare

3502
01:09:50,310 --> 01:09:52,300
 

3503
01:09:50,320 --> 01:09:53,950
 pairs of images about a test time you

3504
01:09:52,290 --> 01:09:53,950
 

3505
01:09:52,300 --> 01:09:55,540
 can then take your test data point

3506
01:09:53,940 --> 01:09:55,540
 

3507
01:09:53,950 --> 01:09:57,940
 compare it to each of the images in your

3508
01:09:55,530 --> 01:09:57,940
 

3509
01:09:55,540 --> 01:09:59,440
 training dataset - and then output the

3510
01:09:57,930 --> 01:09:59,440
 

3511
01:09:57,940 --> 01:10:02,950
 corresponding label to the image that is

3512
01:09:59,430 --> 01:10:02,950
 

3513
01:09:59,440 --> 01:10:03,880
 the closest okay so in this case meta

3514
01:10:02,940 --> 01:10:03,880
 

3515
01:10:02,950 --> 01:10:05,620
 training is give me a two-way

3516
01:10:03,870 --> 01:10:05,620
 

3517
01:10:03,880 --> 01:10:07,180
 classification problem and then meta

3518
01:10:05,610 --> 01:10:07,180
 

3519
01:10:05,620 --> 01:10:09,190
 testing is going to be an anyway

3520
01:10:07,170 --> 01:10:09,190
 

3521
01:10:07,180 --> 01:10:13,270
 classification problem by doing all of

3522
01:10:09,180 --> 01:10:13,270
 

3523
01:10:09,190 --> 01:10:14,500
 these pairwise comparisons so now to

3524
01:10:13,260 --> 01:10:14,500
 

3525
01:10:13,270 --> 01:10:17,590
 improve upon this approach you might ask

3526
01:10:14,490 --> 01:10:17,590
 

3527
01:10:14,500 --> 01:10:19,870
 well can we make meta training and meta

3528
01:10:17,580 --> 01:10:19,870
 

3529
01:10:17,590 --> 01:10:21,940
 testing match can we train it such they

3530
01:10:19,860 --> 01:10:21,940
 

3531
01:10:19,870 --> 01:10:25,060
 can actually perform effective anyway

3532
01:10:21,930 --> 01:10:25,060
 

3533
01:10:21,940 --> 01:10:27,250
 classification and this was kind of the

3534
01:10:25,050 --> 01:10:27,250
 

3535
01:10:25,060 --> 01:10:29,020
 key idea that Sergey alluded to in the

3536
01:10:27,240 --> 01:10:29,020
 

3537
01:10:27,250 --> 01:10:31,390
 problem definition which is to try to

3538
01:10:29,010 --> 01:10:31,390
 

3539
01:10:29,020 --> 01:10:34,240
 match Mediterranean meta testing in this

3540
01:10:31,380 --> 01:10:34,240
 

3541
01:10:31,390 --> 01:10:35,830
 case putting a an NBA classification

3542
01:10:34,230 --> 01:10:35,830
 

3543
01:10:34,240 --> 01:10:37,510
 problem with nearest neighbors into a

3544
01:10:35,820 --> 01:10:37,510
 

3545
01:10:35,830 --> 01:10:38,920
 neural network and so we're gonna be

3546
01:10:37,500 --> 01:10:38,920
 

3547
01:10:37,510 --> 01:10:40,900
 feeding the train data set as shown on

3548
01:10:38,910 --> 01:10:40,900
 

3549
01:10:38,920 --> 01:10:43,510
 the left into an into the neural network

3550
01:10:40,890 --> 01:10:43,510
 

3551
01:10:40,900 --> 01:10:46,990
 comparing this to our test input shown

3552
01:10:43,500 --> 01:10:46,990
 

3553
01:10:43,510 --> 01:10:49,090
 on the bottom - and outputting these

3554
01:10:46,980 --> 01:10:49,090
 

3555
01:10:46,990 --> 01:10:50,320
 similarities and then training it such

3556
01:10:49,080 --> 01:10:50,320
 

3557
01:10:49,090 --> 01:10:51,940
 that the predictions that it gets out

3558
01:10:50,310 --> 01:10:51,940
 

3559
01:10:50,320 --> 01:10:54,760
 which corresponds to weighted nearest

3560
01:10:51,930 --> 01:10:54,760
 

3561
01:10:51,940 --> 01:10:56,620
 neighbors are correct with respect to

3562
01:10:54,750 --> 01:10:56,620
 

3563
01:10:54,760 --> 01:10:58,460
 each of the tasks in our meta training

3564
01:10:56,610 --> 01:10:58,460
 

3565
01:10:56,620 --> 01:11:00,630
 set

3566
01:10:58,450 --> 01:11:00,630
 

3567
01:10:58,460 --> 01:11:02,990
 cool so this is basically embedding

3568
01:11:00,620 --> 01:11:02,990
 

3569
01:11:00,630 --> 01:11:06,360
 nearest neighbors into a neural network

3570
01:11:02,980 --> 01:11:06,360
 

3571
01:11:02,990 --> 01:11:08,040
 the the metric that we're using can be

3572
01:11:06,350 --> 01:11:08,040
 

3573
01:11:06,360 --> 01:11:10,830
 correspond to a convolutional encoder or

3574
01:11:08,030 --> 01:11:10,830
 

3575
01:11:08,040 --> 01:11:12,090
 some other architecture to get the

3576
01:11:10,820 --> 01:11:12,090
 

3577
01:11:10,830 --> 01:11:14,900
 embeddings of the training data points

3578
01:11:12,080 --> 01:11:14,900
 

3579
01:11:12,090 --> 01:11:17,370
 this method uses a bi-directional ostium

3580
01:11:14,890 --> 01:11:17,370
 

3581
01:11:14,900 --> 01:11:19,800
 and then we get a model that can do

3582
01:11:17,360 --> 01:11:19,800
 

3583
01:11:17,370 --> 01:11:24,120
 anyway classification is actually better

3584
01:11:19,790 --> 01:11:24,120
 

3585
01:11:19,800 --> 01:11:26,040
 trained for anyway classification now if

3586
01:11:24,110 --> 01:11:26,040
 

3587
01:11:24,120 --> 01:11:27,030
 you have more than one example per class

3588
01:11:26,030 --> 01:11:27,030
 

3589
01:11:26,040 --> 01:11:29,100
 what this will do is it will

3590
01:11:27,020 --> 01:11:29,100
 

3591
01:11:27,030 --> 01:11:30,810
 independently compare the test image

3592
01:11:29,090 --> 01:11:30,810
 

3593
01:11:29,100 --> 01:11:33,180
 with each of those examples per class

3594
01:11:30,800 --> 01:11:33,180
 

3595
01:11:30,810 --> 01:11:35,070
 and one of the things that might be nice

3596
01:11:33,170 --> 01:11:35,070
 

3597
01:11:33,180 --> 01:11:36,420
 to do this actually to better integrate

3598
01:11:35,060 --> 01:11:36,420
 

3599
01:11:35,070 --> 01:11:39,870
 the information across different

3600
01:11:36,410 --> 01:11:39,870
 

3601
01:11:36,420 --> 01:11:41,430
 examples for our class to aggregate cost

3602
01:11:39,860 --> 01:11:41,430
 

3603
01:11:39,870 --> 01:11:43,890
 information into a sort of prototypical

3604
01:11:41,420 --> 01:11:43,890
 

3605
01:11:41,430 --> 01:11:45,600
 embedding of that class such that when

3606
01:11:43,880 --> 01:11:45,600
 

3607
01:11:43,890 --> 01:11:47,520
 we do comparisons we're comparing at a

3608
01:11:45,590 --> 01:11:47,520
 

3609
01:11:45,600 --> 01:11:49,820
 class level rather than at the example

3610
01:11:47,510 --> 01:11:49,820
 

3611
01:11:47,520 --> 01:11:52,980
 level and this is exactly what

3612
01:11:49,810 --> 01:11:52,980
 

3613
01:11:49,820 --> 01:11:54,210
 prototypical networks does so they embed

3614
01:11:52,970 --> 01:11:54,210
 

3615
01:11:52,980 --> 01:11:55,770
 each of the data points for a given

3616
01:11:54,200 --> 01:11:55,770
 

3617
01:11:54,210 --> 01:12:00,000
 class such as the class corresponding to

3618
01:11:55,760 --> 01:12:00,000
 

3619
01:11:55,770 --> 01:12:05,010
 green blue or orange I average these to

3620
01:11:59,990 --> 01:12:05,010
 

3621
01:12:00,000 --> 01:12:08,040
 compute a prototype shown as CK and then

3622
01:12:05,000 --> 01:12:08,040
 

3623
01:12:05,010 --> 01:12:11,490
 make predictions based off of embedding

3624
01:12:08,030 --> 01:12:11,490
 

3625
01:12:08,040 --> 01:12:13,560
 the test data point X test and comparing

3626
01:12:11,480 --> 01:12:13,560
 

3627
01:12:11,490 --> 01:12:15,030
 it to each of the prototypes and what

3628
01:12:13,550 --> 01:12:15,030
 

3629
01:12:13,560 --> 01:12:16,080
 this looks like is we simply measure the

3630
01:12:15,020 --> 01:12:16,080
 

3631
01:12:15,030 --> 01:12:18,960
 distance X between each of the

3632
01:12:16,070 --> 01:12:18,960
 

3633
01:12:16,080 --> 01:12:21,180
 prototypes this is denoted as D and then

3634
01:12:18,950 --> 01:12:21,180
 

3635
01:12:18,960 --> 01:12:24,690
 perform a softmax operation to decide

3636
01:12:21,170 --> 01:12:24,690
 

3637
01:12:21,180 --> 01:12:26,760
 which class it corresponds to and in

3638
01:12:24,680 --> 01:12:26,760
 

3639
01:12:24,690 --> 01:12:31,560
 this case D can correspond to Euclidean

3640
01:12:26,750 --> 01:12:31,560
 

3641
01:12:26,760 --> 01:12:33,180
 distance or cosine distance great so

3642
01:12:31,550 --> 01:12:33,180
 

3643
01:12:31,560 --> 01:12:36,720
 this is a simple approach that works

3644
01:12:33,170 --> 01:12:36,720
 

3645
01:12:33,180 --> 01:12:38,280
 quite well and there are a couple other

3646
01:12:36,710 --> 01:12:38,280
 

3647
01:12:36,720 --> 01:12:39,810
 extensions that we can make upon it so

3648
01:12:38,270 --> 01:12:39,810
 

3649
01:12:38,280 --> 01:12:41,070
 so far we've looked at Siamese networks

3650
01:12:39,800 --> 01:12:41,070
 

3651
01:12:39,810 --> 01:12:42,780
 about two networks our prototypical that

3652
01:12:41,060 --> 01:12:42,780
 

3653
01:12:41,070 --> 01:12:44,280
 works at all correspond to some sort of

3654
01:12:42,770 --> 01:12:44,280
 

3655
01:12:42,780 --> 01:12:46,370
 embedding in the nearest neighbors

3656
01:12:44,270 --> 01:12:46,370
 

3657
01:12:44,280 --> 01:12:48,630
 either two examples or two prototypes

3658
01:12:46,360 --> 01:12:48,630
 

3659
01:12:46,370 --> 01:12:50,160
 when tota comes up is what if you need a

3660
01:12:48,620 --> 01:12:50,160
 

3661
01:12:48,630 --> 01:12:51,810
 reason about more complex relationships

3662
01:12:50,150 --> 01:12:51,810
 

3663
01:12:50,160 --> 01:12:52,920
 between data points we don't simply want

3664
01:12:51,800 --> 01:12:52,920
 

3665
01:12:51,810 --> 01:12:55,700
 to average to create a single

3666
01:12:52,910 --> 01:12:55,700
 

3667
01:12:52,920 --> 01:12:57,780
 prototypical example pour per per class

3668
01:12:55,690 --> 01:12:57,780
 

3669
01:12:55,700 --> 01:12:59,580
 to handle this we could learn a

3670
01:12:57,770 --> 01:12:59,580
 

3671
01:12:57,780 --> 01:13:01,170
 nonlinear relation module on the

3672
01:12:59,570 --> 01:13:01,170
 

3673
01:12:59,580 --> 01:13:03,240
 embeddings so we can essentially learn

3674
01:13:01,160 --> 01:13:03,240
 

3675
01:13:01,170 --> 01:13:05,100
 that D function by embedding each of the

3676
01:13:03,230 --> 01:13:05,100
 

3677
01:13:03,240 --> 01:13:07,500
 pairwise examples and and producing

3678
01:13:05,090 --> 01:13:07,500
 

3679
01:13:05,100 --> 01:13:09,720
 predictions for each of them we could

3680
01:13:07,490 --> 01:13:09,720
 

3681
01:13:07,500 --> 01:13:10,020
 also learn a different prototype more

3682
01:13:09,710 --> 01:13:10,020
 

3683
01:13:09,720 --> 01:13:11,400
 than one

3684
01:13:10,010 --> 01:13:11,400
 

3685
01:13:10,020 --> 01:13:13,560
 Oh time for each class learning an

3686
01:13:11,390 --> 01:13:13,560
 

3687
01:13:11,400 --> 01:13:15,750
 infinite mixture of prototypes or

3688
01:13:13,550 --> 01:13:15,750
 

3689
01:13:13,560 --> 01:13:17,610
 perform message-passing on the different

3690
01:13:15,740 --> 01:13:17,610
 

3691
01:13:15,750 --> 01:13:22,560
 classes and the different examples in

3692
01:13:17,600 --> 01:13:22,560
 

3693
01:13:17,610 --> 01:13:23,910
 our dataset okay um so stepping up a bit

3694
01:13:22,550 --> 01:13:23,910
 

3695
01:13:22,560 --> 01:13:25,620
 let's actually try to think about how

3696
01:13:23,900 --> 01:13:25,620
 

3697
01:13:23,910 --> 01:13:26,940
 these different approaches compare so

3698
01:13:25,610 --> 01:13:26,940
 

3699
01:13:25,620 --> 01:13:30,060
 you can go back to the computation graph

3700
01:13:26,930 --> 01:13:30,060
 

3701
01:13:26,940 --> 01:13:31,680
 perspective and bring up the kind of

3702
01:13:30,050 --> 01:13:31,680
 

3703
01:13:30,060 --> 01:13:33,090
 computation graph view of the blackbox

3704
01:13:31,670 --> 01:13:33,090
 

3705
01:13:31,680 --> 01:13:35,280
 approaches as well as the optimization

3706
01:13:33,080 --> 01:13:35,280
 

3707
01:13:33,090 --> 01:13:36,960
 based approaches and the nonparametric

3708
01:13:35,270 --> 01:13:36,960
 

3709
01:13:35,280 --> 01:13:40,500
 approach can be viewed in the same exact

3710
01:13:36,950 --> 01:13:40,500
 

3711
01:13:36,960 --> 01:13:42,990
 way except here I we're gonna be having

3712
01:13:40,490 --> 01:13:42,990
 

3713
01:13:40,500 --> 01:13:45,180
 a new function which I'll denote as PN

3714
01:13:42,980 --> 01:13:45,180
 

3715
01:13:42,990 --> 01:13:48,300
 for prototypical networks where the

3716
01:13:45,170 --> 01:13:48,300
 

3717
01:13:45,180 --> 01:13:51,690
 function corresponds to the softmax of

3718
01:13:48,290 --> 01:13:51,690
 

3719
01:13:48,300 --> 01:13:54,810
 the the negative distance between the

3720
01:13:51,680 --> 01:13:54,810
 

3721
01:13:51,690 --> 01:13:56,880
 embedded query tests data point and the

3722
01:13:54,800 --> 01:13:56,880
 

3723
01:13:54,810 --> 01:13:59,370
 prototypes where the prototypes are

3724
01:13:56,870 --> 01:13:59,370
 

3725
01:13:56,880 --> 01:14:03,270
 defined as the average embedding of each

3726
01:13:59,360 --> 01:14:03,270
 

3727
01:13:59,370 --> 01:14:05,070
 of the class for that example and so

3728
01:14:03,260 --> 01:14:05,070
 

3729
01:14:03,270 --> 01:14:06,390
 with this view again we can mix and

3730
01:14:05,060 --> 01:14:06,390
 

3731
01:14:05,070 --> 01:14:08,520
 match components of this computation

3732
01:14:06,380 --> 01:14:08,520
 

3733
01:14:06,390 --> 01:14:10,770
 graph to create hybrid approaches among

3734
01:14:08,510 --> 01:14:10,770
 

3735
01:14:08,520 --> 01:14:12,990
 these different three and this includes

3736
01:14:10,760 --> 01:14:12,990
 

3737
01:14:10,770 --> 01:14:14,400
 things like camel which can both

3738
01:14:12,980 --> 01:14:14,400
 

3739
01:14:12,990 --> 01:14:16,410
 conditions on the data with a black box

3740
01:14:14,390 --> 01:14:16,410
 

3741
01:14:14,400 --> 01:14:18,690
 approach and run great news and runs

3742
01:14:16,400 --> 01:14:18,690
 

3743
01:14:16,410 --> 01:14:21,600
 grain descent on the parameters it's

3744
01:14:18,680 --> 01:14:21,600
 

3745
01:14:18,690 --> 01:14:23,520
 also another hybrid approach is to run

3746
01:14:21,590 --> 01:14:23,520
 

3747
01:14:21,600 --> 01:14:24,960
 gradient descent on an embedding that

3748
01:14:23,510 --> 01:14:24,960
 

3749
01:14:23,520 --> 01:14:26,400
 produces a set of parameters so this

3750
01:14:24,950 --> 01:14:26,400
 

3751
01:14:24,960 --> 01:14:28,470
 sort of combines all three approaches

3752
01:14:26,390 --> 01:14:28,470
 

3753
01:14:26,400 --> 01:14:29,910
 and then finally we can do something

3754
01:14:28,460 --> 01:14:29,910
 

3755
01:14:28,470 --> 01:14:30,960
 like mammal but initialize the last

3756
01:14:29,900 --> 01:14:30,960
 

3757
01:14:29,910 --> 01:14:32,880
 layer to be a linear classifier

3758
01:14:30,950 --> 01:14:32,880
 

3759
01:14:30,960 --> 01:14:38,130
 equivalent to the prototypical networks

3760
01:14:32,870 --> 01:14:38,130
 

3761
01:14:32,880 --> 01:14:39,300
 classifier okay before we move on to the

3762
01:14:38,120 --> 01:14:39,300
 

3763
01:14:38,130 --> 01:14:40,980
 takeaways are there any questions about

3764
01:14:39,290 --> 01:14:40,980
 

3765
01:14:39,300 --> 01:14:47,900
 nonparametric approaches or any

3766
01:14:40,970 --> 01:14:47,900
 

3767
01:14:40,980 --> 01:14:47,900
 questions from slide oh no

3768
01:14:48,160 --> 01:14:48,160
 

3769
01:14:48,170 --> 01:14:53,250
 there are roughly 200 questions and I

3770
01:14:50,510 --> 01:14:53,250
 

3771
01:14:50,520 --> 01:14:55,230
 can't sort through them all but there

3772
01:14:53,240 --> 01:14:55,230
 

3773
01:14:53,250 --> 01:14:56,400
 are a couple of questions that are kind

3774
01:14:55,220 --> 01:14:56,400
 

3775
01:14:55,230 --> 01:14:58,190
 of crowd favorites that are maybe good

3776
01:14:56,390 --> 01:14:58,190
 

3777
01:14:56,400 --> 01:15:01,349
 to take not not to do that primary

3778
01:14:58,180 --> 01:15:01,349
 

3779
01:14:58,190 --> 01:15:03,570
 methods necessarily but one common theme

3780
01:15:01,339 --> 01:15:03,570
 

3781
01:15:01,349 --> 01:15:05,790
 is well can we do meta training where we

3782
01:15:03,560 --> 01:15:05,790
 

3783
01:15:03,570 --> 01:15:07,139
 have different numbers of classes for

3784
01:15:05,780 --> 01:15:07,139
 

3785
01:15:05,790 --> 01:15:10,230
 different tasks or different numbers of

3786
01:15:07,129 --> 01:15:10,230
 

3787
01:15:07,139 --> 01:15:12,540
 data points for different classes yeah

3788
01:15:10,220 --> 01:15:12,540
 

3789
01:15:10,230 --> 01:15:13,980
 absolutely so and actually this is a

3790
01:15:12,530 --> 01:15:13,980
 

3791
01:15:12,540 --> 01:15:16,800
 this is a good question to talk about

3792
01:15:13,970 --> 01:15:16,800
 

3793
01:15:13,980 --> 01:15:18,389
 now too because um these different

3794
01:15:16,790 --> 01:15:18,389
 

3795
01:15:16,800 --> 01:15:21,739
 approaches can handle variable numbers

3796
01:15:18,379 --> 01:15:21,739
 

3797
01:15:18,389 --> 01:15:23,520
 of training data points and numbers of

3798
01:15:21,729 --> 01:15:23,520
 

3799
01:15:21,739 --> 01:15:24,809
 number of data points per class and

3800
01:15:23,510 --> 01:15:24,809
 

3801
01:15:23,520 --> 01:15:27,150
 varied number variable numbers of

3802
01:15:24,799 --> 01:15:27,150
 

3803
01:15:24,809 --> 01:15:28,530
 classes the this is something that

3804
01:15:27,140 --> 01:15:28,530
 

3805
01:15:27,150 --> 01:15:30,000
 optimization based approaches can handle

3806
01:15:28,520 --> 01:15:30,000
 

3807
01:15:28,530 --> 01:15:31,650
 very gracefully because you can simply

3808
01:15:29,990 --> 01:15:31,650
 

3809
01:15:30,000 --> 01:15:33,059
 compete your loss function over

3810
01:15:31,640 --> 01:15:33,059
 

3811
01:15:31,650 --> 01:15:35,610
 different batches of data points very

3812
01:15:33,049 --> 01:15:35,610
 

3813
01:15:33,059 --> 01:15:37,460
 very naturally blackbox approaches you

3814
01:15:35,600 --> 01:15:37,460
 

3815
01:15:35,610 --> 01:15:39,329
 can do it but you need to train it for

3816
01:15:37,450 --> 01:15:39,329
 

3817
01:15:37,460 --> 01:15:40,710
 to be able to do that you need to be

3818
01:15:39,319 --> 01:15:40,710
 

3819
01:15:39,329 --> 01:15:42,239
 able to train it with variable data set

3820
01:15:40,700 --> 01:15:42,239
 

3821
01:15:40,710 --> 01:15:44,429
 sizes and variable numbers of examples

3822
01:15:42,229 --> 01:15:44,429
 

3823
01:15:42,239 --> 01:15:47,250
 per class and nonparametric approaches

3824
01:15:44,419 --> 01:15:47,250
 

3825
01:15:44,429 --> 01:15:49,559
 can also handle it fairly gracefully but

3826
01:15:47,240 --> 01:15:49,559
 

3827
01:15:47,250 --> 01:15:52,260
 in practice we found them not to always

3828
01:15:49,549 --> 01:15:52,260
 

3829
01:15:49,559 --> 01:15:54,659
 work well to to variable numbers of data

3830
01:15:52,250 --> 01:15:54,659
 

3831
01:15:52,260 --> 01:15:56,670
 points per class so another question

3832
01:15:54,649 --> 01:15:56,670
 

3833
01:15:54,659 --> 01:15:57,840
 which is kind of a summary of multiple

3834
01:15:56,660 --> 01:15:57,840
 

3835
01:15:56,670 --> 01:16:00,300
 questions but I think it applies to all

3836
01:15:57,830 --> 01:16:00,300
 

3837
01:15:57,840 --> 01:16:02,040
 three of the methods as well to what

3838
01:16:00,290 --> 01:16:02,040
 

3839
01:16:00,300 --> 01:16:05,099
 degree to meta learning methods actually

3840
01:16:02,030 --> 01:16:05,099
 

3841
01:16:02,040 --> 01:16:07,679
 learn to use the entirety of kind of a

3842
01:16:05,089 --> 01:16:07,679
 

3843
01:16:05,099 --> 01:16:09,540
 test batch meaning that they classify

3844
01:16:07,669 --> 01:16:09,540
 

3845
01:16:07,679 --> 01:16:10,949
 multiple images at once versus are they

3846
01:16:09,530 --> 01:16:10,949
 

3847
01:16:09,540 --> 01:16:12,239
 classifying individual images one at a

3848
01:16:10,939 --> 01:16:12,239
 

3849
01:16:10,949 --> 01:16:13,619
 time this is all the questions about

3850
01:16:12,229 --> 01:16:13,619
 

3851
01:16:12,239 --> 01:16:16,800
 inductive versus transductive are also

3852
01:16:13,609 --> 01:16:16,800
 

3853
01:16:13,619 --> 01:16:18,210
 getting at this point yes I think it

3854
01:16:16,790 --> 01:16:18,210
 

3855
01:16:16,800 --> 01:16:19,590
 depends on your application so if you

3856
01:16:18,200 --> 01:16:19,590
 

3857
01:16:18,210 --> 01:16:20,849
 think that you are going to have just a

3858
01:16:19,580 --> 01:16:20,849
 

3859
01:16:19,590 --> 01:16:24,900
 single example that you need to classify

3860
01:16:20,839 --> 01:16:24,900
 

3861
01:16:20,849 --> 01:16:27,690
 it test time then then all of these

3862
01:16:24,890 --> 01:16:27,690
 

3863
01:16:24,900 --> 01:16:28,949
 approaches can handle that setting and

3864
01:16:27,680 --> 01:16:28,949
 

3865
01:16:27,690 --> 01:16:29,849
 then if you're in the transductive

3866
01:16:28,939 --> 01:16:29,849
 

3867
01:16:28,949 --> 01:16:31,349
 setting and you think that you're gonna

3868
01:16:29,839 --> 01:16:31,349
 

3869
01:16:29,849 --> 01:16:33,599
 actually have a test dataset that you

3870
01:16:31,339 --> 01:16:33,599
 

3871
01:16:31,349 --> 01:16:36,210
 need a label that corresponds to two

3872
01:16:33,589 --> 01:16:36,210
 

3873
01:16:33,599 --> 01:16:37,980
 multiple classes then you could also you

3874
01:16:36,200 --> 01:16:37,980
 

3875
01:16:36,210 --> 01:16:40,199
 could either independently classify them

3876
01:16:37,970 --> 01:16:40,199
 

3877
01:16:37,980 --> 01:16:43,409
 of course or try to actually combine

3878
01:16:40,189 --> 01:16:43,409
 

3879
01:16:40,199 --> 01:16:45,030
 information between them different

3880
01:16:43,399 --> 01:16:45,030
 

3881
01:16:43,409 --> 01:16:47,280
 approaches can handle this in different

3882
01:16:45,020 --> 01:16:47,280
 

3883
01:16:45,030 --> 01:16:50,520
 ways for example you can use the batch

3884
01:16:47,270 --> 01:16:50,520
 

3885
01:16:47,280 --> 01:16:52,530
 storm statistics to I basically transmit

3886
01:16:50,510 --> 01:16:52,530
 

3887
01:16:50,520 --> 01:16:54,030
 information across different tasks or

3888
01:16:52,520 --> 01:16:54,030
 

3889
01:16:52,530 --> 01:16:57,449
 across different examples in your meta

3890
01:16:54,020 --> 01:16:57,449
 

3891
01:16:54,030 --> 01:16:58,829
 test set and with this you can actually

3892
01:16:57,439 --> 01:16:58,829
 

3893
01:16:57,449 --> 01:17:00,929
 transmit a fair amount of information

3894
01:16:58,819 --> 01:17:00,929
 

3895
01:16:58,829 --> 01:17:02,489
 through those about statistics

3896
01:17:00,919 --> 01:17:02,489
 

3897
01:17:00,929 --> 01:17:04,080
 and there are also more recent

3898
01:17:02,479 --> 01:17:04,080
 

3899
01:17:02,489 --> 01:17:05,820
 approaches that try to learn lost

3900
01:17:04,070 --> 01:17:05,820
 

3901
01:17:04,080 --> 01:17:08,330
 functions that can effectively transmit

3902
01:17:05,810 --> 01:17:08,330
 

3903
01:17:05,820 --> 01:17:10,920
 information across unlabeled examples

3904
01:17:08,320 --> 01:17:10,920
 

3905
01:17:08,330 --> 01:17:14,520
 another common theme in regard to mammal

3906
01:17:10,910 --> 01:17:14,520
 

3907
01:17:10,920 --> 01:17:16,170
 and a few questions here is well what

3908
01:17:14,510 --> 01:17:16,170
 

3909
01:17:14,520 --> 01:17:17,429
 are the relative benefits of simply

3910
01:17:16,160 --> 01:17:17,429
 

3911
01:17:16,170 --> 01:17:18,960
 doing pre training followed by

3912
01:17:17,419 --> 01:17:18,960
 

3913
01:17:17,429 --> 01:17:21,239
 fine-tuning versus doing a mammal and

3914
01:17:18,950 --> 01:17:21,239
 

3915
01:17:18,960 --> 01:17:23,280
 also can mammal itself be used as just a

3916
01:17:21,229 --> 01:17:23,280
 

3917
01:17:21,239 --> 01:17:28,800
 pre training stage without any

3918
01:17:23,270 --> 01:17:28,800
 

3919
01:17:23,280 --> 01:17:31,530
 fine-tuning so in practice mammal is

3920
01:17:28,790 --> 01:17:31,530
 

3921
01:17:28,800 --> 01:17:32,849
 certainly training for like fine-tuned

3922
01:17:31,520 --> 01:17:32,849
 

3923
01:17:31,530 --> 01:17:34,409
 ability to a new task and so I'm

3924
01:17:32,839 --> 01:17:34,409
 

3925
01:17:32,849 --> 01:17:36,869
 practice you really do need to actually

3926
01:17:34,399 --> 01:17:36,869
 

3927
01:17:34,409 --> 01:17:38,580
 fine-tune it to a test task although

3928
01:17:36,859 --> 01:17:38,580
 

3929
01:17:36,869 --> 01:17:40,080
 perhaps if you only care about getting

3930
01:17:38,570 --> 01:17:40,080
 

3931
01:17:38,580 --> 01:17:41,610
 good features and maybe the features

3932
01:17:40,070 --> 01:17:41,610
 

3933
01:17:40,080 --> 01:17:47,099
 learned by the method are something that

3934
01:17:41,600 --> 01:17:47,099
 

3935
01:17:41,610 --> 01:17:48,330
 is reasonable okay so some intermediate

3936
01:17:47,089 --> 01:17:48,330
 

3937
01:17:47,099 --> 01:17:50,790
 takeaways of these different approaches

3938
01:17:48,320 --> 01:17:50,790
 

3939
01:17:48,330 --> 01:17:52,920
 one of the benefits of things like

3940
01:17:50,780 --> 01:17:52,920
 

3941
01:17:50,790 --> 01:17:54,210
 blackbox approaches is that they are

3942
01:17:52,910 --> 01:17:54,210
 

3943
01:17:52,920 --> 01:17:56,130
 easy to combine with a variety of

3944
01:17:54,200 --> 01:17:56,130
 

3945
01:17:54,210 --> 01:17:57,449
 learning problems such as supervised

3946
01:17:56,120 --> 01:17:57,449
 

3947
01:17:56,130 --> 01:17:59,670
 learning and as you'll see after the

3948
01:17:57,439 --> 01:17:59,670
 

3949
01:17:57,449 --> 01:18:01,260
 break and reinforcement learning they do

3950
01:17:59,660 --> 01:18:01,260
 

3951
01:17:59,670 --> 01:18:02,969
 involve a challenging optimization

3952
01:18:01,250 --> 01:18:02,969
 

3953
01:18:01,260 --> 01:18:05,540
 problem and this is a bit of a subtle

3954
01:18:02,959 --> 01:18:05,540
 

3955
01:18:02,969 --> 01:18:07,560
 point which is that initialization

3956
01:18:05,530 --> 01:18:07,560
 

3957
01:18:05,540 --> 01:18:08,940
 because you're not embedding a known

3958
01:18:07,550 --> 01:18:08,940
 

3959
01:18:07,560 --> 01:18:09,900
 learning procedure into the meta

3960
01:18:08,930 --> 01:18:09,900
 

3961
01:18:08,940 --> 01:18:12,060
 learning process

3962
01:18:09,890 --> 01:18:12,060
 

3963
01:18:09,900 --> 01:18:13,199
 there's no inductive bias that points

3964
01:18:12,050 --> 01:18:13,199
 

3965
01:18:12,060 --> 01:18:15,750
 you in the right direction for how to

3966
01:18:13,189 --> 01:18:15,750
 

3967
01:18:13,199 --> 01:18:17,580
 learn from the data and as a result you

3968
01:18:15,740 --> 01:18:17,580
 

3969
01:18:15,750 --> 01:18:20,400
 have to learn how to learn from the data

3970
01:18:17,570 --> 01:18:20,400
 

3971
01:18:17,580 --> 01:18:21,480
 completely from scratch and as a result

3972
01:18:20,390 --> 01:18:21,480
 

3973
01:18:20,400 --> 01:18:22,949
 these methods can often be data

3974
01:18:21,470 --> 01:18:22,949
 

3975
01:18:21,480 --> 01:18:25,770
 inefficient because that optimization

3976
01:18:22,939 --> 01:18:25,770
 

3977
01:18:22,949 --> 01:18:28,830
 process requires time to actually learn

3978
01:18:25,760 --> 01:18:28,830
 

3979
01:18:25,770 --> 01:18:31,080
 how to learn from data and lastly these

3980
01:18:28,820 --> 01:18:31,080
 

3981
01:18:28,830 --> 01:18:33,750
 methods do have an intertwined model and

3982
01:18:31,070 --> 01:18:33,750
 

3983
01:18:31,080 --> 01:18:34,710
 architecture essentially the the model

3984
01:18:33,740 --> 01:18:34,710
 

3985
01:18:33,750 --> 01:18:35,820
 that you're using to make predictions

3986
01:18:34,700 --> 01:18:35,820
 

3987
01:18:34,710 --> 01:18:37,349
 about data points is inherently

3988
01:18:35,810 --> 01:18:37,349
 

3989
01:18:35,820 --> 01:18:40,409
 intertwined with the model that you're

3990
01:18:37,339 --> 01:18:40,409
 

3991
01:18:37,349 --> 01:18:43,949
 using to to take in the training data

3992
01:18:40,399 --> 01:18:43,949
 

3993
01:18:40,409 --> 01:18:45,480
 points as input okay as I mentioned

3994
01:18:43,939 --> 01:18:45,480
 

3995
01:18:43,949 --> 01:18:47,639
 before optimization based approaches can

3996
01:18:45,470 --> 01:18:47,639
 

3997
01:18:45,480 --> 01:18:49,980
 very nicely handle varying K and large K

3998
01:18:47,629 --> 01:18:49,980
 

3999
01:18:47,639 --> 01:18:52,650
 the structure also lends well to out of

4000
01:18:49,970 --> 01:18:52,650
 

4001
01:18:49,980 --> 01:18:53,880
 distribution tasks and one of the

4002
01:18:52,640 --> 01:18:53,880
 

4003
01:18:52,650 --> 01:18:55,349
 downsides is that does it involve the

4004
01:18:53,870 --> 01:18:55,349
 

4005
01:18:53,880 --> 01:18:56,820
 second order optimization although there

4006
01:18:55,339 --> 01:18:56,820
 

4007
01:18:55,349 --> 01:18:59,400
 are a couple of approaches towards

4008
01:18:56,810 --> 01:18:59,400
 

4009
01:18:56,820 --> 01:19:01,050
 trying to mitigate this nonparametric

4010
01:18:59,390 --> 01:19:01,050
 

4011
01:18:59,400 --> 01:19:02,969
 pressures are quite simple and they're

4012
01:19:01,040 --> 01:19:02,969
 

4013
01:19:01,050 --> 01:19:04,110
 entirely feed-forward and because

4014
01:19:02,959 --> 01:19:04,110
 

4015
01:19:02,969 --> 01:19:06,119
 they're entirely feed-forward they're

4016
01:19:04,100 --> 01:19:06,119
 

4017
01:19:04,110 --> 01:19:07,260
 computationally fast and easy to

4018
01:19:06,109 --> 01:19:07,260
 

4019
01:19:06,119 --> 01:19:08,670
 optimize because you don't need to worry

4020
01:19:07,250 --> 01:19:08,670
 

4021
01:19:07,260 --> 01:19:09,719
 about by propagating through gradient

4022
01:19:08,660 --> 01:19:09,719
 

4023
01:19:08,670 --> 01:19:12,599
 steps or through recurrent neural

4024
01:19:09,709 --> 01:19:12,599
 

4025
01:19:09,719 --> 01:19:14,280
 networks for example they're harder to

4026
01:19:12,589 --> 01:19:14,280
 

4027
01:19:12,599 --> 01:19:16,110
 generalize to varying K this

4028
01:19:14,270 --> 01:19:16,110
 

4029
01:19:14,280 --> 01:19:18,809
 more of an empirical observation than

4030
01:19:16,100 --> 01:19:18,809
 

4031
01:19:16,110 --> 01:19:19,860
 then a theoretical one and of course

4032
01:19:18,799 --> 01:19:19,860
 

4033
01:19:18,809 --> 01:19:21,059
 because they're using non parametric

4034
01:19:19,850 --> 01:19:21,059
 

4035
01:19:19,860 --> 01:19:24,300
 approaches they're harder to scale to

4036
01:19:21,049 --> 01:19:24,300
 

4037
01:19:21,059 --> 01:19:25,710
 very large K because the the computation

4038
01:19:24,290 --> 01:19:25,710
 

4039
01:19:24,300 --> 01:19:27,960
 grows with with more and more data

4040
01:19:25,700 --> 01:19:27,960
 

4041
01:19:25,710 --> 01:19:29,429
 points okay

4042
01:19:27,950 --> 01:19:29,429
 

4043
01:19:27,960 --> 01:19:30,449
 and the MAS leaders approach is so far

4044
01:19:29,419 --> 01:19:30,449
 

4045
01:19:29,429 --> 01:19:32,159
 have been limited to classification

4046
01:19:30,439 --> 01:19:32,159
 

4047
01:19:30,449 --> 01:19:35,010
 approaches because you're inherently

4048
01:19:32,149 --> 01:19:35,010
 

4049
01:19:32,159 --> 01:19:36,659
 making making decisions based off of the

4050
01:19:35,000 --> 01:19:36,659
 

4051
01:19:35,010 --> 01:19:39,570
 based on the data points that you've

4052
01:19:36,649 --> 01:19:39,570
 

4053
01:19:36,659 --> 01:19:41,039
 seen so far okay and lastly it's worth

4054
01:19:39,560 --> 01:19:41,039
 

4055
01:19:39,570 --> 01:19:42,840
 pointing out that well team versions of

4056
01:19:41,029 --> 01:19:42,840
 

4057
01:19:41,039 --> 01:19:45,690
 each of these approaches I tend to

4058
01:19:42,830 --> 01:19:45,690
 

4059
01:19:42,840 --> 01:19:48,420
 perform compared ibly on existing few

4060
01:19:45,680 --> 01:19:48,420
 

4061
01:19:45,690 --> 01:19:49,860
 taught learning benchmarks so at the

4062
01:19:48,410 --> 01:19:49,860
 

4063
01:19:48,420 --> 01:19:54,980
 performance level they all perform quite

4064
01:19:49,850 --> 01:19:54,980
 

4065
01:19:49,860 --> 01:19:57,989
 well okay so now let's talk about

4066
01:19:54,970 --> 01:19:57,989
 

4067
01:19:54,980 --> 01:20:00,900
 Bayesian meta learning approaches so we

4068
01:19:57,979 --> 01:20:00,900
 

4069
01:19:57,989 --> 01:20:02,070
 had this really nice motivation for for

4070
01:20:00,890 --> 01:20:02,070
 

4071
01:20:00,900 --> 01:20:04,500
 Bayesian methods and then we kind of

4072
01:20:02,060 --> 01:20:04,500
 

4073
01:20:02,070 --> 01:20:07,650
 threw that all away in the last 30

4074
01:20:04,490 --> 01:20:07,650
 

4075
01:20:04,500 --> 01:20:09,719
 minutes or so and so and particular what

4076
01:20:07,640 --> 01:20:09,719
 

4077
01:20:07,650 --> 01:20:11,489
 we did is we assume that this PFI given

4078
01:20:09,709 --> 01:20:11,489
 

4079
01:20:09,719 --> 01:20:12,869
 the data set and the meta parameters is

4080
01:20:11,479 --> 01:20:12,869
 

4081
01:20:11,489 --> 01:20:15,000
 going to be a point estimate or a

4082
01:20:12,859 --> 01:20:15,000
 

4083
01:20:12,869 --> 01:20:17,270
 deterministic function of the training

4084
01:20:14,990 --> 01:20:17,270
 

4085
01:20:15,000 --> 01:20:19,380
 data point and and the meta parameters

4086
01:20:17,260 --> 01:20:19,380
 

4087
01:20:17,270 --> 01:20:20,639
 and you might as well maybe this is all

4088
01:20:19,370 --> 01:20:20,639
 

4089
01:20:19,380 --> 01:20:22,530
 fine like we don't we don't really need

4090
01:20:20,629 --> 01:20:22,530
 

4091
01:20:20,639 --> 01:20:24,989
 Bayesian things but there are many

4092
01:20:22,520 --> 01:20:24,989
 

4093
01:20:22,530 --> 01:20:26,670
 situations where it actually is useful

4094
01:20:24,979 --> 01:20:26,670
 

4095
01:20:24,989 --> 01:20:28,590
 to be representing actually a

4096
01:20:26,660 --> 01:20:28,590
 

4097
01:20:26,670 --> 01:20:30,360
 distribution over the potential

4098
01:20:28,580 --> 01:20:30,360
 

4099
01:20:28,590 --> 01:20:32,639
 functions that we might be encountering

4100
01:20:30,350 --> 01:20:32,639
 

4101
01:20:30,360 --> 01:20:34,050
 and in particular because future

4102
01:20:32,629 --> 01:20:34,050
 

4103
01:20:32,639 --> 01:20:35,460
 learning problems only have a small

4104
01:20:34,040 --> 01:20:35,460
 

4105
01:20:34,050 --> 01:20:37,619
 number of data points they may be

4106
01:20:35,450 --> 01:20:37,619
 

4107
01:20:35,460 --> 01:20:40,020
 ambiguous even with when you do have a

4108
01:20:37,609 --> 01:20:40,020
 

4109
01:20:37,619 --> 01:20:41,340
 prior for example if your goal is to

4110
01:20:40,010 --> 01:20:41,340
 

4111
01:20:40,020 --> 01:20:43,050
 classify between all the images on the

4112
01:20:41,330 --> 01:20:43,050
 

4113
01:20:41,340 --> 01:20:44,280
 left and all the images on the right all

4114
01:20:43,040 --> 01:20:44,280
 

4115
01:20:43,050 --> 01:20:45,989
 the folks on the left are smiling

4116
01:20:44,270 --> 01:20:45,989
 

4117
01:20:44,280 --> 01:20:47,460
 wearing hats and young all the folks on

4118
01:20:45,979 --> 01:20:47,460
 

4119
01:20:45,989 --> 01:20:49,349
 the right or not and so if you get an

4120
01:20:47,450 --> 01:20:49,349
 

4121
01:20:47,460 --> 01:20:51,420
 image that has someone that smiling and

4122
01:20:49,339 --> 01:20:51,420
 

4123
01:20:49,349 --> 01:20:53,070
 wearing hot and not young or not smiling

4124
01:20:51,410 --> 01:20:53,070
 

4125
01:20:51,420 --> 01:20:54,840
 and wearing a hot and young that it's

4126
01:20:53,060 --> 01:20:54,840
 

4127
01:20:53,070 --> 01:20:56,520
 inherently ambiguous on what the correct

4128
01:20:54,830 --> 01:20:56,520
 

4129
01:20:54,840 --> 01:20:57,449
 label is for these classes because you

4130
01:20:56,510 --> 01:20:57,449
 

4131
01:20:56,520 --> 01:21:00,300
 don't know if you're supposed to be

4132
01:20:57,439 --> 01:21:00,300
 

4133
01:20:57,449 --> 01:21:01,829
 classifying on the attribute of smiling

4134
01:21:00,290 --> 01:21:01,829
 

4135
01:21:00,300 --> 01:21:04,980
 on the attribute of wearing a hat or on

4136
01:21:01,819 --> 01:21:04,980
 

4137
01:21:01,829 --> 01:21:06,090
 the attribute of being young and so what

4138
01:21:04,970 --> 01:21:06,090
 

4139
01:21:04,980 --> 01:21:07,949
 you might ask this can be generate

4140
01:21:06,080 --> 01:21:07,949
 

4141
01:21:06,090 --> 01:21:09,840
 hypotheses about the underlying function

4142
01:21:07,939 --> 01:21:09,840
 

4143
01:21:07,949 --> 01:21:11,520
 I like the three hypotheses that I

4144
01:21:09,830 --> 01:21:11,520
 

4145
01:21:09,840 --> 01:21:15,059
 mentioned before essentially can you

4146
01:21:11,510 --> 01:21:15,059
 

4147
01:21:11,520 --> 01:21:17,070
 sample from this function PFI and this

4148
01:21:15,049 --> 01:21:17,070
 

4149
01:21:15,059 --> 01:21:19,170
 is gonna be important for for ambiguous

4150
01:21:17,060 --> 01:21:19,170
 

4151
01:21:17,070 --> 01:21:20,610
 problems but also for safety critical

4152
01:21:19,160 --> 01:21:20,610
 

4153
01:21:19,170 --> 01:21:23,550
 few shot learning such as medical

4154
01:21:20,600 --> 01:21:23,550
 

4155
01:21:20,610 --> 01:21:25,110
 imaging for learning to actively learn

4156
01:21:23,540 --> 01:21:25,110
 

4157
01:21:23,550 --> 01:21:26,909
 because if you care about actually

4158
01:21:25,100 --> 01:21:26,909
 

4159
01:21:25,110 --> 01:21:27,650
 reducing your uncertainty about a given

4160
01:21:26,899 --> 01:21:27,650
 

4161
01:21:26,909 --> 01:21:29,480
 function and get

4162
01:21:27,640 --> 01:21:29,480
 

4163
01:21:27,650 --> 01:21:30,380
 labels for new data points and you want

4164
01:21:29,470 --> 01:21:30,380
 

4165
01:21:29,480 --> 01:21:31,699
 to be able to reason about your

4166
01:21:30,370 --> 01:21:31,699
 

4167
01:21:30,380 --> 01:21:35,030
 uncertainty over your function space

4168
01:21:31,689 --> 01:21:35,030
 

4169
01:21:31,699 --> 01:21:36,350
 right now and it's also and there have

4170
01:21:35,020 --> 01:21:36,350
 

4171
01:21:35,030 --> 01:21:38,270
 been approaches for using active

4172
01:21:36,340 --> 01:21:38,270
 

4173
01:21:36,350 --> 01:21:40,250
 learning with meta learning and it's

4174
01:21:38,260 --> 01:21:40,250
 

4175
01:21:38,270 --> 01:21:41,390
 also useful for learning to explore and

4176
01:21:40,240 --> 01:21:41,390
 

4177
01:21:40,250 --> 01:21:43,070
 meta reinforcement learning because

4178
01:21:41,380 --> 01:21:43,070
 

4179
01:21:41,390 --> 01:21:45,730
 again we can try to explicitly reduce

4180
01:21:43,060 --> 01:21:45,730
 

4181
01:21:43,070 --> 01:21:46,909
 our uncertainty by collecting more data

4182
01:21:45,720 --> 01:21:46,909
 

4183
01:21:45,730 --> 01:21:49,250
 okay

4184
01:21:46,899 --> 01:21:49,250
 

4185
01:21:46,909 --> 01:21:50,929
 so how do we go about trying to medal

4186
01:21:49,240 --> 01:21:50,929
 

4187
01:21:49,250 --> 01:21:52,670
 learn in a way that allows us to

4188
01:21:50,919 --> 01:21:52,670
 

4189
01:21:50,929 --> 01:21:55,010
 generate hypotheses and reason about

4190
01:21:52,660 --> 01:21:55,010
 

4191
01:21:52,670 --> 01:21:56,360
 distributions so we can bring up the

4192
01:21:55,000 --> 01:21:56,360
 

4193
01:21:55,010 --> 01:21:57,739
 graphical model that Sergei mentioned

4194
01:21:56,350 --> 01:21:57,739
 

4195
01:21:56,360 --> 01:21:59,360
 before where we're going to have a

4196
01:21:57,729 --> 01:21:59,360
 

4197
01:21:57,739 --> 01:22:01,280
 distribution over our prior parameters

4198
01:21:59,350 --> 01:22:01,280
 

4199
01:21:59,360 --> 01:22:03,890
 theta we will have a distribution over

4200
01:22:01,270 --> 01:22:03,890
 

4201
01:22:01,280 --> 01:22:06,580
 our task specific parameters by I given

4202
01:22:03,880 --> 01:22:06,580
 

4203
01:22:03,890 --> 01:22:08,600
 theta and then we'll also have

4204
01:22:06,570 --> 01:22:08,600
 

4205
01:22:06,580 --> 01:22:09,739
 probability distributions corresponding

4206
01:22:08,590 --> 01:22:09,739
 

4207
01:22:08,600 --> 01:22:11,690
 the probability of our training dataset

4208
01:22:09,729 --> 01:22:11,690
 

4209
01:22:09,739 --> 01:22:13,190
 giving our given our task parameters and

4210
01:22:11,680 --> 01:22:13,190
 

4211
01:22:11,690 --> 01:22:16,040
 our test dataset given our task

4212
01:22:13,180 --> 01:22:16,040
 

4213
01:22:13,190 --> 01:22:18,400
 parameters and then our goal will be can

4214
01:22:16,030 --> 01:22:18,400
 

4215
01:22:16,040 --> 01:22:21,199
 we sample task Pacific parameters Phi I

4216
01:22:18,390 --> 01:22:21,199
 

4217
01:22:18,400 --> 01:22:23,120
 given our training data points and

4218
01:22:21,189 --> 01:22:23,120
 

4219
01:22:21,199 --> 01:22:25,370
 ex-house essentially given all of the

4220
01:22:23,110 --> 01:22:25,370
 

4221
01:22:23,120 --> 01:22:26,659
 observed variables and because of

4222
01:22:25,360 --> 01:22:26,659
 

4223
01:22:25,370 --> 01:22:28,460
 conditional independence a test time we

4224
01:22:26,649 --> 01:22:28,460
 

4225
01:22:26,659 --> 01:22:29,900
 we don't really need to worry too much

4226
01:22:28,450 --> 01:22:29,900
 

4227
01:22:28,460 --> 01:22:32,000
 about X test we just want to be able to

4228
01:22:29,890 --> 01:22:32,000
 

4229
01:22:29,900 --> 01:22:35,420
 sample Phi I given x training in a white

4230
01:22:31,990 --> 01:22:35,420
 

4231
01:22:32,000 --> 01:22:38,510
 train so blackbox adaptation approaches

4232
01:22:35,410 --> 01:22:38,510
 

4233
01:22:35,420 --> 01:22:40,520
 are quite easy to to extend to this

4234
01:22:38,500 --> 01:22:40,520
 

4235
01:22:38,510 --> 01:22:42,080
 Bayesian setting so what we can do is we

4236
01:22:40,510 --> 01:22:42,080
 

4237
01:22:40,520 --> 01:22:43,969
 can use amortized variational inference

4238
01:22:42,070 --> 01:22:43,969
 

4239
01:22:42,080 --> 01:22:46,489
 we can train a neural network to

4240
01:22:43,959 --> 01:22:46,489
 

4241
01:22:43,969 --> 01:22:48,290
 represent a distribution over sufficient

4242
01:22:46,479 --> 01:22:48,290
 

4243
01:22:46,489 --> 01:22:49,880
 statistics we can also represent

4244
01:22:48,280 --> 01:22:49,880
 

4245
01:22:48,290 --> 01:22:51,409
 distribution over full parameter vectors

4246
01:22:49,870 --> 01:22:51,409
 

4247
01:22:49,880 --> 01:22:54,290
 Phi I although that might get a bit

4248
01:22:51,399 --> 01:22:54,290
 

4249
01:22:51,409 --> 01:22:55,730
 unwieldy at some point and so we could

4250
01:22:54,280 --> 01:22:55,730
 

4251
01:22:54,290 --> 01:22:57,679
 train a neural network to represent a

4252
01:22:55,720 --> 01:22:57,679
 

4253
01:22:55,730 --> 01:22:59,510
 Gaussian distribution over H and then

4254
01:22:57,669 --> 01:22:59,510
 

4255
01:22:57,679 --> 01:23:03,560
 feed this H into another neural network

4256
01:22:59,500 --> 01:23:03,560
 

4257
01:22:59,510 --> 01:23:04,790
 and use ideas from from amortized

4258
01:23:03,550 --> 01:23:04,790
 

4259
01:23:03,560 --> 01:23:06,560
 variational inference such as the

4260
01:23:04,780 --> 01:23:06,560
 

4261
01:23:04,790 --> 01:23:08,600
 reprioritization trick to train this

4262
01:23:06,550 --> 01:23:08,600
 

4263
01:23:06,560 --> 01:23:11,870
 model in order to effectively produce

4264
01:23:08,590 --> 01:23:11,870
 

4265
01:23:08,600 --> 01:23:13,159
 this distribution indeed various

4266
01:23:11,860 --> 01:23:13,159
 

4267
01:23:11,870 --> 01:23:16,190
 approaches have used this type of

4268
01:23:13,149 --> 01:23:16,190
 

4269
01:23:13,159 --> 01:23:17,929
 approach for example Gordon at all I use

4270
01:23:16,180 --> 01:23:17,929
 

4271
01:23:16,190 --> 01:23:19,580
 this approach where H correspond to the

4272
01:23:17,919 --> 01:23:19,580
 

4273
01:23:17,929 --> 01:23:24,739
 the weights of the last layer of a

4274
01:23:19,570 --> 01:23:24,739
 

4275
01:23:19,580 --> 01:23:27,260
 neural network okay so now now that

4276
01:23:24,729 --> 01:23:27,260
 

4277
01:23:24,739 --> 01:23:28,460
 we've gone over how you can a very

4278
01:23:27,250 --> 01:23:28,460
 

4279
01:23:27,260 --> 01:23:30,620
 naturally apply this with blackbox

4280
01:23:28,450 --> 01:23:30,620
 

4281
01:23:28,460 --> 01:23:32,960
 approaches what about optimization based

4282
01:23:30,610 --> 01:23:32,960
 

4283
01:23:30,620 --> 01:23:36,489
 metal earning approaches can we further

4284
01:23:32,950 --> 01:23:36,489
 

4285
01:23:32,960 --> 01:23:38,420
 extend these to the Bayesian setting

4286
01:23:36,479 --> 01:23:38,420
 

4287
01:23:36,489 --> 01:23:39,890
 there's a few different ways that you go

4288
01:23:38,410 --> 01:23:39,890
 

4289
01:23:38,420 --> 01:23:41,500
 about doing this all this is this

4290
01:23:39,880 --> 01:23:41,500
 

4291
01:23:39,890 --> 01:23:44,120
 her than the simple black box approach

4292
01:23:41,490 --> 01:23:44,120
 

4293
01:23:41,500 --> 01:23:46,190
 one thing we could do is model the

4294
01:23:44,110 --> 01:23:46,190
 

4295
01:23:44,120 --> 01:23:49,190
 distribution of Phi given theta as

4296
01:23:46,180 --> 01:23:49,190
 

4297
01:23:46,190 --> 01:23:50,540
 Gaussian and use the same sort of

4298
01:23:49,180 --> 01:23:50,540
 

4299
01:23:49,190 --> 01:23:52,460
 variational inference for training as I

4300
01:23:50,530 --> 01:23:52,460
 

4301
01:23:50,540 --> 01:23:55,130
 mentioned before but have our inference

4302
01:23:52,450 --> 01:23:55,130
 

4303
01:23:52,460 --> 01:23:58,580
 network you perform an optimization over

4304
01:23:55,120 --> 01:23:58,580
 

4305
01:23:55,130 --> 01:24:02,450
 Phi and this is what was done in Ravi

4306
01:23:58,570 --> 01:24:02,450
 

4307
01:23:58,580 --> 01:24:03,440
 and Beatson in 2019 another thing that

4308
01:24:02,440 --> 01:24:03,440
 

4309
01:24:02,450 --> 01:24:06,500
 we want to do is if you don't want to

4310
01:24:03,430 --> 01:24:06,500
 

4311
01:24:03,440 --> 01:24:09,050
 model p 5 theta i give p PF i given

4312
01:24:06,490 --> 01:24:09,050
 

4313
01:24:06,500 --> 01:24:12,200
 theta as gaussian we can use stein

4314
01:24:09,040 --> 01:24:12,200
 

4315
01:24:09,050 --> 01:24:16,070
 variational gradient too on the last

4316
01:24:12,190 --> 01:24:16,070
 

4317
01:24:12,200 --> 01:24:17,150
 layer of the neural network and do

4318
01:24:16,060 --> 01:24:17,150
 

4319
01:24:16,070 --> 01:24:18,740
 gradient based inference on the last

4320
01:24:17,140 --> 01:24:18,740
 

4321
01:24:17,150 --> 01:24:20,720
 layer and only have a single set of

4322
01:24:18,730 --> 01:24:20,720
 

4323
01:24:18,740 --> 01:24:23,390
 parameters similar to theta G for all

4324
01:24:20,710 --> 01:24:23,390
 

4325
01:24:20,720 --> 01:24:26,020
 the other layers and also we could use

4326
01:24:23,380 --> 01:24:26,020
 

4327
01:24:23,390 --> 01:24:28,130
 something like an ensemble of mammals

4328
01:24:26,010 --> 01:24:28,130
 

4329
01:24:26,020 --> 01:24:29,600
 ensemble of meta trainer all networks

4330
01:24:28,120 --> 01:24:29,600
 

4331
01:24:28,130 --> 01:24:32,630
 that allows us to represent basically

4332
01:24:29,590 --> 01:24:32,630
 

4333
01:24:29,600 --> 01:24:34,490
 particles at the distribution and both

4334
01:24:32,620 --> 01:24:34,490
 

4335
01:24:32,630 --> 01:24:38,390
 of these were proposed by Kim at all in

4336
01:24:34,480 --> 01:24:38,390
 

4337
01:24:34,490 --> 01:24:39,890
 2018 know one thing you might ask is

4338
01:24:38,380 --> 01:24:39,890
 

4339
01:24:38,390 --> 01:24:41,060
 well can we can we get some of the kind

4340
01:24:39,880 --> 01:24:41,060
 

4341
01:24:39,890 --> 01:24:43,330
 of the benefits of both of these can we

4342
01:24:41,050 --> 01:24:43,330
 

4343
01:24:41,060 --> 01:24:47,450
 model both a non Gaussian posterior and

4344
01:24:43,320 --> 01:24:47,450
 

4345
01:24:43,330 --> 01:24:49,370
 do so over all of the parameters and

4346
01:24:47,440 --> 01:24:49,370
 

4347
01:24:47,450 --> 01:24:51,110
 there is a way that we can do this in

4348
01:24:49,360 --> 01:24:51,110
 

4349
01:24:49,370 --> 01:24:52,730
 fact and so let's go back to our

4350
01:24:51,100 --> 01:24:52,730
 

4351
01:24:51,110 --> 01:24:55,250
 graphical model say that we want to

4352
01:24:52,720 --> 01:24:55,250
 

4353
01:24:52,730 --> 01:24:56,660
 sample from this this distribution you

4354
01:24:55,240 --> 01:24:56,660
 

4355
01:24:55,250 --> 01:25:00,080
 can write out as the integral where

4356
01:24:56,650 --> 01:25:00,080
 

4357
01:24:56,660 --> 01:25:01,310
 we're integrating out P theta of course

4358
01:25:00,070 --> 01:25:01,310
 

4359
01:25:00,080 --> 01:25:02,990
 this integral is completely intractable

4360
01:25:01,300 --> 01:25:02,990
 

4361
01:25:01,310 --> 01:25:05,720
 if they are the parameters of a neural

4362
01:25:02,980 --> 01:25:05,720
 

4363
01:25:02,990 --> 01:25:08,450
 network but one thing you might ask is

4364
01:25:05,710 --> 01:25:08,450
 

4365
01:25:05,720 --> 01:25:11,870
 well what if we knew p if i given theta

4366
01:25:08,440 --> 01:25:11,870
 

4367
01:25:08,450 --> 01:25:13,640
 and the training data set if we knew

4368
01:25:11,860 --> 01:25:13,640
 

4369
01:25:11,870 --> 01:25:15,220
 this distribution our graphical model

4370
01:25:13,630 --> 01:25:15,220
 

4371
01:25:13,640 --> 01:25:17,210
 would be transformed in this way

4372
01:25:15,210 --> 01:25:17,210
 

4373
01:25:15,220 --> 01:25:20,090
 switching the arrows from the training

4374
01:25:17,200 --> 01:25:20,090
 

4375
01:25:17,210 --> 01:25:22,040
 data set to 5 and then we could simply

4376
01:25:20,080 --> 01:25:22,040
 

4377
01:25:20,090 --> 01:25:24,080
 sample with ancestral sampling we could

4378
01:25:22,030 --> 01:25:24,080
 

4379
01:25:22,040 --> 01:25:28,510
 sample with theta from the prior sample

4380
01:25:24,070 --> 01:25:28,510
 

4381
01:25:24,080 --> 01:25:32,570
 5 from this distribution and and and get

4382
01:25:28,500 --> 01:25:32,570
 

4383
01:25:28,510 --> 01:25:33,680
 get a get a sampled 5 and so the key

4384
01:25:32,560 --> 01:25:33,680
 

4385
01:25:32,570 --> 01:25:36,290
 idea is what we can do here is we can

4386
01:25:33,670 --> 01:25:36,290
 

4387
01:25:33,680 --> 01:25:39,710
 approximate this distribution Phi given

4388
01:25:36,280 --> 01:25:39,710
 

4389
01:25:36,290 --> 01:25:42,590
 theta and D train as a a point estimate

4390
01:25:39,700 --> 01:25:42,590
 

4391
01:25:39,710 --> 01:25:44,030
 using map of course this is an extremely

4392
01:25:42,580 --> 01:25:44,030
 

4393
01:25:42,590 --> 01:25:46,160
 crude approximation similar to the one

4394
01:25:44,020 --> 01:25:46,160
 

4395
01:25:44,030 --> 01:25:48,170
 that we use in mammal but it's also

4396
01:25:46,150 --> 01:25:48,170
 

4397
01:25:46,160 --> 01:25:50,630
 extremely convenient and in particular

4398
01:25:48,160 --> 01:25:50,630
 

4399
01:25:48,170 --> 01:25:52,789
 if we do this approximation we can still

4400
01:25:50,620 --> 01:25:52,789
 

4401
01:25:50,630 --> 01:25:58,429
 get samples from

4402
01:25:52,779 --> 01:25:58,429
 

4403
01:25:52,789 --> 01:25:59,840
 PFI given the training data and in fact

4404
01:25:58,419 --> 01:25:59,840
 

4405
01:25:58,429 --> 01:26:01,699
 we can to do this approximation we can

4406
01:25:59,830 --> 01:26:01,699
 

4407
01:25:59,840 --> 01:26:04,249
 use the same gradient based map

4408
01:26:01,689 --> 01:26:04,249
 

4409
01:26:01,699 --> 01:26:07,909
 approximation as shown before but we

4410
01:26:04,239 --> 01:26:07,909
 

4411
01:26:04,249 --> 01:26:11,599
 don't require we don't require doing the

4412
01:26:07,899 --> 01:26:11,599
 

4413
01:26:07,909 --> 01:26:13,190
 intractable inner girl at the top okay

4414
01:26:11,589 --> 01:26:13,190
 

4415
01:26:11,599 --> 01:26:15,199
 so at test time what this corresponds to

4416
01:26:13,180 --> 01:26:15,199
 

4417
01:26:13,190 --> 01:26:19,039
 is we'll have initial set of parameters

4418
01:26:15,189 --> 01:26:19,039
 

4419
01:26:15,199 --> 01:26:20,539
 theta will add noise to sample both add

4420
01:26:19,029 --> 01:26:20,539
 

4421
01:26:19,039 --> 01:26:23,059
 noise so that that parameter vector to

4422
01:26:20,529 --> 01:26:23,059
 

4423
01:26:20,539 --> 01:26:25,459
 basically sample a new theta and then

4424
01:26:23,049 --> 01:26:25,459
 

4425
01:26:23,059 --> 01:26:28,039
 we'll run gradient descent from there so

4426
01:26:25,449 --> 01:26:28,039
 

4427
01:26:25,459 --> 01:26:31,280
 it's essentially a way to make mammal Li

4428
01:26:28,029 --> 01:26:31,280
 

4429
01:26:28,039 --> 01:26:35,449
 give you estimates and sample from PFI

4430
01:26:31,270 --> 01:26:35,449
 

4431
01:26:31,280 --> 01:26:37,010
 given your training data but doing so in

4432
01:26:35,439 --> 01:26:37,010
 

4433
01:26:35,449 --> 01:26:40,849
 a way that actually will lead to the

4434
01:26:37,000 --> 01:26:40,849
 

4435
01:26:37,010 --> 01:26:42,469
 correct posterior distribution I didn't

4436
01:26:40,839 --> 01:26:42,469
 

4437
01:26:40,849 --> 01:26:44,809
 go through actually how we trained this

4438
01:26:42,459 --> 01:26:44,809
 

4439
01:26:42,469 --> 01:26:46,099
 that's a bit harder and for details you

4440
01:26:44,799 --> 01:26:46,099
 

4441
01:26:44,809 --> 01:26:48,530
 can look at the probabilistic mammal

4442
01:26:46,089 --> 01:26:48,530
 

4443
01:26:46,099 --> 01:26:50,059
 paper on the bottom we refer to this

4444
01:26:48,520 --> 01:26:50,059
 

4445
01:26:48,530 --> 01:26:51,619
 algorithm as platypus or a probabilistic

4446
01:26:50,049 --> 01:26:51,619
 

4447
01:26:50,059 --> 01:26:54,289
 latent model for incorporating priors

4448
01:26:51,609 --> 01:26:54,289
 

4449
01:26:51,619 --> 01:26:55,639
 and uncertainty in future learning which

4450
01:26:54,279 --> 01:26:55,639
 

4451
01:26:54,289 --> 01:26:58,219
 is a bit of a tortured acronym but we

4452
01:26:55,629 --> 01:26:58,219
 

4453
01:26:55,639 --> 01:26:59,449
 can then get another type of male and

4454
01:26:58,209 --> 01:26:59,449
 

4455
01:26:58,219 --> 01:27:00,849
 what this gives us is it gives us the

4456
01:26:59,439 --> 01:27:00,849
 

4457
01:26:59,449 --> 01:27:03,050
 ability to sample different functions

4458
01:27:00,839 --> 01:27:03,050
 

4459
01:27:00,849 --> 01:27:06,320
 with only a few data points such as

4460
01:27:03,040 --> 01:27:06,320
 

4461
01:27:03,050 --> 01:27:07,849
 linear functions sinusoid functions but

4462
01:27:06,310 --> 01:27:07,849
 

4463
01:27:06,320 --> 01:27:09,920
 ever inherently ambiguous because of

4464
01:27:07,839 --> 01:27:09,920
 

4465
01:27:07,849 --> 01:27:11,749
 noise in the data as well as represent

4466
01:27:09,910 --> 01:27:11,749
 

4467
01:27:09,920 --> 01:27:12,979
 different classification problems for

4468
01:27:11,739 --> 01:27:12,979
 

4469
01:27:11,749 --> 01:27:14,690
 different classifiers with different

4470
01:27:12,969 --> 01:27:14,690
 

4471
01:27:12,979 --> 01:27:17,570
 decision boundaries as again shown in

4472
01:27:14,680 --> 01:27:17,570
 

4473
01:27:14,690 --> 01:27:19,639
 the colored dashed lines and perhaps

4474
01:27:17,560 --> 01:27:19,639
 

4475
01:27:17,570 --> 01:27:21,739
 most importantly it's also it's better

4476
01:27:19,629 --> 01:27:21,739
 

4477
01:27:19,639 --> 01:27:23,780
 able to model ambiguous future learning

4478
01:27:21,729 --> 01:27:23,780
 

4479
01:27:21,739 --> 01:27:28,010
 problems like I showed on the first

4480
01:27:23,770 --> 01:27:28,010
 

4481
01:27:23,780 --> 01:27:29,329
 slide ok for further reading on Bayesian

4482
01:27:28,000 --> 01:27:29,329
 

4483
01:27:28,010 --> 01:27:33,019
 buy learning approaches you can look at

4484
01:27:29,319 --> 01:27:33,019
 

4485
01:27:29,329 --> 01:27:34,670
 the papers shown here and next we can go

4486
01:27:33,009 --> 01:27:34,670
 

4487
01:27:33,019 --> 01:27:40,159
 to a couple questions before moving on

4488
01:27:34,660 --> 01:27:40,159
 

4489
01:27:34,670 --> 01:27:41,989
 to applications so one question is if we

4490
01:27:40,149 --> 01:27:41,989
 

4491
01:27:40,159 --> 01:27:43,519
 want to do a Bayesian metal learning

4492
01:27:41,979 --> 01:27:43,519
 

4493
01:27:41,989 --> 01:27:48,800
 approach can we just use drop out at a

4494
01:27:43,509 --> 01:27:48,800
 

4495
01:27:43,519 --> 01:27:51,519
 test time right so if you just drop out

4496
01:27:48,790 --> 01:27:51,519
 

4497
01:27:48,800 --> 01:27:54,440
 at test time you will certainly get a

4498
01:27:51,509 --> 01:27:54,440
 

4499
01:27:51,519 --> 01:27:56,449
 distribution there's no guarantee if you

4500
01:27:54,430 --> 01:27:56,449
 

4501
01:27:54,440 --> 01:27:57,559
 don't train for that distribution to

4502
01:27:56,439 --> 01:27:57,559
 

4503
01:27:56,449 --> 01:27:59,719
 actually represent represent the

4504
01:27:57,549 --> 01:27:59,719
 

4505
01:27:57,559 --> 01:28:03,860
 posterior then you won't actually get

4506
01:27:59,709 --> 01:28:03,860
 

4507
01:27:59,719 --> 01:28:05,380
 the correct posterior over functions for

4508
01:28:03,850 --> 01:28:05,380
 

4509
01:28:03,860 --> 01:28:07,000
 probabilistic mammal

4510
01:28:05,370 --> 01:28:07,000
 

4511
01:28:05,380 --> 01:28:08,530
 you assume that there's no relationship

4512
01:28:06,990 --> 01:28:08,530
 

4513
01:28:07,000 --> 01:28:15,790
 between the training X and the training

4514
01:28:08,520 --> 01:28:15,790
 

4515
01:28:08,530 --> 01:28:19,780
 Y the graphical model assumes that the

4516
01:28:15,780 --> 01:28:19,780
 

4517
01:28:15,790 --> 01:28:23,110
 that there is that the labels Y are

4518
01:28:19,770 --> 01:28:23,110
 

4519
01:28:19,780 --> 01:28:24,610
 dependent on the inputs the the labels Y

4520
01:28:23,100 --> 01:28:24,610
 

4521
01:28:23,110 --> 01:28:31,270
 train are dependent on the inputs X

4522
01:28:24,600 --> 01:28:31,270
 

4523
01:28:24,610 --> 01:28:32,410
 train and the the parameters Phi okay so

4524
01:28:31,260 --> 01:28:32,410
 

4525
01:28:31,270 --> 01:28:34,390
 let's go through a couple applications

4526
01:28:32,400 --> 01:28:34,390
 

4527
01:28:32,410 --> 01:28:36,580
 this will be quite quick meta-learning

4528
01:28:34,380 --> 01:28:36,580
 

4529
01:28:34,390 --> 01:28:38,620
 has been used in a variety of computer

4530
01:28:36,570 --> 01:28:38,620
 

4531
01:28:36,580 --> 01:28:41,070
 vision applications such as image

4532
01:28:38,610 --> 01:28:41,070
 

4533
01:28:38,620 --> 01:28:45,190
 recognition modeling

4534
01:28:41,060 --> 01:28:45,190
 

4535
01:28:41,070 --> 01:28:46,870
 the motion and the pose of humans uses

4536
01:28:45,180 --> 01:28:46,870
 

4537
01:28:45,190 --> 01:28:48,430
 for domain adaptation for example when

4538
01:28:46,860 --> 01:28:48,430
 

4539
01:28:46,870 --> 01:28:50,920
 you want to adapt to new domains and you

4540
01:28:48,420 --> 01:28:50,920
 

4541
01:28:48,430 --> 01:28:52,930
 have a variety of domains to train on as

4542
01:28:50,910 --> 01:28:52,930
 

4543
01:28:50,920 --> 01:28:54,400
 well as for a few shots segmentation

4544
01:28:52,920 --> 01:28:54,400
 

4545
01:28:52,930 --> 01:28:58,480
 problems or you want to segment images

4546
01:28:54,390 --> 01:28:58,480
 

4547
01:28:54,400 --> 01:29:00,550
 given only a few label pixels beyond

4548
01:28:58,470 --> 01:29:00,550
 

4549
01:28:58,480 --> 01:29:02,140
 some of those kind of typical supervised

4550
01:29:00,540 --> 01:29:02,140
 

4551
01:29:00,550 --> 01:29:04,870
 computer vision problems people have

4552
01:29:02,130 --> 01:29:04,870
 

4553
01:29:02,140 --> 01:29:06,850
 also looked at tired of modeling with

4554
01:29:04,860 --> 01:29:06,850
 

4555
01:29:04,870 --> 01:29:08,890
 with meta learning methods this includes

4556
01:29:06,840 --> 01:29:08,890
 

4557
01:29:06,850 --> 01:29:10,420
 few shot image generation if you shot

4558
01:29:08,880 --> 01:29:10,420
 

4559
01:29:08,890 --> 01:29:11,730
 image the image translation when you

4560
01:29:10,410 --> 01:29:11,730
 

4561
01:29:10,420 --> 01:29:13,840
 where you want to translate between

4562
01:29:11,720 --> 01:29:13,840
 

4563
01:29:11,730 --> 01:29:15,610
 different types of images where you want

4564
01:29:13,830 --> 01:29:15,610
 

4565
01:29:13,840 --> 01:29:17,100
 to translate things to a new type of

4566
01:29:15,600 --> 01:29:17,100
 

4567
01:29:15,610 --> 01:29:19,660
 animal that that you haven't seen before

4568
01:29:17,090 --> 01:29:19,660
 

4569
01:29:17,100 --> 01:29:21,970
 also a generation of novel viewpoints

4570
01:29:19,650 --> 01:29:21,970
 

4571
01:29:19,660 --> 01:29:24,430
 given a single viewpoint as well as

4572
01:29:21,960 --> 01:29:24,430
 

4573
01:29:21,970 --> 01:29:29,110
 generating videos of people from just a

4574
01:29:24,420 --> 01:29:29,110
 

4575
01:29:24,430 --> 01:29:30,160
 single image of that person okay and

4576
01:29:29,100 --> 01:29:30,160
 

4577
01:29:29,110 --> 01:29:31,540
 then another application that people

4578
01:29:30,150 --> 01:29:31,540
 

4579
01:29:30,160 --> 01:29:33,520
 have looked at is imitation learning

4580
01:29:31,530 --> 01:29:33,520
 

4581
01:29:31,540 --> 01:29:35,260
 where the goal is given one

4582
01:29:33,510 --> 01:29:35,260
 

4583
01:29:33,520 --> 01:29:39,280
 demonstration of a task can you learn a

4584
01:29:35,250 --> 01:29:39,280
 

4585
01:29:35,260 --> 01:29:41,260
 policy for that task and this is quite a

4586
01:29:39,270 --> 01:29:41,260
 

4587
01:29:39,280 --> 01:29:42,580
 fairly simple extension of the

4588
01:29:41,250 --> 01:29:42,580
 

4589
01:29:41,260 --> 01:29:43,660
 supervised meta learning approaches that

4590
01:29:42,570 --> 01:29:43,660
 

4591
01:29:42,580 --> 01:29:44,830
 we showed before because you can treat

4592
01:29:43,650 --> 01:29:44,830
 

4593
01:29:43,660 --> 01:29:47,110
 imitation learning as a supervised

4594
01:29:44,820 --> 01:29:47,110
 

4595
01:29:44,830 --> 01:29:49,750
 learning problem this has been looked at

4596
01:29:47,100 --> 01:29:49,750
 

4597
01:29:47,110 --> 01:29:51,700
 for blocking blocks on top of each other

4598
01:29:49,740 --> 01:29:51,700
 

4599
01:29:49,750 --> 01:29:55,440
 for extended step temporarily extended

4600
01:29:51,690 --> 01:29:55,440
 

4601
01:29:51,700 --> 01:29:58,210
 tasks also for other kind of robotics

4602
01:29:55,430 --> 01:29:58,210
 

4603
01:29:55,440 --> 01:30:00,010
 control tasks in the real world as well

4604
01:29:58,200 --> 01:30:00,010
 

4605
01:29:58,210 --> 01:30:01,630
 as for high fidelity imitation where you

4606
01:30:00,000 --> 01:30:01,630
 

4607
01:30:00,010 --> 01:30:05,230
 want to very closely match the the

4608
01:30:01,620 --> 01:30:05,230
 

4609
01:30:01,630 --> 01:30:06,880
 training demonstration and also it has

4610
01:30:05,220 --> 01:30:06,880
 

4611
01:30:05,230 --> 01:30:10,630
 also been used with optimization based

4612
01:30:06,870 --> 01:30:10,630
 

4613
01:30:06,880 --> 01:30:12,970
 patient techniques such as shout here so

4614
01:30:10,620 --> 01:30:12,970
 

4615
01:30:10,630 --> 01:30:16,330
 here the goal is to after seeing a

4616
01:30:12,960 --> 01:30:16,330
 

4617
01:30:12,970 --> 01:30:18,329
 single demonstration of placing an apple

4618
01:30:16,320 --> 01:30:18,329
 

4619
01:30:16,330 --> 01:30:19,590
 into a bowl through teleoperation

4620
01:30:18,319 --> 01:30:19,590
 

4621
01:30:18,329 --> 01:30:21,570
 throughout what is able to figure out

4622
01:30:19,580 --> 01:30:21,570
 

4623
01:30:19,590 --> 01:30:25,559
 how to place the dapple on to the ball

4624
01:30:21,560 --> 01:30:25,559
 

4625
01:30:21,570 --> 01:30:26,610
 in new situations in this case I the the

4626
01:30:25,549 --> 01:30:26,610
 

4627
01:30:25,559 --> 01:30:28,500
 robot had never seen any of these

4628
01:30:26,600 --> 01:30:28,500
 

4629
01:30:26,610 --> 01:30:30,780
 objects before and so it's adapting to

4630
01:30:28,490 --> 01:30:30,780
 

4631
01:30:28,500 --> 01:30:34,020
 being able to manipulate objects in new

4632
01:30:30,770 --> 01:30:34,020
 

4633
01:30:30,780 --> 01:30:35,699
 objects in in this setting okay and also

4634
01:30:34,010 --> 01:30:35,699
 

4635
01:30:34,020 --> 01:30:37,260
 for more advanced topics you can look at

4636
01:30:35,689 --> 01:30:37,260
 

4637
01:30:35,699 --> 01:30:39,000
 approaches for wanton inverse

4638
01:30:37,250 --> 01:30:39,000
 

4639
01:30:37,260 --> 01:30:42,480
 reinforcement learning and one-shot

4640
01:30:38,990 --> 01:30:42,480
 

4641
01:30:39,000 --> 01:30:43,530
 hierarchical imitation learning now one

4642
01:30:42,470 --> 01:30:43,530
 

4643
01:30:42,480 --> 01:30:45,360
 thing you might say is can we take this

4644
01:30:43,520 --> 01:30:45,360
 

4645
01:30:43,530 --> 01:30:47,010
 one step further can we do imitation

4646
01:30:45,350 --> 01:30:47,010
 

4647
01:30:45,360 --> 01:30:48,690
 from a video of a human per se where our

4648
01:30:47,000 --> 01:30:48,690
 

4649
01:30:47,010 --> 01:30:50,190
 goal is given a video of a human

4650
01:30:48,680 --> 01:30:50,190
 

4651
01:30:48,690 --> 01:30:52,020
 performing a task such as placing into

4652
01:30:50,180 --> 01:30:52,020
 

4653
01:30:50,190 --> 01:30:56,010
 the red bowl can the robot figure out a

4654
01:30:52,010 --> 01:30:56,010
 

4655
01:30:52,020 --> 01:30:57,179
 policy for placing for performing the

4656
01:30:56,000 --> 01:30:57,179
 

4657
01:30:56,010 --> 01:30:59,849
 toss stone in the video such as placing

4658
01:30:57,169 --> 01:30:59,849
 

4659
01:30:57,179 --> 01:31:01,980
 the peach into the red bowl and in this

4660
01:30:59,839 --> 01:31:01,980
 

4661
01:30:59,849 --> 01:31:03,900
 case you need to learn how to learn from

4662
01:31:01,970 --> 01:31:03,900
 

4663
01:31:01,980 --> 01:31:05,340
 weak supervision and what I mean by weak

4664
01:31:03,890 --> 01:31:05,340
 

4665
01:31:03,900 --> 01:31:07,170
 supervision is that the video of the

4666
01:31:05,330 --> 01:31:07,170
 

4667
01:31:05,340 --> 01:31:09,809
 human has all the information about the

4668
01:31:07,160 --> 01:31:09,809
 

4669
01:31:07,170 --> 01:31:12,059
 task but is not accessible in a way of

4670
01:31:09,799 --> 01:31:12,059
 

4671
01:31:09,809 --> 01:31:14,400
 kind of a standard machine learning data

4672
01:31:12,049 --> 01:31:14,400
 

4673
01:31:12,059 --> 01:31:16,679
 set and so we need to do something a bit

4674
01:31:14,390 --> 01:31:16,679
 

4675
01:31:14,400 --> 01:31:18,389
 different here and in particular what

4676
01:31:16,669 --> 01:31:18,389
 

4677
01:31:16,679 --> 01:31:21,030
 we're going to do is we can take the the

4678
01:31:18,379 --> 01:31:21,030
 

4679
01:31:18,389 --> 01:31:24,630
 mammal objective or really any metal

4680
01:31:21,020 --> 01:31:24,630
 

4681
01:31:21,030 --> 01:31:25,980
 learning objective and the the the meta

4682
01:31:24,620 --> 01:31:25,980
 

4683
01:31:24,630 --> 01:31:27,869
 objective the outer objective is going

4684
01:31:25,970 --> 01:31:27,869
 

4685
01:31:25,980 --> 01:31:29,190
 to be fully supervised and the inner

4686
01:31:27,859 --> 01:31:29,190
 

4687
01:31:27,869 --> 01:31:30,960
 objective is going to be weekly

4688
01:31:29,180 --> 01:31:30,960
 

4689
01:31:29,190 --> 01:31:33,449
 supervised so we're be learning how to

4690
01:31:30,950 --> 01:31:33,449
 

4691
01:31:30,960 --> 01:31:36,000
 learn from weekly supervised data using

4692
01:31:33,439 --> 01:31:36,000
 

4693
01:31:33,449 --> 01:31:37,739
 fully supervised data and then a test

4694
01:31:35,990 --> 01:31:37,739
 

4695
01:31:36,000 --> 01:31:40,230
 time you'll run gradient descent using

4696
01:31:37,729 --> 01:31:40,230
 

4697
01:31:37,739 --> 01:31:41,280
 the weekly supervised data now you one

4698
01:31:40,220 --> 01:31:41,280
 

4699
01:31:40,230 --> 01:31:42,510
 question you might ask is well what if

4700
01:31:41,270 --> 01:31:42,510
 

4701
01:31:41,280 --> 01:31:44,309
 the weekly supervised loss function

4702
01:31:42,500 --> 01:31:44,309
 

4703
01:31:42,510 --> 01:31:45,960
 isn't available like from a video of a

4704
01:31:44,299 --> 01:31:45,960
 

4705
01:31:44,309 --> 01:31:48,059
 human in that case you could actually

4706
01:31:45,950 --> 01:31:48,059
 

4707
01:31:45,960 --> 01:31:50,429
 learn a loss function for performing

4708
01:31:48,049 --> 01:31:50,429
 

4709
01:31:48,059 --> 01:31:52,170
 adaptation such as I roll Network loss

4710
01:31:50,419 --> 01:31:52,170
 

4711
01:31:50,429 --> 01:31:53,550
 function that outputs a scalar value and

4712
01:31:52,160 --> 01:31:53,550
 

4713
01:31:52,170 --> 01:31:55,590
 you train this loss function such that

4714
01:31:53,540 --> 01:31:55,590
 

4715
01:31:53,550 --> 01:31:59,340
 the gradients it provides are effective

4716
01:31:55,580 --> 01:31:59,340
 

4717
01:31:55,590 --> 01:32:00,960
 for adaptation okay and then lastly it's

4718
01:31:59,330 --> 01:32:00,960
 

4719
01:31:59,340 --> 01:32:02,880
 worth mentioning that meta learning for

4720
01:32:00,950 --> 01:32:02,880
 

4721
01:32:00,960 --> 01:32:06,030
 languages been explored in a variety of

4722
01:32:02,870 --> 01:32:06,030
 

4723
01:32:02,880 --> 01:32:08,909
 contexts this includes adapting link

4724
01:32:06,020 --> 01:32:08,909
 

4725
01:32:06,030 --> 01:32:11,130
 adapting models to modeling new programs

4726
01:32:08,899 --> 01:32:11,130
 

4727
01:32:08,909 --> 01:32:13,619
 such as program induction and program

4728
01:32:11,120 --> 01:32:13,619
 

4729
01:32:11,130 --> 01:32:15,599
 synthesis this also includes adapting to

4730
01:32:13,609 --> 01:32:15,599
 

4731
01:32:13,619 --> 01:32:17,489
 new languages such as translating

4732
01:32:15,589 --> 01:32:17,489
 

4733
01:32:15,599 --> 01:32:19,230
 between two language pairs that you only

4734
01:32:17,479 --> 01:32:19,230
 

4735
01:32:17,489 --> 01:32:20,699
 have a small amount of data for by

4736
01:32:19,220 --> 01:32:20,699
 

4737
01:32:19,230 --> 01:32:23,130
 meditating on language pairs that you

4738
01:32:20,689 --> 01:32:23,130
 

4739
01:32:20,699 --> 01:32:25,079
 have a lot of data for this also

4740
01:32:23,120 --> 01:32:25,079
 

4741
01:32:23,130 --> 01:32:26,400
 includes learning new words so this is

4742
01:32:25,069 --> 01:32:26,400
 

4743
01:32:25,079 --> 01:32:28,170
 actually done in the original matching

4744
01:32:26,390 --> 01:32:28,170
 

4745
01:32:26,400 --> 01:32:30,179
 networks paper that learns how to use a

4746
01:32:28,160 --> 01:32:30,179
 

4747
01:32:28,170 --> 01:32:30,790
 new word in the new context from a

4748
01:32:30,169 --> 01:32:30,790
 

4749
01:32:30,179 --> 01:32:33,760
 single

4750
01:32:30,780 --> 01:32:33,760
 

4751
01:32:30,790 --> 01:32:35,470
 example usage of that word and lastly

4752
01:32:33,750 --> 01:32:35,470
 

4753
01:32:33,760 --> 01:32:38,380
 adapting to new personas so training

4754
01:32:35,460 --> 01:32:38,380
 

4755
01:32:35,470 --> 01:32:40,060
 dialog agents to be able to generate

4756
01:32:38,370 --> 01:32:40,060
 

4757
01:32:38,380 --> 01:32:43,540
 dialogue from a particular persona from

4758
01:32:40,050 --> 01:32:43,540
 

4759
01:32:40,060 --> 01:32:45,460
 only a few examples of that persona okay

4760
01:32:43,530 --> 01:32:45,460
 

4761
01:32:43,540 --> 01:32:47,380
 great so next we'll you will take a

4762
01:32:45,450 --> 01:32:47,380
 

4763
01:32:45,460 --> 01:32:49,240
 short five-minute break we will be

4764
01:32:47,370 --> 01:32:49,240
 

4765
01:32:47,380 --> 01:32:50,380
 taking questions during the break and

4766
01:32:49,230 --> 01:32:50,380
 

4767
01:32:49,240 --> 01:32:51,700
 then after that Sergei we will talk

4768
01:32:50,370 --> 01:32:51,700
 

4769
01:32:50,380 --> 01:32:54,310
 about meta reinforcement learning

4770
01:32:51,690 --> 01:32:54,310
 

4771
01:32:51,700 --> 01:32:55,510
 so feel free to stand up and and and

4772
01:32:54,300 --> 01:32:55,510
 

4773
01:32:54,310 --> 01:32:57,660
 rest your legs it will be answering some

4774
01:32:55,500 --> 01:32:57,660
 

4775
01:32:55,510 --> 01:32:57,660
 questions

4776
01:33:17,860 --> 01:33:17,860
 

4777
01:33:17,870 --> 01:33:21,020
[Music]

4778
01:33:23,770 --> 01:33:23,770
 

4779
01:33:23,780 --> 01:33:26,939
[Music]

4780
01:33:37,200 --> 01:33:37,200
 

4781
01:33:37,210 --> 01:33:41,650
[Music]

4782
01:33:38,640 --> 01:33:41,650
 

4783
01:33:38,650 --> 01:33:41,650
 behind

4784
01:33:43,850 --> 01:33:43,850
 

4785
01:33:43,860 --> 01:33:47,280
[Music]

4786
01:33:49,650 --> 01:33:49,650
 

4787
01:33:49,660 --> 01:33:53,000
[Applause]

4788
01:34:08,000 --> 01:34:08,000
 

4789
01:34:08,010 --> 01:34:15,210
[Music]

4790
01:34:25,990 --> 01:34:25,990
 

4791
01:34:26,000 --> 01:34:29,199
[Music]

4792
01:34:31,790 --> 01:34:31,790
 

4793
01:34:31,800 --> 01:34:34,979
[Applause]