1
00:00:00,000 --> 00:01:36,510
[Music]

2
00:01:42,830 --> 00:01:42,830
 

3
00:01:42,840 --> 00:02:39,679
[Music]

4
00:02:41,910 --> 00:02:41,910
 

5
00:02:41,920 --> 00:04:05,949
[Music]

6
00:04:15,980 --> 00:04:15,980
 

7
00:04:15,990 --> 00:05:10,779
[Music]

8
00:05:13,030 --> 00:05:13,030
 

9
00:05:13,040 --> 00:05:40,480
[Music]

10
00:05:44,950 --> 00:05:44,950
 

11
00:05:44,960 --> 00:06:06,010
[Music]

12
00:06:04,220 --> 00:06:06,010
 

13
00:06:04,230 --> 00:06:09,570
[Applause]

14
00:06:06,000 --> 00:06:09,570
 

15
00:06:06,010 --> 00:06:11,340
[Music]

16
00:06:09,560 --> 00:06:11,340
 

17
00:06:09,570 --> 00:06:14,910
[Applause]

18
00:06:11,330 --> 00:06:14,910
 

19
00:06:11,340 --> 00:06:16,680
[Music]

20
00:06:14,900 --> 00:06:16,680
 

21
00:06:14,910 --> 00:06:51,959
[Applause]

22
00:06:16,670 --> 00:06:51,959
 

23
00:06:16,680 --> 00:06:51,959
[Music]

24
00:06:54,200 --> 00:06:54,200
 

25
00:06:54,210 --> 00:07:10,010
[Music]

26
00:07:08,220 --> 00:07:10,010
 

27
00:07:08,230 --> 00:07:24,260
[Applause]

28
00:07:10,000 --> 00:07:24,260
 

29
00:07:10,010 --> 00:07:26,010
[Music]

30
00:07:24,250 --> 00:07:26,010
 

31
00:07:24,260 --> 00:08:19,070
[Applause]

32
00:07:26,000 --> 00:08:19,070
 

33
00:07:26,010 --> 00:08:19,070
[Music]

34
00:08:24,320 --> 00:08:24,320
 

35
00:08:24,330 --> 00:08:30,060
[Music]

36
00:08:32,740 --> 00:08:32,740
 

37
00:08:32,750 --> 00:08:54,519
[Music]

38
00:08:56,980 --> 00:08:56,980
 

39
00:08:56,990 --> 00:10:05,940
[Music]

40
00:10:06,400 --> 00:10:06,400
 

41
00:10:06,410 --> 00:10:12,540
 all right so hi everyone let's get

42
00:10:09,590 --> 00:10:12,540
 

43
00:10:09,600 --> 00:10:14,670
 started so welcome back from lunch so as

44
00:10:12,530 --> 00:10:14,670
 

45
00:10:12,540 --> 00:10:18,000
 you know this tutorial is about active

46
00:10:14,660 --> 00:10:18,000
 

47
00:10:14,670 --> 00:10:19,380
 learning from theory to practice and I'm

48
00:10:17,990 --> 00:10:19,380
 

49
00:10:18,000 --> 00:10:22,920
 delighted to introduce our two speakers

50
00:10:19,370 --> 00:10:22,920
 

51
00:10:19,380 --> 00:10:27,420
 today our first tutorial speaker is Rob

52
00:10:22,910 --> 00:10:27,420
 

53
00:10:22,920 --> 00:10:29,030
 Norwalk Rob is a knob nose push

54
00:10:27,410 --> 00:10:29,030
 

55
00:10:27,420 --> 00:10:31,980
 professor in engineering at the

56
00:10:29,020 --> 00:10:31,980
 

57
00:10:29,030 --> 00:10:34,590
 University of wisconsin-madison he is

58
00:10:31,970 --> 00:10:34,590
 

59
00:10:31,980 --> 00:10:36,480
 one of the leaders of the field with

60
00:10:34,580 --> 00:10:36,480
 

61
00:10:34,590 --> 00:10:37,710
 variety of interesting results at the

62
00:10:36,470 --> 00:10:37,710
 

63
00:10:36,480 --> 00:10:39,870
 intersection of signal processing

64
00:10:37,700 --> 00:10:39,870
 

65
00:10:37,710 --> 00:10:42,810
 machine learning optimization and

66
00:10:39,860 --> 00:10:42,810
 

67
00:10:39,870 --> 00:10:44,820
 statistics including seminal results in

68
00:10:42,800 --> 00:10:44,820
 

69
00:10:42,810 --> 00:10:47,460
 active learning which is a topic of the

70
00:10:44,810 --> 00:10:47,460
 

71
00:10:44,820 --> 00:10:51,960
 tutorial today ok and our second speaker

72
00:10:47,450 --> 00:10:51,960
 

73
00:10:47,460 --> 00:10:53,550
 is Steve Janaki he is currently a

74
00:10:51,950 --> 00:10:53,550
 

75
00:10:51,960 --> 00:10:55,680
 research assistant professor at the

76
00:10:53,540 --> 00:10:55,680
 

77
00:10:53,550 --> 00:10:58,380
 Toyota Technological Institute in

78
00:10:55,670 --> 00:10:58,380
 

79
00:10:55,680 --> 00:11:01,140
 Chicago and he was broadly in learning

80
00:10:58,370 --> 00:11:01,140
 

81
00:10:58,380 --> 00:11:03,210
 theory and just like Rob he has done a

82
00:11:01,130 --> 00:11:03,210
 

83
00:11:01,140 --> 00:11:05,520
 lot of seminal work on active learning

84
00:11:03,200 --> 00:11:05,520
 

85
00:11:03,210 --> 00:11:07,260
 and so we're really really excited to

86
00:11:05,510 --> 00:11:07,260
 

87
00:11:05,520 --> 00:11:09,980
 have both of them here today telling us

88
00:11:07,250 --> 00:11:09,980
 

89
00:11:07,260 --> 00:11:09,980
 about active learning

90
00:11:10,280 --> 00:11:10,280
 

91
00:11:10,290 --> 00:11:17,860
[Applause]

92
00:11:14,930 --> 00:11:17,860
 

93
00:11:14,940 --> 00:11:19,540
 Thank You Nina for that great

94
00:11:17,850 --> 00:11:19,540
 

95
00:11:17,860 --> 00:11:21,520
 introduction it's a real pleasure to be

96
00:11:19,530 --> 00:11:21,520
 

97
00:11:19,540 --> 00:11:23,740
 here and I'm really thrilled to see so

98
00:11:21,510 --> 00:11:23,740
 

99
00:11:21,520 --> 00:11:26,500
 many people out in the audience I I hope

100
00:11:23,730 --> 00:11:26,500
 

101
00:11:23,740 --> 00:11:30,010
 you can learn a lot during this tutorial

102
00:11:26,490 --> 00:11:30,010
 

103
00:11:26,500 --> 00:11:31,420
 and we'll be around to answer questions

104
00:11:30,000 --> 00:11:31,420
 

105
00:11:30,010 --> 00:11:34,180
 and so forth and I'll explain a little

106
00:11:31,410 --> 00:11:34,180
 

107
00:11:31,420 --> 00:11:36,010
 bit about the format in a minute here I

108
00:11:34,170 --> 00:11:36,010
 

109
00:11:34,180 --> 00:11:37,930
 just wanted to mention that Steve and I

110
00:11:36,000 --> 00:11:37,930
 

111
00:11:36,010 --> 00:11:39,940
 have probably been Nina for that matter

112
00:11:37,920 --> 00:11:39,940
 

113
00:11:37,930 --> 00:11:42,190
 we've known each other since those two

114
00:11:39,930 --> 00:11:42,190
 

115
00:11:39,940 --> 00:11:43,810
 were graduate students I was already a

116
00:11:42,180 --> 00:11:43,810
 

117
00:11:42,190 --> 00:11:46,270
 professor I have to admit I'm a bit

118
00:11:43,800 --> 00:11:46,270
 

119
00:11:43,810 --> 00:11:48,790
 older but I've known them for many years

120
00:11:46,260 --> 00:11:48,790
 

121
00:11:46,270 --> 00:11:51,070
 we've worked together on organizing

122
00:11:48,780 --> 00:11:51,070
 

123
00:11:48,790 --> 00:11:52,540
 things and all sorts of stuff so it's

124
00:11:51,060 --> 00:11:52,540
 

125
00:11:51,070 --> 00:11:55,540
 really nice to be together with them

126
00:11:52,530 --> 00:11:55,540
 

127
00:11:52,540 --> 00:11:57,790
 today so this tutorial is going to be

128
00:11:55,530 --> 00:11:57,790
 

129
00:11:55,540 --> 00:12:00,700
 about active machine learning and we're

130
00:11:57,780 --> 00:12:00,700
 

131
00:11:57,790 --> 00:12:03,400
 going to be largely focusing on the

132
00:12:00,690 --> 00:12:03,400
 

133
00:12:00,700 --> 00:12:05,470
 theory aspects of active learning but

134
00:12:03,390 --> 00:12:05,470
 

135
00:12:03,400 --> 00:12:07,900
 I'm going to sprinkle in some of the

136
00:12:05,460 --> 00:12:07,900
 

137
00:12:05,470 --> 00:12:10,600
 practical applications and so forth as

138
00:12:07,890 --> 00:12:10,600
 

139
00:12:07,900 --> 00:12:13,510
 we go and I'd also be happy to talk

140
00:12:10,590 --> 00:12:13,510
 

141
00:12:10,600 --> 00:12:16,330
 about some of that practical stuff in

142
00:12:13,500 --> 00:12:16,330
 

143
00:12:13,510 --> 00:12:18,010
 the breaks and during questions so

144
00:12:16,320 --> 00:12:18,010
 

145
00:12:16,330 --> 00:12:20,350
 here's the outline for our tutorial

146
00:12:18,000 --> 00:12:20,350
 

147
00:12:18,010 --> 00:12:22,390
 we're breaking it into four parts the

148
00:12:20,340 --> 00:12:22,390
 

149
00:12:20,350 --> 00:12:24,520
 first part is going to be an

150
00:12:22,380 --> 00:12:24,520
 

151
00:12:22,390 --> 00:12:27,190
 introduction and I'll do that that'll be

152
00:12:24,510 --> 00:12:27,190
 

153
00:12:24,520 --> 00:12:29,620
 about the first half hour then Steve

154
00:12:27,180 --> 00:12:29,620
 

155
00:12:27,190 --> 00:12:31,810
 will take over for parts 2 & 3 which

156
00:12:29,610 --> 00:12:31,810
 

157
00:12:29,620 --> 00:12:33,760
 will be really diving into the theory of

158
00:12:31,800 --> 00:12:33,760
 

159
00:12:31,810 --> 00:12:36,280
 active learning and then discussing some

160
00:12:33,750 --> 00:12:36,280
 

161
00:12:33,760 --> 00:12:38,110
 advanced and open advanced topics and

162
00:12:36,270 --> 00:12:38,110
 

163
00:12:36,280 --> 00:12:39,730
 open problems and then I'm going to wrap

164
00:12:38,100 --> 00:12:39,730
 

165
00:12:38,110 --> 00:12:41,740
 up talking a little bit about

166
00:12:39,720 --> 00:12:41,740
 

167
00:12:39,730 --> 00:12:44,710
 nonparametric learning this is going to

168
00:12:41,730 --> 00:12:44,710
 

169
00:12:41,740 --> 00:12:47,650
 be really kind of talking about some

170
00:12:44,700 --> 00:12:47,650
 

171
00:12:44,710 --> 00:12:49,330
 very very ongoing work that I'm doing

172
00:12:47,640 --> 00:12:49,330
 

173
00:12:47,650 --> 00:12:52,570
 with some other folks too so I hope

174
00:12:49,320 --> 00:12:52,570
 

175
00:12:49,330 --> 00:12:54,790
 you'll like that and I'm noting at the

176
00:12:52,560 --> 00:12:54,790
 

177
00:12:52,570 --> 00:12:58,570
 bottom here that the slides for all of

178
00:12:54,780 --> 00:12:58,570
 

179
00:12:54,790 --> 00:13:00,940
 our parts here are available at URL

180
00:12:58,560 --> 00:13:00,940
 

181
00:12:58,570 --> 00:13:04,090
 there so I'll flash that up again at the

182
00:13:00,930 --> 00:13:04,090
 

183
00:13:00,940 --> 00:13:05,980
 end of my first part here but if you

184
00:13:04,080 --> 00:13:05,980
 

185
00:13:04,090 --> 00:13:10,120
 want to follow along the slides are

186
00:13:05,970 --> 00:13:10,120
 

187
00:13:05,980 --> 00:13:13,690
 online there so what we're going to do

188
00:13:10,110 --> 00:13:13,690
 

189
00:13:10,120 --> 00:13:15,460
 is we'll do these four parts during and

190
00:13:13,680 --> 00:13:15,460
 

191
00:13:13,690 --> 00:13:16,930
 maybe just given the size of the

192
00:13:15,450 --> 00:13:16,930
 

193
00:13:15,460 --> 00:13:20,770
 audience and so forth it might make

194
00:13:16,920 --> 00:13:20,770
 

195
00:13:16,930 --> 00:13:22,720
 sense to take questions after each part

196
00:13:20,760 --> 00:13:22,720
 

197
00:13:20,770 --> 00:13:24,340
 so after part one we'll take a little

198
00:13:22,710 --> 00:13:24,340
 

199
00:13:22,720 --> 00:13:26,470
 time for some questions people can come

200
00:13:24,330 --> 00:13:26,470
 

201
00:13:24,340 --> 00:13:27,860
 up to the microphones again after parts

202
00:13:26,460 --> 00:13:27,860
 

203
00:13:26,470 --> 00:13:29,000
 2 3 & 4

204
00:13:27,850 --> 00:13:29,000
 

205
00:13:27,860 --> 00:13:31,130
 we'll do it like that but if there's

206
00:13:28,990 --> 00:13:31,130
 

207
00:13:29,000 --> 00:13:34,220
 something really confusing or bothering

208
00:13:31,120 --> 00:13:34,220
 

209
00:13:31,130 --> 00:13:36,140
 you while I'm speaking jump up to a mic

210
00:13:34,210 --> 00:13:36,140
 

211
00:13:34,220 --> 00:13:38,209
 and just say hey Rob I don't get what

212
00:13:36,130 --> 00:13:38,209
 

213
00:13:36,140 --> 00:13:39,290
 you just said and I'll try to feel it

214
00:13:38,199 --> 00:13:39,290
 

215
00:13:38,209 --> 00:13:41,620
 right there on the spot

216
00:13:39,280 --> 00:13:41,620
 

217
00:13:39,290 --> 00:13:44,990
 does that sound like a good plan awesome

218
00:13:41,610 --> 00:13:44,990
 

219
00:13:41,620 --> 00:13:46,880
 okay so I have to kind of be a little

220
00:13:44,980 --> 00:13:46,880
 

221
00:13:44,990 --> 00:13:48,740
 bit in control here normally when I give

222
00:13:46,870 --> 00:13:48,740
 

223
00:13:46,880 --> 00:13:50,510
 talks I'm running around on the stage

224
00:13:48,730 --> 00:13:50,510
 

225
00:13:48,740 --> 00:13:52,339
 moving around pointing and things like

226
00:13:50,500 --> 00:13:52,339
 

227
00:13:50,510 --> 00:13:55,339
 that but for this one I have to stay

228
00:13:52,329 --> 00:13:55,339
 

229
00:13:52,339 --> 00:13:57,620
 here at least within facing this

230
00:13:55,329 --> 00:13:57,620
 

231
00:13:55,339 --> 00:13:59,420
 microphone so I mean if I walk away and

232
00:13:57,610 --> 00:13:59,420
 

233
00:13:57,620 --> 00:14:01,579
 you're saying Rob we can't hear you

234
00:13:59,410 --> 00:14:01,579
 

235
00:13:59,420 --> 00:14:03,140
 anymore yell out and tell me so here's

236
00:14:01,569 --> 00:14:03,140
 

237
00:14:01,579 --> 00:14:05,510
 the conventional machine-learning

238
00:14:03,130 --> 00:14:05,510
 

239
00:14:03,140 --> 00:14:07,970
 pipeline way it works is we have a large

240
00:14:05,500 --> 00:14:07,970
 

241
00:14:05,510 --> 00:14:09,279
 unlabeled data set I'm showing it here

242
00:14:07,960 --> 00:14:09,279
 

243
00:14:07,970 --> 00:14:13,610
 in the case of an image classification

244
00:14:09,269 --> 00:14:13,610
 

245
00:14:09,279 --> 00:14:16,160
 problem and what we do is we take a sub

246
00:14:13,600 --> 00:14:16,160
 

247
00:14:13,610 --> 00:14:19,760
 sample of that very large unlabeled data

248
00:14:16,150 --> 00:14:19,760
 

249
00:14:16,160 --> 00:14:21,589
 set and we ask a human to annotate some

250
00:14:19,750 --> 00:14:21,589
 

251
00:14:19,760 --> 00:14:23,450
 subset of it so we get some human labels

252
00:14:21,579 --> 00:14:23,450
 

253
00:14:21,589 --> 00:14:25,820
 for things like dogs and boats and so

254
00:14:23,440 --> 00:14:25,820
 

255
00:14:23,450 --> 00:14:27,800
 forth that gives us a smaller set of

256
00:14:25,810 --> 00:14:27,800
 

257
00:14:25,820 --> 00:14:29,269
 label data we turn that over to a

258
00:14:27,790 --> 00:14:29,269
 

259
00:14:27,800 --> 00:14:32,000
 machine learning algorithm it tries to

260
00:14:29,259 --> 00:14:32,000
 

261
00:14:29,269 --> 00:14:33,860
 then learn a predictive model mimicking

262
00:14:31,990 --> 00:14:33,860
 

263
00:14:32,000 --> 00:14:36,199
 the labeling process that this human

264
00:14:33,850 --> 00:14:36,199
 

265
00:14:33,860 --> 00:14:38,560
 expert is doing so that's a conventional

266
00:14:36,189 --> 00:14:38,560
 

267
00:14:36,199 --> 00:14:41,480
 pipeline it's been really successful

268
00:14:38,550 --> 00:14:41,480
 

269
00:14:38,560 --> 00:14:42,769
 these are just a few headlines there are

270
00:14:41,470 --> 00:14:42,769
 

271
00:14:41,480 --> 00:14:45,320
 a few years old now but they kind of

272
00:14:42,759 --> 00:14:45,320
 

273
00:14:42,769 --> 00:14:47,089
 capture all the excitement that has been

274
00:14:45,310 --> 00:14:47,089
 

275
00:14:45,320 --> 00:14:50,630
 going on in the machine learning area

276
00:14:47,079 --> 00:14:50,630
 

277
00:14:47,089 --> 00:14:54,260
 recently and all systems go is actually

278
00:14:50,620 --> 00:14:54,260
 

279
00:14:50,630 --> 00:14:56,329
 the title of a nature journal article

280
00:14:54,250 --> 00:14:56,329
 

281
00:14:54,260 --> 00:15:01,610
 talking about the success with the

282
00:14:56,319 --> 00:15:01,610
 

283
00:14:56,329 --> 00:15:04,279
 game-playing system that alphago so all

284
00:15:01,600 --> 00:15:04,279
 

285
00:15:01,610 --> 00:15:05,740
 systems go that's true except when you

286
00:15:04,269 --> 00:15:05,740
 

287
00:15:04,279 --> 00:15:08,029
 kind of start to look under the hood

288
00:15:05,730 --> 00:15:08,029
 

289
00:15:05,740 --> 00:15:10,399
 training these machine learning systems

290
00:15:08,019 --> 00:15:10,399
 

291
00:15:08,029 --> 00:15:12,589
 takes a huge amount of human effort in

292
00:15:10,389 --> 00:15:12,589
 

293
00:15:10,399 --> 00:15:15,459
 many cases so millions of labeled images

294
00:15:12,579 --> 00:15:15,459
 

295
00:15:12,589 --> 00:15:18,199
 required say for image net for

296
00:15:15,449 --> 00:15:18,199
 

297
00:15:15,459 --> 00:15:19,970
 translation these machines are learning

298
00:15:18,189 --> 00:15:19,970
 

299
00:15:18,199 --> 00:15:22,220
 to translate between languages by

300
00:15:19,960 --> 00:15:22,220
 

301
00:15:19,970 --> 00:15:24,470
 effectively reading more text than a

302
00:15:22,210 --> 00:15:24,470
 

303
00:15:22,220 --> 00:15:26,870
 human could ever learn in a lifetime and

304
00:15:24,460 --> 00:15:26,870
 

305
00:15:24,470 --> 00:15:28,699
 in game playing they're learning by

306
00:15:26,860 --> 00:15:28,699
 

307
00:15:26,870 --> 00:15:30,920
 playing more games than even my teenage

308
00:15:28,689 --> 00:15:30,920
 

309
00:15:28,699 --> 00:15:35,180
 son can stomach so this is kind of a

310
00:15:30,910 --> 00:15:35,180
 

311
00:15:30,920 --> 00:15:37,699
 little bit of a confusing sort of

312
00:15:35,170 --> 00:15:37,699
 

313
00:15:35,180 --> 00:15:39,380
 situation machines are doing fantastic

314
00:15:37,689 --> 00:15:39,380
 

315
00:15:37,699 --> 00:15:40,380
 things but they're not exactly learning

316
00:15:39,370 --> 00:15:40,380
 

317
00:15:39,380 --> 00:15:42,330
 the way humans

318
00:15:40,370 --> 00:15:42,330
 

319
00:15:40,380 --> 00:15:44,070
 our learning and so there's a lot of

320
00:15:42,320 --> 00:15:44,070
 

321
00:15:42,330 --> 00:15:45,420
 interest and active research right now

322
00:15:44,060 --> 00:15:45,420
 

323
00:15:44,070 --> 00:15:47,310
 and trying to figure out how we can

324
00:15:45,410 --> 00:15:47,310
 

325
00:15:45,420 --> 00:15:52,710
 train machines with less label data and

326
00:15:47,300 --> 00:15:52,710
 

327
00:15:47,310 --> 00:15:53,910
 less human supervision so the approach

328
00:15:52,700 --> 00:15:53,910
 

329
00:15:52,710 --> 00:15:55,950
 that we're going to talk about today

330
00:15:53,900 --> 00:15:55,950
 

331
00:15:53,910 --> 00:15:58,440
 specifically is active machine learning

332
00:15:55,940 --> 00:15:58,440
 

333
00:15:55,950 --> 00:16:00,420
 and this picture serve modifies that

334
00:15:58,430 --> 00:16:00,420
 

335
00:15:58,440 --> 00:16:02,460
 earlier pipeline to show you how active

336
00:16:00,410 --> 00:16:02,460
 

337
00:16:00,420 --> 00:16:04,470
 machine learning works the idea is that

338
00:16:02,450 --> 00:16:04,470
 

339
00:16:02,460 --> 00:16:06,360
 rather than just taking a random subset

340
00:16:04,460 --> 00:16:06,360
 

341
00:16:04,470 --> 00:16:08,820
 of the unlabeled data and asking our

342
00:16:06,350 --> 00:16:08,820
 

343
00:16:06,360 --> 00:16:11,130
 human expert to label it instead we're

344
00:16:08,810 --> 00:16:11,130
 

345
00:16:08,820 --> 00:16:13,230
 going to design an extra algorithmic

346
00:16:11,120 --> 00:16:13,230
 

347
00:16:11,130 --> 00:16:15,210
 component to the system which is a data

348
00:16:13,220 --> 00:16:15,210
 

349
00:16:13,230 --> 00:16:16,770
 selection algorithm and that data

350
00:16:15,200 --> 00:16:16,770
 

351
00:16:15,210 --> 00:16:18,930
 selection algorithm is going to be

352
00:16:16,760 --> 00:16:18,930
 

353
00:16:16,770 --> 00:16:20,910
 looking at all the unlabeled data as the

354
00:16:18,920 --> 00:16:20,910
 

355
00:16:18,930 --> 00:16:23,400
 machine is learning and judiciously

356
00:16:20,900 --> 00:16:23,400
 

357
00:16:20,910 --> 00:16:25,800
 selecting specific examples and asking

358
00:16:23,390 --> 00:16:25,800
 

359
00:16:23,400 --> 00:16:27,270
 the human expert to label those the idea

360
00:16:25,790 --> 00:16:27,270
 

361
00:16:25,800 --> 00:16:29,070
 is basically the machine is it's

362
00:16:27,260 --> 00:16:29,070
 

363
00:16:27,270 --> 00:16:33,720
 starting to build up a model of how to

364
00:16:29,060 --> 00:16:33,720
 

365
00:16:29,070 --> 00:16:35,460
 predict it will flag confusing or cases

366
00:16:33,710 --> 00:16:35,460
 

367
00:16:33,720 --> 00:16:37,590
 or examples that is very uncertain about

368
00:16:35,450 --> 00:16:37,590
 

369
00:16:35,460 --> 00:16:41,460
 a NASA human expert for help on those

370
00:16:37,580 --> 00:16:41,460
 

371
00:16:37,590 --> 00:16:44,190
 specific cases so let me just kind of

372
00:16:41,450 --> 00:16:44,190
 

373
00:16:41,460 --> 00:16:46,290
 explain this idea with a motivating

374
00:16:44,180 --> 00:16:46,290
 

375
00:16:44,190 --> 00:16:48,440
 application here this is something that

376
00:16:46,280 --> 00:16:48,440
 

377
00:16:46,290 --> 00:16:52,050
 I've worked on a bit with people in

378
00:16:48,430 --> 00:16:52,050
 

379
00:16:48,440 --> 00:16:53,850
 health and medicine they're interested

380
00:16:52,040 --> 00:16:53,850
 

381
00:16:52,050 --> 00:16:56,520
 in developing machine learning systems

382
00:16:53,840 --> 00:16:56,520
 

383
00:16:53,850 --> 00:16:59,100
 that can predict disease outcomes from

384
00:16:56,510 --> 00:16:59,100
 

385
00:16:56,520 --> 00:17:01,260
 electronic health records and this kind

386
00:16:59,090 --> 00:17:01,260
 

387
00:16:59,100 --> 00:17:02,580
 of closed-loop human-in-the-loop active

388
00:17:01,250 --> 00:17:02,580
 

389
00:17:01,260 --> 00:17:04,439
 learning process looks something like

390
00:17:02,570 --> 00:17:04,439
 

391
00:17:02,580 --> 00:17:07,439
 this we have a large database of

392
00:17:04,429 --> 00:17:07,439
 

393
00:17:04,439 --> 00:17:09,660
 unlabeled electronic health records we

394
00:17:07,429 --> 00:17:09,660
 

395
00:17:07,439 --> 00:17:11,670
 have a team or an individual human

396
00:17:09,650 --> 00:17:11,670
 

397
00:17:09,660 --> 00:17:15,000
 expert who provides labels to a machine

398
00:17:11,660 --> 00:17:15,000
 

399
00:17:11,670 --> 00:17:17,850
 learner labeling one electronic health

400
00:17:14,990 --> 00:17:17,850
 

401
00:17:15,000 --> 00:17:19,560
 record can take an expert several

402
00:17:17,840 --> 00:17:19,560
 

403
00:17:17,850 --> 00:17:21,300
 minutes to do so it's a kind of

404
00:17:19,550 --> 00:17:21,300
 

405
00:17:19,560 --> 00:17:24,240
 expensive labeling process and these

406
00:17:21,290 --> 00:17:24,240
 

407
00:17:21,300 --> 00:17:26,490
 experts are clinicians who typically get

408
00:17:24,230 --> 00:17:26,490
 

409
00:17:24,240 --> 00:17:29,190
 paid maybe 100 200 bucks an hour so

410
00:17:26,480 --> 00:17:29,190
 

411
00:17:26,490 --> 00:17:30,570
 these labels are expensive and the idea

412
00:17:29,180 --> 00:17:30,570
 

413
00:17:29,190 --> 00:17:34,470
 is that we're going to try to get that

414
00:17:30,560 --> 00:17:34,470
 

415
00:17:30,570 --> 00:17:36,360
 human expert or team of experts to label

416
00:17:34,460 --> 00:17:36,360
 

417
00:17:34,470 --> 00:17:38,970
 some subset of these electronic health

418
00:17:36,350 --> 00:17:38,970
 

419
00:17:36,360 --> 00:17:40,620
 records to teach the Machine and the

420
00:17:38,960 --> 00:17:40,620
 

421
00:17:38,970 --> 00:17:42,780
 active learning ideas that as the

422
00:17:40,610 --> 00:17:42,780
 

423
00:17:40,620 --> 00:17:45,270
 machine is learning to build up this

424
00:17:42,770 --> 00:17:45,270
 

425
00:17:42,780 --> 00:17:47,670
 model is going to automatically select

426
00:17:45,260 --> 00:17:47,670
 

427
00:17:45,270 --> 00:17:49,410
 specific unlabeled electronic health

428
00:17:47,660 --> 00:17:49,410
 

429
00:17:47,670 --> 00:17:52,710
 records that it wants the team of

430
00:17:49,400 --> 00:17:52,710
 

431
00:17:49,410 --> 00:17:54,270
 experts to label so just kind of at a

432
00:17:52,700 --> 00:17:54,270
 

433
00:17:52,710 --> 00:17:56,490
 very high level

434
00:17:54,260 --> 00:17:56,490
 

435
00:17:54,270 --> 00:17:58,050
 the the one of the most intuitive ways

436
00:17:56,480 --> 00:17:58,050
 

437
00:17:56,490 --> 00:18:00,780
 you might go about this active learning

438
00:17:58,040 --> 00:18:00,780
 

439
00:17:58,050 --> 00:18:02,730
 process is depicted in this figure here

440
00:18:00,770 --> 00:18:02,730
 

441
00:18:00,780 --> 00:18:04,050
 so I'm just showing you two features

442
00:18:02,720 --> 00:18:04,050
 

443
00:18:02,730 --> 00:18:06,210
 from an electronic health record

444
00:18:04,040 --> 00:18:06,210
 

445
00:18:04,050 --> 00:18:08,550
 typically we're looking more like ten

446
00:18:06,200 --> 00:18:08,550
 

447
00:18:06,210 --> 00:18:10,110
 thousand different features or so but to

448
00:18:08,540 --> 00:18:10,110
 

449
00:18:08,550 --> 00:18:11,670
 visualize it we'll just look at two so

450
00:18:10,100 --> 00:18:11,670
 

451
00:18:10,110 --> 00:18:13,530
 suppose we have electronic health record

452
00:18:11,660 --> 00:18:13,530
 

453
00:18:11,670 --> 00:18:15,870
 feature one and two those are our

454
00:18:13,520 --> 00:18:15,870
 

455
00:18:13,530 --> 00:18:17,940
 horizontal and vertical access the grey

456
00:18:15,860 --> 00:18:17,940
 

457
00:18:15,870 --> 00:18:19,860
 dots are unlabeled electronic health

458
00:18:17,930 --> 00:18:19,860
 

459
00:18:17,940 --> 00:18:21,630
 records and the red and blue dots are

460
00:18:19,850 --> 00:18:21,630
 

461
00:18:19,860 --> 00:18:23,700
 labeled one so we're labeling into two

462
00:18:21,620 --> 00:18:23,700
 

463
00:18:21,630 --> 00:18:25,860
 classes a disease and no disease and

464
00:18:23,690 --> 00:18:25,860
 

465
00:18:23,700 --> 00:18:29,040
 that dashed green line is the best

466
00:18:25,850 --> 00:18:29,040
 

467
00:18:25,860 --> 00:18:31,590
 linear classifier so if we're thinking

468
00:18:29,030 --> 00:18:31,590
 

469
00:18:29,040 --> 00:18:33,780
 about which example to label next the

470
00:18:31,580 --> 00:18:33,780
 

471
00:18:31,590 --> 00:18:35,250
 conventional machine learning pipeline

472
00:18:33,770 --> 00:18:35,250
 

473
00:18:33,780 --> 00:18:37,320
 would just pick one of the unlabeled

474
00:18:35,240 --> 00:18:37,320
 

475
00:18:35,250 --> 00:18:39,150
 examples at random and ask the human

476
00:18:37,310 --> 00:18:39,150
 

477
00:18:37,320 --> 00:18:41,100
 expert to label it and that would give

478
00:18:39,140 --> 00:18:41,100
 

479
00:18:39,150 --> 00:18:43,560
 us a certain learning curve so as we

480
00:18:41,090 --> 00:18:43,560
 

481
00:18:41,100 --> 00:18:45,420
 collect more labeled examples of course

482
00:18:43,550 --> 00:18:45,420
 

483
00:18:43,560 --> 00:18:49,110
 the error rate of our classifier will

484
00:18:45,410 --> 00:18:49,110
 

485
00:18:45,420 --> 00:18:51,300
 improve go down an active procedure

486
00:18:49,100 --> 00:18:51,300
 

487
00:18:49,110 --> 00:18:53,850
 would somehow the machine would decide

488
00:18:51,290 --> 00:18:53,850
 

489
00:18:51,300 --> 00:18:56,100
 well which examples are is it more

490
00:18:53,840 --> 00:18:56,100
 

491
00:18:53,850 --> 00:18:58,830
 uncertain about and for example a

492
00:18:56,090 --> 00:18:58,830
 

493
00:18:56,100 --> 00:19:00,900
 natural heuristic is to look at examples

494
00:18:58,820 --> 00:19:00,900
 

495
00:18:58,830 --> 00:19:02,940
 that are unlabeled and close to the

496
00:19:00,890 --> 00:19:02,940
 

497
00:19:00,900 --> 00:19:04,980
 current best decision boundary like the

498
00:19:02,930 --> 00:19:04,980
 

499
00:19:02,940 --> 00:19:06,840
 orange one I've circled here and the

500
00:19:04,970 --> 00:19:06,840
 

501
00:19:04,980 --> 00:19:08,970
 idea is that hopefully that by

502
00:19:06,830 --> 00:19:08,970
 

503
00:19:06,840 --> 00:19:11,550
 strategically selecting those examples

504
00:19:08,960 --> 00:19:11,550
 

505
00:19:08,970 --> 00:19:13,380
 that are most confusing this will help

506
00:19:11,540 --> 00:19:13,380
 

507
00:19:11,550 --> 00:19:15,510
 the Machine quickly learn a good

508
00:19:13,370 --> 00:19:15,510
 

509
00:19:13,380 --> 00:19:17,610
 predictive rule and reduce the total

510
00:19:15,500 --> 00:19:17,610
 

511
00:19:15,510 --> 00:19:22,230
 amount of human supervision required in

512
00:19:17,600 --> 00:19:22,230
 

513
00:19:17,610 --> 00:19:25,320
 this learning task so this is just some

514
00:19:22,220 --> 00:19:25,320
 

515
00:19:22,230 --> 00:19:28,440
 experimental data from some actual

516
00:19:25,310 --> 00:19:28,440
 

517
00:19:25,320 --> 00:19:31,710
 electronic health record cases we have

518
00:19:28,430 --> 00:19:31,710
 

519
00:19:28,440 --> 00:19:33,210
 about 11,000 patients here roughly 6,000

520
00:19:31,700 --> 00:19:33,210
 

521
00:19:31,710 --> 00:19:35,610
 different features that we're looking at

522
00:19:33,200 --> 00:19:35,610
 

523
00:19:33,210 --> 00:19:37,860
 and we're basically using an active

524
00:19:35,600 --> 00:19:37,860
 

525
00:19:35,610 --> 00:19:39,780
 version of logistic regression and also

526
00:19:37,850 --> 00:19:39,780
 

527
00:19:37,860 --> 00:19:41,850
 a passive version and you can see that

528
00:19:39,770 --> 00:19:41,850
 

529
00:19:39,780 --> 00:19:44,400
 in this particular example active

530
00:19:41,840 --> 00:19:44,400
 

531
00:19:41,850 --> 00:19:46,260
 learning was able to reduce the number

532
00:19:44,390 --> 00:19:46,260
 

533
00:19:44,400 --> 00:19:49,830
 of labeled examples needed to learn a

534
00:19:46,250 --> 00:19:49,830
 

535
00:19:46,260 --> 00:19:52,650
 good classifier by roughly 50% about

536
00:19:49,820 --> 00:19:52,650
 

537
00:19:49,830 --> 00:19:53,940
 half as many required so there are some

538
00:19:52,640 --> 00:19:53,940
 

539
00:19:52,650 --> 00:19:55,830
 significant gains that you can get

540
00:19:53,930 --> 00:19:55,830
 

541
00:19:53,940 --> 00:19:57,560
 especially in cases where the human

542
00:19:55,820 --> 00:19:57,560
 

543
00:19:55,830 --> 00:20:00,540
 labeling is very very expensive

544
00:19:57,550 --> 00:20:00,540
 

545
00:19:57,560 --> 00:20:01,890
 so one of the things why you say well

546
00:20:00,530 --> 00:20:01,890
 

547
00:20:00,540 --> 00:20:03,780
 that's great why don't we just always do

548
00:20:01,880 --> 00:20:03,780
 

549
00:20:01,890 --> 00:20:05,850
 active learning well one of the reasons

550
00:20:03,770 --> 00:20:05,850
 

551
00:20:03,780 --> 00:20:07,080
 is that it's a little hard to implement

552
00:20:05,840 --> 00:20:07,080
 

553
00:20:05,850 --> 00:20:09,000
 these things your your

554
00:20:07,070 --> 00:20:09,000
 

555
00:20:07,080 --> 00:20:11,309
 having sort of an online learning system

556
00:20:08,990 --> 00:20:11,309
 

557
00:20:09,000 --> 00:20:13,230
 as soon as you active eyes it that

558
00:20:11,299 --> 00:20:13,230
 

559
00:20:11,309 --> 00:20:14,760
 becomes a much more intensive process

560
00:20:13,220 --> 00:20:14,760
 

561
00:20:13,230 --> 00:20:17,580
 it's not like you just have a label data

562
00:20:14,750 --> 00:20:17,580
 

563
00:20:14,760 --> 00:20:18,929
 set and then you're going to take your

564
00:20:17,570 --> 00:20:18,929
 

565
00:20:17,580 --> 00:20:20,940
 favorite machine learning algorithm and

566
00:20:18,919 --> 00:20:20,940
 

567
00:20:18,929 --> 00:20:23,190
 apply it to that label data set in

568
00:20:20,930 --> 00:20:23,190
 

569
00:20:20,940 --> 00:20:24,659
 active learning collecting the data as

570
00:20:23,180 --> 00:20:24,659
 

571
00:20:23,190 --> 00:20:27,480
 part of the machine learning algorithm

572
00:20:24,649 --> 00:20:27,480
 

573
00:20:24,659 --> 00:20:30,299
 and so that kind of gives you some more

574
00:20:27,470 --> 00:20:30,299
 

575
00:20:27,480 --> 00:20:33,350
 demanding system requirements one of the

576
00:20:30,289 --> 00:20:33,350
 

577
00:20:30,299 --> 00:20:35,580
 ways that we've tried to allow

578
00:20:33,340 --> 00:20:35,580
 

579
00:20:33,350 --> 00:20:37,710
 researchers and practitioners to

580
00:20:35,570 --> 00:20:37,710
 

581
00:20:35,580 --> 00:20:40,440
 prototype active learning ideas as we

582
00:20:37,700 --> 00:20:40,440
 

583
00:20:37,710 --> 00:20:42,960
 built this open-source software system

584
00:20:40,430 --> 00:20:42,960
 

585
00:20:40,440 --> 00:20:45,059
 that allows you to easily build up

586
00:20:42,950 --> 00:20:45,059
 

587
00:20:42,960 --> 00:20:46,139
 active learning systems a lot of people

588
00:20:45,049 --> 00:20:46,139
 

589
00:20:45,059 --> 00:20:48,120
 have used it for many different

590
00:20:46,129 --> 00:20:48,120
 

591
00:20:46,139 --> 00:20:49,860
 applications I'm just going to tell you

592
00:20:48,110 --> 00:20:49,860
 

593
00:20:48,120 --> 00:20:51,389
 about two of my favorite applications

594
00:20:49,850 --> 00:20:51,389
 

595
00:20:49,860 --> 00:20:53,940
 that I've been involved with where we've

596
00:20:51,379 --> 00:20:53,940
 

597
00:20:51,389 --> 00:20:57,090
 really seen active learning give helpful

598
00:20:53,930 --> 00:20:57,090
 

599
00:20:53,940 --> 00:20:59,130
 gains one has been in active learning to

600
00:20:57,080 --> 00:20:59,130
 

601
00:20:57,090 --> 00:21:01,980
 optimize crowdsourcing in The New Yorker

602
00:20:59,120 --> 00:21:01,980
 

603
00:20:59,130 --> 00:21:03,809
 caption contest and I'm not going to say

604
00:21:01,970 --> 00:21:03,809
 

605
00:21:01,980 --> 00:21:05,340
 much more about this but if anybody

606
00:21:03,799 --> 00:21:05,340
 

607
00:21:03,809 --> 00:21:06,570
 wants to talk to me during the breaks or

608
00:21:05,330 --> 00:21:06,570
 

609
00:21:05,340 --> 00:21:08,700
 afterwards I could tell you more about

610
00:21:06,560 --> 00:21:08,700
 

611
00:21:06,570 --> 00:21:10,889
 our collaboration with The New Yorker

612
00:21:08,690 --> 00:21:10,889
 

613
00:21:08,700 --> 00:21:13,139
 using active learning for crowdsourcing

614
00:21:10,879 --> 00:21:13,139
 

615
00:21:10,889 --> 00:21:15,510
 and another one this is a fun one that

616
00:21:13,129 --> 00:21:15,510
 

617
00:21:13,139 --> 00:21:18,690
 came out of one of my former students

618
00:21:15,500 --> 00:21:18,690
 

619
00:21:15,510 --> 00:21:20,010
 Kevin Jameson's work he decided he

620
00:21:18,680 --> 00:21:20,010
 

621
00:21:18,690 --> 00:21:22,049
 wanted to implement some active learning

622
00:21:20,000 --> 00:21:22,049
 

623
00:21:20,010 --> 00:21:23,940
 in the real world so he built an app for

624
00:21:22,039 --> 00:21:23,940
 

625
00:21:22,049 --> 00:21:26,340
 the iPhone which was a beer

626
00:21:23,930 --> 00:21:26,340
 

627
00:21:23,940 --> 00:21:28,500
 recommendation system and basically it

628
00:21:26,330 --> 00:21:28,500
 

629
00:21:26,340 --> 00:21:31,559
 actively learns a user's preferences for

630
00:21:28,490 --> 00:21:31,559
 

631
00:21:28,500 --> 00:21:33,779
 beers quickly and just again the next

632
00:21:31,549 --> 00:21:33,779
 

633
00:21:31,559 --> 00:21:35,250
 system and this app are both free so if

634
00:21:33,769 --> 00:21:35,250
 

635
00:21:33,779 --> 00:21:37,799
 you're interested in checking about no

636
00:21:35,240 --> 00:21:37,799
 

637
00:21:35,250 --> 00:21:41,039
 charge so I'm not trying to milk you for

638
00:21:37,789 --> 00:21:41,039
 

639
00:21:37,799 --> 00:21:43,320
 money or anything here all right so what

640
00:21:41,029 --> 00:21:43,320
 

641
00:21:41,039 --> 00:21:45,419
 I want to do with the rest of this first

642
00:21:43,310 --> 00:21:45,419
 

643
00:21:43,320 --> 00:21:47,580
 part is just kind of go into a little

644
00:21:45,409 --> 00:21:47,580
 

645
00:21:45,419 --> 00:21:49,470
 bit of some of the basic principles of

646
00:21:47,570 --> 00:21:49,470
 

647
00:21:47,580 --> 00:21:52,049
 active learning some of the theory just

648
00:21:49,460 --> 00:21:52,049
 

649
00:21:49,470 --> 00:21:54,559
 touching on it at a kind of high level

650
00:21:52,039 --> 00:21:54,559
 

651
00:21:52,049 --> 00:21:57,240
 and then Steve's going to dive more into

652
00:21:54,549 --> 00:21:57,240
 

653
00:21:54,559 --> 00:22:00,750
 kind of more complex ideas in more

654
00:21:57,230 --> 00:22:00,750
 

655
00:21:57,240 --> 00:22:02,580
 detail so to begin with one of the

656
00:22:00,740 --> 00:22:02,580
 

657
00:22:00,750 --> 00:22:05,250
 things are ways I like to think about

658
00:22:02,570 --> 00:22:05,250
 

659
00:22:02,580 --> 00:22:07,019
 active learning is by distinguishing

660
00:22:05,240 --> 00:22:07,019
 

661
00:22:05,250 --> 00:22:10,380
 between what I call what and where

662
00:22:07,009 --> 00:22:10,380
 

663
00:22:07,019 --> 00:22:13,350
 information so in the case of just

664
00:22:10,370 --> 00:22:13,350
 

665
00:22:10,380 --> 00:22:15,320
 learning a density what information

666
00:22:13,340 --> 00:22:15,320
 

667
00:22:13,350 --> 00:22:18,090
 would be something like what is the

668
00:22:15,310 --> 00:22:18,090
 

669
00:22:15,320 --> 00:22:19,889
 conditional probability of Y given X so

670
00:22:18,080 --> 00:22:19,889
 

671
00:22:18,090 --> 00:22:20,700
 here I'm showing you say Y is the label

672
00:22:19,879 --> 00:22:20,700
 

673
00:22:19,889 --> 00:22:22,440
 X is a feature

674
00:22:20,690 --> 00:22:22,440
 

675
00:22:20,700 --> 00:22:25,409
 that's the conditional probability

676
00:22:22,430 --> 00:22:25,409
 

677
00:22:22,440 --> 00:22:27,419
 function a where question is where is

678
00:22:25,399 --> 00:22:27,419
 

679
00:22:25,409 --> 00:22:29,190
 this conditional probability function

680
00:22:27,409 --> 00:22:29,190
 

681
00:22:27,419 --> 00:22:31,139
 greater than zero so that would be

682
00:22:29,180 --> 00:22:31,139
 

683
00:22:29,190 --> 00:22:34,350
 basically telling us what our what's the

684
00:22:31,129 --> 00:22:34,350
 

685
00:22:31,139 --> 00:22:37,830
 decision set for label equals a plus one

686
00:22:34,340 --> 00:22:37,830
 

687
00:22:34,350 --> 00:22:39,659
 instead of minus one okay so what

688
00:22:37,820 --> 00:22:39,659
 

689
00:22:37,830 --> 00:22:41,609
 information in this case what is this

690
00:22:39,649 --> 00:22:41,609
 

691
00:22:39,659 --> 00:22:44,489
 function versus where is this function

692
00:22:41,599 --> 00:22:44,489
 

693
00:22:41,609 --> 00:22:47,100
 have a certain property another what and

694
00:22:44,479 --> 00:22:47,100
 

695
00:22:44,489 --> 00:22:49,350
 where example is just in straight-up

696
00:22:47,090 --> 00:22:49,350
 

697
00:22:47,100 --> 00:22:51,779
 density estimation we could say we want

698
00:22:49,340 --> 00:22:51,779
 

699
00:22:49,350 --> 00:22:53,840
 to estimate the density of our data X or

700
00:22:51,769 --> 00:22:53,840
 

701
00:22:51,779 --> 00:22:56,759
 we could ask where is that density

702
00:22:53,830 --> 00:22:56,759
 

703
00:22:53,840 --> 00:22:57,989
 greater than some value Epsilon that's

704
00:22:56,749 --> 00:22:57,989
 

705
00:22:56,759 --> 00:23:01,100
 something you might want to do if you

706
00:22:57,979 --> 00:23:01,100
 

707
00:22:57,989 --> 00:23:03,720
 were doing clustering and then finally

708
00:23:01,090 --> 00:23:03,720
 

709
00:23:01,100 --> 00:23:05,879
 we could think of a function estimation

710
00:23:03,710 --> 00:23:05,879
 

711
00:23:03,720 --> 00:23:07,559
 case like we want to estimate or or

712
00:23:05,869 --> 00:23:07,559
 

713
00:23:05,879 --> 00:23:09,899
 learn what is the conditional

714
00:23:07,549 --> 00:23:09,899
 

715
00:23:07,559 --> 00:23:12,659
 expectation of Y given X that's a what

716
00:23:09,889 --> 00:23:12,659
 

717
00:23:09,899 --> 00:23:16,109
 question a where question would be where

718
00:23:12,649 --> 00:23:16,109
 

719
00:23:12,659 --> 00:23:18,480
 is the maximum of that function so for

720
00:23:16,099 --> 00:23:18,480
 

721
00:23:16,109 --> 00:23:20,759
 what value of x do we have a maximal

722
00:23:18,470 --> 00:23:20,759
 

723
00:23:18,480 --> 00:23:22,889
 reward and this is basically the bandit

724
00:23:20,749 --> 00:23:22,889
 

725
00:23:20,759 --> 00:23:25,710
 setting so that would be a certain

726
00:23:22,879 --> 00:23:25,710
 

727
00:23:22,889 --> 00:23:28,289
 location so the point of this slide is

728
00:23:25,700 --> 00:23:28,289
 

729
00:23:25,710 --> 00:23:31,230
 just to kind of impress upon you that

730
00:23:28,279 --> 00:23:31,230
 

731
00:23:28,289 --> 00:23:32,999
 one of the most effective places for

732
00:23:31,220 --> 00:23:32,999
 

733
00:23:31,230 --> 00:23:34,320
 active learning is situations where you

734
00:23:32,989 --> 00:23:34,320
 

735
00:23:32,999 --> 00:23:36,749
 want to learn about where information

736
00:23:34,310 --> 00:23:36,749
 

737
00:23:34,320 --> 00:23:39,299
 you want to localize something a

738
00:23:36,739 --> 00:23:39,299
 

739
00:23:36,749 --> 00:23:41,429
 decision boundary a maximum of a

740
00:23:39,289 --> 00:23:41,429
 

741
00:23:39,299 --> 00:23:44,549
 function the support of a function

742
00:23:41,419 --> 00:23:44,549
 

743
00:23:41,429 --> 00:23:46,710
 something like that so all these kinds

744
00:23:44,539 --> 00:23:46,710
 

745
00:23:44,549 --> 00:23:48,419
 of problems clustering bandits and just

746
00:23:46,700 --> 00:23:48,419
 

747
00:23:46,710 --> 00:23:50,730
 general classification and prediction

748
00:23:48,409 --> 00:23:50,730
 

749
00:23:48,419 --> 00:23:52,019
 have active learning versions we're

750
00:23:50,720 --> 00:23:52,019
 

751
00:23:50,730 --> 00:23:53,999
 going to be focusing mainly on

752
00:23:52,009 --> 00:23:53,999
 

753
00:23:52,019 --> 00:23:56,340
 classification and especially than a

754
00:23:53,989 --> 00:23:56,340
 

755
00:23:53,999 --> 00:23:58,919
 binary classification just to keep all

756
00:23:56,330 --> 00:23:58,919
 

757
00:23:56,340 --> 00:24:00,840
 of the math and so forth clear and as

758
00:23:58,909 --> 00:24:00,840
 

759
00:23:58,919 --> 00:24:02,190
 simple as possible but a lot of the

760
00:24:00,830 --> 00:24:02,190
 

761
00:24:00,840 --> 00:24:03,659
 principles and ideas we'll be talking

762
00:24:02,180 --> 00:24:03,659
 

763
00:24:02,190 --> 00:24:08,220
 about can generalize to many many

764
00:24:03,649 --> 00:24:08,220
 

765
00:24:03,659 --> 00:24:10,950
 different situations okay so here is

766
00:24:08,210 --> 00:24:10,950
 

767
00:24:08,220 --> 00:24:13,409
 kind of a meta algorithm if you will for

768
00:24:10,940 --> 00:24:13,409
 

769
00:24:10,950 --> 00:24:16,200
 active learning and it basically works

770
00:24:13,399 --> 00:24:16,200
 

771
00:24:13,409 --> 00:24:18,509
 like this suppose we have a collection

772
00:24:16,190 --> 00:24:18,509
 

773
00:24:16,200 --> 00:24:20,669
 of models or hypotheses I'll call that

774
00:24:18,499 --> 00:24:20,669
 

775
00:24:18,509 --> 00:24:22,350
 script H so that's the set of all models

776
00:24:20,659 --> 00:24:22,350
 

777
00:24:20,669 --> 00:24:23,789
 we're considering if you want to keep

778
00:24:22,340 --> 00:24:23,789
 

779
00:24:22,350 --> 00:24:25,889
 something in your mind think of all

780
00:24:23,779 --> 00:24:25,889
 

781
00:24:23,789 --> 00:24:28,590
 possible linear classifiers in some

782
00:24:25,879 --> 00:24:28,590
 

783
00:24:25,889 --> 00:24:31,260
 feature space and then we're going to

784
00:24:28,580 --> 00:24:31,260
 

785
00:24:28,590 --> 00:24:33,510
 basically iterate over a kind of loop

786
00:24:31,250 --> 00:24:33,510
 

787
00:24:31,260 --> 00:24:36,570
 in which at each step we're going to

788
00:24:33,500 --> 00:24:36,570
 

789
00:24:33,510 --> 00:24:38,220
 first sample a random sample of from the

790
00:24:36,560 --> 00:24:38,220
 

791
00:24:36,570 --> 00:24:40,530
 data set from the unlabeled data set

792
00:24:38,210 --> 00:24:40,530
 

793
00:24:38,220 --> 00:24:42,810
 we're going to then label only those

794
00:24:40,520 --> 00:24:42,810
 

795
00:24:40,530 --> 00:24:45,390
 examples that distinguish models in our

796
00:24:42,800 --> 00:24:45,390
 

797
00:24:42,810 --> 00:24:47,700
 current hypothesis class or version

798
00:24:45,380 --> 00:24:47,700
 

799
00:24:45,390 --> 00:24:50,010
 space and then we're going to reduce

800
00:24:47,690 --> 00:24:50,010
 

801
00:24:47,700 --> 00:24:51,740
 that hypothesis class or version space

802
00:24:50,000 --> 00:24:51,740
 

803
00:24:50,010 --> 00:24:53,460
 by removing all models that are

804
00:24:51,730 --> 00:24:53,460
 

805
00:24:51,740 --> 00:24:55,620
 inconsistent with the labels we've

806
00:24:53,450 --> 00:24:55,620
 

807
00:24:53,460 --> 00:24:57,420
 received and then we'll repeat this

808
00:24:55,610 --> 00:24:57,420
 

809
00:24:55,620 --> 00:24:59,700
 process until we've sort of winnowed

810
00:24:57,410 --> 00:24:59,700
 

811
00:24:57,420 --> 00:25:01,560
 things down to a a small pool of very

812
00:24:59,690 --> 00:25:01,560
 

813
00:24:59,700 --> 00:25:04,650
 good models or maybe just a single model

814
00:25:01,550 --> 00:25:04,650
 

815
00:25:01,560 --> 00:25:07,170
 and output that so here's how it looks

816
00:25:04,640 --> 00:25:07,170
 

817
00:25:04,650 --> 00:25:08,580
 in a picture and sorry this loop so we

818
00:25:07,160 --> 00:25:08,580
 

819
00:25:07,170 --> 00:25:11,040
 have a model space that's why I was

820
00:25:08,570 --> 00:25:11,040
 

821
00:25:08,580 --> 00:25:12,780
 calling H we look at that model space

822
00:25:11,030 --> 00:25:12,780
 

823
00:25:11,040 --> 00:25:14,880
 the structure of it the geometric

824
00:25:12,770 --> 00:25:14,880
 

825
00:25:12,780 --> 00:25:16,650
 structure what have you we use that

826
00:25:14,870 --> 00:25:16,650
 

827
00:25:14,880 --> 00:25:19,140
 structure to decide how to select

828
00:25:16,640 --> 00:25:19,140
 

829
00:25:16,650 --> 00:25:21,750
 examples for labeling we asked our human

830
00:25:19,130 --> 00:25:21,750
 

831
00:25:19,140 --> 00:25:23,700
 expert for to label those we get those

832
00:25:21,740 --> 00:25:23,700
 

833
00:25:21,750 --> 00:25:26,850
 labelled data and then we use those

834
00:25:23,690 --> 00:25:26,850
 

835
00:25:23,700 --> 00:25:28,800
 labeled examples to prune or narrow our

836
00:25:26,840 --> 00:25:28,800
 

837
00:25:26,850 --> 00:25:31,050
 search over this model space basically

838
00:25:28,790 --> 00:25:31,050
 

839
00:25:28,800 --> 00:25:32,730
 effectively reducing it and again this

840
00:25:31,040 --> 00:25:32,730
 

841
00:25:31,050 --> 00:25:35,400
 reduction is because we're throwing out

842
00:25:32,720 --> 00:25:35,400
 

843
00:25:32,730 --> 00:25:38,760
 models that are inconsistent with the

844
00:25:35,390 --> 00:25:38,760
 

845
00:25:35,400 --> 00:25:41,070
 labels we've received so let me give you

846
00:25:38,750 --> 00:25:41,070
 

847
00:25:38,760 --> 00:25:42,780
 like the very simplest example of this

848
00:25:41,060 --> 00:25:42,780
 

849
00:25:41,070 --> 00:25:44,670
 this would be learning a one dimensional

850
00:25:42,770 --> 00:25:44,670
 

851
00:25:42,780 --> 00:25:46,560
 classifier so what I'm showing you here

852
00:25:44,660 --> 00:25:46,560
 

853
00:25:44,670 --> 00:25:49,080
 these dots are supposed to demand to

854
00:25:46,550 --> 00:25:49,080
 

855
00:25:46,560 --> 00:25:50,910
 know unlabeled examples and you could

856
00:25:49,070 --> 00:25:50,910
 

857
00:25:49,080 --> 00:25:53,700
 imagine that if we labeled all of them

858
00:25:50,900 --> 00:25:53,700
 

859
00:25:50,910 --> 00:25:55,710
 we could learn a nice linear classifier

860
00:25:53,690 --> 00:25:55,710
 

861
00:25:53,700 --> 00:25:57,840
 so this would be for example if your

862
00:25:55,700 --> 00:25:57,840
 

863
00:25:55,710 --> 00:25:59,550
 blood pressure is above a certain level

864
00:25:57,830 --> 00:25:59,550
 

865
00:25:57,840 --> 00:26:03,150
 then you probably have heart disease and

866
00:25:59,540 --> 00:26:03,150
 

867
00:25:59,550 --> 00:26:04,830
 the red would indicate that so we can

868
00:26:03,140 --> 00:26:04,830
 

869
00:26:03,150 --> 00:26:06,900
 contrast passive learning and active

870
00:26:04,820 --> 00:26:06,900
 

871
00:26:04,830 --> 00:26:09,810
 learning very easily and I'll show you

872
00:26:06,890 --> 00:26:09,810
 

873
00:26:06,900 --> 00:26:11,580
 these two so passive learning would just

874
00:26:09,800 --> 00:26:11,580
 

875
00:26:09,810 --> 00:26:14,250
 be randomly selecting unlabeled examples

876
00:26:11,570 --> 00:26:14,250
 

877
00:26:11,580 --> 00:26:15,870
 for labeling and active learning in this

878
00:26:14,240 --> 00:26:15,870
 

879
00:26:14,250 --> 00:26:17,790
 case could be implemented just as a

880
00:26:15,860 --> 00:26:17,790
 

881
00:26:15,870 --> 00:26:20,280
 binary search for bisection procedure

882
00:26:17,780 --> 00:26:20,280
 

883
00:26:17,790 --> 00:26:22,320
 where we keep trying to sample between

884
00:26:20,270 --> 00:26:22,320
 

885
00:26:20,280 --> 00:26:24,540
 the two closest oppositely labelled

886
00:26:22,310 --> 00:26:24,540
 

887
00:26:22,320 --> 00:26:26,790
 points and you can see after a few

888
00:26:24,530 --> 00:26:26,790
 

889
00:26:24,540 --> 00:26:28,350
 samples the binary search the active

890
00:26:26,780 --> 00:26:28,350
 

891
00:26:26,790 --> 00:26:30,120
 procedure has really nailed down where

892
00:26:28,340 --> 00:26:30,120
 

893
00:26:28,350 --> 00:26:32,250
 that decision boundary is but the

894
00:26:30,110 --> 00:26:32,250
 

895
00:26:30,120 --> 00:26:33,990
 passive one is sort of very very loose

896
00:26:32,240 --> 00:26:33,990
 

897
00:26:32,250 --> 00:26:36,690
 it hasn't really figured out where to

898
00:26:33,980 --> 00:26:36,690
 

899
00:26:33,990 --> 00:26:38,100
 put that decision boundary and in fact

900
00:26:36,680 --> 00:26:38,100
 

901
00:26:36,690 --> 00:26:40,770
 just a little back of the envelope math

902
00:26:38,090 --> 00:26:40,770
 

903
00:26:38,100 --> 00:26:42,840
 shows you that the error of the passive

904
00:26:40,760 --> 00:26:42,840
 

905
00:26:40,770 --> 00:26:43,400
 learning method goes down like 1 over n

906
00:26:42,830 --> 00:26:43,400
 

907
00:26:42,840 --> 00:26:44,660
 where

908
00:26:43,390 --> 00:26:44,660
 

909
00:26:43,400 --> 00:26:48,110
 is the number of samples you've

910
00:26:44,650 --> 00:26:48,110
 

911
00:26:44,660 --> 00:26:50,510
 collected versus the active method

912
00:26:48,100 --> 00:26:50,510
 

913
00:26:48,110 --> 00:26:52,160
 binary search is going down like true

914
00:26:50,500 --> 00:26:52,160
 

915
00:26:50,510 --> 00:26:54,980
 the myosin so we get an exponential

916
00:26:52,150 --> 00:26:54,980
 

917
00:26:52,160 --> 00:26:56,720
 speed-up in how quickly we can learn

918
00:26:54,970 --> 00:26:56,720
 

919
00:26:54,980 --> 00:26:59,360
 from a small set of labeled examples

920
00:26:56,710 --> 00:26:59,360
 

921
00:26:56,720 --> 00:27:00,950
 this is kind of a classical starting

922
00:26:59,350 --> 00:27:00,950
 

923
00:26:59,360 --> 00:27:02,780
 point for trying to understand why

924
00:27:00,940 --> 00:27:02,780
 

925
00:27:00,950 --> 00:27:06,770
 active learning could help and that's

926
00:27:02,770 --> 00:27:06,770
 

927
00:27:02,780 --> 00:27:08,570
 binary search so then you can say well

928
00:27:06,760 --> 00:27:08,570
 

929
00:27:06,770 --> 00:27:10,760
 that's great I learned about binary

930
00:27:08,560 --> 00:27:10,760
 

931
00:27:08,570 --> 00:27:12,380
 search in my freshman computer science

932
00:27:10,750 --> 00:27:12,380
 

933
00:27:10,760 --> 00:27:14,570
 class is that all there is to it well

934
00:27:12,370 --> 00:27:14,570
 

935
00:27:12,380 --> 00:27:16,010
 there's a lot more to it we're going to

936
00:27:14,560 --> 00:27:16,010
 

937
00:27:14,570 --> 00:27:17,600
 tell you about that but the starting

938
00:27:16,000 --> 00:27:17,600
 

939
00:27:16,010 --> 00:27:19,550
 point for just saying how would we

940
00:27:17,590 --> 00:27:19,550
 

941
00:27:17,600 --> 00:27:22,160
 generalize binary search to more

942
00:27:19,540 --> 00:27:22,160
 

943
00:27:19,550 --> 00:27:23,960
 complicated situations as VC theory and

944
00:27:22,150 --> 00:27:23,960
 

945
00:27:22,160 --> 00:27:25,490
 so let me just give you a kind of a

946
00:27:23,950 --> 00:27:25,490
 

947
00:27:23,960 --> 00:27:26,690
 little bit of an introduction to this

948
00:27:25,480 --> 00:27:26,690
 

949
00:27:25,490 --> 00:27:28,130
 and then Steve's going to talk a lot

950
00:27:26,680 --> 00:27:28,130
 

951
00:27:26,690 --> 00:27:30,800
 more about the theory of active learning

952
00:27:28,120 --> 00:27:30,800
 

953
00:27:28,130 --> 00:27:32,929
 in the next couple of parts so suppose

954
00:27:30,790 --> 00:27:32,929
 

955
00:27:30,800 --> 00:27:34,970
 we're giving a set of training data X Y

956
00:27:32,919 --> 00:27:34,970
 

957
00:27:32,929 --> 00:27:36,679
 so those are our label Dexamyl

958
00:27:34,960 --> 00:27:36,679
 

959
00:27:34,970 --> 00:27:41,000
 we want to learn a prediction of

960
00:27:36,669 --> 00:27:41,000
 

961
00:27:36,679 --> 00:27:42,890
 function f to predict Y from X so we're

962
00:27:40,990 --> 00:27:42,890
 

963
00:27:41,000 --> 00:27:45,530
 going to set consider a possibly

964
00:27:42,880 --> 00:27:45,530
 

965
00:27:42,890 --> 00:27:47,000
 infinite set of hypotheses I'll call it

966
00:27:45,520 --> 00:27:47,000
 

967
00:27:45,530 --> 00:27:49,160
 script F I guess I was calling this

968
00:27:46,990 --> 00:27:49,160
 

969
00:27:47,000 --> 00:27:51,500
 script H before so my apologies for

970
00:27:49,150 --> 00:27:51,500
 

971
00:27:49,160 --> 00:27:53,300
 changing the notation a little bit but

972
00:27:51,490 --> 00:27:53,300
 

973
00:27:51,500 --> 00:27:55,640
 basically I'm saying suppose we have a

974
00:27:53,290 --> 00:27:55,640
 

975
00:27:53,300 --> 00:27:58,850
 big pool of hypotheses or models and

976
00:27:55,630 --> 00:27:58,850
 

977
00:27:55,640 --> 00:28:01,040
 they have a finite VC dimension D and so

978
00:27:58,840 --> 00:28:01,040
 

979
00:27:58,850 --> 00:28:02,720
 if you aren't familiar VC Theory just

980
00:28:01,030 --> 00:28:02,720
 

981
00:28:01,040 --> 00:28:04,610
 think about D is the dimension of your

982
00:28:02,710 --> 00:28:04,610
 

983
00:28:02,720 --> 00:28:06,590
 feature space that's what it would be in

984
00:28:04,600 --> 00:28:06,590
 

985
00:28:04,610 --> 00:28:10,670
 the case of a linear classifier for

986
00:28:06,580 --> 00:28:10,670
 

987
00:28:06,590 --> 00:28:12,559
 example so we have with that for every F

988
00:28:10,660 --> 00:28:12,559
 

989
00:28:10,670 --> 00:28:14,360
 for every model in this class we can

990
00:28:12,549 --> 00:28:14,360
 

991
00:28:12,559 --> 00:28:15,890
 talk about its error rate and we'll call

992
00:28:14,350 --> 00:28:15,890
 

993
00:28:14,360 --> 00:28:18,080
 that the risk and so that's just the

994
00:28:15,880 --> 00:28:18,080
 

995
00:28:15,890 --> 00:28:23,390
 probability that f of X is not equal to

996
00:28:18,070 --> 00:28:23,390
 

997
00:28:18,080 --> 00:28:25,100
 Y for a new pair X Y so the first thing

998
00:28:23,380 --> 00:28:25,100
 

999
00:28:23,390 --> 00:28:26,570
 you can do is you could say for any F I

1000
00:28:25,090 --> 00:28:26,570
 

1001
00:28:25,100 --> 00:28:29,390
 could look at how well does it do in the

1002
00:28:26,560 --> 00:28:29,390
 

1003
00:28:26,570 --> 00:28:31,670
 training data so this R hat of F is

1004
00:28:29,380 --> 00:28:31,670
 

1005
00:28:29,390 --> 00:28:33,320
 simply counting up how many mistakes do

1006
00:28:31,660 --> 00:28:33,320
 

1007
00:28:31,670 --> 00:28:35,330
 I make on my training set and then

1008
00:28:33,310 --> 00:28:35,330
 

1009
00:28:33,320 --> 00:28:38,090
 normalizing by n to get an average we

1010
00:28:35,320 --> 00:28:38,090
 

1011
00:28:35,330 --> 00:28:40,460
 call this the empirical risk and what VC

1012
00:28:38,080 --> 00:28:40,460
 

1013
00:28:38,090 --> 00:28:42,650
 Theory does for us is it allows us to

1014
00:28:40,450 --> 00:28:42,650
 

1015
00:28:40,460 --> 00:28:45,170
 bound uniformly over all the models in

1016
00:28:42,640 --> 00:28:45,170
 

1017
00:28:42,650 --> 00:28:47,240
 our class the deviation or difference

1018
00:28:45,160 --> 00:28:47,240
 

1019
00:28:45,170 --> 00:28:50,090
 between the true probability of error or

1020
00:28:47,230 --> 00:28:50,090
 

1021
00:28:47,240 --> 00:28:53,150
 true risk and the empirical risk of each

1022
00:28:50,080 --> 00:28:53,150
 

1023
00:28:50,090 --> 00:28:55,680
 and every classifier F and so this bound

1024
00:28:53,140 --> 00:28:55,680
 

1025
00:28:53,150 --> 00:28:57,570
 is saying that the maximum absolute

1026
00:28:55,670 --> 00:28:57,570
 

1027
00:28:55,680 --> 00:28:59,790
 difference between our and our hat for

1028
00:28:57,560 --> 00:28:59,790
 

1029
00:28:57,570 --> 00:29:02,340
 any F is less than or equal to six times

1030
00:28:59,780 --> 00:29:02,340
 

1031
00:28:59,790 --> 00:29:04,320
 the square root of that VC dimension D

1032
00:29:02,330 --> 00:29:04,320
 

1033
00:29:02,340 --> 00:29:07,290
 and then there's a log fact that their

1034
00:29:04,310 --> 00:29:07,290
 

1035
00:29:04,320 --> 00:29:09,000
 log n over Delta divided by N and as

1036
00:29:07,280 --> 00:29:09,000
 

1037
00:29:07,290 --> 00:29:10,830
 your sample size here and Delta is your

1038
00:29:08,990 --> 00:29:10,830
 

1039
00:29:09,000 --> 00:29:14,820
 confidence level so we think of Delta as

1040
00:29:10,820 --> 00:29:14,820
 

1041
00:29:10,830 --> 00:29:16,950
 some small number like 0.01 1% when I

1042
00:29:14,810 --> 00:29:16,950
 

1043
00:29:14,820 --> 00:29:21,210
 have 99% confidence that this bound

1044
00:29:16,940 --> 00:29:21,210
 

1045
00:29:16,950 --> 00:29:23,430
 holds so how do we use something like

1046
00:29:21,200 --> 00:29:23,430
 

1047
00:29:21,210 --> 00:29:25,170
 this so this is empirical risk

1048
00:29:23,420 --> 00:29:25,170
 

1049
00:29:23,430 --> 00:29:26,940
 minimization I'm sure many of you are

1050
00:29:25,160 --> 00:29:26,940
 

1051
00:29:25,170 --> 00:29:28,170
 familiar with it but bear with me I just

1052
00:29:26,930 --> 00:29:28,170
 

1053
00:29:26,940 --> 00:29:30,030
 kind of want to go through this and then

1054
00:29:28,160 --> 00:29:30,030
 

1055
00:29:28,170 --> 00:29:32,610
 I'll talk about how we active eyes this

1056
00:29:30,020 --> 00:29:32,610
 

1057
00:29:30,030 --> 00:29:34,470
 type of procedure so empirical risk

1058
00:29:32,600 --> 00:29:34,470
 

1059
00:29:32,610 --> 00:29:36,630
 minimization the goal is to select a

1060
00:29:34,460 --> 00:29:36,630
 

1061
00:29:34,470 --> 00:29:39,120
 hypothesis whose true error rate is

1062
00:29:36,620 --> 00:29:39,120
 

1063
00:29:36,630 --> 00:29:41,760
 within epsilon of the best we could

1064
00:29:39,110 --> 00:29:41,760
 

1065
00:29:39,120 --> 00:29:44,340
 achieve from any element of this class

1066
00:29:41,750 --> 00:29:44,340
 

1067
00:29:41,760 --> 00:29:46,830
 script F so I'm going to talk about two

1068
00:29:44,330 --> 00:29:46,830
 

1069
00:29:44,340 --> 00:29:50,700
 different classifiers F star which is

1070
00:29:46,820 --> 00:29:50,700
 

1071
00:29:46,830 --> 00:29:52,860
 the true risk minimizer is the F in our

1072
00:29:50,690 --> 00:29:52,860
 

1073
00:29:50,700 --> 00:29:54,510
 class that actually minimizes the true

1074
00:29:52,850 --> 00:29:54,510
 

1075
00:29:52,860 --> 00:29:59,790
 probability of error sort of the test

1076
00:29:54,500 --> 00:29:59,790
 

1077
00:29:54,510 --> 00:30:01,770
 error if you will F hat is the model

1078
00:29:59,780 --> 00:30:01,770
 

1079
00:29:59,790 --> 00:30:03,390
 that minimizes the empirical risk that's

1080
00:30:01,760 --> 00:30:03,390
 

1081
00:30:01,770 --> 00:30:05,820
 something we can actually compute by

1082
00:30:03,380 --> 00:30:05,820
 

1083
00:30:03,390 --> 00:30:07,890
 just looking at the training data and so

1084
00:30:05,810 --> 00:30:07,890
 

1085
00:30:05,820 --> 00:30:09,960
 since F hat minimizes the empirical risk

1086
00:30:07,880 --> 00:30:09,960
 

1087
00:30:07,890 --> 00:30:11,790
 we know that the empirical risk of f hat

1088
00:30:09,950 --> 00:30:11,790
 

1089
00:30:09,960 --> 00:30:15,030
 has to be less than or equal to the

1090
00:30:11,780 --> 00:30:15,030
 

1091
00:30:11,790 --> 00:30:17,280
 empirical risk of f star okay so that's

1092
00:30:15,020 --> 00:30:17,280
 

1093
00:30:15,030 --> 00:30:18,930
 a kind of the key insight and what it

1094
00:30:17,270 --> 00:30:18,930
 

1095
00:30:17,280 --> 00:30:20,550
 allows us to do is make a picture like

1096
00:30:18,920 --> 00:30:20,550
 

1097
00:30:18,930 --> 00:30:23,520
 this so what I'm showing you is the

1098
00:30:20,540 --> 00:30:23,520
 

1099
00:30:20,550 --> 00:30:25,200
 empirical risk of f hat and f star those

1100
00:30:23,510 --> 00:30:25,200
 

1101
00:30:23,520 --> 00:30:28,080
 are the blue dots and then those

1102
00:30:25,190 --> 00:30:28,080
 

1103
00:30:25,200 --> 00:30:30,300
 confidence intervals are the confidence

1104
00:30:28,070 --> 00:30:30,300
 

1105
00:30:28,080 --> 00:30:32,310
 intervals we get from the VC inequality

1106
00:30:30,290 --> 00:30:32,310
 

1107
00:30:30,300 --> 00:30:33,810
 I showed you on the previous slide so

1108
00:30:32,300 --> 00:30:33,810
 

1109
00:30:32,310 --> 00:30:35,910
 basically what that saying is that the

1110
00:30:33,800 --> 00:30:35,910
 

1111
00:30:33,810 --> 00:30:38,940
 true risk of f hat is somewhere within

1112
00:30:35,900 --> 00:30:38,940
 

1113
00:30:35,910 --> 00:30:43,500
 the red bars around the R hat of F hat

1114
00:30:38,930 --> 00:30:43,500
 

1115
00:30:38,940 --> 00:30:45,090
 blue dot and similarly for F star so

1116
00:30:43,490 --> 00:30:45,090
 

1117
00:30:43,500 --> 00:30:46,560
 what does that do for us so now we can

1118
00:30:45,080 --> 00:30:46,560
 

1119
00:30:45,090 --> 00:30:48,750
 sort of say well what do we know we know

1120
00:30:46,550 --> 00:30:48,750
 

1121
00:30:46,560 --> 00:30:50,550
 that the true probability of error the

1122
00:30:48,740 --> 00:30:50,550
 

1123
00:30:48,750 --> 00:30:52,320
 true error rate of F hat with high

1124
00:30:50,540 --> 00:30:52,320
 

1125
00:30:50,550 --> 00:30:54,000
 probability is going to be less than or

1126
00:30:52,310 --> 00:30:54,000
 

1127
00:30:52,320 --> 00:30:56,760
 equal to that upper bound there which is

1128
00:30:53,990 --> 00:30:56,760
 

1129
00:30:54,000 --> 00:31:00,630
 our hat of F hat plus that term coming

1130
00:30:56,750 --> 00:31:00,630
 

1131
00:30:56,760 --> 00:31:03,870
 from the VC inequality and we also know

1132
00:31:00,620 --> 00:31:03,870
 

1133
00:31:00,630 --> 00:31:07,360
 that with high probability the true

1134
00:31:03,860 --> 00:31:07,360
 

1135
00:31:03,870 --> 00:31:11,470
 error rate of F star is

1136
00:31:07,350 --> 00:31:11,470
 

1137
00:31:07,360 --> 00:31:15,190
 you know at best this our hat of F star

1138
00:31:11,460 --> 00:31:15,190
 

1139
00:31:11,470 --> 00:31:17,620
 plus or minus that bound coming from the

1140
00:31:15,180 --> 00:31:17,620
 

1141
00:31:15,190 --> 00:31:19,420
 BC inequality so we know what those

1142
00:31:17,610 --> 00:31:19,420
 

1143
00:31:17,620 --> 00:31:20,830
 upper and lower limits are and with high

1144
00:31:19,410 --> 00:31:20,830
 

1145
00:31:19,420 --> 00:31:22,630
 probability those are where those true

1146
00:31:20,820 --> 00:31:22,630
 

1147
00:31:20,830 --> 00:31:24,940
 risks or true probabilities of errors

1148
00:31:22,620 --> 00:31:24,940
 

1149
00:31:22,630 --> 00:31:27,010
 set and so what we can do now is we can

1150
00:31:24,930 --> 00:31:27,010
 

1151
00:31:24,940 --> 00:31:28,900
 just look at the difference okay so the

1152
00:31:27,000 --> 00:31:28,900
 

1153
00:31:27,010 --> 00:31:31,390
 difference between those two bounds is

1154
00:31:28,890 --> 00:31:31,390
 

1155
00:31:28,900 --> 00:31:33,429
 just 12 times the square root of d log n

1156
00:31:31,380 --> 00:31:33,429
 

1157
00:31:31,390 --> 00:31:35,650
 over Delta divided by and the sample

1158
00:31:33,419 --> 00:31:35,650
 

1159
00:31:33,429 --> 00:31:38,110
 size and what that means that is we can

1160
00:31:35,640 --> 00:31:38,110
 

1161
00:31:35,650 --> 00:31:39,970
 immediately identify how many training

1162
00:31:38,100 --> 00:31:39,970
 

1163
00:31:38,110 --> 00:31:42,220
 samples how many labeled examples do we

1164
00:31:39,960 --> 00:31:42,220
 

1165
00:31:39,970 --> 00:31:43,570
 need in order to find an epsilon good

1166
00:31:42,210 --> 00:31:43,570
 

1167
00:31:42,220 --> 00:31:45,280
 classifier where we're going to need

1168
00:31:43,560 --> 00:31:45,280
 

1169
00:31:43,570 --> 00:31:47,890
 this difference to be less than or equal

1170
00:31:45,270 --> 00:31:47,890
 

1171
00:31:45,280 --> 00:31:49,240
 to Epsilon so that's where the 12 times

1172
00:31:47,880 --> 00:31:49,240
 

1173
00:31:47,890 --> 00:31:50,860
 the square root of that business less

1174
00:31:49,230 --> 00:31:50,860
 

1175
00:31:49,240 --> 00:31:53,200
 than or equal to Epsilon and then we

1176
00:31:50,850 --> 00:31:53,200
 

1177
00:31:50,860 --> 00:31:55,210
 just invert that to solve for what

1178
00:31:53,190 --> 00:31:55,210
 

1179
00:31:53,200 --> 00:31:59,110
 should n be and we find that n should be

1180
00:31:55,200 --> 00:31:59,110
 

1181
00:31:55,210 --> 00:32:00,820
 something like d log 1 over epsilon a d

1182
00:31:59,100 --> 00:32:00,820
 

1183
00:31:59,110 --> 00:32:03,250
 log 1 over Delta divided by epsilon

1184
00:32:00,810 --> 00:32:03,250
 

1185
00:32:00,820 --> 00:32:04,630
 squared so what is that telling us it's

1186
00:32:03,240 --> 00:32:04,630
 

1187
00:32:03,250 --> 00:32:07,270
 saying that if you're in a higher

1188
00:32:04,620 --> 00:32:07,270
 

1189
00:32:04,630 --> 00:32:09,520
 dimensional feature space or a larger VC

1190
00:32:07,260 --> 00:32:09,520
 

1191
00:32:07,270 --> 00:32:11,710
 class a higher VC dimension you're going

1192
00:32:09,510 --> 00:32:11,710
 

1193
00:32:09,520 --> 00:32:13,450
 to need more training data it also says

1194
00:32:11,700 --> 00:32:13,450
 

1195
00:32:11,710 --> 00:32:15,010
 that if you want to push epsilon down if

1196
00:32:13,440 --> 00:32:15,010
 

1197
00:32:13,450 --> 00:32:16,990
 you want to get closer and closer to

1198
00:32:15,000 --> 00:32:16,990
 

1199
00:32:15,010 --> 00:32:18,610
 that true best classifier within that

1200
00:32:16,980 --> 00:32:18,610
 

1201
00:32:16,990 --> 00:32:20,500
 class you're also going to have to pay a

1202
00:32:18,600 --> 00:32:20,500
 

1203
00:32:18,610 --> 00:32:22,510
 price and how many labeled examples you

1204
00:32:20,490 --> 00:32:22,510
 

1205
00:32:20,500 --> 00:32:25,660
 need and Steve's going to be elaborating

1206
00:32:22,500 --> 00:32:25,660
 

1207
00:32:22,510 --> 00:32:27,309
 on all this as we go forward so let me

1208
00:32:25,650 --> 00:32:27,309
 

1209
00:32:25,660 --> 00:32:29,230
 just show you how this might look in

1210
00:32:27,299 --> 00:32:29,230
 

1211
00:32:27,309 --> 00:32:30,040
 pictures then so what I'm doing here is

1212
00:32:29,220 --> 00:32:30,040
 

1213
00:32:29,230 --> 00:32:32,320
 I'm showing you

1214
00:32:30,030 --> 00:32:32,320
 

1215
00:32:30,040 --> 00:32:34,720
 empirical risks of a bunch of different

1216
00:32:32,310 --> 00:32:34,720
 

1217
00:32:32,320 --> 00:32:36,700
 classifiers say each blue dot is the

1218
00:32:34,710 --> 00:32:36,700
 

1219
00:32:34,720 --> 00:32:38,260
 empirical risk and the red bars again

1220
00:32:36,690 --> 00:32:38,260
 

1221
00:32:36,700 --> 00:32:40,450
 are those confidence intervals coming

1222
00:32:38,250 --> 00:32:40,450
 

1223
00:32:38,260 --> 00:32:43,809
 from the VC inequality so we could order

1224
00:32:40,440 --> 00:32:43,809
 

1225
00:32:40,450 --> 00:32:46,030
 them like this from lowest empirical

1226
00:32:43,799 --> 00:32:46,030
 

1227
00:32:43,809 --> 00:32:47,650
 risk to highest empirical risk and what

1228
00:32:46,020 --> 00:32:47,650
 

1229
00:32:46,030 --> 00:32:49,690
 happens is we collect more and more

1230
00:32:47,640 --> 00:32:49,690
 

1231
00:32:47,650 --> 00:32:51,010
 training data we'll shrink these

1232
00:32:49,680 --> 00:32:51,010
 

1233
00:32:49,690 --> 00:32:53,290
 confidence intervals because remember

1234
00:32:51,000 --> 00:32:53,290
 

1235
00:32:51,010 --> 00:32:54,429
 those VC bounds are going down like 1

1236
00:32:53,280 --> 00:32:54,429
 

1237
00:32:53,290 --> 00:32:57,309
 over the square root of N or

1238
00:32:54,419 --> 00:32:57,309
 

1239
00:32:54,429 --> 00:32:58,630
 proportional to that and we'll get to

1240
00:32:57,299 --> 00:32:58,630
 

1241
00:32:57,309 --> 00:33:00,730
 some point like this if we've got enough

1242
00:32:58,620 --> 00:33:00,730
 

1243
00:32:58,630 --> 00:33:03,000
 training data and at that point we can

1244
00:33:00,720 --> 00:33:03,000
 

1245
00:33:00,730 --> 00:33:05,290
 clearly see that with high confidence

1246
00:33:02,990 --> 00:33:05,290
 

1247
00:33:03,000 --> 00:33:07,300
 model one is better than all the other

1248
00:33:05,280 --> 00:33:07,300
 

1249
00:33:05,290 --> 00:33:08,740
 models and that's the idea of empirical

1250
00:33:07,290 --> 00:33:08,740
 

1251
00:33:07,300 --> 00:33:12,490
 risk minimization you could keep going

1252
00:33:08,730 --> 00:33:12,490
 

1253
00:33:08,740 --> 00:33:14,110
 until you get this epsilon good model or

1254
00:33:12,480 --> 00:33:14,110
 

1255
00:33:12,490 --> 00:33:15,700
 you could keep going you know if you

1256
00:33:14,100 --> 00:33:15,700
 

1257
00:33:14,110 --> 00:33:17,260
 have a finite set of models until one's

1258
00:33:15,690 --> 00:33:17,260
 

1259
00:33:15,700 --> 00:33:21,070
 clearly better than all the others like

1260
00:33:17,250 --> 00:33:21,070
 

1261
00:33:17,260 --> 00:33:23,559
 I'm showing here so what's the issue

1262
00:33:21,060 --> 00:33:23,559
 

1263
00:33:21,070 --> 00:33:25,539
 is that empirical risk minimization may

1264
00:33:23,549 --> 00:33:25,539
 

1265
00:33:23,559 --> 00:33:27,039
 be wasting labeled examples and why

1266
00:33:25,529 --> 00:33:27,039
 

1267
00:33:25,539 --> 00:33:29,620
 would that be so let's kind of back up

1268
00:33:27,029 --> 00:33:29,620
 

1269
00:33:27,039 --> 00:33:31,059
 to where we started let's collect some

1270
00:33:29,610 --> 00:33:31,059
 

1271
00:33:29,620 --> 00:33:32,289
 more data shrink those confidence

1272
00:33:31,049 --> 00:33:32,289
 

1273
00:33:31,059 --> 00:33:35,259
 intervals and suppose we're at this

1274
00:33:32,279 --> 00:33:35,259
 

1275
00:33:32,289 --> 00:33:37,000
 stage right here now what could I what

1276
00:33:35,249 --> 00:33:37,000
 

1277
00:33:35,259 --> 00:33:38,830
 could i green from this picture well if

1278
00:33:36,990 --> 00:33:38,830
 

1279
00:33:37,000 --> 00:33:42,130
 I had this picture I can see that at

1280
00:33:38,820 --> 00:33:42,130
 

1281
00:33:38,830 --> 00:33:44,799
 this point we know that hypothesis one

1282
00:33:42,120 --> 00:33:44,799
 

1283
00:33:42,130 --> 00:33:46,330
 or model one has to be better or at

1284
00:33:44,789 --> 00:33:46,330
 

1285
00:33:44,799 --> 00:33:48,669
 least with high probability is better

1286
00:33:46,320 --> 00:33:48,669
 

1287
00:33:46,330 --> 00:33:50,380
 than model three and all the others

1288
00:33:48,659 --> 00:33:50,380
 

1289
00:33:48,669 --> 00:33:52,600
 beyond model three right and so we could

1290
00:33:50,370 --> 00:33:52,600
 

1291
00:33:50,380 --> 00:33:54,190
 actually remove that from further

1292
00:33:52,590 --> 00:33:54,190
 

1293
00:33:52,600 --> 00:33:55,899
 consideration at this point and remove

1294
00:33:54,180 --> 00:33:55,899
 

1295
00:33:54,190 --> 00:33:57,460
 all the others we basically boiled

1296
00:33:55,889 --> 00:33:57,460
 

1297
00:33:55,899 --> 00:33:59,919
 things down at this point to just a race

1298
00:33:57,450 --> 00:33:59,919
 

1299
00:33:57,460 --> 00:34:03,429
 between who's better model one or two

1300
00:33:59,909 --> 00:34:03,429
 

1301
00:33:59,919 --> 00:34:05,110
 okay so rather than continually asking

1302
00:34:03,419 --> 00:34:05,110
 

1303
00:34:03,429 --> 00:34:06,759
 for random examples at this point we're

1304
00:34:05,100 --> 00:34:06,759
 

1305
00:34:05,110 --> 00:34:08,919
 going to kind of strategically select

1306
00:34:06,749 --> 00:34:08,919
 

1307
00:34:06,759 --> 00:34:11,950
 samples we're going to only require

1308
00:34:08,909 --> 00:34:11,950
 

1309
00:34:08,919 --> 00:34:15,429
 labels for examples where hypotheses or

1310
00:34:11,940 --> 00:34:15,429
 

1311
00:34:11,950 --> 00:34:17,319
 models one and two label an example

1312
00:34:15,419 --> 00:34:17,319
 

1313
00:34:15,429 --> 00:34:18,909
 differently where they disagree and so

1314
00:34:17,309 --> 00:34:18,909
 

1315
00:34:17,319 --> 00:34:21,069
 this is the idea of disagreement based

1316
00:34:18,899 --> 00:34:21,069
 

1317
00:34:18,909 --> 00:34:22,419
 learning Steve is the world expert on

1318
00:34:21,059 --> 00:34:22,419
 

1319
00:34:21,069 --> 00:34:24,490
 this topic and he's gonna be talking a

1320
00:34:22,409 --> 00:34:24,490
 

1321
00:34:22,419 --> 00:34:25,960
 lot about it but the idea is that now

1322
00:34:24,480 --> 00:34:25,960
 

1323
00:34:24,490 --> 00:34:28,179
 what the machine's gonna do is is gonna

1324
00:34:25,950 --> 00:34:28,179
 

1325
00:34:25,960 --> 00:34:30,310
 say okay now I know I just have to worry

1326
00:34:28,169 --> 00:34:30,310
 

1327
00:34:28,179 --> 00:34:32,950
 is one or two better model one or two

1328
00:34:30,300 --> 00:34:32,950
 

1329
00:34:30,310 --> 00:34:34,839
 I'm gonna ask my human expert I'm gonna

1330
00:34:32,940 --> 00:34:34,839
 

1331
00:34:32,950 --> 00:34:36,940
 search you my data set of unlabeled

1332
00:34:34,829 --> 00:34:36,940
 

1333
00:34:34,839 --> 00:34:38,919
 examples find examples where models one

1334
00:34:36,930 --> 00:34:38,919
 

1335
00:34:36,940 --> 00:34:40,569
 and two predict different output labels

1336
00:34:38,909 --> 00:34:40,569
 

1337
00:34:38,919 --> 00:34:42,190
 and I'm gonna ask the human expert to

1338
00:34:40,559 --> 00:34:42,190
 

1339
00:34:40,569 --> 00:34:44,169
 label those those are the ones that I'm

1340
00:34:42,180 --> 00:34:44,169
 

1341
00:34:42,190 --> 00:34:46,839
 confused about me being the machine in

1342
00:34:44,159 --> 00:34:46,839
 

1343
00:34:44,169 --> 00:34:48,700
 this little story so let me show you how

1344
00:34:46,829 --> 00:34:48,700
 

1345
00:34:46,839 --> 00:34:51,099
 that might look and this is based on

1346
00:34:48,690 --> 00:34:51,099
 

1347
00:34:48,700 --> 00:34:52,810
 some of Nina's work actually how would

1348
00:34:51,089 --> 00:34:52,810
 

1349
00:34:51,099 --> 00:34:54,280
 you do this disagreement based active

1350
00:34:52,800 --> 00:34:54,280
 

1351
00:34:52,810 --> 00:34:56,530
 learning in the case of a linear

1352
00:34:54,270 --> 00:34:56,530
 

1353
00:34:54,280 --> 00:34:58,690
 classifier so what we're going to do

1354
00:34:56,520 --> 00:34:58,690
 

1355
00:34:56,530 --> 00:35:01,569
 here is consider points for unlabeled

1356
00:34:58,680 --> 00:35:01,569
 

1357
00:34:58,690 --> 00:35:03,400
 examples sprinkled uniformly on the unit

1358
00:35:01,559 --> 00:35:03,400
 

1359
00:35:01,569 --> 00:35:05,589
 ball in this picture and we're going to

1360
00:35:03,390 --> 00:35:05,589
 

1361
00:35:03,400 --> 00:35:07,210
 focus on linear classifiers that pass

1362
00:35:05,579 --> 00:35:07,210
 

1363
00:35:05,589 --> 00:35:09,430
 through the origin or the center of this

1364
00:35:07,200 --> 00:35:09,430
 

1365
00:35:07,210 --> 00:35:11,079
 ball so that's what one of the

1366
00:35:09,420 --> 00:35:11,079
 

1367
00:35:09,430 --> 00:35:12,609
 classifiers looks like we don't know

1368
00:35:11,069 --> 00:35:12,609
 

1369
00:35:11,079 --> 00:35:15,579
 which one it is so it could be in any

1370
00:35:12,599 --> 00:35:15,579
 

1371
00:35:12,609 --> 00:35:17,230
 orientation so now what we're going to

1372
00:35:15,569 --> 00:35:17,230
 

1373
00:35:15,579 --> 00:35:18,730
 do we're going to follow this empirical

1374
00:35:17,220 --> 00:35:18,730
 

1375
00:35:17,230 --> 00:35:20,589
 risk minimization idea we're going to

1376
00:35:18,720 --> 00:35:20,589
 

1377
00:35:18,730 --> 00:35:22,780
 take a bunch of examples draw them at

1378
00:35:20,579 --> 00:35:22,780
 

1379
00:35:20,589 --> 00:35:24,790
 random from our pool of unlabeled cases

1380
00:35:22,770 --> 00:35:24,790
 

1381
00:35:22,780 --> 00:35:26,950
 ask our human expert to label them and

1382
00:35:24,780 --> 00:35:26,950
 

1383
00:35:24,790 --> 00:35:28,720
 say we get these red and blue dots so

1384
00:35:26,940 --> 00:35:28,720
 

1385
00:35:26,950 --> 00:35:32,650
 now what does that mean it means that

1386
00:35:28,710 --> 00:35:32,650
 

1387
00:35:28,720 --> 00:35:34,329
 we know that the only linear classifiers

1388
00:35:32,640 --> 00:35:34,329
 

1389
00:35:32,650 --> 00:35:36,190
 that could be consistent with these

1390
00:35:34,319 --> 00:35:36,190
 

1391
00:35:34,329 --> 00:35:37,359
 labeled examples are the ones I've shown

1392
00:35:36,180 --> 00:35:37,359
 

1393
00:35:36,190 --> 00:35:39,190
 here okay

1394
00:35:37,349 --> 00:35:39,190
 

1395
00:35:37,359 --> 00:35:41,980
 all the others we can rule out because

1396
00:35:39,180 --> 00:35:41,980
 

1397
00:35:39,190 --> 00:35:43,180
 they'd start to miss classify things so

1398
00:35:41,970 --> 00:35:43,180
 

1399
00:35:41,980 --> 00:35:46,180
 what we're going to do then is when to

1400
00:35:43,170 --> 00:35:46,180
 

1401
00:35:43,180 --> 00:35:48,250
 focus now on this region where new

1402
00:35:46,170 --> 00:35:48,250
 

1403
00:35:46,180 --> 00:35:50,260
 examples might distinguish among those

1404
00:35:48,240 --> 00:35:50,260
 

1405
00:35:48,250 --> 00:35:52,420
 remaining hypotheses this is the region

1406
00:35:50,250 --> 00:35:52,420
 

1407
00:35:50,260 --> 00:35:54,280
 of disagreement we're going to now look

1408
00:35:52,410 --> 00:35:54,280
 

1409
00:35:52,420 --> 00:35:55,920
 at it the machine will go and look at a

1410
00:35:54,270 --> 00:35:55,920
 

1411
00:35:54,280 --> 00:35:58,869
 large set of unlabeled examples

1412
00:35:55,910 --> 00:35:58,869
 

1413
00:35:55,920 --> 00:35:59,980
 remaining in that data set but it's not

1414
00:35:58,859 --> 00:35:59,980
 

1415
00:35:58,869 --> 00:36:01,599
 going to label all of them it's only

1416
00:35:59,970 --> 00:36:01,599
 

1417
00:35:59,980 --> 00:36:03,069
 going to label ones in the green region

1418
00:36:01,589 --> 00:36:03,069
 

1419
00:36:01,599 --> 00:36:04,630
 and the disagreement region so it's

1420
00:36:03,059 --> 00:36:04,630
 

1421
00:36:03,069 --> 00:36:06,730
 going to say forget about all those just

1422
00:36:04,620 --> 00:36:06,730
 

1423
00:36:04,630 --> 00:36:08,800
 label those two and update your

1424
00:36:06,720 --> 00:36:08,800
 

1425
00:36:06,730 --> 00:36:10,690
 hypothesis so we improve it a little bit

1426
00:36:08,790 --> 00:36:10,690
 

1427
00:36:08,800 --> 00:36:12,819
 and all we improve it rather than having

1428
00:36:10,680 --> 00:36:12,819
 

1429
00:36:10,690 --> 00:36:14,650
 to label a bunch of examples we focused

1430
00:36:12,809 --> 00:36:14,650
 

1431
00:36:12,819 --> 00:36:16,900
 our sampling and only labeled two here

1432
00:36:14,640 --> 00:36:16,900
 

1433
00:36:14,650 --> 00:36:19,329
 and so that's the idea of disagreement

1434
00:36:16,890 --> 00:36:19,329
 

1435
00:36:16,900 --> 00:36:22,569
 based learning and basically what you

1436
00:36:19,319 --> 00:36:22,569
 

1437
00:36:19,329 --> 00:36:24,280
 can say about that is kind of this sort

1438
00:36:22,559 --> 00:36:24,280
 

1439
00:36:22,569 --> 00:36:26,079
 of general type of result and again

1440
00:36:24,270 --> 00:36:26,079
 

1441
00:36:24,280 --> 00:36:29,040
 Steve's going to go into a lot more in a

1442
00:36:26,069 --> 00:36:29,040
 

1443
00:36:26,079 --> 00:36:32,589
 moment but if we assume that our Bayes

1444
00:36:29,030 --> 00:36:32,589
 

1445
00:36:29,040 --> 00:36:34,780
 classifier the optimal classifier for

1446
00:36:32,579 --> 00:36:34,780
 

1447
00:36:32,589 --> 00:36:37,089
 our problem f star is in this VC class

1448
00:36:34,770 --> 00:36:37,089
 

1449
00:36:34,780 --> 00:36:39,339
 with dimension D and we have some nice

1450
00:36:37,079 --> 00:36:39,339
 

1451
00:36:37,089 --> 00:36:41,170
 distribution for example the label noise

1452
00:36:39,329 --> 00:36:41,170
 

1453
00:36:39,339 --> 00:36:42,970
 is bounded I don't want to get too much

1454
00:36:41,160 --> 00:36:42,970
 

1455
00:36:41,170 --> 00:36:44,500
 into these details we can talk about it

1456
00:36:42,960 --> 00:36:44,500
 

1457
00:36:42,970 --> 00:36:46,300
 later if you like but we're going to

1458
00:36:44,490 --> 00:36:46,300
 

1459
00:36:44,500 --> 00:36:47,530
 assume those two nice things and we're

1460
00:36:46,290 --> 00:36:47,530
 

1461
00:36:46,300 --> 00:36:49,780
 going to let epsilon denote the

1462
00:36:47,520 --> 00:36:49,780
 

1463
00:36:47,530 --> 00:36:52,720
 difference between the error rate of F

1464
00:36:49,770 --> 00:36:52,720
 

1465
00:36:49,780 --> 00:36:55,690
 hat and the error rate of the best Bayes

1466
00:36:52,710 --> 00:36:55,690
 

1467
00:36:52,720 --> 00:36:57,880
 classifier F star so if we were in this

1468
00:36:55,680 --> 00:36:57,880
 

1469
00:36:55,690 --> 00:37:00,579
 kind of situation and we use the normal

1470
00:36:57,870 --> 00:37:00,579
 

1471
00:36:57,880 --> 00:37:02,829
 passive learning machine pipeline then

1472
00:37:00,569 --> 00:37:02,829
 

1473
00:37:00,579 --> 00:37:05,410
 our error after n samples would be

1474
00:37:02,819 --> 00:37:05,410
 

1475
00:37:02,829 --> 00:37:07,780
 roughly like D over at the dimension of

1476
00:37:05,400 --> 00:37:07,780
 

1477
00:37:05,410 --> 00:37:09,520
 our problem the VC dimension or the

1478
00:37:07,770 --> 00:37:09,520
 

1479
00:37:07,780 --> 00:37:11,290
 dimension of the feature space just in a

1480
00:37:09,510 --> 00:37:11,290
 

1481
00:37:09,520 --> 00:37:13,210
 simple linear classifier problem and

1482
00:37:11,280 --> 00:37:13,210
 

1483
00:37:11,290 --> 00:37:15,880
 that's what we call the parametric rate

1484
00:37:13,200 --> 00:37:15,880
 

1485
00:37:13,210 --> 00:37:17,470
 D over N if we're active though the

1486
00:37:15,870 --> 00:37:17,470
 

1487
00:37:15,880 --> 00:37:19,359
 error goes down much faster it goes down

1488
00:37:17,460 --> 00:37:19,359
 

1489
00:37:17,470 --> 00:37:21,609
 exponentially fast just like it did in

1490
00:37:19,349 --> 00:37:21,609
 

1491
00:37:19,359 --> 00:37:23,890
 that binary search example it goes down

1492
00:37:21,599 --> 00:37:23,890
 

1493
00:37:21,609 --> 00:37:26,470
 like e to the minus some constant C and

1494
00:37:23,880 --> 00:37:26,470
 

1495
00:37:23,890 --> 00:37:28,780
 over D and so we get this exponential

1496
00:37:26,460 --> 00:37:28,780
 

1497
00:37:26,470 --> 00:37:31,630
 speed-up and how quickly we can learn a

1498
00:37:28,770 --> 00:37:31,630
 

1499
00:37:28,780 --> 00:37:36,040
 good classifier or a good prediction

1500
00:37:31,620 --> 00:37:36,040
 

1501
00:37:31,630 --> 00:37:38,020
 rule in this case okay so

1502
00:37:36,030 --> 00:37:38,020
 

1503
00:37:36,040 --> 00:37:40,420
 that's what I wanted to say in this

1504
00:37:38,010 --> 00:37:40,420
 

1505
00:37:38,020 --> 00:37:42,880
 first part is as a means of introduction

1506
00:37:40,410 --> 00:37:42,880
 

1507
00:37:40,420 --> 00:37:45,070
 this is just again an outline of how

1508
00:37:42,870 --> 00:37:45,070
 

1509
00:37:42,880 --> 00:37:47,740
 we're going to proceed Steve's going to

1510
00:37:45,060 --> 00:37:47,740
 

1511
00:37:45,070 --> 00:37:49,390
 take the next two half hours so about 1

1512
00:37:47,730 --> 00:37:49,390
 

1513
00:37:47,740 --> 00:37:51,220
 hour to talk about theory of active

1514
00:37:49,380 --> 00:37:51,220
 

1515
00:37:49,390 --> 00:37:53,200
 learning some advanced topics after that

1516
00:37:51,210 --> 00:37:53,200
 

1517
00:37:51,220 --> 00:37:55,150
 we'll probably take a little bit of a

1518
00:37:53,190 --> 00:37:55,150
 

1519
00:37:53,200 --> 00:37:56,470
 break between part 2 and 3 in case

1520
00:37:55,140 --> 00:37:56,470
 

1521
00:37:55,150 --> 00:37:59,350
 people who just need to stretch their

1522
00:37:56,460 --> 00:37:59,350
 

1523
00:37:56,470 --> 00:38:01,450
 legs and then in part 4 I'll I'll talk

1524
00:37:59,340 --> 00:38:01,450
 

1525
00:37:59,350 --> 00:38:03,520
 about some ongoing work in the area of

1526
00:38:01,440 --> 00:38:03,520
 

1527
00:38:01,450 --> 00:38:06,910
 nonparametric active learning and again

1528
00:38:03,510 --> 00:38:06,910
 

1529
00:38:03,520 --> 00:38:08,410
 the URL for the slides is there at the

1530
00:38:06,900 --> 00:38:08,410
 

1531
00:38:06,910 --> 00:38:10,300
 bottom you can follow along if you like

1532
00:38:08,400 --> 00:38:10,300
 

1533
00:38:08,410 --> 00:38:11,080
 or download the slides at your

1534
00:38:10,290 --> 00:38:11,080
 

1535
00:38:10,300 --> 00:38:14,800
 convenience

1536
00:38:11,070 --> 00:38:14,800
 

1537
00:38:11,080 --> 00:38:17,260
 so with that introduction I'll stop here

1538
00:38:14,790 --> 00:38:17,260
 

1539
00:38:14,800 --> 00:38:19,120
 and I'll take questions if there are any

1540
00:38:17,250 --> 00:38:19,120
 

1541
00:38:17,260 --> 00:38:22,680
 and I'll ask Steve to come up and he can

1542
00:38:19,110 --> 00:38:22,680
 

1543
00:38:19,120 --> 00:38:22,680
 start setting up while we're doing that

1544
00:38:28,359 --> 00:38:28,359
 

1545
00:38:28,369 --> 00:38:33,569
 thanks for the presentation it was

1546
00:38:30,559 --> 00:38:33,569
 

1547
00:38:30,569 --> 00:38:36,270
 really really instructed my question was

1548
00:38:33,559 --> 00:38:36,270
 

1549
00:38:33,569 --> 00:38:38,730
 about what if you have neural nets that

1550
00:38:36,260 --> 00:38:38,730
 

1551
00:38:36,270 --> 00:38:40,920
 are very powerful so your vc-dimension

1552
00:38:38,720 --> 00:38:40,920
 

1553
00:38:38,730 --> 00:38:44,609
 allottee becomes very high with the same

1554
00:38:40,910 --> 00:38:44,609
 

1555
00:38:40,920 --> 00:38:47,790
 type of arguments apply so if you have a

1556
00:38:44,599 --> 00:38:47,790
 

1557
00:38:44,609 --> 00:38:49,710
 super high v c-dimension so in principle

1558
00:38:47,780 --> 00:38:49,710
 

1559
00:38:47,790 --> 00:38:51,210
 so as you're in a finite VC class the

1560
00:38:49,700 --> 00:38:51,210
 

1561
00:38:49,710 --> 00:38:52,920
 sorts of things I talked about would

1562
00:38:51,200 --> 00:38:52,920
 

1563
00:38:51,210 --> 00:38:55,490
 still apply but of course they might

1564
00:38:52,910 --> 00:38:55,490
 

1565
00:38:52,920 --> 00:38:57,809
 become practically a little bit

1566
00:38:55,480 --> 00:38:57,809
 

1567
00:38:55,490 --> 00:39:00,359
 meaningless because again you're going

1568
00:38:57,799 --> 00:39:00,359
 

1569
00:38:57,809 --> 00:39:02,400
 to need at least as may labeled examples

1570
00:39:00,349 --> 00:39:02,400
 

1571
00:39:00,359 --> 00:39:04,859
 as the VC dimension so if the VC

1572
00:39:02,390 --> 00:39:04,859
 

1573
00:39:02,400 --> 00:39:06,839
 dimension is extremely high either you

1574
00:39:04,849 --> 00:39:06,839
 

1575
00:39:04,859 --> 00:39:09,000
 have a ton of labeled examples either

1576
00:39:06,829 --> 00:39:09,000
 

1577
00:39:06,839 --> 00:39:13,040
 passively or actively or you don't and

1578
00:39:08,990 --> 00:39:13,040
 

1579
00:39:09,000 --> 00:39:16,290
 so that is a kind of a fundamental issue

1580
00:39:13,030 --> 00:39:16,290
 

1581
00:39:13,040 --> 00:39:18,150
 because large VC classes are just so

1582
00:39:16,280 --> 00:39:18,150
 

1583
00:39:16,290 --> 00:39:19,859
 expressive right they could kind of

1584
00:39:18,140 --> 00:39:19,859
 

1585
00:39:18,150 --> 00:39:21,510
 represent many many complicated

1586
00:39:19,849 --> 00:39:21,510
 

1587
00:39:19,859 --> 00:39:24,150
 functions of course you would expect it

1588
00:39:21,500 --> 00:39:24,150
 

1589
00:39:21,510 --> 00:39:27,240
 to take a lot more effort to learn such

1590
00:39:24,140 --> 00:39:27,240
 

1591
00:39:24,150 --> 00:39:30,359
 functions I'm going to in the last part

1592
00:39:27,230 --> 00:39:30,359
 

1593
00:39:27,240 --> 00:39:32,490
 of the talk cover nonparametric and also

1594
00:39:30,349 --> 00:39:32,490
 

1595
00:39:30,359 --> 00:39:34,650
 sort of the over parametrized say where

1596
00:39:32,480 --> 00:39:34,650
 

1597
00:39:32,490 --> 00:39:36,780
 we're always talking about basically

1598
00:39:34,640 --> 00:39:36,780
 

1599
00:39:34,650 --> 00:39:38,819
 more dimensions than we have training

1600
00:39:36,770 --> 00:39:38,819
 

1601
00:39:36,780 --> 00:39:40,559
 data there we kind of have to depart

1602
00:39:38,809 --> 00:39:40,559
 

1603
00:39:38,819 --> 00:39:42,599
 from the usual statistical learning

1604
00:39:40,549 --> 00:39:42,599
 

1605
00:39:40,559 --> 00:39:44,819
 theory approach and VC theory and so

1606
00:39:42,589 --> 00:39:44,819
 

1607
00:39:42,599 --> 00:39:46,890
 forth and and pursue other types of

1608
00:39:44,809 --> 00:39:46,890
 

1609
00:39:44,819 --> 00:39:49,680
 approaches and so I'll be touching at

1610
00:39:46,880 --> 00:39:49,680
 

1611
00:39:46,890 --> 00:39:51,480
 the end but that's very kind of nascent

1612
00:39:49,670 --> 00:39:51,480
 

1613
00:39:49,680 --> 00:39:53,160
 work that's it's just starting now so

1614
00:39:51,470 --> 00:39:53,160
 

1615
00:39:51,480 --> 00:39:54,869
 it's a great question but we'll be

1616
00:39:53,150 --> 00:39:54,869
 

1617
00:39:53,160 --> 00:39:56,609
 covering some of that and hopefully you

1618
00:39:54,859 --> 00:39:56,609
 

1619
00:39:54,869 --> 00:39:59,869
 have more questions along those lines

1620
00:39:56,599 --> 00:39:59,869
 

1621
00:39:56,609 --> 00:39:59,869
 just ask me again

1622
00:40:56,000 --> 00:40:56,000
 

1623
00:40:56,010 --> 00:41:03,600
 okay great Rob I gave of course this

1624
00:41:00,859 --> 00:41:03,600
 

1625
00:41:00,869 --> 00:41:06,390
 wonderful introduction to active

1626
00:41:03,590 --> 00:41:06,390
 

1627
00:41:03,600 --> 00:41:09,000
 learning as a topic and the theory of

1628
00:41:06,380 --> 00:41:09,000
 

1629
00:41:06,390 --> 00:41:13,290
 active learning and sort of the

1630
00:41:08,990 --> 00:41:13,290
 

1631
00:41:09,000 --> 00:41:14,760
 beginnings of why in special cases you

1632
00:41:13,280 --> 00:41:14,760
 

1633
00:41:13,290 --> 00:41:16,710
 can have a theory of active learning a

1634
00:41:14,750 --> 00:41:16,710
 

1635
00:41:14,760 --> 00:41:20,100
 theoretical theoretically sound approach

1636
00:41:16,700 --> 00:41:20,100
 

1637
00:41:16,710 --> 00:41:21,869
 to active learning and also some ideas

1638
00:41:20,090 --> 00:41:21,869
 

1639
00:41:20,100 --> 00:41:24,720
 about why maybe there was a general

1640
00:41:21,859 --> 00:41:24,720
 

1641
00:41:21,869 --> 00:41:25,770
 theory of active learning as well what

1642
00:41:24,710 --> 00:41:25,770
 

1643
00:41:24,720 --> 00:41:29,070
 I'm going to be talking about in the

1644
00:41:25,760 --> 00:41:29,070
 

1645
00:41:25,770 --> 00:41:32,150
 second part here is indeed a general

1646
00:41:29,060 --> 00:41:32,150
 

1647
00:41:29,070 --> 00:41:34,730
 theory of active learning in more detail

1648
00:41:32,140 --> 00:41:34,730
 

1649
00:41:32,150 --> 00:41:38,070
 general descriptions of algorithms and

1650
00:41:34,720 --> 00:41:38,070
 

1651
00:41:34,730 --> 00:41:40,410
 descriptions of the number of labels

1652
00:41:38,060 --> 00:41:40,410
 

1653
00:41:38,070 --> 00:41:44,670
 that are sufficient to achieve

1654
00:41:40,400 --> 00:41:44,670
 

1655
00:41:40,410 --> 00:41:48,330
 guarantees on the error rate expressed

1656
00:41:44,660 --> 00:41:48,330
 

1657
00:41:44,670 --> 00:41:49,859
 in terms of complexity measures I'll say

1658
00:41:48,320 --> 00:41:49,859
 

1659
00:41:48,330 --> 00:41:53,820
 that the theory I'm going to be

1660
00:41:49,849 --> 00:41:53,820
 

1661
00:41:49,859 --> 00:41:55,920
 describing here in referring to the

1662
00:41:53,810 --> 00:41:55,920
 

1663
00:41:53,820 --> 00:41:58,050
 question that was asked I'm gonna be

1664
00:41:55,910 --> 00:41:58,050
 

1665
00:41:55,920 --> 00:42:00,720
 expressing everything in terms of the VC

1666
00:41:58,040 --> 00:42:00,720
 

1667
00:41:58,050 --> 00:42:02,700
 dimension that's mainly just for

1668
00:42:00,710 --> 00:42:02,700
 

1669
00:42:00,720 --> 00:42:05,580
 simplicity many of these results would

1670
00:42:02,690 --> 00:42:05,580
 

1671
00:42:02,700 --> 00:42:07,500
 also generalize to to things like

1672
00:42:05,570 --> 00:42:07,500
 

1673
00:42:05,580 --> 00:42:08,850
 covering number eight you could express

1674
00:42:07,490 --> 00:42:08,850
 

1675
00:42:07,500 --> 00:42:11,490
 them in terms of things like covering

1676
00:42:08,840 --> 00:42:11,490
 

1677
00:42:08,850 --> 00:42:12,990
 numbers and even localized covering

1678
00:42:11,480 --> 00:42:12,990
 

1679
00:42:11,490 --> 00:42:17,220
 numbers and that would actually also

1680
00:42:12,980 --> 00:42:17,220
 

1681
00:42:12,990 --> 00:42:19,710
 address many things that aren't covered

1682
00:42:17,210 --> 00:42:19,710
 

1683
00:42:17,220 --> 00:42:21,810
 by bounds expressed in terms of VC

1684
00:42:19,700 --> 00:42:21,810
 

1685
00:42:19,710 --> 00:42:25,050
 dimension but first implicit II will

1686
00:42:21,800 --> 00:42:25,050
 

1687
00:42:21,810 --> 00:42:28,170
 stick with VC dimension throughout okay

1688
00:42:25,040 --> 00:42:28,170
 

1689
00:42:25,050 --> 00:42:30,780
 and I'll mention that this segment the

1690
00:42:28,160 --> 00:42:30,780
 

1691
00:42:28,170 --> 00:42:33,030
 the second part of the tutorial is

1692
00:42:30,770 --> 00:42:33,030
 

1693
00:42:30,780 --> 00:42:35,280
 covered very thoroughly in monograph

1694
00:42:33,020 --> 00:42:35,280
 

1695
00:42:33,030 --> 00:42:37,050
 that I wrote a few years back a theory

1696
00:42:35,270 --> 00:42:37,050
 

1697
00:42:35,280 --> 00:42:40,040
 of disagreement based active learning it

1698
00:42:37,040 --> 00:42:40,040
 

1699
00:42:37,050 --> 00:42:44,070
 is freely downloadable off my website

1700
00:42:40,030 --> 00:42:44,070
 

1701
00:42:40,040 --> 00:42:45,630
 and or if you want a bound version I

1702
00:42:44,060 --> 00:42:45,630
 

1703
00:42:44,070 --> 00:42:48,230
 think you can purchase it off the

1704
00:42:45,620 --> 00:42:48,230
 

1705
00:42:45,630 --> 00:42:48,230
 publishers website

1706
00:42:48,730 --> 00:42:48,730
 

1707
00:42:48,740 --> 00:42:54,680
 okay so Rob introduced Hosting's

1708
00:42:52,690 --> 00:42:54,680
 

1709
00:42:52,700 --> 00:42:58,010
 inequality and a uniform version of

1710
00:42:54,670 --> 00:42:58,010
 

1711
00:42:54,680 --> 00:43:00,859
 Hosting's inequality and for the purpose

1712
00:42:58,000 --> 00:43:00,859
 

1713
00:42:58,010 --> 00:43:02,510
 of stating sort of the tightest results

1714
00:43:00,849 --> 00:43:02,510
 

1715
00:43:00,859 --> 00:43:05,510
 that we can get in disagreement based

1716
00:43:02,500 --> 00:43:05,510
 

1717
00:43:02,510 --> 00:43:07,040
 active learning I'm gonna need something

1718
00:43:05,500 --> 00:43:07,040
 

1719
00:43:05,510 --> 00:43:08,930
 just slightly tighter than that so this

1720
00:43:07,030 --> 00:43:08,930
 

1721
00:43:07,040 --> 00:43:12,440
 is what's sometimes called Bernstein's

1722
00:43:08,920 --> 00:43:12,440
 

1723
00:43:08,930 --> 00:43:14,630
 inequality basically it's just like hook

1724
00:43:12,430 --> 00:43:14,630
 

1725
00:43:12,440 --> 00:43:16,730
 tangs inequality so if you have any two

1726
00:43:14,620 --> 00:43:16,730
 

1727
00:43:14,630 --> 00:43:19,280
 classifiers the difference of their

1728
00:43:16,720 --> 00:43:19,280
 

1729
00:43:16,730 --> 00:43:22,460
 risks is bounded by the difference of

1730
00:43:19,270 --> 00:43:22,460
 

1731
00:43:19,280 --> 00:43:24,109
 their empirical risks plus a term that's

1732
00:43:22,450 --> 00:43:24,109
 

1733
00:43:22,460 --> 00:43:26,960
 like looks like the hoeffding bound

1734
00:43:24,099 --> 00:43:26,960
 

1735
00:43:24,109 --> 00:43:28,369
 right the square root of a log 1 over

1736
00:43:26,950 --> 00:43:28,369
 

1737
00:43:26,960 --> 00:43:30,890
 Delta over m except we're now

1738
00:43:28,359 --> 00:43:30,890
 

1739
00:43:28,369 --> 00:43:32,510
 multiplying inside by the fraction of

1740
00:43:30,880 --> 00:43:32,510
 

1741
00:43:30,890 --> 00:43:34,369
 the data on which the two classifiers

1742
00:43:32,500 --> 00:43:34,369
 

1743
00:43:32,510 --> 00:43:35,869
 disagree with each other and we're just

1744
00:43:34,359 --> 00:43:35,869
 

1745
00:43:34,369 --> 00:43:37,730
 talking about binary classifiers so this

1746
00:43:35,859 --> 00:43:37,730
 

1747
00:43:35,869 --> 00:43:39,470
 is saying one of them is 0 or sorry

1748
00:43:37,720 --> 00:43:39,470
 

1749
00:43:37,730 --> 00:43:46,280
 let's say plus minus one so one of them

1750
00:43:39,460 --> 00:43:46,280
 

1751
00:43:39,470 --> 00:43:47,960
 is plus one of them is - ok so that's a

1752
00:43:46,270 --> 00:43:47,960
 

1753
00:43:46,280 --> 00:43:49,190
 Bernstein's inequality and just like

1754
00:43:47,950 --> 00:43:49,190
 

1755
00:43:47,960 --> 00:43:51,380
 with Haughton you can get a uniform

1756
00:43:49,180 --> 00:43:51,380
 

1757
00:43:49,190 --> 00:43:55,160
 version of this and here we're saying

1758
00:43:51,370 --> 00:43:55,160
 

1759
00:43:51,380 --> 00:43:57,050
 for a given hypothesis class the with

1760
00:43:55,150 --> 00:43:57,050
 

1761
00:43:55,160 --> 00:43:58,609
 high probability for any two classifiers

1762
00:43:57,040 --> 00:43:58,609
 

1763
00:43:57,050 --> 00:44:00,970
 in the hypothesis class the same

1764
00:43:58,599 --> 00:44:00,970
 

1765
00:43:58,609 --> 00:44:03,320
 inequality holds except now we've

1766
00:44:00,960 --> 00:44:03,320
 

1767
00:44:00,970 --> 00:44:06,470
 multiplied by a VC dimension and we've

1768
00:44:03,310 --> 00:44:06,470
 

1769
00:44:03,320 --> 00:44:07,609
 changed the log factors a little bit ok

1770
00:44:06,460 --> 00:44:07,609
 

1771
00:44:06,470 --> 00:44:08,960
 and we're going to be expressing

1772
00:44:07,599 --> 00:44:08,960
 

1773
00:44:07,609 --> 00:44:13,480
 algorithms in terms of this we're gonna

1774
00:44:08,950 --> 00:44:13,480
 

1775
00:44:08,960 --> 00:44:17,540
 be using this to design algorithms and

1776
00:44:13,470 --> 00:44:17,540
 

1777
00:44:13,480 --> 00:44:20,180
 right so just so I can sort of reduce

1778
00:44:17,530 --> 00:44:20,180
 

1779
00:44:17,540 --> 00:44:24,020
 clutter on the slides I'm actually gonna

1780
00:44:20,170 --> 00:44:24,020
 

1781
00:44:20,180 --> 00:44:27,290
 use a sort of a simplified version of it

1782
00:44:24,010 --> 00:44:27,290
 

1783
00:44:24,020 --> 00:44:29,000
 which is technically not accurate but

1784
00:44:27,280 --> 00:44:29,000
 

1785
00:44:27,290 --> 00:44:31,400
 basically I'm shaving off the constants

1786
00:44:28,990 --> 00:44:31,400
 

1787
00:44:29,000 --> 00:44:33,530
 and log factors and lower order terms

1788
00:44:31,390 --> 00:44:33,530
 

1789
00:44:31,400 --> 00:44:36,080
 just to reduce the clutter it's sort of

1790
00:44:33,520 --> 00:44:36,080
 

1791
00:44:33,530 --> 00:44:38,330
 a conceptualization of the important

1792
00:44:36,070 --> 00:44:38,330
 

1793
00:44:36,080 --> 00:44:40,550
 parts of the uniform Bernstein

1794
00:44:38,320 --> 00:44:40,550
 

1795
00:44:38,330 --> 00:44:42,859
 inequality and so this down here is what

1796
00:44:40,540 --> 00:44:42,859
 

1797
00:44:40,550 --> 00:44:44,060
 I'm actually going to be using but just

1798
00:44:42,849 --> 00:44:44,060
 

1799
00:44:42,859 --> 00:44:45,650
 keep in mind that if you want to make

1800
00:44:44,050 --> 00:44:45,650
 

1801
00:44:44,060 --> 00:44:47,810
 these things formal you really do have

1802
00:44:45,640 --> 00:44:47,810
 

1803
00:44:45,650 --> 00:44:52,210
 to take into account these the constants

1804
00:44:47,800 --> 00:44:52,210
 

1805
00:44:47,810 --> 00:44:52,210
 and the logs and lower order terms

1806
00:44:52,650 --> 00:44:52,650
 

1807
00:44:52,660 --> 00:45:00,200
 ok so with with all of that background

1808
00:44:56,950 --> 00:45:00,200
 

1809
00:44:56,960 --> 00:45:01,250
 let's now dive into the general theory

1810
00:45:00,190 --> 00:45:01,250
 

1811
00:45:00,200 --> 00:45:05,720
 of disagreement post

1812
00:45:01,240 --> 00:45:05,720
 

1813
00:45:01,250 --> 00:45:09,560
 learning so first I'll just maybe rehash

1814
00:45:05,710 --> 00:45:09,560
 

1815
00:45:05,720 --> 00:45:12,170
 in a formal notation what Rob was

1816
00:45:09,550 --> 00:45:12,170
 

1817
00:45:09,560 --> 00:45:13,760
 already defined the the region of

1818
00:45:12,160 --> 00:45:13,760
 

1819
00:45:12,170 --> 00:45:16,160
 disagreement of a set of classifiers H

1820
00:45:13,750 --> 00:45:16,160
 

1821
00:45:13,760 --> 00:45:19,280
 it's just the set of points such that

1822
00:45:16,150 --> 00:45:19,280
 

1823
00:45:16,160 --> 00:45:22,490
 there exists two classifiers in H that

1824
00:45:19,270 --> 00:45:22,490
 

1825
00:45:19,280 --> 00:45:25,580
 disagree on the label of that point it's

1826
00:45:22,480 --> 00:45:25,580
 

1827
00:45:22,490 --> 00:45:28,820
 a very simple notion it's a region of

1828
00:45:25,570 --> 00:45:28,820
 

1829
00:45:25,580 --> 00:45:31,520
 the instance space and with that

1830
00:45:28,810 --> 00:45:31,520
 

1831
00:45:28,820 --> 00:45:34,340
 definition we have here a definition of

1832
00:45:31,510 --> 00:45:34,340
 

1833
00:45:31,520 --> 00:45:36,440
 the classic disagreement based active

1834
00:45:34,330 --> 00:45:36,440
 

1835
00:45:34,340 --> 00:45:39,410
 learning algorithm that the a squared or

1836
00:45:36,430 --> 00:45:39,410
 

1837
00:45:36,440 --> 00:45:42,110
 agnostic active algorithm from balcan

1838
00:45:39,400 --> 00:45:42,110
 

1839
00:45:39,410 --> 00:45:46,100
 bagels Emory and Langford

1840
00:45:42,100 --> 00:45:46,100
 

1841
00:45:42,110 --> 00:45:50,000
 so this is a sort of a slimmed down

1842
00:45:46,090 --> 00:45:50,000
 

1843
00:45:46,100 --> 00:45:53,470
 version of the algorithm so basically

1844
00:45:49,990 --> 00:45:53,470
 

1845
00:45:50,000 --> 00:45:56,360
 what you do on each round you get some

1846
00:45:53,460 --> 00:45:56,360
 

1847
00:45:53,470 --> 00:45:58,070
 number of unlabeled samples as say on

1848
00:45:56,350 --> 00:45:58,070
 

1849
00:45:56,360 --> 00:46:02,270
 round t you take two to the T on labeled

1850
00:45:58,060 --> 00:46:02,270
 

1851
00:45:58,070 --> 00:46:03,950
 samples and we'll go ahead and just I'll

1852
00:46:02,260 --> 00:46:03,950
 

1853
00:46:02,270 --> 00:46:06,530
 just do it in pictures at the same time

1854
00:46:03,940 --> 00:46:06,530
 

1855
00:46:03,950 --> 00:46:07,820
 so we'll go ahead and query everything

1856
00:46:06,520 --> 00:46:07,820
 

1857
00:46:06,530 --> 00:46:09,500
 that's in the region a disagreement was

1858
00:46:07,810 --> 00:46:09,500
 

1859
00:46:07,820 --> 00:46:14,210
 maybe let's say initially everything is

1860
00:46:09,490 --> 00:46:14,210
 

1861
00:46:09,500 --> 00:46:15,590
 in the region and then we'll minimize

1862
00:46:14,200 --> 00:46:15,590
 

1863
00:46:14,210 --> 00:46:17,750
 the number of mistakes we'll find some

1864
00:46:15,580 --> 00:46:17,750
 

1865
00:46:15,590 --> 00:46:20,990
 classifier an H that minimizes the

1866
00:46:17,740 --> 00:46:20,990
 

1867
00:46:17,750 --> 00:46:23,540
 number mistakes on that data and then

1868
00:46:20,980 --> 00:46:23,540
 

1869
00:46:20,990 --> 00:46:26,330
 we'll actually look at all the other

1870
00:46:23,530 --> 00:46:26,330
 

1871
00:46:23,540 --> 00:46:29,300
 functions in H let's say this one and if

1872
00:46:26,320 --> 00:46:29,300
 

1873
00:46:26,330 --> 00:46:32,000
 they make a lot more mistakes than the F

1874
00:46:29,290 --> 00:46:32,000
 

1875
00:46:29,300 --> 00:46:33,920
 hat minimizer then we actually just

1876
00:46:31,990 --> 00:46:33,920
 

1877
00:46:32,000 --> 00:46:36,560
 remove them from H we throw them away so

1878
00:46:33,910 --> 00:46:36,560
 

1879
00:46:33,920 --> 00:46:38,390
 I can maybe this other curve that I've

1880
00:46:36,550 --> 00:46:38,390
 

1881
00:46:36,560 --> 00:46:39,710
 drawn makes a lot more mistakes so I go

1882
00:46:38,380 --> 00:46:39,710
 

1883
00:46:38,390 --> 00:46:42,410
 ahead and show it away maybe this one

1884
00:46:39,700 --> 00:46:42,410
 

1885
00:46:39,710 --> 00:46:44,390
 also go ahead and throw it away so

1886
00:46:42,400 --> 00:46:44,390
 

1887
00:46:42,410 --> 00:46:47,420
 what's left is a smaller set we sort of

1888
00:46:44,380 --> 00:46:47,420
 

1889
00:46:44,390 --> 00:46:50,720
 pruned the set H down to a smaller set

1890
00:46:47,410 --> 00:46:50,720
 

1891
00:46:47,420 --> 00:46:52,150
 of candidate hypotheses and the region

1892
00:46:50,710 --> 00:46:52,150
 

1893
00:46:50,720 --> 00:46:54,800
 of disagreement of that set is

1894
00:46:52,140 --> 00:46:54,800
 

1895
00:46:52,150 --> 00:46:59,030
 potentially smaller and then we iterate

1896
00:46:54,790 --> 00:46:59,030
 

1897
00:46:54,800 --> 00:47:01,010
 we then draw more on labeled samples we

1898
00:46:59,020 --> 00:47:01,010
 

1899
00:46:59,030 --> 00:47:03,920
 just query the ones that are in the

1900
00:47:01,000 --> 00:47:03,920
 

1901
00:47:01,010 --> 00:47:05,630
 region of disagreement again find some F

1902
00:47:03,910 --> 00:47:05,630
 

1903
00:47:03,920 --> 00:47:08,750
 hat minimizer of the number of mistakes

1904
00:47:05,620 --> 00:47:08,750
 

1905
00:47:05,630 --> 00:47:10,090
 there if there's some other classifiers

1906
00:47:08,740 --> 00:47:10,090
 

1907
00:47:08,750 --> 00:47:11,290
 in H

1908
00:47:10,080 --> 00:47:11,290
 

1909
00:47:10,090 --> 00:47:13,450
 that are that make a lot more mistakes

1910
00:47:11,280 --> 00:47:13,450
 

1911
00:47:11,290 --> 00:47:16,810
 we throw them away

1912
00:47:13,440 --> 00:47:16,810
 

1913
00:47:13,450 --> 00:47:17,950
 and so H is getting smaller and the

1914
00:47:16,800 --> 00:47:17,950
 

1915
00:47:16,810 --> 00:47:22,090
 region of disagreement is getting

1916
00:47:17,940 --> 00:47:22,090
 

1917
00:47:17,950 --> 00:47:23,470
 smaller hopefully and and we do this for

1918
00:47:22,080 --> 00:47:23,470
 

1919
00:47:22,090 --> 00:47:28,300
 a number of rounds until some stopping

1920
00:47:23,460 --> 00:47:28,300
 

1921
00:47:23,470 --> 00:47:30,160
 criterion okay so what's the point of

1922
00:47:28,290 --> 00:47:30,160
 

1923
00:47:28,300 --> 00:47:35,260
 all this why are we doing like this step

1924
00:47:30,150 --> 00:47:35,260
 

1925
00:47:30,160 --> 00:47:36,940
 in particular is you know the sort of

1926
00:47:35,250 --> 00:47:36,940
 

1927
00:47:35,260 --> 00:47:39,880
 looks like Bernstein's inequality so so

1928
00:47:36,930 --> 00:47:39,880
 

1929
00:47:36,940 --> 00:47:42,120
 why are we doing that so the point of it

1930
00:47:39,870 --> 00:47:42,120
 

1931
00:47:39,880 --> 00:47:45,820
 is that if at the beginning of a round

1932
00:47:42,110 --> 00:47:45,820
 

1933
00:47:42,120 --> 00:47:47,530
 the best let's say F star is the best

1934
00:47:45,810 --> 00:47:47,530
 

1935
00:47:45,820 --> 00:47:49,510
 classifier in the hypothesis class if

1936
00:47:47,520 --> 00:47:49,510
 

1937
00:47:47,530 --> 00:47:51,100
 it's still in the hypothesis class at

1938
00:47:49,500 --> 00:47:51,100
 

1939
00:47:49,510 --> 00:47:53,020
 the beginning or in the in the version

1940
00:47:51,090 --> 00:47:53,020
 

1941
00:47:51,100 --> 00:47:55,450
 space or the prune down thoughts this

1942
00:47:53,010 --> 00:47:55,450
 

1943
00:47:53,020 --> 00:47:57,820
 class then because all the classifiers

1944
00:47:55,440 --> 00:47:57,820
 

1945
00:47:55,450 --> 00:48:00,310
 in H at the beginning of a round agree

1946
00:47:57,810 --> 00:48:00,310
 

1947
00:47:57,820 --> 00:48:02,560
 outside of the region of disagreement if

1948
00:48:00,300 --> 00:48:02,560
 

1949
00:48:00,310 --> 00:48:04,500
 f star is the best classifier overall

1950
00:48:02,550 --> 00:48:04,500
 

1951
00:48:02,560 --> 00:48:06,130
 then it's still the best classifier

1952
00:48:04,490 --> 00:48:06,130
 

1953
00:48:04,500 --> 00:48:08,260
 conditioned on the region of

1954
00:48:06,120 --> 00:48:08,260
 

1955
00:48:06,130 --> 00:48:12,160
 disagreement so it still has the

1956
00:48:08,250 --> 00:48:12,160
 

1957
00:48:08,260 --> 00:48:13,240
 smallest risk under the conditional

1958
00:48:12,150 --> 00:48:13,240
 

1959
00:48:12,160 --> 00:48:15,700
 distribution given the region of

1960
00:48:13,230 --> 00:48:15,700
 

1961
00:48:13,240 --> 00:48:17,290
 disagreement so if we sort of plug that

1962
00:48:15,690 --> 00:48:17,290
 

1963
00:48:15,700 --> 00:48:19,150
 into the reasoning from Bernstein's

1964
00:48:17,280 --> 00:48:19,150
 

1965
00:48:17,290 --> 00:48:21,190
 inequality what you're getting here is

1966
00:48:19,140 --> 00:48:21,190
 

1967
00:48:19,150 --> 00:48:25,830
 that the empirical risk the excess

1968
00:48:21,180 --> 00:48:25,830
 

1969
00:48:21,190 --> 00:48:29,470
 empirical risk of F star is bounded by

1970
00:48:25,820 --> 00:48:29,470
 

1971
00:48:25,830 --> 00:48:30,970
 you know it's it's excess risk under the

1972
00:48:29,460 --> 00:48:30,970
 

1973
00:48:29,470 --> 00:48:32,590
 conditional distribution given the

1974
00:48:30,960 --> 00:48:32,590
 

1975
00:48:30,970 --> 00:48:35,860
 region of disagreement because that's

1976
00:48:32,580 --> 00:48:35,860
 

1977
00:48:32,590 --> 00:48:39,130
 what Q is sampled from Plus this sort of

1978
00:48:35,850 --> 00:48:39,130
 

1979
00:48:35,860 --> 00:48:41,710
 Bernstein looking term this term from

1980
00:48:39,120 --> 00:48:41,710
 

1981
00:48:39,130 --> 00:48:43,300
 Bernstein's inequality and then as I

1982
00:48:41,700 --> 00:48:43,300
 

1983
00:48:41,710 --> 00:48:45,010
 just said F star is still the minimizer

1984
00:48:43,290 --> 00:48:45,010
 

1985
00:48:43,300 --> 00:48:47,500
 under the conditional given the region

1986
00:48:45,000 --> 00:48:47,500
 

1987
00:48:45,010 --> 00:48:51,190
 of disagreement so that that first

1988
00:48:47,490 --> 00:48:51,190
 

1989
00:48:47,500 --> 00:48:52,660
 difference is at most zero and so what

1990
00:48:51,180 --> 00:48:52,660
 

1991
00:48:51,190 --> 00:48:54,970
 you get to conclude is that F star is

1992
00:48:52,650 --> 00:48:54,970
 

1993
00:48:52,660 --> 00:48:56,950
 never removed so f star if f star is in

1994
00:48:54,960 --> 00:48:56,950
 

1995
00:48:54,970 --> 00:48:58,390
 there at the beginning of the round then

1996
00:48:56,940 --> 00:48:58,390
 

1997
00:48:56,950 --> 00:49:00,190
 given that Bernstein's inequality holds

1998
00:48:58,380 --> 00:49:00,190
 

1999
00:48:58,390 --> 00:49:02,350
 it's still going to be in there at the

2000
00:49:00,180 --> 00:49:02,350
 

2001
00:49:00,190 --> 00:49:05,050
 end of the round so so this algorithm is

2002
00:49:02,340 --> 00:49:05,050
 

2003
00:49:02,350 --> 00:49:07,270
 sort of preserving this invariant that

2004
00:49:05,040 --> 00:49:07,270
 

2005
00:49:05,050 --> 00:49:09,880
 you never remove the best classifier

2006
00:49:07,260 --> 00:49:09,880
 

2007
00:49:07,270 --> 00:49:11,680
 from the hypothesis class okay so it's

2008
00:49:09,870 --> 00:49:11,680
 

2009
00:49:09,880 --> 00:49:15,820
 definitely doing something reasonable in

2010
00:49:11,670 --> 00:49:15,820
 

2011
00:49:11,680 --> 00:49:17,410
 that sense and then we can ask well then

2012
00:49:15,810 --> 00:49:17,410
 

2013
00:49:15,820 --> 00:49:18,510
 how many labels does it use in order to

2014
00:49:17,400 --> 00:49:18,510
 

2015
00:49:17,410 --> 00:49:22,890
 get a guarantee

2016
00:49:18,500 --> 00:49:22,890
 

2017
00:49:18,510 --> 00:49:24,690
 on the excess error rate and for this

2018
00:49:22,880 --> 00:49:24,690
 

2019
00:49:22,890 --> 00:49:28,859
 we're going to need to introduce some

2020
00:49:24,680 --> 00:49:28,859
 

2021
00:49:24,690 --> 00:49:30,720
 additional notions of complexity and

2022
00:49:28,849 --> 00:49:30,720
 

2023
00:49:28,859 --> 00:49:33,780
 this is where this notion of the

2024
00:49:30,710 --> 00:49:33,780
 

2025
00:49:30,720 --> 00:49:36,030
 disagreement coefficient comes in

2026
00:49:33,770 --> 00:49:36,030
 

2027
00:49:33,780 --> 00:49:37,770
 so I'll define the disagreement

2028
00:49:36,020 --> 00:49:37,770
 

2029
00:49:36,030 --> 00:49:40,109
 coefficient there's several different

2030
00:49:37,760 --> 00:49:40,109
 

2031
00:49:37,770 --> 00:49:46,500
 parts to it there's first part is this

2032
00:49:40,099 --> 00:49:46,500
 

2033
00:49:40,109 --> 00:49:49,230
 ball of radius R so this is a an l1 ball

2034
00:49:46,490 --> 00:49:49,230
 

2035
00:49:46,500 --> 00:49:51,450
 essentially so you take say the ball

2036
00:49:49,220 --> 00:49:51,450
 

2037
00:49:49,230 --> 00:49:54,530
 centered at F star of radius R is just

2038
00:49:51,440 --> 00:49:54,530
 

2039
00:49:51,450 --> 00:49:57,090
 the set of classifiers in H such that

2040
00:49:54,520 --> 00:49:57,090
 

2041
00:49:54,530 --> 00:49:58,710
 the probability that they disagree with

2042
00:49:57,080 --> 00:49:58,710
 

2043
00:49:57,090 --> 00:50:03,060
 that star and a random point is that

2044
00:49:58,700 --> 00:50:03,060
 

2045
00:49:58,710 --> 00:50:05,220
 most R and then the region of

2046
00:50:03,050 --> 00:50:05,220
 

2047
00:50:03,060 --> 00:50:07,109
 disagreement of that ball is just s as

2048
00:50:05,210 --> 00:50:07,109
 

2049
00:50:05,220 --> 00:50:08,940
 usual it's the set of points such that

2050
00:50:07,099 --> 00:50:08,940
 

2051
00:50:07,109 --> 00:50:12,150
 there exist two classifiers in the ball

2052
00:50:08,930 --> 00:50:12,150
 

2053
00:50:08,940 --> 00:50:15,750
 that disagree on the the label of that

2054
00:50:12,140 --> 00:50:15,750
 

2055
00:50:12,150 --> 00:50:17,369
 point so then to define the disagreement

2056
00:50:15,740 --> 00:50:17,369
 

2057
00:50:15,750 --> 00:50:20,310
 coefficient you just you take the ball

2058
00:50:17,359 --> 00:50:20,310
 

2059
00:50:17,369 --> 00:50:21,780
 of radius R Center to that star you

2060
00:50:20,300 --> 00:50:21,780
 

2061
00:50:20,310 --> 00:50:25,050
 calculate the region of disagreement of

2062
00:50:21,770 --> 00:50:25,050
 

2063
00:50:21,780 --> 00:50:26,310
 that ball you calculate the probability

2064
00:50:25,040 --> 00:50:26,310
 

2065
00:50:25,050 --> 00:50:30,000
 mass in the region of disagreement

2066
00:50:26,300 --> 00:50:30,000
 

2067
00:50:26,310 --> 00:50:34,070
 divided by R and take the supremum over

2068
00:50:29,990 --> 00:50:34,070
 

2069
00:50:30,000 --> 00:50:40,050
 R okay so that's the definition of the

2070
00:50:34,060 --> 00:50:40,050
 

2071
00:50:34,070 --> 00:50:41,880
 disagreement coefficient I'll state some

2072
00:50:40,040 --> 00:50:41,880
 

2073
00:50:40,050 --> 00:50:43,520
 guarantees in terms of it but first

2074
00:50:41,870 --> 00:50:43,520
 

2075
00:50:41,880 --> 00:50:46,560
 maybe let's go through some examples

2076
00:50:43,510 --> 00:50:46,560
 

2077
00:50:43,520 --> 00:50:49,590
 just to make sure everybody follows the

2078
00:50:46,550 --> 00:50:49,590
 

2079
00:50:46,560 --> 00:50:51,270
 definition so the first example might be

2080
00:50:49,580 --> 00:50:51,270
 

2081
00:50:49,590 --> 00:50:53,850
 thresholds on the real line this is sort

2082
00:50:51,260 --> 00:50:53,850
 

2083
00:50:51,270 --> 00:50:56,040
 of the simplest example if you have

2084
00:50:53,840 --> 00:50:56,040
 

2085
00:50:53,850 --> 00:51:00,510
 let's say a uniform distribution between

2086
00:50:56,030 --> 00:51:00,510
 

2087
00:50:56,040 --> 00:51:02,460
 0 & 1 and the set of classifiers are

2088
00:51:00,500 --> 00:51:02,460
 

2089
00:51:00,510 --> 00:51:04,830
 threshold so there's some point T such

2090
00:51:02,450 --> 00:51:04,830
 

2091
00:51:02,460 --> 00:51:08,030
 that everything to the right is labeled

2092
00:51:04,820 --> 00:51:08,030
 

2093
00:51:04,830 --> 00:51:11,040
 plus everything to the left is labeled -

2094
00:51:08,020 --> 00:51:11,040
 

2095
00:51:08,030 --> 00:51:13,109
 ok so then there's some t star that

2096
00:51:11,030 --> 00:51:13,109
 

2097
00:51:11,040 --> 00:51:18,330
 corresponds to the F star optimal

2098
00:51:13,099 --> 00:51:18,330
 

2099
00:51:13,109 --> 00:51:21,510
 function and any threshold that is

2100
00:51:18,320 --> 00:51:21,510
 

2101
00:51:18,330 --> 00:51:23,430
 bigger than T star plus R is definitely

2102
00:51:21,500 --> 00:51:23,430
 

2103
00:51:21,510 --> 00:51:26,100
 not in the bar not in the ball of radius

2104
00:51:23,420 --> 00:51:26,100
 

2105
00:51:23,430 --> 00:51:29,369
 R because it has distance further than R

2106
00:51:26,090 --> 00:51:29,369
 

2107
00:51:26,100 --> 00:51:29,770
 and anything less than T star minus R is

2108
00:51:29,359 --> 00:51:29,770
 

2109
00:51:29,369 --> 00:51:33,280
 definite

2110
00:51:29,760 --> 00:51:33,280
 

2111
00:51:29,770 --> 00:51:35,230
 not contained in the ball so that's the

2112
00:51:33,270 --> 00:51:35,230
 

2113
00:51:33,280 --> 00:51:37,840
 set that's the ball it's the set of

2114
00:51:35,220 --> 00:51:37,840
 

2115
00:51:35,230 --> 00:51:40,240
 classifiers that are have their

2116
00:51:37,830 --> 00:51:40,240
 

2117
00:51:37,840 --> 00:51:43,360
 threshold between T star minus R and T

2118
00:51:40,230 --> 00:51:43,360
 

2119
00:51:40,240 --> 00:51:44,710
 star plus R okay so there and then the

2120
00:51:43,350 --> 00:51:44,710
 

2121
00:51:43,360 --> 00:51:47,440
 region of disagreement of that set of

2122
00:51:44,700 --> 00:51:47,440
 

2123
00:51:44,710 --> 00:51:49,920
 classifiers is just that same segment

2124
00:51:47,430 --> 00:51:49,920
 

2125
00:51:47,440 --> 00:51:52,840
 the probability mass there is two R

2126
00:51:49,910 --> 00:51:52,840
 

2127
00:51:49,920 --> 00:51:57,310
 divided by R and so the disagreement

2128
00:51:52,830 --> 00:51:57,310
 

2129
00:51:52,840 --> 00:51:58,450
 coefficient is 2 so for thresholds under

2130
00:51:57,300 --> 00:51:58,450
 

2131
00:51:57,310 --> 00:52:03,490
 this uniform distribution the

2132
00:51:58,440 --> 00:52:03,490
 

2133
00:51:58,450 --> 00:52:05,710
 disagreement coefficient is just 2 okay

2134
00:52:03,480 --> 00:52:05,710
 

2135
00:52:03,490 --> 00:52:08,140
 to go up to a higher dimensional setting

2136
00:52:05,700 --> 00:52:08,140
 

2137
00:52:05,710 --> 00:52:10,540
 let's consider let's say a uniform

2138
00:52:08,130 --> 00:52:10,540
 

2139
00:52:08,140 --> 00:52:15,100
 distribution on the surface of a ball

2140
00:52:10,530 --> 00:52:15,100
 

2141
00:52:10,540 --> 00:52:17,950
 and of a sphere in n dimensions say

2142
00:52:15,090 --> 00:52:17,950
 

2143
00:52:15,100 --> 00:52:19,540
 centered at the origin and let's talk

2144
00:52:17,940 --> 00:52:19,540
 

2145
00:52:17,950 --> 00:52:20,860
 about the set of linear separators that

2146
00:52:19,530 --> 00:52:20,860
 

2147
00:52:19,540 --> 00:52:25,240
 pass through the origin so homogeneous

2148
00:52:20,850 --> 00:52:25,240
 

2149
00:52:20,860 --> 00:52:28,630
 linear separators so f star would be

2150
00:52:25,230 --> 00:52:28,630
 

2151
00:52:25,240 --> 00:52:30,730
 some separator like this and the

2152
00:52:28,620 --> 00:52:30,730
 

2153
00:52:28,630 --> 00:52:34,270
 separators other separators in the ball

2154
00:52:30,720 --> 00:52:34,270
 

2155
00:52:30,730 --> 00:52:35,800
 of radius R would be sort of just like f

2156
00:52:34,260 --> 00:52:35,800
 

2157
00:52:34,270 --> 00:52:38,740
 star except tilted a little bit just a

2158
00:52:35,790 --> 00:52:38,740
 

2159
00:52:35,800 --> 00:52:40,210
 slightly different angle where the

2160
00:52:38,730 --> 00:52:40,210
 

2161
00:52:38,740 --> 00:52:45,580
 probability mass in that wedge between

2162
00:52:40,200 --> 00:52:45,580
 

2163
00:52:40,210 --> 00:52:47,350
 the two is at most R and then if you

2164
00:52:45,570 --> 00:52:47,350
 

2165
00:52:45,580 --> 00:52:49,030
 think about all of the classifiers that

2166
00:52:47,340 --> 00:52:49,030
 

2167
00:52:47,350 --> 00:52:51,160
 are just like f star except tilted a

2168
00:52:49,020 --> 00:52:51,160
 

2169
00:52:49,030 --> 00:52:53,680
 little bit the region of disagreement

2170
00:52:51,150 --> 00:52:53,680
 

2171
00:52:51,160 --> 00:52:57,580
 that they form is this sort of fixed

2172
00:52:53,670 --> 00:52:57,580
 

2173
00:52:53,680 --> 00:52:59,620
 width slab around f star and if you

2174
00:52:57,570 --> 00:52:59,620
 

2175
00:52:57,580 --> 00:53:01,300
 actually do the geometry the width of

2176
00:52:59,610 --> 00:53:01,300
 

2177
00:52:59,620 --> 00:53:04,180
 that slab is going to be proportional to

2178
00:53:01,290 --> 00:53:04,180
 

2179
00:53:01,300 --> 00:53:07,390
 R and it turns out that the probability

2180
00:53:04,170 --> 00:53:07,390
 

2181
00:53:04,180 --> 00:53:12,190
 mass contained in a region a slab of

2182
00:53:07,380 --> 00:53:12,190
 

2183
00:53:07,390 --> 00:53:15,580
 radius sorry of with R is roughly square

2184
00:53:12,180 --> 00:53:15,580
 

2185
00:53:12,190 --> 00:53:16,990
 root of the dimension times R so if you

2186
00:53:15,570 --> 00:53:16,990
 

2187
00:53:15,580 --> 00:53:18,490
 divide that by our you find out the

2188
00:53:16,980 --> 00:53:18,490
 

2189
00:53:16,990 --> 00:53:20,260
 disagreement coefficient for this case

2190
00:53:18,480 --> 00:53:20,260
 

2191
00:53:18,490 --> 00:53:24,430
 is roughly the square root of the

2192
00:53:20,250 --> 00:53:24,430
 

2193
00:53:20,260 --> 00:53:26,290
 dimension okay so these are some

2194
00:53:24,420 --> 00:53:26,290
 

2195
00:53:24,430 --> 00:53:27,820
 examples to show that this is a quantity

2196
00:53:26,280 --> 00:53:27,820
 

2197
00:53:26,290 --> 00:53:29,410
 that can be calculated indeed it has

2198
00:53:27,810 --> 00:53:29,410
 

2199
00:53:27,820 --> 00:53:32,890
 been calculated for quite a few

2200
00:53:29,400 --> 00:53:32,890
 

2201
00:53:29,410 --> 00:53:34,810
 different scenarios and I'll mention a

2202
00:53:32,880 --> 00:53:34,810
 

2203
00:53:32,890 --> 00:53:37,810
 few general things a little bit later

2204
00:53:34,800 --> 00:53:37,810
 

2205
00:53:34,810 --> 00:53:42,440
 but first let's get to stating the

2206
00:53:37,800 --> 00:53:42,440
 

2207
00:53:37,810 --> 00:53:44,720
 results okay so start with

2208
00:53:42,430 --> 00:53:44,720
 

2209
00:53:42,440 --> 00:53:48,289
 case bounded noise Rob mentioned this

2210
00:53:44,710 --> 00:53:48,289
 

2211
00:53:44,720 --> 00:53:50,630
 sometimes called mass art noise so this

2212
00:53:48,279 --> 00:53:50,630
 

2213
00:53:48,289 --> 00:53:52,520
 is an assumption on the the type of

2214
00:53:50,620 --> 00:53:52,520
 

2215
00:53:50,630 --> 00:53:54,410
 noise that we're dealing with and we'll

2216
00:53:52,510 --> 00:53:54,410
 

2217
00:53:52,520 --> 00:53:56,839
 look at two different assumptions in

2218
00:53:54,400 --> 00:53:56,839
 

2219
00:53:54,410 --> 00:53:58,910
 this segment of the talk

2220
00:53:56,829 --> 00:53:58,910
 

2221
00:53:56,839 --> 00:54:01,190
 so under bounded noise what we're

2222
00:53:58,900 --> 00:54:01,190
 

2223
00:53:58,910 --> 00:54:03,559
 assuming is that there's some beta so

2224
00:54:01,180 --> 00:54:03,559
 

2225
00:54:01,190 --> 00:54:05,270
 that it's strictly less than one half

2226
00:54:03,549 --> 00:54:05,270
 

2227
00:54:03,559 --> 00:54:08,690
 such that for every point the

2228
00:54:05,260 --> 00:54:08,690
 

2229
00:54:05,270 --> 00:54:13,160
 probability that the label is different

2230
00:54:08,680 --> 00:54:13,160
 

2231
00:54:08,690 --> 00:54:15,410
 from f stars label given that point it's

2232
00:54:13,150 --> 00:54:15,410
 

2233
00:54:13,160 --> 00:54:18,319
 at most beta so this is saying that

2234
00:54:15,400 --> 00:54:18,319
 

2235
00:54:15,410 --> 00:54:23,450
 there's some bias toward F stars label

2236
00:54:18,309 --> 00:54:23,450
 

2237
00:54:18,319 --> 00:54:26,750
 at every point okay so this is bounded

2238
00:54:23,440 --> 00:54:26,750
 

2239
00:54:23,450 --> 00:54:28,130
 noise and the the number of samples the

2240
00:54:26,740 --> 00:54:28,130
 

2241
00:54:26,750 --> 00:54:29,660
 number of label sample is needed for

2242
00:54:28,120 --> 00:54:29,660
 

2243
00:54:28,130 --> 00:54:33,559
 passive learning it's known to be about

2244
00:54:29,650 --> 00:54:33,559
 

2245
00:54:29,660 --> 00:54:35,809
 D over epsilon to get the error rate to

2246
00:54:33,549 --> 00:54:35,809
 

2247
00:54:33,559 --> 00:54:39,319
 be at most the error rate of F star plus

2248
00:54:35,799 --> 00:54:39,319
 

2249
00:54:35,809 --> 00:54:41,630
 epsilon and for active learning the

2250
00:54:39,309 --> 00:54:41,630
 

2251
00:54:39,319 --> 00:54:44,089
 guarantee here is vc-dimension times

2252
00:54:41,620 --> 00:54:44,089
 

2253
00:54:41,630 --> 00:54:46,099
 theta log 1 over epsilon okay so if

2254
00:54:44,079 --> 00:54:46,099
 

2255
00:54:44,089 --> 00:54:47,210
 theta is small this is actually an

2256
00:54:46,089 --> 00:54:47,210
 

2257
00:54:46,099 --> 00:54:49,520
 exponential improvement in the

2258
00:54:47,200 --> 00:54:49,520
 

2259
00:54:47,210 --> 00:54:50,990
 dependence on epsilon another way to

2260
00:54:49,510 --> 00:54:50,990
 

2261
00:54:49,520 --> 00:54:53,180
 view that is it's a rate of convergence

2262
00:54:50,980 --> 00:54:53,180
 

2263
00:54:50,990 --> 00:54:55,549
 in terms of the number of labels it'd be

2264
00:54:53,170 --> 00:54:55,549
 

2265
00:54:53,180 --> 00:55:00,740
 D over N for passive it would be like e

2266
00:54:55,539 --> 00:55:00,740
 

2267
00:54:55,549 --> 00:55:02,630
 to the minus and over D theta for active

2268
00:55:00,730 --> 00:55:02,630
 

2269
00:55:00,740 --> 00:55:09,380
 so again if theta is small that's that's

2270
00:55:02,620 --> 00:55:09,380
 

2271
00:55:02,630 --> 00:55:13,789
 an exponential rate okay I'll just very

2272
00:55:09,370 --> 00:55:13,789
 

2273
00:55:09,380 --> 00:55:17,119
 quickly go through some high-level of

2274
00:55:13,779 --> 00:55:17,119
 

2275
00:55:13,789 --> 00:55:19,940
 how that proof works so for any given

2276
00:55:17,109 --> 00:55:19,940
 

2277
00:55:17,119 --> 00:55:22,160
 round recall that all of the classifiers

2278
00:55:19,930 --> 00:55:22,160
 

2279
00:55:19,940 --> 00:55:24,470
 in H agree on all the points that

2280
00:55:22,150 --> 00:55:24,470
 

2281
00:55:22,160 --> 00:55:29,510
 weren't queried on all the points that

2282
00:55:24,460 --> 00:55:29,510
 

2283
00:55:24,470 --> 00:55:30,849
 are in outside of the set Q and so that

2284
00:55:29,500 --> 00:55:30,849
 

2285
00:55:29,510 --> 00:55:34,309
 means that actually in step four

2286
00:55:30,839 --> 00:55:34,309
 

2287
00:55:30,849 --> 00:55:36,890
 basically if you were to dump all of the

2288
00:55:34,299 --> 00:55:36,890
 

2289
00:55:34,309 --> 00:55:39,380
 samples that weren't queried back into

2290
00:55:36,880 --> 00:55:39,380
 

2291
00:55:36,890 --> 00:55:41,839
 the dataset it wouldn't change anything

2292
00:55:39,370 --> 00:55:41,839
 

2293
00:55:39,380 --> 00:55:43,460
 in step four okay so you would still

2294
00:55:41,829 --> 00:55:43,460
 

2295
00:55:41,839 --> 00:55:46,900
 basically be evaluating bernstein's

2296
00:55:43,450 --> 00:55:46,900
 

2297
00:55:43,460 --> 00:55:50,059
 inequality but now with all of the data

2298
00:55:46,890 --> 00:55:50,059
 

2299
00:55:46,900 --> 00:55:51,890
 and and so basically the the guarantee

2300
00:55:50,049 --> 00:55:51,890
 

2301
00:55:50,059 --> 00:55:54,350
 that that gives you is that the X after

2302
00:55:51,880 --> 00:55:54,350
 

2303
00:55:51,890 --> 00:55:59,810
 step four the only classifiers that are

2304
00:55:54,340 --> 00:55:59,810
 

2305
00:55:54,350 --> 00:56:03,200
 around have their excess risk bounded by

2306
00:55:59,800 --> 00:56:03,200
 

2307
00:55:59,810 --> 00:56:07,190
 roughly this square root of the distance

2308
00:56:03,190 --> 00:56:07,190
 

2309
00:56:03,200 --> 00:56:08,840
 to start D over to to the teeth and then

2310
00:56:07,180 --> 00:56:08,840
 

2311
00:56:07,190 --> 00:56:10,460
 we're going to use the bounded noise

2312
00:56:08,830 --> 00:56:10,460
 

2313
00:56:08,840 --> 00:56:13,310
 condition and this condition turns out

2314
00:56:10,450 --> 00:56:13,310
 

2315
00:56:10,460 --> 00:56:15,650
 to imply that the distance between the

2316
00:56:13,300 --> 00:56:15,650
 

2317
00:56:13,310 --> 00:56:18,950
 two classifiers that shows up inside

2318
00:56:15,640 --> 00:56:18,950
 

2319
00:56:15,650 --> 00:56:23,110
 that square root it's bounded by a

2320
00:56:18,940 --> 00:56:23,110
 

2321
00:56:18,950 --> 00:56:25,730
 quantity proportional to the excess risk

2322
00:56:23,100 --> 00:56:25,730
 

2323
00:56:23,110 --> 00:56:27,380
 so you can sort of plug in the excess

2324
00:56:25,720 --> 00:56:27,380
 

2325
00:56:25,730 --> 00:56:29,690
 risk inside of that square root and

2326
00:56:27,370 --> 00:56:29,690
 

2327
00:56:27,380 --> 00:56:33,410
 solve a little quadratic equation and

2328
00:56:29,680 --> 00:56:33,410
 

2329
00:56:29,690 --> 00:56:35,630
 what you get out is that after at the

2330
00:56:33,400 --> 00:56:35,630
 

2331
00:56:33,410 --> 00:56:37,430
 end of each round the classifiers that

2332
00:56:35,620 --> 00:56:37,430
 

2333
00:56:35,630 --> 00:56:41,360
 survive have an excess risk that's

2334
00:56:37,420 --> 00:56:41,360
 

2335
00:56:37,430 --> 00:56:43,820
 bounded by like D over two to the T and

2336
00:56:41,350 --> 00:56:43,820
 

2337
00:56:41,360 --> 00:56:46,040
 so you know that like roughly log D over

2338
00:56:43,810 --> 00:56:46,040
 

2339
00:56:43,820 --> 00:56:51,290
 epsilon rounds will suffice to give you

2340
00:56:46,030 --> 00:56:51,290
 

2341
00:56:46,040 --> 00:56:52,760
 that epsilon excess risk okay and you

2342
00:56:51,280 --> 00:56:52,760
 

2343
00:56:51,290 --> 00:56:54,950
 can also use this bounded noise

2344
00:56:52,750 --> 00:56:54,950
 

2345
00:56:52,760 --> 00:56:58,310
 condition in combination with that

2346
00:56:54,940 --> 00:56:58,310
 

2347
00:56:54,950 --> 00:56:59,780
 guarantee to say that to give you a

2348
00:56:58,300 --> 00:56:59,780
 

2349
00:56:58,310 --> 00:57:02,390
 bound on how many queries we made well

2350
00:56:59,770 --> 00:57:02,390
 

2351
00:56:59,780 --> 00:57:04,580
 what it implies is that when you enter

2352
00:57:02,380 --> 00:57:04,580
 

2353
00:57:02,390 --> 00:57:07,220
 the round H is contained within a ball

2354
00:57:04,570 --> 00:57:07,220
 

2355
00:57:04,580 --> 00:57:08,510
 of radius like roughly D over two to the

2356
00:57:07,210 --> 00:57:08,510
 

2357
00:57:07,220 --> 00:57:13,220
 t minus one because that's the guarantee

2358
00:57:08,500 --> 00:57:13,220
 

2359
00:57:08,510 --> 00:57:14,930
 left from the previous round and so then

2360
00:57:13,210 --> 00:57:14,930
 

2361
00:57:13,220 --> 00:57:17,150
 the number of queries is like roughly

2362
00:57:14,920 --> 00:57:17,150
 

2363
00:57:14,930 --> 00:57:18,530
 the probability in the region of

2364
00:57:17,140 --> 00:57:18,530
 

2365
00:57:17,150 --> 00:57:20,810
 disagreement of that ball

2366
00:57:18,520 --> 00:57:20,810
 

2367
00:57:18,530 --> 00:57:23,690
 we're upper bounded by it anyway times

2368
00:57:20,800 --> 00:57:23,690
 

2369
00:57:20,810 --> 00:57:25,310
 the number of samples and then so you

2370
00:57:23,680 --> 00:57:25,310
 

2371
00:57:23,690 --> 00:57:27,230
 just go ahead and evaluate that and it

2372
00:57:25,300 --> 00:57:27,230
 

2373
00:57:25,310 --> 00:57:31,370
 comes out to you make roughly theta

2374
00:57:27,220 --> 00:57:31,370
 

2375
00:57:27,230 --> 00:57:33,470
 times D queries on each round okay so if

2376
00:57:31,360 --> 00:57:33,470
 

2377
00:57:31,370 --> 00:57:35,090
 you just sum up over that log D over

2378
00:57:33,460 --> 00:57:35,090
 

2379
00:57:33,470 --> 00:57:38,840
 epsilon rounds you're gonna get roughly

2380
00:57:35,080 --> 00:57:38,840
 

2381
00:57:35,090 --> 00:57:43,720
 theta d log D over epsilon s your total

2382
00:57:38,830 --> 00:57:43,720
 

2383
00:57:38,840 --> 00:57:47,770
 number of queries okay

2384
00:57:43,710 --> 00:57:47,770
 

2385
00:57:43,720 --> 00:57:49,750
 so that was the bounded noise case we

2386
00:57:47,760 --> 00:57:49,750
 

2387
00:57:47,770 --> 00:57:51,190
 can also look at weaker assumptions so

2388
00:57:49,740 --> 00:57:51,190
 

2389
00:57:49,750 --> 00:57:55,480
 bounded noise actually make some

2390
00:57:51,180 --> 00:57:55,480
 

2391
00:57:51,190 --> 00:57:58,300
 assumptions on how the probability of Y

2392
00:57:55,470 --> 00:57:58,300
 

2393
00:57:55,480 --> 00:58:00,160
 given X behaves we can also just look at

2394
00:57:58,290 --> 00:58:00,160
 

2395
00:57:58,300 --> 00:58:04,540
 the general case we have an arbitrary

2396
00:58:00,150 --> 00:58:04,540
 

2397
00:58:00,160 --> 00:58:06,609
 distribution on x and y and let's just

2398
00:58:04,530 --> 00:58:06,609
 

2399
00:58:04,540 --> 00:58:09,040
 denote it as it's called the agnostic

2400
00:58:06,599 --> 00:58:09,040
 

2401
00:58:06,609 --> 00:58:12,180
 case so let's just denote by beta the

2402
00:58:09,030 --> 00:58:12,180
 

2403
00:58:09,040 --> 00:58:15,069
 risk of the best classifier in the class

2404
00:58:12,170 --> 00:58:15,069
 

2405
00:58:12,180 --> 00:58:17,200
 and then we can get this epsilon excess

2406
00:58:15,059 --> 00:58:17,200
 

2407
00:58:15,069 --> 00:58:20,530
 risk guarantee with passive learning

2408
00:58:17,190 --> 00:58:20,530
 

2409
00:58:17,200 --> 00:58:23,819
 turns out you require about D times beta

2410
00:58:20,520 --> 00:58:23,819
 

2411
00:58:20,530 --> 00:58:26,470
 over epsilon squared labeled samples and

2412
00:58:23,809 --> 00:58:26,470
 

2413
00:58:23,819 --> 00:58:30,550
 with active learning again with the same

2414
00:58:26,460 --> 00:58:30,550
 

2415
00:58:26,470 --> 00:58:32,020
 algorithm D times theta beta squared

2416
00:58:30,540 --> 00:58:32,020
 

2417
00:58:30,550 --> 00:58:33,970
 over epsilon squared so what the

2418
00:58:32,010 --> 00:58:33,970
 

2419
00:58:32,020 --> 00:58:37,030
 difference between these two bounds is

2420
00:58:33,960 --> 00:58:37,030
 

2421
00:58:33,970 --> 00:58:39,880
 we've squared the beta so we got an

2422
00:58:37,020 --> 00:58:39,880
 

2423
00:58:37,030 --> 00:58:42,369
 improvement in proportional to the the

2424
00:58:39,870 --> 00:58:42,369
 

2425
00:58:39,880 --> 00:58:45,910
 error rate of the best classifier in the

2426
00:58:42,359 --> 00:58:45,910
 

2427
00:58:42,369 --> 00:58:47,950
 class but we also multiply by theta okay

2428
00:58:45,900 --> 00:58:47,950
 

2429
00:58:45,910 --> 00:58:51,250
 so this is a an improvement if theta is

2430
00:58:47,940 --> 00:58:51,250
 

2431
00:58:47,950 --> 00:58:57,970
 smaller and if the error rate of the

2432
00:58:51,240 --> 00:58:57,970
 

2433
00:58:51,250 --> 00:59:01,300
 best classifier is small okay the proof

2434
00:58:57,960 --> 00:59:01,300
 

2435
00:58:57,970 --> 00:59:03,970
 runs essentially similar and so I'm just

2436
00:59:01,290 --> 00:59:03,970
 

2437
00:59:01,300 --> 00:59:08,829
 going to maybe skip this for this sake

2438
00:59:03,960 --> 00:59:08,829
 

2439
00:59:03,970 --> 00:59:10,960
 of time okay so as I said there have

2440
00:59:08,819 --> 00:59:10,960
 

2441
00:59:08,829 --> 00:59:14,829
 been some studies of when is theta small

2442
00:59:10,950 --> 00:59:14,829
 

2443
00:59:10,960 --> 00:59:17,170
 and we have some understanding of

2444
00:59:14,819 --> 00:59:17,170
 

2445
00:59:14,829 --> 00:59:18,880
 various general conditions in addition

2446
00:59:17,160 --> 00:59:18,880
 

2447
00:59:17,170 --> 00:59:21,670
 to many examples that where it's been

2448
00:59:18,870 --> 00:59:21,670
 

2449
00:59:18,880 --> 00:59:24,300
 calculated so for instance if we're

2450
00:59:21,660 --> 00:59:24,300
 

2451
00:59:21,670 --> 00:59:28,589
 working with linear separators and the

2452
00:59:24,290 --> 00:59:28,589
 

2453
00:59:24,300 --> 00:59:32,530
 distribution of X has a density and the

2454
00:59:28,579 --> 00:59:32,530
 

2455
00:59:28,589 --> 00:59:34,900
 f star separator passes through the

2456
00:59:32,520 --> 00:59:34,900
 

2457
00:59:32,530 --> 00:59:38,050
 interior of the support of the density

2458
00:59:34,890 --> 00:59:38,050
 

2459
00:59:34,900 --> 00:59:42,690
 turns out then theta is bounded meaning

2460
00:59:38,040 --> 00:59:42,690
 

2461
00:59:38,050 --> 00:59:45,430
 it's not a function of the epsilon and

2462
00:59:42,680 --> 00:59:45,430
 

2463
00:59:42,690 --> 00:59:47,170
 even if it you don't have that second

2464
00:59:45,420 --> 00:59:47,170
 

2465
00:59:45,430 --> 00:59:48,970
 condition even if you're just in the

2466
00:59:47,160 --> 00:59:48,970
 

2467
00:59:47,170 --> 00:59:50,589
 case where the density function but

2468
00:59:48,960 --> 00:59:50,589
 

2469
00:59:48,970 --> 00:59:52,599
 where the distribution of x has a

2470
00:59:50,579 --> 00:59:52,599
 

2471
00:59:50,589 --> 00:59:56,010
 density with respect to the lebesgue

2472
00:59:52,589 --> 00:59:56,010
 

2473
00:59:52,599 --> 00:59:58,980
 measure then at least you can guarantee

2474
00:59:56,000 --> 00:59:58,980
 

2475
00:59:56,010 --> 01:00:01,260
 theta is little oh one over epsilon and

2476
00:59:58,970 --> 01:00:01,260
 

2477
00:59:58,980 --> 01:00:04,110
 it turns out that that's sufficient to

2478
01:00:01,250 --> 01:00:04,110
 

2479
01:00:01,260 --> 01:00:07,190
 guarantee some benefits of the active

2480
01:00:04,100 --> 01:00:07,190
 

2481
01:00:04,110 --> 01:00:10,410
 learning algorithm compared to passive

2482
01:00:07,180 --> 01:00:10,410
 

2483
01:00:07,190 --> 01:00:12,510
 and then there are some more general

2484
01:00:10,400 --> 01:00:12,510
 

2485
01:00:10,410 --> 01:00:17,030
 conditions that that for general

2486
01:00:12,500 --> 01:00:17,030
 

2487
01:00:12,510 --> 01:00:20,460
 function classes H there's some work by

2488
01:00:17,020 --> 01:00:20,460
 

2489
01:00:17,030 --> 01:00:22,470
 Friedman from Cole 2009 showing if your

2490
01:00:20,450 --> 01:00:22,470
 

2491
01:00:20,460 --> 01:00:24,720
 hypothesis class is in some sense

2492
01:00:22,460 --> 01:00:24,720
 

2493
01:00:22,470 --> 01:00:26,690
 smoothly parameterised and you have some

2494
01:00:24,710 --> 01:00:26,690
 

2495
01:00:24,720 --> 01:00:30,120
 regularity conditions on the density of

2496
01:00:26,680 --> 01:00:30,120
 

2497
01:00:26,690 --> 01:00:32,760
 the X distribution and you have some

2498
01:00:30,110 --> 01:00:32,760
 

2499
01:00:30,120 --> 01:00:34,670
 other technical conditions the functions

2500
01:00:32,750 --> 01:00:34,670
 

2501
01:00:32,760 --> 01:00:36,960
 are specified as so real-valued

2502
01:00:34,660 --> 01:00:36,960
 

2503
01:00:34,670 --> 01:00:39,690
 thresholded real valued functions and

2504
01:00:36,950 --> 01:00:39,690
 

2505
01:00:36,960 --> 01:00:43,380
 they they cross the decision they cross

2506
01:00:39,680 --> 01:00:43,380
 

2507
01:00:39,690 --> 01:00:46,350
 the threshold at a certain rate and

2508
01:00:43,370 --> 01:00:46,350
 

2509
01:00:43,380 --> 01:00:47,940
 things of this sort then the

2510
01:00:46,340 --> 01:00:47,940
 

2511
01:00:46,350 --> 01:00:49,860
 disagreement coefficient is going to be

2512
01:00:47,930 --> 01:00:49,860
 

2513
01:00:47,940 --> 01:00:51,150
 roughly proportional to the number of

2514
01:00:49,850 --> 01:00:51,150
 

2515
01:00:49,860 --> 01:00:54,830
 parameters in that smooth

2516
01:00:51,140 --> 01:00:54,830
 

2517
01:00:51,150 --> 01:00:54,830
 parameterization of the function class

2518
01:00:55,510 --> 01:00:55,510
 

2519
01:00:55,520 --> 01:00:59,540
 refer you to the original work for all

2520
01:00:57,650 --> 01:00:59,540
 

2521
01:00:57,660 --> 01:01:03,210
 of the what the technical conditions are

2522
01:00:59,530 --> 01:01:03,210
 

2523
01:00:59,540 --> 01:01:05,850
 and yeah I'll just mention there lots

2524
01:01:03,200 --> 01:01:05,850
 

2525
01:01:03,210 --> 01:01:13,980
 more examples and and general conditions

2526
01:01:05,840 --> 01:01:13,980
 

2527
01:01:05,850 --> 01:01:15,480
 explored in that monograph 2014 ok a few

2528
01:01:13,970 --> 01:01:15,480
 

2529
01:01:13,980 --> 01:01:19,350
 things remaining that I should talk

2530
01:01:15,470 --> 01:01:19,350
 

2531
01:01:15,480 --> 01:01:21,090
 about one is I've so far I left this

2532
01:01:19,340 --> 01:01:21,090
 

2533
01:01:19,350 --> 01:01:24,090
 stopping criterion in the active

2534
01:01:21,080 --> 01:01:24,090
 

2535
01:01:21,090 --> 01:01:26,070
 learning algorithm sort of vague it

2536
01:01:24,080 --> 01:01:26,070
 

2537
01:01:24,090 --> 01:01:29,310
 really just depends on what context

2538
01:01:26,060 --> 01:01:29,310
 

2539
01:01:26,070 --> 01:01:31,440
 you're in you're using this as to what

2540
01:01:29,300 --> 01:01:31,440
 

2541
01:01:29,310 --> 01:01:33,510
 stopping criterion you want to use you

2542
01:01:31,430 --> 01:01:33,510
 

2543
01:01:31,440 --> 01:01:37,710
 could run this as an anytime algorithm

2544
01:01:33,500 --> 01:01:37,710
 

2545
01:01:33,510 --> 01:01:40,560
 right so if you want a classifier Friday

2546
01:01:37,700 --> 01:01:40,560
 

2547
01:01:37,710 --> 01:01:43,290
 at 4:00 p.m. you can just run it and and

2548
01:01:40,550 --> 01:01:43,290
 

2549
01:01:40,560 --> 01:01:45,720
 whatever the the f hat it got on the

2550
01:01:43,280 --> 01:01:45,720
 

2551
01:01:43,290 --> 01:01:47,070
 previous round it's going to be it's

2552
01:01:45,710 --> 01:01:47,070
 

2553
01:01:45,720 --> 01:01:50,310
 going to have those guarantees in terms

2554
01:01:47,060 --> 01:01:50,310
 

2555
01:01:47,070 --> 01:01:53,340
 of however many label queries it's made

2556
01:01:50,300 --> 01:01:53,340
 

2557
01:01:50,310 --> 01:01:56,700
 up to that time you could set a label

2558
01:01:53,330 --> 01:01:56,700
 

2559
01:01:53,340 --> 01:02:00,090
 budget if you only can afford a certain

2560
01:01:56,690 --> 01:02:00,090
 

2561
01:01:56,700 --> 01:02:01,650
 number of labels and it'll just run up

2562
01:02:00,080 --> 01:02:01,650
 

2563
01:02:00,090 --> 01:02:02,870
 until it makes that many queries and

2564
01:02:01,640 --> 01:02:02,870
 

2565
01:02:01,650 --> 01:02:06,260
 again you'll get a guarantee

2566
01:02:02,860 --> 01:02:06,260
 

2567
01:02:02,870 --> 01:02:10,070
 on that F hat in terms of what the label

2568
01:02:06,250 --> 01:02:10,070
 

2569
01:02:06,260 --> 01:02:11,450
 budget is if you have a bounded amount

2570
01:02:10,060 --> 01:02:11,450
 

2571
01:02:10,070 --> 01:02:13,730
 of unlabeled data of course you can just

2572
01:02:11,440 --> 01:02:13,730
 

2573
01:02:11,450 --> 01:02:15,500
 run until you run out of unlabeled data

2574
01:02:13,720 --> 01:02:15,500
 

2575
01:02:13,730 --> 01:02:17,780
 right so on each round it's using some

2576
01:02:15,490 --> 01:02:17,780
 

2577
01:02:15,500 --> 01:02:19,520
 number of unlabeled samples you know if

2578
01:02:17,770 --> 01:02:19,520
 

2579
01:02:17,780 --> 01:02:21,350
 you if you run out of unlabeled samples

2580
01:02:19,510 --> 01:02:21,350
 

2581
01:02:19,520 --> 01:02:23,240
 it doesn't really make sense to sort of

2582
01:02:21,340 --> 01:02:23,240
 

2583
01:02:21,350 --> 01:02:26,300
 cycle through this the unlabeled data

2584
01:02:23,230 --> 01:02:26,300
 

2585
01:02:23,240 --> 01:02:29,120
 again or something like that so that's

2586
01:02:26,290 --> 01:02:29,120
 

2587
01:02:26,300 --> 01:02:30,850
 another way that you can sort of have to

2588
01:02:29,110 --> 01:02:30,850
 

2589
01:02:29,120 --> 01:02:33,830
 stop if you run out of unlabeled data

2590
01:02:30,840 --> 01:02:33,830
 

2591
01:02:30,850 --> 01:02:36,290
 another thing is that this algorithm and

2592
01:02:33,820 --> 01:02:36,290
 

2593
01:02:33,830 --> 01:02:38,510
 this generally disagreement based active

2594
01:02:36,280 --> 01:02:38,510
 

2595
01:02:36,290 --> 01:02:41,570
 learning algorithms have this property

2596
01:02:38,500 --> 01:02:41,570
 

2597
01:02:38,510 --> 01:02:43,460
 of being self verifying and what that

2598
01:02:41,560 --> 01:02:43,460
 

2599
01:02:41,570 --> 01:02:44,780
 means is if you give if you have an

2600
01:02:43,450 --> 01:02:44,780
 

2601
01:02:43,460 --> 01:02:47,300
 epsilon that you're trying to achieve

2602
01:02:44,770 --> 01:02:47,300
 

2603
01:02:44,780 --> 01:02:50,330
 and so you know what excess error rate

2604
01:02:47,290 --> 01:02:50,330
 

2605
01:02:47,300 --> 01:02:53,360
 you're aiming for you could just you

2606
01:02:50,320 --> 01:02:53,360
 

2607
01:02:50,330 --> 01:02:57,410
 could evaluate this bernstein inequality

2608
01:02:53,350 --> 01:02:57,410
 

2609
01:02:53,360 --> 01:02:59,390
 bound after each round and it will sort

2610
01:02:57,400 --> 01:02:59,390
 

2611
01:02:57,410 --> 01:03:01,880
 of tell you whether you've gotten there

2612
01:02:59,380 --> 01:03:01,880
 

2613
01:02:59,390 --> 01:03:04,310
 yet or not you can check whether you're

2614
01:03:01,870 --> 01:03:04,310
 

2615
01:03:01,880 --> 01:03:11,000
 guaranteed excess error rate is at most

2616
01:03:04,300 --> 01:03:11,000
 

2617
01:03:04,310 --> 01:03:12,200
 epsilon after each round and so that so

2618
01:03:10,990 --> 01:03:12,200
 

2619
01:03:11,000 --> 01:03:17,630
 that's another way that you can set a

2620
01:03:12,190 --> 01:03:17,630
 

2621
01:03:12,200 --> 01:03:19,700
 stopping criterion okay I'll mention

2622
01:03:17,620 --> 01:03:19,700
 

2623
01:03:17,630 --> 01:03:21,950
 also there's a there is a simplification

2624
01:03:19,690 --> 01:03:21,950
 

2625
01:03:19,700 --> 01:03:26,120
 of this algorithm that makes a lot of

2626
01:03:21,940 --> 01:03:26,120
 

2627
01:03:21,950 --> 01:03:28,250
 practical sense and this is based on

2628
01:03:26,110 --> 01:03:28,250
 

2629
01:03:26,120 --> 01:03:33,950
 work from Daniel Hsu and and other

2630
01:03:28,240 --> 01:03:33,950
 

2631
01:03:28,250 --> 01:03:35,540
 co-authors over the years basically it

2632
01:03:33,940 --> 01:03:35,540
 

2633
01:03:33,950 --> 01:03:37,460
 would be nice if you can implement this

2634
01:03:35,530 --> 01:03:37,460
 

2635
01:03:35,540 --> 01:03:41,830
 as a reduction to empirical risk

2636
01:03:37,450 --> 01:03:41,830
 

2637
01:03:37,460 --> 01:03:45,620
 minimization and that has practical

2638
01:03:41,820 --> 01:03:45,620
 

2639
01:03:41,830 --> 01:03:48,470
 implications because okay empirical risk

2640
01:03:45,610 --> 01:03:48,470
 

2641
01:03:45,620 --> 01:03:50,930
 minimization can be difficult it can be

2642
01:03:48,460 --> 01:03:50,930
 

2643
01:03:48,470 --> 01:03:52,640
 challenging to run in practice but you

2644
01:03:50,920 --> 01:03:52,640
 

2645
01:03:50,930 --> 01:03:53,900
 could just maybe plug in a different

2646
01:03:52,630 --> 01:03:53,900
 

2647
01:03:52,640 --> 01:03:57,080
 algorithm there are different passive

2648
01:03:53,890 --> 01:03:57,080
 

2649
01:03:53,900 --> 01:03:59,810
 learning algorithm and sort of still get

2650
01:03:57,070 --> 01:03:59,810
 

2651
01:03:57,080 --> 01:04:02,540
 a reasonable behavior in your algorithm

2652
01:03:59,800 --> 01:04:02,540
 

2653
01:03:59,810 --> 01:04:05,630
 so so here's a strategy that does that

2654
01:04:02,530 --> 01:04:05,630
 

2655
01:04:02,540 --> 01:04:09,700
 that does this reduction basically

2656
01:04:05,620 --> 01:04:09,700
 

2657
01:04:05,630 --> 01:04:13,520
 sample points now just one at a time and

2658
01:04:09,690 --> 01:04:13,520
 

2659
01:04:09,700 --> 01:04:16,310
 for each X that you sample these are

2660
01:04:13,510 --> 01:04:16,310
 

2661
01:04:13,520 --> 01:04:19,109
 unlabeled points from the pool

2662
01:04:16,300 --> 01:04:19,109
 

2663
01:04:16,310 --> 01:04:20,970
 for each label why constrain the label

2664
01:04:19,099 --> 01:04:20,970
 

2665
01:04:19,109 --> 01:04:22,619
 to be Y and then minimize the number of

2666
01:04:20,960 --> 01:04:22,619
 

2667
01:04:20,970 --> 01:04:23,490
 mistakes so in binary classification

2668
01:04:22,609 --> 01:04:23,490
 

2669
01:04:22,619 --> 01:04:27,839
 you're going to end up with two

2670
01:04:23,480 --> 01:04:27,839
 

2671
01:04:23,490 --> 01:04:31,410
 classifiers one that definitely labels X

2672
01:04:27,829 --> 01:04:31,410
 

2673
01:04:27,839 --> 01:04:32,970
 as plus and then minimizes the number of

2674
01:04:31,400 --> 01:04:32,970
 

2675
01:04:31,410 --> 01:04:35,280
 mistakes and another that definitely

2676
01:04:32,960 --> 01:04:35,280
 

2677
01:04:32,970 --> 01:04:37,829
 labels X as minus and minimizes the

2678
01:04:35,270 --> 01:04:37,829
 

2679
01:04:35,280 --> 01:04:39,630
 number of mistakes and then with those

2680
01:04:37,819 --> 01:04:39,630
 

2681
01:04:37,829 --> 01:04:42,900
 two classifiers just do this sort of

2682
01:04:39,620 --> 01:04:42,900
 

2683
01:04:39,630 --> 01:04:45,599
 bernstein check is one of them much

2684
01:04:42,890 --> 01:04:45,599
 

2685
01:04:42,900 --> 01:04:47,730
 worse than the other basically if the

2686
01:04:45,589 --> 01:04:47,730
 

2687
01:04:45,599 --> 01:04:51,630
 empirical error rates are very close to

2688
01:04:47,720 --> 01:04:51,630
 

2689
01:04:47,730 --> 01:04:53,900
 each other then go ahead and request the

2690
01:04:51,620 --> 01:04:53,900
 

2691
01:04:51,630 --> 01:04:57,930
 label for X and add it to the data set

2692
01:04:53,890 --> 01:04:57,930
 

2693
01:04:53,900 --> 01:04:59,400
 otherwise just skip over it because we

2694
01:04:57,920 --> 01:04:59,400
 

2695
01:04:57,930 --> 01:05:00,900
 can sort of conclude that one of them is

2696
01:04:59,390 --> 01:05:00,900
 

2697
01:04:59,400 --> 01:05:03,119
 definitely worse than the other so we

2698
01:05:00,890 --> 01:05:03,119
 

2699
01:05:00,900 --> 01:05:07,349
 know we can kind of infer what F stars

2700
01:05:03,109 --> 01:05:07,349
 

2701
01:05:03,119 --> 01:05:08,880
 label is there and again I'm simplifying

2702
01:05:07,339 --> 01:05:08,880
 

2703
01:05:07,349 --> 01:05:12,859
 things a little bit but not too much

2704
01:05:08,870 --> 01:05:12,859
 

2705
01:05:08,880 --> 01:05:16,170
 this is a very nice simple way to do

2706
01:05:12,849 --> 01:05:16,170
 

2707
01:05:12,859 --> 01:05:17,730
 effectively the same thing as these

2708
01:05:16,160 --> 01:05:17,730
 

2709
01:05:16,170 --> 01:05:19,440
 disagreement based algorithms like a

2710
01:05:17,720 --> 01:05:19,440
 

2711
01:05:17,730 --> 01:05:21,240
 squared and in fact this will achieve

2712
01:05:19,430 --> 01:05:21,240
 

2713
01:05:19,440 --> 01:05:25,349
 roughly the same sample complexity si

2714
01:05:21,230 --> 01:05:25,349
 

2715
01:05:21,240 --> 01:05:31,050
 squared and the convenience here is that

2716
01:05:25,339 --> 01:05:31,050
 

2717
01:05:25,349 --> 01:05:32,310
 as I said this step this too can be you

2718
01:05:31,040 --> 01:05:32,310
 

2719
01:05:31,050 --> 01:05:34,109
 know this is an empirical risk

2720
01:05:32,300 --> 01:05:34,109
 

2721
01:05:32,310 --> 01:05:36,569
 minimization step okay so technically

2722
01:05:34,099 --> 01:05:36,569
 

2723
01:05:34,109 --> 01:05:38,250
 there's a constraint there but you know

2724
01:05:36,559 --> 01:05:38,250
 

2725
01:05:36,569 --> 01:05:40,079
 and you can implement a constraint by

2726
01:05:38,240 --> 01:05:40,079
 

2727
01:05:38,250 --> 01:05:42,599
 just sort of putting in many copies of

2728
01:05:40,069 --> 01:05:42,599
 

2729
01:05:40,079 --> 01:05:44,010
 that example in the data set and so you

2730
01:05:42,589 --> 01:05:44,010
 

2731
01:05:42,599 --> 01:05:49,589
 can just run it empirical risk

2732
01:05:44,000 --> 01:05:49,589
 

2733
01:05:44,010 --> 01:05:51,930
 minimization and okay basically what you

2734
01:05:49,579 --> 01:05:51,930
 

2735
01:05:49,589 --> 01:05:53,550
 would do in practice you know as I said

2736
01:05:51,920 --> 01:05:53,550
 

2737
01:05:51,930 --> 01:05:57,300
 empirical risk minimization can be

2738
01:05:53,540 --> 01:05:57,300
 

2739
01:05:53,550 --> 01:05:58,920
 challenging to run so let's just use a

2740
01:05:57,290 --> 01:05:58,920
 

2741
01:05:57,300 --> 01:06:01,829
 different passive learning algorithm

2742
01:05:58,910 --> 01:06:01,829
 

2743
01:05:58,920 --> 01:06:05,300
 there and that's kind of a one way to

2744
01:06:01,819 --> 01:06:05,300
 

2745
01:06:01,829 --> 01:06:07,560
 make these algorithms practical is is

2746
01:06:05,290 --> 01:06:07,560
 

2747
01:06:05,300 --> 01:06:08,940
 express it as this kind of reduction and

2748
01:06:07,550 --> 01:06:08,940
 

2749
01:06:07,560 --> 01:06:12,540
 then plug in a different learning

2750
01:06:08,930 --> 01:06:12,540
 

2751
01:06:08,940 --> 01:06:14,190
 algorithm and in particular so so in the

2752
01:06:12,530 --> 01:06:14,190
 

2753
01:06:12,540 --> 01:06:15,329
 general case we plug in any passive

2754
01:06:14,180 --> 01:06:15,329
 

2755
01:06:14,190 --> 01:06:18,089
 learning algorithm there may be you'll

2756
01:06:15,319 --> 01:06:18,089
 

2757
01:06:15,329 --> 01:06:19,950
 lose the theoretical guarantees but in

2758
01:06:18,079 --> 01:06:19,950
 

2759
01:06:18,089 --> 01:06:22,290
 some cases you can still maintain some

2760
01:06:19,940 --> 01:06:22,290
 

2761
01:06:19,950 --> 01:06:24,089
 guarantees so in particular if you

2762
01:06:22,280 --> 01:06:24,089
 

2763
01:06:22,290 --> 01:06:26,599
 consider a passive learning algorithm

2764
01:06:24,079 --> 01:06:26,599
 

2765
01:06:24,089 --> 01:06:30,740
 that minimizes we call a surrogate loss

2766
01:06:26,589 --> 01:06:30,740
 

2767
01:06:26,599 --> 01:06:33,290
 just a relaxation of the zero-one loss

2768
01:06:30,730 --> 01:06:33,290
 

2769
01:06:30,740 --> 01:06:35,270
 hinge loss or squared loss exponential

2770
01:06:33,280 --> 01:06:35,270
 

2771
01:06:33,290 --> 01:06:37,520
 loss these are some popular losses right

2772
01:06:35,260 --> 01:06:37,520
 

2773
01:06:35,270 --> 01:06:41,320
 so now you have some function class that

2774
01:06:37,510 --> 01:06:41,320
 

2775
01:06:37,520 --> 01:06:44,200
 has real valued functions and then

2776
01:06:41,310 --> 01:06:44,200
 

2777
01:06:41,320 --> 01:06:47,000
 basically you can minimize this

2778
01:06:44,190 --> 01:06:47,000
 

2779
01:06:44,200 --> 01:06:51,920
 surrogate loss function in that step

2780
01:06:46,990 --> 01:06:51,920
 

2781
01:06:47,000 --> 01:06:53,450
 instead of the zero one loss and that's

2782
01:06:51,910 --> 01:06:53,450
 

2783
01:06:51,920 --> 01:06:57,500
 an algorithm that often can be run

2784
01:06:53,440 --> 01:06:57,500
 

2785
01:06:53,450 --> 01:06:59,540
 efficiently and if you make some

2786
01:06:57,490 --> 01:06:59,540
 

2787
01:06:57,500 --> 01:07:00,859
 additional assumptions so if you have

2788
01:06:59,530 --> 01:07:00,859
 

2789
01:06:59,540 --> 01:07:04,130
 this bounded noise and then some

2790
01:07:00,849 --> 01:07:04,130
 

2791
01:07:00,859 --> 01:07:08,240
 additional fairly strong assumptions

2792
01:07:04,120 --> 01:07:08,240
 

2793
01:07:04,130 --> 01:07:09,980
 actually on how the hypothesis class and

2794
01:07:08,230 --> 01:07:09,980
 

2795
01:07:08,240 --> 01:07:13,010
 loss function and distribution all sort

2796
01:07:09,970 --> 01:07:13,010
 

2797
01:07:09,980 --> 01:07:15,230
 of work together well then you can still

2798
01:07:13,000 --> 01:07:15,230
 

2799
01:07:13,010 --> 01:07:17,150
 get this kind of guarantee that you get

2800
01:07:15,220 --> 01:07:17,150
 

2801
01:07:15,230 --> 01:07:19,369
 excess error rate under the zero one

2802
01:07:17,140 --> 01:07:19,369
 

2803
01:07:17,150 --> 01:07:21,380
 loss at most Epsilon using a number of

2804
01:07:19,359 --> 01:07:21,380
 

2805
01:07:19,369 --> 01:07:25,460
 labels that's like still like theta d

2806
01:07:21,370 --> 01:07:25,460
 

2807
01:07:21,380 --> 01:07:28,190
 log 1 over Epsilon and there are various

2808
01:07:25,450 --> 01:07:28,190
 

2809
01:07:25,460 --> 01:07:34,130
 other kinds of results that you can also

2810
01:07:28,180 --> 01:07:34,130
 

2811
01:07:28,190 --> 01:07:35,780
 express okay and then I'll just conclude

2812
01:07:34,120 --> 01:07:35,780
 

2813
01:07:34,130 --> 01:07:39,589
 with one other modification one other

2814
01:07:35,770 --> 01:07:39,589
 

2815
01:07:35,780 --> 01:07:44,869
 variant of this that that is definitely

2816
01:07:39,579 --> 01:07:44,869
 

2817
01:07:39,589 --> 01:07:48,440
 worth mentioning so we've been talking

2818
01:07:44,859 --> 01:07:48,440
 

2819
01:07:44,869 --> 01:07:50,000
 about strategies that have like a set

2820
01:07:48,430 --> 01:07:50,000
 

2821
01:07:48,440 --> 01:07:51,640
 and they'll query if the point is in the

2822
01:07:49,990 --> 01:07:51,640
 

2823
01:07:50,000 --> 01:07:55,670
 set but you can also talk about

2824
01:07:51,630 --> 01:07:55,670
 

2825
01:07:51,640 --> 01:07:57,890
 strategies that set a probability of

2826
01:07:55,660 --> 01:07:57,890
 

2827
01:07:55,670 --> 01:07:59,859
 querying and this gets into this

2828
01:07:57,880 --> 01:07:59,859
 

2829
01:07:57,890 --> 01:08:04,130
 importance weighted active learning idea

2830
01:07:59,849 --> 01:08:04,130
 

2831
01:07:59,859 --> 01:08:06,170
 so the idea there is on each round you

2832
01:08:04,120 --> 01:08:06,170
 

2833
01:08:04,130 --> 01:08:08,660
 get this X and now let's just set a

2834
01:08:06,160 --> 01:08:08,660
 

2835
01:08:06,170 --> 01:08:10,550
 probability and say P sub X that you

2836
01:08:08,650 --> 01:08:10,550
 

2837
01:08:08,660 --> 01:08:14,060
 query that point then you flip a coin

2838
01:08:10,540 --> 01:08:14,060
 

2839
01:08:10,550 --> 01:08:17,239
 with that probability as its bias if it

2840
01:08:14,050 --> 01:08:17,239
 

2841
01:08:14,060 --> 01:08:19,900
 lands heads you query if it lands tails

2842
01:08:17,229 --> 01:08:19,900
 

2843
01:08:17,239 --> 01:08:22,940
 you don't but if you do query the label

2844
01:08:19,890 --> 01:08:22,940
 

2845
01:08:19,900 --> 01:08:27,730
 add it to the data set but edit with a

2846
01:08:22,930 --> 01:08:27,730
 

2847
01:08:22,940 --> 01:08:30,200
 weight 1 over P or 1 over P sub X ok and

2848
01:08:27,720 --> 01:08:30,200
 

2849
01:08:27,730 --> 01:08:33,859
 the reason you do this is because the

2850
01:08:30,190 --> 01:08:33,859
 

2851
01:08:30,200 --> 01:08:37,489
 the weighted empirical risk with those

2852
01:08:33,849 --> 01:08:37,489
 

2853
01:08:33,859 --> 01:08:38,970
 weights on the loss ends up being an

2854
01:08:37,479 --> 01:08:38,970
 

2855
01:08:37,489 --> 01:08:43,740
 unbiased estimator

2856
01:08:38,960 --> 01:08:43,740
 

2857
01:08:38,970 --> 01:08:45,780
 of the actual risk and that's a very

2858
01:08:43,730 --> 01:08:45,780
 

2859
01:08:43,740 --> 01:08:47,970
 general statement so no matter how you

2860
01:08:45,770 --> 01:08:47,970
 

2861
01:08:45,780 --> 01:08:50,100
 set these peace of X's you're going to

2862
01:08:47,960 --> 01:08:50,100
 

2863
01:08:47,970 --> 01:08:52,050
 get an unbiased estimator of the

2864
01:08:50,090 --> 01:08:52,050
 

2865
01:08:50,100 --> 01:08:54,300
 empirical risk you don't want to set

2866
01:08:52,040 --> 01:08:54,300
 

2867
01:08:52,050 --> 01:08:55,470
 them too small if you want to get you

2868
01:08:54,290 --> 01:08:55,470
 

2869
01:08:54,300 --> 01:08:58,440
 know because there's some issues with

2870
01:08:55,460 --> 01:08:58,440
 

2871
01:08:55,470 --> 01:09:01,970
 variance and and and so on but you know

2872
01:08:58,430 --> 01:09:01,970
 

2873
01:08:58,440 --> 01:09:04,260
 basically any choice of setting these px

2874
01:09:01,960 --> 01:09:04,260
 

2875
01:09:01,970 --> 01:09:07,430
 probabilities is fine you know maybe

2876
01:09:04,250 --> 01:09:07,430
 

2877
01:09:04,260 --> 01:09:10,470
 just don't make it too small and that

2878
01:09:07,420 --> 01:09:10,470
 

2879
01:09:07,430 --> 01:09:14,600
 gives you some flexibility to plug in

2880
01:09:10,460 --> 01:09:14,600
 

2881
01:09:10,470 --> 01:09:16,830
 various ideas various heuristics for

2882
01:09:14,590 --> 01:09:16,830
 

2883
01:09:14,600 --> 01:09:21,600
 designing active learning algorithms and

2884
01:09:16,820 --> 01:09:21,600
 

2885
01:09:16,830 --> 01:09:23,520
 out of that you'll get still some kind

2886
01:09:21,590 --> 01:09:23,520
 

2887
01:09:21,600 --> 01:09:25,290
 of a safety guarantee because you're

2888
01:09:23,510 --> 01:09:25,290
 

2889
01:09:23,520 --> 01:09:28,380
 dealing with an unbiased estimator of

2890
01:09:25,280 --> 01:09:28,380
 

2891
01:09:25,290 --> 01:09:31,020
 the risk and I'll mention it's still

2892
01:09:28,370 --> 01:09:31,020
 

2893
01:09:28,380 --> 01:09:32,670
 possible to recover the disagreement

2894
01:09:31,010 --> 01:09:32,670
 

2895
01:09:31,020 --> 01:09:34,980
 based active learning just by plugging

2896
01:09:32,660 --> 01:09:34,980
 

2897
01:09:32,670 --> 01:09:38,910
 in a sort of Bernstein indicator

2898
01:09:34,970 --> 01:09:38,910
 

2899
01:09:34,980 --> 01:09:43,250
 function for the the px value so you

2900
01:09:38,900 --> 01:09:43,250
 

2901
01:09:38,910 --> 01:09:45,270
 don't really lose something in terms of

2902
01:09:43,240 --> 01:09:45,270
 

2903
01:09:43,250 --> 01:09:46,860
 theoretical guarantees by expressing

2904
01:09:45,260 --> 01:09:46,860
 

2905
01:09:45,270 --> 01:09:48,240
 things this ways but you do gain a

2906
01:09:46,850 --> 01:09:48,240
 

2907
01:09:46,860 --> 01:09:50,580
 little bit of flexibility and for that

2908
01:09:48,230 --> 01:09:50,580
 

2909
01:09:48,240 --> 01:09:55,440
 reason that this has been used actually

2910
01:09:50,570 --> 01:09:55,440
 

2911
01:09:50,580 --> 01:09:57,270
 in practical context and right and of

2912
01:09:55,430 --> 01:09:57,270
 

2913
01:09:55,440 --> 01:09:58,920
 course it yeah at the end we are running

2914
01:09:57,260 --> 01:09:58,920
 

2915
01:09:57,270 --> 01:10:00,630
 an empirical risk minimization step but

2916
01:09:58,910 --> 01:10:00,630
 

2917
01:09:58,920 --> 01:10:06,570
 you could use again any passive learning

2918
01:10:00,620 --> 01:10:06,570
 

2919
01:10:00,630 --> 01:10:08,640
 algorithm there in practice and there is

2920
01:10:06,560 --> 01:10:08,640
 

2921
01:10:06,570 --> 01:10:11,970
 an approximate implementation of this

2922
01:10:08,630 --> 01:10:11,970
 

2923
01:10:08,640 --> 01:10:15,780
 strategy in the vocal webbot software

2924
01:10:11,960 --> 01:10:15,780
 

2925
01:10:11,970 --> 01:10:21,480
 library it's maintained by Langford it's

2926
01:10:15,770 --> 01:10:21,480
 

2927
01:10:15,780 --> 01:10:25,200
 an cohorts okay so that's the conclusion

2928
01:10:21,470 --> 01:10:25,200
 

2929
01:10:21,480 --> 01:10:27,090
 of the second part we will get into

2930
01:10:25,190 --> 01:10:27,090
 

2931
01:10:25,200 --> 01:10:29,130
 other techniques beyond disagreement

2932
01:10:27,080 --> 01:10:29,130
 

2933
01:10:27,090 --> 01:10:30,540
 based active learning in the third part

2934
01:10:29,120 --> 01:10:30,540
 

2935
01:10:29,130 --> 01:10:33,690
 but I'm happy to take any questions

2936
01:10:30,530 --> 01:10:33,690
 

2937
01:10:30,540 --> 01:10:37,670
 right now and otherwise we will be

2938
01:10:33,680 --> 01:10:37,670
 

2939
01:10:33,690 --> 01:10:39,730
 taking a short maybe five minute break

2940
01:10:37,660 --> 01:10:39,730
 

2941
01:10:37,670 --> 01:10:41,740
 between

2942
01:10:39,720 --> 01:10:41,740
 

2943
01:10:39,730 --> 01:10:49,930
 so yeah if there any questions please

2944
01:10:41,730 --> 01:10:49,930
 

2945
01:10:41,740 --> 01:10:54,460
 step up to a microphone so they the

2946
01:10:49,920 --> 01:10:54,460
 

2947
01:10:49,930 --> 01:10:56,830
 second part the algorithms had sampling

2948
01:10:54,450 --> 01:10:56,830
 

2949
01:10:54,460 --> 01:10:59,290
 just one X but in the first algorithm

2950
01:10:56,820 --> 01:10:59,290
 

2951
01:10:56,830 --> 01:11:03,100
 you were sampling two to the TX where

2952
01:10:59,280 --> 01:11:03,100
 

2953
01:10:59,290 --> 01:11:07,270
 these day duration so can you comment on

2954
01:11:03,090 --> 01:11:07,270
 

2955
01:11:03,100 --> 01:11:09,460
 the practicality of that and what is the

2956
01:11:07,260 --> 01:11:09,460
 

2957
01:11:07,270 --> 01:11:10,960
 trade-off between the number of samples

2958
01:11:09,450 --> 01:11:10,960
 

2959
01:11:09,460 --> 01:11:12,430
 and you know it seems very extreme that

2960
01:11:10,950 --> 01:11:12,430
 

2961
01:11:10,960 --> 01:11:16,030
 one is exponential and the other one is

2962
01:11:12,420 --> 01:11:16,030
 

2963
01:11:12,430 --> 01:11:18,070
 just one right okay so I'll give you two

2964
01:11:16,020 --> 01:11:18,070
 

2965
01:11:16,030 --> 01:11:22,140
 answers for this one from a theory

2966
01:11:18,060 --> 01:11:22,140
 

2967
01:11:18,070 --> 01:11:24,670
 perspective the sampling and batches of

2968
01:11:22,130 --> 01:11:24,670
 

2969
01:11:22,140 --> 01:11:26,530
 exponentially increasing size gave us an

2970
01:11:24,660 --> 01:11:26,530
 

2971
01:11:24,670 --> 01:11:28,270
 actual actually a simplification of the

2972
01:11:26,520 --> 01:11:28,270
 

2973
01:11:26,530 --> 01:11:31,360
 proof that the proof for sampling one at

2974
01:11:28,260 --> 01:11:31,360
 

2975
01:11:28,270 --> 01:11:33,070
 a time is a bit Messier because we were

2976
01:11:31,350 --> 01:11:33,070
 

2977
01:11:31,360 --> 01:11:35,140
 sampling two to the T actually in that

2978
01:11:33,060 --> 01:11:35,140
 

2979
01:11:33,070 --> 01:11:37,720
 proof we ended up querying about D times

2980
01:11:35,130 --> 01:11:37,720
 

2981
01:11:35,140 --> 01:11:39,490
 theta on each iteration and that's kind

2982
01:11:37,710 --> 01:11:39,490
 

2983
01:11:37,720 --> 01:11:41,770
 of a nice number because you're gonna

2984
01:11:39,480 --> 01:11:41,770
 

2985
01:11:39,490 --> 01:11:43,300
 add it up a logarithmic number of rounds

2986
01:11:41,760 --> 01:11:43,300
 

2987
01:11:41,770 --> 01:11:45,190
 and yeah so there's a lot of

2988
01:11:43,290 --> 01:11:45,190
 

2989
01:11:43,300 --> 01:11:47,320
 simplification in the proof by having

2990
01:11:45,180 --> 01:11:47,320
 

2991
01:11:45,190 --> 01:11:50,440
 this exponential increase in the

2992
01:11:47,310 --> 01:11:50,440
 

2993
01:11:47,320 --> 01:11:52,150
 unlabeled data from a practical

2994
01:11:50,430 --> 01:11:52,150
 

2995
01:11:50,440 --> 01:11:54,720
 perspective there may actually be some

2996
01:11:52,140 --> 01:11:54,720
 

2997
01:11:52,150 --> 01:11:57,250
 reason to want larger batch sizes

2998
01:11:54,710 --> 01:11:57,250
 

2999
01:11:54,720 --> 01:11:59,380
 because of course you have some team of

3000
01:11:57,240 --> 01:11:59,380
 

3001
01:11:57,250 --> 01:12:01,330
 labelers and you if you give them more

3002
01:11:59,370 --> 01:12:01,330
 

3003
01:11:59,380 --> 01:12:03,580
 at once they can label pretty fast

3004
01:12:01,320 --> 01:12:03,580
 

3005
01:12:01,330 --> 01:12:05,110
 actually if it's one at a time then

3006
01:12:03,570 --> 01:12:05,110
 

3007
01:12:03,580 --> 01:12:07,750
 they're sort of waiting on the algorithm

3008
01:12:05,100 --> 01:12:07,750
 

3009
01:12:05,110 --> 01:12:10,720
 to give them the next query so yeah

3010
01:12:07,740 --> 01:12:10,720
 

3011
01:12:07,750 --> 01:12:14,200
 batches make sense in practice in many

3012
01:12:10,710 --> 01:12:14,200
 

3013
01:12:10,720 --> 01:12:15,820
 contexts also but it is kind of nice to

3014
01:12:14,190 --> 01:12:15,820
 

3015
01:12:14,200 --> 01:12:17,740
 have a simple algorithm that just has

3016
01:12:15,810 --> 01:12:17,740
 

3017
01:12:15,820 --> 01:12:19,890
 one at a time also expensive there's

3018
01:12:17,730 --> 01:12:19,890
 

3019
01:12:17,740 --> 01:12:19,890
 some

3020
01:12:22,859 --> 01:12:22,859
 

3021
01:12:22,869 --> 01:12:29,980
 okay can I have as a question so my

3022
01:12:26,970 --> 01:12:29,980
 

3023
01:12:26,980 --> 01:12:33,340
 question is is there any no minimax

3024
01:12:29,970 --> 01:12:33,340
 

3025
01:12:29,980 --> 01:12:36,340
 lower bound regarding the label

3026
01:12:33,330 --> 01:12:36,340
 

3027
01:12:33,340 --> 01:12:38,949
 complexity of this problem and if there

3028
01:12:36,330 --> 01:12:38,949
 

3029
01:12:36,340 --> 01:12:41,110
 is there any minimax lower bound so how

3030
01:12:38,939 --> 01:12:41,110
 

3031
01:12:38,949 --> 01:12:44,560
 is the performance of the algorithm that

3032
01:12:41,100 --> 01:12:44,560
 

3033
01:12:41,110 --> 01:12:46,929
 you just introduced compared with the

3034
01:12:44,550 --> 01:12:46,929
 

3035
01:12:44,560 --> 01:12:48,010
 minimax lower bound in terms of the

3036
01:12:46,919 --> 01:12:48,010
 

3037
01:12:46,929 --> 01:12:50,130
 label complexity

3038
01:12:48,000 --> 01:12:50,130
 

3039
01:12:48,010 --> 01:12:52,060
 thank you yeah it's a great question

3040
01:12:50,120 --> 01:12:52,060
 

3041
01:12:50,130 --> 01:12:55,800
 we're actually going to talk a bit about

3042
01:12:52,050 --> 01:12:55,800
 

3043
01:12:52,060 --> 01:12:58,480
 lower balance in the the next segment

3044
01:12:55,790 --> 01:12:58,480
 

3045
01:12:55,800 --> 01:13:00,969
 the lower bound I'll say the lower bound

3046
01:12:58,470 --> 01:13:00,969
 

3047
01:12:58,480 --> 01:13:02,830
 for agnostic learning doesn't have the

3048
01:13:00,959 --> 01:13:02,830
 

3049
01:13:00,969 --> 01:13:04,150
 disagreement coefficient in it and this

3050
01:13:02,820 --> 01:13:04,150
 

3051
01:13:02,830 --> 01:13:05,949
 is actually an open problem about

3052
01:13:04,140 --> 01:13:05,949
 

3053
01:13:04,150 --> 01:13:08,949
 whether you can generally remove that

3054
01:13:05,939 --> 01:13:08,949
 

3055
01:13:05,949 --> 01:13:11,440
 and I'll get into a lot more detail

3056
01:13:08,939 --> 01:13:11,440
 

3057
01:13:08,949 --> 01:13:15,429
 about that in the next segment and

3058
01:13:11,430 --> 01:13:15,429
 

3059
01:13:11,440 --> 01:13:16,780
 bounded noise case it's it can sometimes

3060
01:13:15,419 --> 01:13:16,780
 

3061
01:13:15,429 --> 01:13:19,330
 be loose but we actually there is

3062
01:13:16,770 --> 01:13:19,330
 

3063
01:13:16,780 --> 01:13:22,750
 actually a known tight bound for the

3064
01:13:19,320 --> 01:13:22,750
 

3065
01:13:19,330 --> 01:13:24,099
 bounded noise case which I won't

3066
01:13:22,740 --> 01:13:24,099
 

3067
01:13:22,750 --> 01:13:25,420
 unfortunately I won't have time to cover

3068
01:13:24,089 --> 01:13:25,420
 

3069
01:13:24,099 --> 01:13:27,969
 here but if you're interested you can

3070
01:13:25,410 --> 01:13:27,969
 

3071
01:13:25,420 --> 01:13:30,489
 feel free to talk to me offline yeah

3072
01:13:27,959 --> 01:13:30,489
 

3073
01:13:27,969 --> 01:13:31,900
 we'll talk about there a tight bound for

3074
01:13:30,479 --> 01:13:31,900
 

3075
01:13:30,489 --> 01:13:33,790
 the distribution free analysis of

3076
01:13:31,890 --> 01:13:33,790
 

3077
01:13:31,900 --> 01:13:35,619
 bounded noise case but even in the

3078
01:13:33,780 --> 01:13:35,619
 

3079
01:13:33,790 --> 01:13:39,010
 distribution dependent case we know of a

3080
01:13:35,609 --> 01:13:39,010
 

3081
01:13:35,619 --> 01:13:41,800
 tight bound and we can talk my question

3082
01:13:39,000 --> 01:13:41,800
 

3083
01:13:39,010 --> 01:13:43,659
 is also related to the batch setting in

3084
01:13:41,790 --> 01:13:43,659
 

3085
01:13:41,800 --> 01:13:45,760
 my experience is that starting is quite

3086
01:13:43,649 --> 01:13:45,760
 

3087
01:13:43,659 --> 01:13:48,130
 complicated usually because you need

3088
01:13:45,750 --> 01:13:48,130
 

3089
01:13:45,760 --> 01:13:49,900
 some kind of diversity term or something

3090
01:13:48,120 --> 01:13:49,900
 

3091
01:13:48,130 --> 01:13:52,389
 that tells you which vegetables to

3092
01:13:49,890 --> 01:13:52,389
 

3093
01:13:49,900 --> 01:13:54,429
 select because just taking the and best

3094
01:13:52,379 --> 01:13:54,429
 

3095
01:13:52,389 --> 01:13:57,550
 or whatever based on a certain criteria

3096
01:13:54,419 --> 01:13:57,550
 

3097
01:13:54,429 --> 01:14:01,630
 usually doesn't lead to enough diversity

3098
01:13:57,540 --> 01:14:01,630
 

3099
01:13:57,550 --> 01:14:06,429
 in the samples do you have any take on

3100
01:14:01,620 --> 01:14:06,429
 

3101
01:14:01,630 --> 01:14:09,190
 good criteria for better selection right

3102
01:14:06,419 --> 01:14:09,190
 

3103
01:14:06,429 --> 01:14:12,340
 so this approach is not actually doing

3104
01:14:09,180 --> 01:14:12,340
 

3105
01:14:09,190 --> 01:14:14,170
 anything with we don't actually need

3106
01:14:12,330 --> 01:14:14,170
 

3107
01:14:12,340 --> 01:14:15,520
 anything to do with diversity in this

3108
01:14:14,160 --> 01:14:15,520
 

3109
01:14:14,170 --> 01:14:22,030
 approach we so we just have this notion

3110
01:14:15,510 --> 01:14:22,030
 

3111
01:14:15,520 --> 01:14:23,889
 of disagreement and yeah it's it doesn't

3112
01:14:22,020 --> 01:14:23,889
 

3113
01:14:22,030 --> 01:14:27,099
 sort of come up in this context I think

3114
01:14:23,879 --> 01:14:27,099
 

3115
01:14:23,889 --> 01:14:28,960
 it will come up maybe in Rob's fourth

3116
01:14:27,089 --> 01:14:28,960
 

3117
01:14:27,099 --> 01:14:30,580
 part when we talk about he's going to

3118
01:14:28,950 --> 01:14:30,580
 

3119
01:14:28,960 --> 01:14:31,489
 mention a little bit about a cluster

3120
01:14:30,570 --> 01:14:31,489
 

3121
01:14:30,580 --> 01:14:33,410
 based active

3122
01:14:31,479 --> 01:14:33,410
 

3123
01:14:31,489 --> 01:14:36,290
 and then definitely there's some

3124
01:14:33,400 --> 01:14:36,290
 

3125
01:14:33,410 --> 01:14:37,910
 diversity that comes into play but in

3126
01:14:36,280 --> 01:14:37,910
 

3127
01:14:36,290 --> 01:14:40,790
 this context yeah that's not really

3128
01:14:37,900 --> 01:14:40,790
 

3129
01:14:37,910 --> 01:14:44,290
 modeled it might be implicit in there

3130
01:14:40,780 --> 01:14:44,290
 

3131
01:14:40,790 --> 01:14:44,290
 somewhere but it's not really

3132
01:14:47,170 --> 01:14:47,170
 

3133
01:14:47,180 --> 01:14:52,040
 yeah with so with the same amount of

3134
01:14:49,540 --> 01:14:52,040
 

3135
01:14:49,550 --> 01:14:53,870
 data points as active learning and

3136
01:14:52,030 --> 01:14:53,870
 

3137
01:14:52,040 --> 01:14:58,730
 passive learning a converging the end or

3138
01:14:53,860 --> 01:14:58,730
 

3139
01:14:53,870 --> 01:15:01,250
 it's doing better I guess converge like

3140
01:14:58,720 --> 01:15:01,250
 

3141
01:14:58,730 --> 01:15:03,469
 if I do active learning but use the same

3142
01:15:01,240 --> 01:15:03,469
 

3143
01:15:01,250 --> 01:15:05,780
 amount of data points as I would do with

3144
01:15:03,459 --> 01:15:05,780
 

3145
01:15:03,469 --> 01:15:07,730
 passive learning what I keep the same

3146
01:15:05,770 --> 01:15:07,730
 

3147
01:15:05,780 --> 01:15:14,270
 like classifier saying the same number

3148
01:15:07,720 --> 01:15:14,270
 

3149
01:15:07,730 --> 01:15:16,250
 of unlabeled data points yes I don't

3150
01:15:14,260 --> 01:15:16,250
 

3151
01:15:14,270 --> 01:15:18,969
 think you get the exact same classifier

3152
01:15:16,240 --> 01:15:18,969
 

3153
01:15:16,250 --> 01:15:18,969
 just because of

3154
01:15:23,330 --> 01:15:23,330
 

3155
01:15:23,340 --> 01:15:27,750
 okay you might be able to prove like

3156
01:15:25,760 --> 01:15:27,750
 

3157
01:15:25,770 --> 01:15:29,370
 with some high probability that it would

3158
01:15:27,740 --> 01:15:29,370
 

3159
01:15:27,750 --> 01:15:32,070
 be the exact same classifier actually

3160
01:15:29,360 --> 01:15:32,070
 

3161
01:15:29,370 --> 01:15:33,660
 but that some of that might depend on

3162
01:15:32,060 --> 01:15:33,660
 

3163
01:15:32,070 --> 01:15:35,940
 how you break ties in your empirical

3164
01:15:33,650 --> 01:15:35,940
 

3165
01:15:33,660 --> 01:15:37,590
 risk minimization and things like that

3166
01:15:35,930 --> 01:15:37,590
 

3167
01:15:35,940 --> 01:15:39,300
 so what you would have to show is that

3168
01:15:37,580 --> 01:15:39,300
 

3169
01:15:37,590 --> 01:15:41,310
 on each round you're not just keeping a

3170
01:15:39,290 --> 01:15:41,310
 

3171
01:15:39,300 --> 01:15:43,500
 star you're also keeping the global

3172
01:15:41,300 --> 01:15:43,500
 

3173
01:15:41,310 --> 01:15:45,510
 empirical risk minimization but the

3174
01:15:43,490 --> 01:15:45,510
 

3175
01:15:43,500 --> 01:15:47,610
 global empirical risk minimizer which I

3176
01:15:45,500 --> 01:15:47,610
 

3177
01:15:45,510 --> 01:15:49,590
 think you do with high probability but

3178
01:15:47,600 --> 01:15:49,590
 

3179
01:15:47,610 --> 01:15:52,140
 that would be something that comes out

3180
01:15:49,580 --> 01:15:52,140
 

3181
01:15:49,590 --> 01:15:59,610
 of the analysis not necessarily I

3182
01:15:52,130 --> 01:15:59,610
 

3183
01:15:52,140 --> 01:16:02,550
 guarantee you Thanks so I have a

3184
01:15:59,600 --> 01:16:02,550
 

3185
01:15:59,610 --> 01:16:06,210
 question about the choice of epsilon

3186
01:16:02,540 --> 01:16:06,210
 

3187
01:16:02,550 --> 01:16:08,700
 since the bounds that you use to define

3188
01:16:06,200 --> 01:16:08,700
 

3189
01:16:06,210 --> 01:16:11,880
 the region of disagreement seem to

3190
01:16:08,690 --> 01:16:11,880
 

3191
01:16:08,700 --> 01:16:15,090
 depend on epsilon it seems like the

3192
01:16:11,870 --> 01:16:15,090
 

3193
01:16:11,880 --> 01:16:16,800
 algorithm is not a priori independent to

3194
01:16:15,080 --> 01:16:16,800
 

3195
01:16:15,090 --> 01:16:21,630
 like your choice Absalon so how does

3196
01:16:16,790 --> 01:16:21,630
 

3197
01:16:16,800 --> 01:16:23,570
 your choice of epsilon affect like how

3198
01:16:21,620 --> 01:16:23,570
 

3199
01:16:21,630 --> 01:16:26,690
 quickly the algorithm will eventually

3200
01:16:23,560 --> 01:16:26,690
 

3201
01:16:23,570 --> 01:16:29,730
 reduce the size of the hypothesis and so

3202
01:16:26,680 --> 01:16:29,730
 

3203
01:16:26,690 --> 01:16:32,700
 this gets back actually to I have this

3204
01:16:29,720 --> 01:16:32,700
 

3205
01:16:29,730 --> 01:16:34,140
 slide here the stopping criterion so you

3206
01:16:32,690 --> 01:16:34,140
 

3207
01:16:32,700 --> 01:16:35,580
 know if you use the stopping criteria

3208
01:16:34,130 --> 01:16:35,580
 

3209
01:16:34,140 --> 01:16:36,840
 that don't involve epsilon then there's

3210
01:16:35,570 --> 01:16:36,840
 

3211
01:16:35,580 --> 01:16:38,910
 there's no epsilon showing up in the

3212
01:16:36,830 --> 01:16:38,910
 

3213
01:16:36,840 --> 01:16:40,980
 algorithm it's just the guarantee there

3214
01:16:38,900 --> 01:16:40,980
 

3215
01:16:38,910 --> 01:16:42,630
 is just saying if I happen to have that

3216
01:16:40,970 --> 01:16:42,630
 

3217
01:16:40,980 --> 01:16:44,670
 many labels if I happen to have queried

3218
01:16:42,620 --> 01:16:44,670
 

3219
01:16:42,630 --> 01:16:46,580
 that many labels then I'll have this

3220
01:16:44,660 --> 01:16:46,580
 

3221
01:16:44,670 --> 01:16:50,730
 excess risk guarantee at most Epsilon

3222
01:16:46,570 --> 01:16:50,730
 

3223
01:16:46,580 --> 01:16:52,380
 but you can also express it just in

3224
01:16:50,720 --> 01:16:52,380
 

3225
01:16:50,730 --> 01:16:54,120
 terms of how many labels did I query you

3226
01:16:52,370 --> 01:16:54,120
 

3227
01:16:52,380 --> 01:16:55,860
 can get a bound basically just solved

3228
01:16:54,110 --> 01:16:55,860
 

3229
01:16:54,120 --> 01:16:58,010
 that set and equal to that and solve for

3230
01:16:55,850 --> 01:16:58,010
 

3231
01:16:55,860 --> 01:17:02,760
 N and that would give you the bound on

3232
01:16:58,000 --> 01:17:02,760
 

3233
01:16:58,010 --> 01:17:04,680
 exactly I don't know yeah like this

3234
01:17:02,750 --> 01:17:04,680
 

3235
01:17:02,760 --> 01:17:06,570
 right so here we've also expressed

3236
01:17:04,670 --> 01:17:06,570
 

3237
01:17:04,680 --> 01:17:10,010
 things in terms of the number of labels

3238
01:17:06,560 --> 01:17:10,010
 

3239
01:17:06,570 --> 01:17:12,360
 that we queried on the right hand side

3240
01:17:10,000 --> 01:17:12,360
 

3241
01:17:10,010 --> 01:17:13,950
 right so that so if you didn't have

3242
01:17:12,350 --> 01:17:13,950
 

3243
01:17:12,360 --> 01:17:16,200
 epsilon if you don't want to set an

3244
01:17:13,940 --> 01:17:16,200
 

3245
01:17:13,950 --> 01:17:17,700
 epsilon then you can just run the

3246
01:17:16,190 --> 01:17:17,700
 

3247
01:17:16,200 --> 01:17:23,060
 algorithm up to whatever stopping

3248
01:17:17,690 --> 01:17:23,060
 

3249
01:17:17,700 --> 01:17:23,060
 criterion you want okay thank you

3250
01:17:26,359 --> 01:17:26,359
 

3251
01:17:26,369 --> 01:17:36,920
 okay so maybe we'll just start on into

3252
01:17:29,479 --> 01:17:36,920
 

3253
01:17:29,489 --> 01:17:36,920
 the next part we're all good questions

3254
01:17:53,600 --> 01:17:53,600
 

3255
01:17:53,610 --> 01:17:59,910
 okay so we've covered sort of the basic

3256
01:17:57,770 --> 01:17:59,910
 

3257
01:17:57,780 --> 01:18:03,270
 theory that a lot of the literature is

3258
01:17:59,900 --> 01:18:03,270
 

3259
01:17:59,910 --> 01:18:08,790
 based on what we're gonna get into in

3260
01:18:03,260 --> 01:18:08,790
 

3261
01:18:03,270 --> 01:18:11,310
 this third part is essentially a sampler

3262
01:18:08,780 --> 01:18:11,310
 

3263
01:18:08,790 --> 01:18:14,100
 platter of different various ways that

3264
01:18:11,300 --> 01:18:14,100
 

3265
01:18:11,310 --> 01:18:16,170
 people have proposed to get a little bit

3266
01:18:14,090 --> 01:18:16,170
 

3267
01:18:14,100 --> 01:18:18,180
 better than disagreement based active

3268
01:18:16,160 --> 01:18:18,180
 

3269
01:18:16,170 --> 01:18:20,220
 learning so it's knowing that it's

3270
01:18:18,170 --> 01:18:20,220
 

3271
01:18:18,180 --> 01:18:22,260
 disagreement base active learning so

3272
01:18:20,210 --> 01:18:22,260
 

3273
01:18:20,220 --> 01:18:23,580
 it's it's a it's a popular technique in

3274
01:18:22,250 --> 01:18:23,580
 

3275
01:18:22,260 --> 01:18:25,650
 the theory literature because it's

3276
01:18:23,570 --> 01:18:25,650
 

3277
01:18:23,580 --> 01:18:27,150
 simple it has a very simple complexity

3278
01:18:25,640 --> 01:18:27,150
 

3279
01:18:25,650 --> 01:18:30,720
 measure that you can calculate very

3280
01:18:27,140 --> 01:18:30,720
 

3281
01:18:27,150 --> 01:18:33,000
 easily for a lot of spaces and it's

3282
01:18:30,710 --> 01:18:33,000
 

3283
01:18:30,720 --> 01:18:35,610
 robust to noise and for those reasons

3284
01:18:32,990 --> 01:18:35,610
 

3285
01:18:33,000 --> 01:18:38,790
 it's sort of a lot of papers build on

3286
01:18:35,600 --> 01:18:38,790
 

3287
01:18:35,610 --> 01:18:39,750
 that but there if you're more careful in

3288
01:18:38,780 --> 01:18:39,750
 

3289
01:18:38,790 --> 01:18:41,880
 the way that you approach the problem

3290
01:18:39,740 --> 01:18:41,880
 

3291
01:18:39,750 --> 01:18:43,860
 you can get improvements over it as I

3292
01:18:41,870 --> 01:18:43,860
 

3293
01:18:41,880 --> 01:18:46,170
 say it's it's no not that it's not

3294
01:18:43,850 --> 01:18:46,170
 

3295
01:18:43,860 --> 01:18:49,800
 always optimal and so we'll talk about

3296
01:18:46,160 --> 01:18:49,800
 

3297
01:18:46,170 --> 01:18:52,560
 some of those here the first I may not

3298
01:18:49,790 --> 01:18:52,560
 

3299
01:18:49,800 --> 01:18:55,080
 get to all these topics but the the

3300
01:18:52,550 --> 01:18:55,080
 

3301
01:18:52,560 --> 01:18:58,160
 first we'll talk about is a sub-region

3302
01:18:55,070 --> 01:18:58,160
 

3303
01:18:55,080 --> 01:19:01,260
 based active learning so the idea here

3304
01:18:58,150 --> 01:19:01,260
 

3305
01:18:58,160 --> 01:19:02,940
 and this is from Shannon Chowdhury but

3306
01:19:01,250 --> 01:19:02,940
 

3307
01:19:01,260 --> 01:19:04,770
 it builds on a lot of earlier work and

3308
01:19:02,930 --> 01:19:04,770
 

3309
01:19:02,940 --> 01:19:06,270
 there's there's a whole line of work on

3310
01:19:04,760 --> 01:19:06,270
 

3311
01:19:04,770 --> 01:19:10,250
 what's called margin based active

3312
01:19:06,260 --> 01:19:10,250
 

3313
01:19:06,270 --> 01:19:13,050
 learning and we'll talk about that also

3314
01:19:10,240 --> 01:19:13,050
 

3315
01:19:10,250 --> 01:19:14,930
 okay so this is the basic a squared

3316
01:19:13,040 --> 01:19:14,930
 

3317
01:19:13,050 --> 01:19:18,450
 algorithm that we've just talked about

3318
01:19:14,920 --> 01:19:18,450
 

3319
01:19:14,930 --> 01:19:20,610
 and the idea in sub region based active

3320
01:19:18,440 --> 01:19:20,610
 

3321
01:19:18,450 --> 01:19:22,350
 learning instead of taking the whole

3322
01:19:20,600 --> 01:19:22,350
 

3323
01:19:20,610 --> 01:19:25,320
 region of disagreement let's take a sub

3324
01:19:22,340 --> 01:19:25,320
 

3325
01:19:22,350 --> 01:19:27,240
 region let's take a smaller set that has

3326
01:19:25,310 --> 01:19:27,240
 

3327
01:19:25,320 --> 01:19:29,370
 some guarantee that for any two

3328
01:19:27,230 --> 01:19:29,370
 

3329
01:19:27,240 --> 01:19:31,290
 classifiers and that are left in the

3330
01:19:29,360 --> 01:19:31,290
 

3331
01:19:29,370 --> 01:19:33,990
 hypothesis class the probability that

3332
01:19:31,280 --> 01:19:33,990
 

3333
01:19:31,290 --> 01:19:36,990
 they disagree on a point that's outside

3334
01:19:33,980 --> 01:19:36,990
 

3335
01:19:33,990 --> 01:19:40,080
 of that region is at most some epsilon

3336
01:19:36,980 --> 01:19:40,080
 

3337
01:19:36,990 --> 01:19:41,610
 prime okay so they don't disagree too

3338
01:19:40,070 --> 01:19:41,610
 

3339
01:19:40,080 --> 01:19:44,070
 much outside of the region now of course

3340
01:19:41,600 --> 01:19:44,070
 

3341
01:19:41,610 --> 01:19:45,750
 we could just take this R sub epsilon

3342
01:19:44,060 --> 01:19:45,750
 

3343
01:19:44,070 --> 01:19:47,130
 prime to be the region of disagreement

3344
01:19:45,740 --> 01:19:47,130
 

3345
01:19:45,750 --> 01:19:48,810
 and then they never disagree outside the

3346
01:19:47,120 --> 01:19:48,810
 

3347
01:19:47,130 --> 01:19:51,660
 region but if you're careful you can

3348
01:19:48,800 --> 01:19:51,660
 

3349
01:19:48,810 --> 01:19:54,960
 sometimes choose a smaller set and get

3350
01:19:51,650 --> 01:19:54,960
 

3351
01:19:51,660 --> 01:19:56,400
 some benefits out of that and yet if you

3352
01:19:54,950 --> 01:19:56,400
 

3353
01:19:54,960 --> 01:19:58,020
 pick this epsilon Prime carefully you

3354
01:19:56,390 --> 01:19:58,020
 

3355
01:19:56,400 --> 01:20:00,060
 can still get this guarantee that after

3356
01:19:58,010 --> 01:20:00,060
 

3357
01:19:58,020 --> 01:20:02,910
 you run it for a some number of rounds

3358
01:20:00,050 --> 01:20:02,910
 

3359
01:20:00,060 --> 01:20:04,670
 you're going to get excess error rate at

3360
01:20:02,900 --> 01:20:04,670
 

3361
01:20:02,910 --> 01:20:07,190
 most Epsilon

3362
01:20:04,660 --> 01:20:07,190
 

3363
01:20:04,670 --> 01:20:09,800
 in particular for bounded noise the the

3364
01:20:07,180 --> 01:20:09,800
 

3365
01:20:07,190 --> 01:20:11,870
 if you run this algorithm with epsilon

3366
01:20:09,790 --> 01:20:11,870
 

3367
01:20:09,800 --> 01:20:13,790
 prime it's like D times two to the minus

3368
01:20:11,860 --> 01:20:13,790
 

3369
01:20:11,870 --> 01:20:17,840
 T something like that you'll end up

3370
01:20:13,780 --> 01:20:17,840
 

3371
01:20:13,790 --> 01:20:19,310
 still having this algorithm will still

3372
01:20:17,830 --> 01:20:19,310
 

3373
01:20:17,840 --> 01:20:21,200
 have that property of Garrett of

3374
01:20:19,300 --> 01:20:21,200
 

3375
01:20:19,310 --> 01:20:24,950
 preserving F star in the hypothesis

3376
01:20:21,190 --> 01:20:24,950
 

3377
01:20:21,200 --> 01:20:27,380
 class and and get the label complexity

3378
01:20:24,940 --> 01:20:27,380
 

3379
01:20:24,950 --> 01:20:30,580
 guarantee okay so then we can quantify

3380
01:20:27,370 --> 01:20:30,580
 

3381
01:20:27,380 --> 01:20:33,350
 the sample complexity of this algorithm

3382
01:20:30,570 --> 01:20:33,350
 

3383
01:20:30,580 --> 01:20:34,820
 right with this again replacing just

3384
01:20:33,340 --> 01:20:34,820
 

3385
01:20:33,350 --> 01:20:38,990
 replacing the region of disagreement

3386
01:20:34,810 --> 01:20:38,990
 

3387
01:20:34,820 --> 01:20:41,660
 with this sub region and basically what

3388
01:20:38,980 --> 01:20:41,660
 

3389
01:20:38,990 --> 01:20:43,070
 I've written here is is just like the

3390
01:20:41,650 --> 01:20:43,070
 

3391
01:20:41,660 --> 01:20:45,020
 disagreement coefficient except we

3392
01:20:43,060 --> 01:20:45,020
 

3393
01:20:43,070 --> 01:20:48,610
 replaced the region of disagreement with

3394
01:20:45,010 --> 01:20:48,610
 

3395
01:20:45,020 --> 01:20:51,260
 this sort of sub region of disagreement

3396
01:20:48,600 --> 01:20:51,260
 

3397
01:20:48,610 --> 01:20:53,720
 of the of the region of disagreement and

3398
01:20:51,250 --> 01:20:53,720
 

3399
01:20:51,260 --> 01:20:55,190
 and here we'll get basically the same

3400
01:20:53,710 --> 01:20:55,190
 

3401
01:20:53,720 --> 01:20:56,450
 kind of guarantee that we got for

3402
01:20:55,180 --> 01:20:56,450
 

3403
01:20:55,190 --> 01:20:58,460
 disagreement based active learning

3404
01:20:56,440 --> 01:20:58,460
 

3405
01:20:56,450 --> 01:21:02,630
 except we replace the disagreement

3406
01:20:58,450 --> 01:21:02,630
 

3407
01:20:58,460 --> 01:21:06,740
 coefficient with this new quantity this

3408
01:21:02,620 --> 01:21:06,740
 

3409
01:21:02,630 --> 01:21:10,520
 fee and that's for bounded noise there's

3410
01:21:06,730 --> 01:21:10,520
 

3411
01:21:06,740 --> 01:21:11,870
 also a version for agnostic learning the

3412
01:21:10,510 --> 01:21:11,870
 

3413
01:21:10,520 --> 01:21:13,670
 quantities defined a little bit

3414
01:21:11,860 --> 01:21:13,670
 

3415
01:21:11,870 --> 01:21:15,200
 differently where the the beta the the

3416
01:21:13,660 --> 01:21:15,200
 

3417
01:21:13,670 --> 01:21:16,940
 error rate of the best classifier sort

3418
01:21:15,190 --> 01:21:16,940
 

3419
01:21:15,200 --> 01:21:19,550
 of shows up inside of the radius and in

3420
01:21:16,930 --> 01:21:19,550
 

3421
01:21:16,940 --> 01:21:21,560
 the denominator but otherwise it's is

3422
01:21:19,540 --> 01:21:21,560
 

3423
01:21:19,550 --> 01:21:24,470
 defined essentially through the same way

3424
01:21:21,550 --> 01:21:24,470
 

3425
01:21:21,560 --> 01:21:26,210
 and here again we recover basically the

3426
01:21:24,460 --> 01:21:26,210
 

3427
01:21:24,470 --> 01:21:27,770
 same guarantee that disagreement based

3428
01:21:26,200 --> 01:21:27,770
 

3429
01:21:26,210 --> 01:21:29,510
 active learning cat except we replace

3430
01:21:27,760 --> 01:21:29,510
 

3431
01:21:27,770 --> 01:21:32,870
 the disagreement coefficient with this

3432
01:21:29,500 --> 01:21:32,870
 

3433
01:21:29,510 --> 01:21:35,420
 other quantity and and because this sub

3434
01:21:32,860 --> 01:21:35,420
 

3435
01:21:32,870 --> 01:21:38,110
 region is potentially smaller than the

3436
01:21:35,410 --> 01:21:38,110
 

3437
01:21:35,420 --> 01:21:41,690
 region of disagreement these this

3438
01:21:38,100 --> 01:21:41,690
 

3439
01:21:38,110 --> 01:21:43,670
 complexity measure is often it can be

3440
01:21:41,680 --> 01:21:43,670
 

3441
01:21:41,690 --> 01:21:45,530
 smaller than the disagreement

3442
01:21:43,660 --> 01:21:45,530
 

3443
01:21:43,670 --> 01:21:50,530
 coefficient and we'll look at some cases

3444
01:21:45,520 --> 01:21:50,530
 

3445
01:21:45,530 --> 01:21:55,220
 where it's actually quantifiably smaller

3446
01:21:50,520 --> 01:21:55,220
 

3447
01:21:50,530 --> 01:21:58,760
 okay so this is one idea for getting

3448
01:21:55,210 --> 01:21:58,760
 

3449
01:21:55,220 --> 01:22:01,100
 beyond to an improvement over a

3450
01:21:58,750 --> 01:22:01,100
 

3451
01:21:58,760 --> 01:22:02,510
 disagreement based active learning so so

3452
01:22:01,090 --> 01:22:02,510
 

3453
01:22:01,100 --> 01:22:06,380
 the question comes up then of course how

3454
01:22:02,500 --> 01:22:06,380
 

3455
01:22:02,510 --> 01:22:07,730
 do you pick this region as I say you

3456
01:22:06,370 --> 01:22:07,730
 

3457
01:22:06,380 --> 01:22:09,590
 could just pick it to be the region of

3458
01:22:07,720 --> 01:22:09,590
 

3459
01:22:07,730 --> 01:22:11,870
 disagreement but that's kind of okay you

3460
01:22:09,580 --> 01:22:11,870
 

3461
01:22:09,590 --> 01:22:14,960
 just recover exactly the a squared

3462
01:22:11,860 --> 01:22:14,960
 

3463
01:22:11,870 --> 01:22:16,289
 algorithm you can choose it empirically

3464
01:22:14,950 --> 01:22:16,289
 

3465
01:22:14,960 --> 01:22:20,280
 and this is an entry

3466
01:22:16,279 --> 01:22:20,280
 

3467
01:22:16,289 --> 01:22:22,049
 the idea from ginger Tory they sort of

3468
01:22:20,270 --> 01:22:22,049
 

3469
01:22:20,280 --> 01:22:26,159
 set up this kind of linear program

3470
01:22:22,039 --> 01:22:26,159
 

3471
01:22:22,049 --> 01:22:27,629
 approach to use the data set to figure

3472
01:22:26,149 --> 01:22:27,629
 

3473
01:22:26,159 --> 01:22:31,079
 out which points to include in the

3474
01:22:27,619 --> 01:22:31,079
 

3475
01:22:27,629 --> 01:22:34,169
 region and which ones not to and that's

3476
01:22:31,069 --> 01:22:34,169
 

3477
01:22:31,079 --> 01:22:35,999
 a general approach but maybe you have to

3478
01:22:34,159 --> 01:22:35,999
 

3479
01:22:34,169 --> 01:22:40,229
 do some more work to get a priority

3480
01:22:35,989 --> 01:22:40,229
 

3481
01:22:35,999 --> 01:22:42,179
 guarantees out of that and the but then

3482
01:22:40,219 --> 01:22:42,179
 

3483
01:22:40,229 --> 01:22:43,859
 another approach which goes into

3484
01:22:42,169 --> 01:22:43,859
 

3485
01:22:42,179 --> 01:22:47,309
 literature that actually predates this

3486
01:22:43,849 --> 01:22:47,309
 

3487
01:22:43,859 --> 01:22:49,079
 paper is using some nice structure in

3488
01:22:47,299 --> 01:22:49,079
 

3489
01:22:47,309 --> 01:22:52,319
 the hypothesis class and distribution

3490
01:22:49,069 --> 01:22:52,319
 

3491
01:22:49,079 --> 01:22:54,030
 and this this is where we get into four

3492
01:22:52,309 --> 01:22:54,030
 

3493
01:22:52,319 --> 01:22:56,929
 linear separators for example this

3494
01:22:54,020 --> 01:22:56,929
 

3495
01:22:54,030 --> 01:23:00,479
 notion of margin based active learning

3496
01:22:56,919 --> 01:23:00,479
 

3497
01:22:56,929 --> 01:23:03,469
 okay so I'll just describe that in some

3498
01:23:00,469 --> 01:23:03,469
 

3499
01:23:00,479 --> 01:23:06,179
 detail here so remember let's talk about

3500
01:23:03,459 --> 01:23:06,179
 

3501
01:23:03,469 --> 01:23:13,349
 for simplicity a uniform distribution on

3502
01:23:06,169 --> 01:23:13,349
 

3503
01:23:06,179 --> 01:23:15,269
 a d-dimensional sphere and let's take F

3504
01:23:13,339 --> 01:23:15,269
 

3505
01:23:13,349 --> 01:23:17,839
 star to be the the red separator here

3506
01:23:15,259 --> 01:23:17,839
 

3507
01:23:15,269 --> 01:23:21,629
 and the blue separator is some other

3508
01:23:17,829 --> 01:23:21,629
 

3509
01:23:17,839 --> 01:23:23,819
 well let's call them W star and W linear

3510
01:23:21,619 --> 01:23:23,819
 

3511
01:23:21,629 --> 01:23:26,699
 separators okay so then what we can do

3512
01:23:23,809 --> 01:23:26,699
 

3513
01:23:23,819 --> 01:23:30,209
 is project so we have to wait vectors

3514
01:23:26,689 --> 01:23:30,209
 

3515
01:23:26,699 --> 01:23:34,349
 right two linear separators and let's

3516
01:23:30,199 --> 01:23:34,349
 

3517
01:23:30,209 --> 01:23:36,539
 project onto the span of the two weight

3518
01:23:34,339 --> 01:23:36,539
 

3519
01:23:34,349 --> 01:23:39,569
 vectors okay so that so what you end up

3520
01:23:36,529 --> 01:23:39,569
 

3521
01:23:36,539 --> 01:23:40,409
 with is this project on the span of two

3522
01:23:39,559 --> 01:23:40,409
 

3523
01:23:39,569 --> 01:23:42,030
 vectors you're going to get a

3524
01:23:40,399 --> 01:23:42,030
 

3525
01:23:40,409 --> 01:23:44,039
 two-dimensional space and that

3526
01:23:42,020 --> 01:23:44,039
 

3527
01:23:42,030 --> 01:23:47,010
 distribution projects on to a disk in

3528
01:23:44,029 --> 01:23:47,010
 

3529
01:23:44,039 --> 01:23:48,510
 the two-dimensional space and the region

3530
01:23:47,000 --> 01:23:48,510
 

3531
01:23:47,010 --> 01:23:52,229
 where the two classifiers disagree with

3532
01:23:48,500 --> 01:23:52,229
 

3533
01:23:48,510 --> 01:23:54,299
 each other is just this green region and

3534
01:23:52,219 --> 01:23:54,299
 

3535
01:23:52,229 --> 01:23:55,559
 the interesting thing that happens when

3536
01:23:54,289 --> 01:23:55,559
 

3537
01:23:54,299 --> 01:23:57,749
 you project from a high dimensional

3538
01:23:55,549 --> 01:23:57,749
 

3539
01:23:55,559 --> 01:23:59,249
 space to a two dimensional space if you

3540
01:23:57,739 --> 01:23:59,249
 

3541
01:23:57,749 --> 01:24:01,709
 started with a uniform distribution on a

3542
01:23:59,239 --> 01:24:01,709
 

3543
01:23:59,249 --> 01:24:03,030
 sphere most of that probability mass is

3544
01:24:01,699 --> 01:24:03,030
 

3545
01:24:01,709 --> 01:24:06,239
 going to end up kind of around the

3546
01:24:03,020 --> 01:24:06,239
 

3547
01:24:03,030 --> 01:24:07,979
 middle it's not gonna be like all over

3548
01:24:06,229 --> 01:24:07,979
 

3549
01:24:06,239 --> 01:24:09,780
 the place like a uniform it's it's

3550
01:24:07,969 --> 01:24:09,780
 

3551
01:24:07,979 --> 01:24:11,549
 actually the density function of the

3552
01:24:09,770 --> 01:24:11,549
 

3553
01:24:09,780 --> 01:24:13,919
 projected distribution drops off

3554
01:24:11,539 --> 01:24:13,919
 

3555
01:24:11,549 --> 01:24:16,769
 exponentially as you move away from the

3556
01:24:13,909 --> 01:24:16,769
 

3557
01:24:13,919 --> 01:24:18,959
 center and what that means is you can

3558
01:24:16,759 --> 01:24:18,959
 

3559
01:24:16,769 --> 01:24:21,959
 draw like let's say a region like this

3560
01:24:18,949 --> 01:24:21,959
 

3561
01:24:18,959 --> 01:24:25,139
 and you'll still capture most of the

3562
01:24:21,949 --> 01:24:25,139
 

3563
01:24:21,959 --> 01:24:26,519
 probability mass and particularly you'll

3564
01:24:25,129 --> 01:24:26,519
 

3565
01:24:25,139 --> 01:24:28,169
 capture most of the probability of mass

3566
01:24:26,509 --> 01:24:28,169
 

3567
01:24:26,519 --> 01:24:30,040
 where the two classifiers disagree with

3568
01:24:28,159 --> 01:24:30,040
 

3569
01:24:28,169 --> 01:24:33,090
 each other so you can put like a slab

3570
01:24:30,030 --> 01:24:33,090
 

3571
01:24:30,040 --> 01:24:36,850
 around one of the the two separators and

3572
01:24:33,080 --> 01:24:36,850
 

3573
01:24:33,090 --> 01:24:38,260
 you'll if you take that let's say the

3574
01:24:36,840 --> 01:24:38,260
 

3575
01:24:36,850 --> 01:24:39,910
 two the two separators were within

3576
01:24:38,250 --> 01:24:39,910
 

3577
01:24:38,260 --> 01:24:41,980
 distance R of each other if you take

3578
01:24:39,900 --> 01:24:41,980
 

3579
01:24:39,910 --> 01:24:43,600
 that slab of width like roughly R over

3580
01:24:41,970 --> 01:24:43,600
 

3581
01:24:41,980 --> 01:24:45,070
 square root D you're still going to

3582
01:24:43,590 --> 01:24:45,070
 

3583
01:24:43,600 --> 01:24:50,260
 capture most of the probability mass

3584
01:24:45,060 --> 01:24:50,260
 

3585
01:24:45,070 --> 01:24:52,870
 where the two disagree and so basically

3586
01:24:50,250 --> 01:24:52,870
 

3587
01:24:50,260 --> 01:24:54,970
 the idea is well let's do that for all

3588
01:24:52,860 --> 01:24:54,970
 

3589
01:24:52,870 --> 01:24:57,160
 of the separators and and what you end

3590
01:24:54,960 --> 01:24:57,160
 

3591
01:24:54,970 --> 01:24:58,750
 up with it in if you think about this in

3592
01:24:57,150 --> 01:24:58,750
 

3593
01:24:57,160 --> 01:25:00,760
 fact in the higher dimensional space is

3594
01:24:58,740 --> 01:25:00,760
 

3595
01:24:58,750 --> 01:25:05,020
 you're just talking about taking a slab

3596
01:25:00,750 --> 01:25:05,020
 

3597
01:25:00,760 --> 01:25:07,630
 around the the actual W star separator

3598
01:25:05,010 --> 01:25:07,630
 

3599
01:25:05,020 --> 01:25:09,460
 in the higher dimensional space and so

3600
01:25:07,620 --> 01:25:09,460
 

3601
01:25:07,630 --> 01:25:11,710
 let's take that as our region it's just

3602
01:25:09,450 --> 01:25:11,710
 

3603
01:25:09,460 --> 01:25:18,190
 a slab of width roughly our over square

3604
01:25:11,700 --> 01:25:18,190
 

3605
01:25:11,710 --> 01:25:19,660
 root D and okay so so as I mentioned in

3606
01:25:18,180 --> 01:25:19,660
 

3607
01:25:18,190 --> 01:25:22,600
 the previous part the region of

3608
01:25:19,650 --> 01:25:22,600
 

3609
01:25:19,660 --> 01:25:25,810
 disagreement is actually a slab of width

3610
01:25:22,590 --> 01:25:25,810
 

3611
01:25:22,600 --> 01:25:27,220
 roughly our so here we've reduced the

3612
01:25:25,800 --> 01:25:27,220
 

3613
01:25:25,810 --> 01:25:31,420
 width of the slab to like our over

3614
01:25:27,210 --> 01:25:31,420
 

3615
01:25:27,220 --> 01:25:32,680
 square root D and yet the little regions

3616
01:25:31,410 --> 01:25:32,680
 

3617
01:25:31,420 --> 01:25:34,750
 these little purple regions that we

3618
01:25:32,670 --> 01:25:34,750
 

3619
01:25:32,680 --> 01:25:36,250
 clipped off didn't add up to very much

3620
01:25:34,740 --> 01:25:36,250
 

3621
01:25:34,750 --> 01:25:39,010
 probability mass so we could still

3622
01:25:36,240 --> 01:25:39,010
 

3623
01:25:36,250 --> 01:25:40,780
 maintain some kind of bound on how much

3624
01:25:39,000 --> 01:25:40,780
 

3625
01:25:39,010 --> 01:25:45,700
 disagreement there is outside of this

3626
01:25:40,770 --> 01:25:45,700
 

3627
01:25:40,780 --> 01:25:48,490
 smaller slab and then it turns out if

3628
01:25:45,690 --> 01:25:48,490
 

3629
01:25:45,700 --> 01:25:50,530
 you have a slab of a given width in d

3630
01:25:48,480 --> 01:25:50,530
 

3631
01:25:48,490 --> 01:25:52,450
 dimensional space the probability mass

3632
01:25:50,520 --> 01:25:52,450
 

3633
01:25:50,530 --> 01:25:56,620
 in that slab is like roughly square root

3634
01:25:52,440 --> 01:25:56,620
 

3635
01:25:52,450 --> 01:25:58,360
 D times the width of the slab so right

3636
01:25:56,610 --> 01:25:58,360
 

3637
01:25:56,620 --> 01:26:00,610
 so here we have a slab of width R over

3638
01:25:58,350 --> 01:26:00,610
 

3639
01:25:58,360 --> 01:26:03,850
 square root D if you divide by if you

3640
01:26:00,600 --> 01:26:03,850
 

3641
01:26:00,610 --> 01:26:06,010
 multiply by square d you get R and just

3642
01:26:03,840 --> 01:26:06,010
 

3643
01:26:03,850 --> 01:26:07,120
 to throw that definition back up and so

3644
01:26:06,000 --> 01:26:07,120
 

3645
01:26:06,010 --> 01:26:09,340
 that's the probability mass in the

3646
01:26:07,110 --> 01:26:09,340
 

3647
01:26:07,120 --> 01:26:10,660
 numerator if you divide by R then you

3648
01:26:09,330 --> 01:26:10,660
 

3649
01:26:09,340 --> 01:26:13,690
 turn out turns out this thing is

3650
01:26:10,650 --> 01:26:13,690
 

3651
01:26:10,660 --> 01:26:15,880
 constant so this complexity measure is

3652
01:26:13,680 --> 01:26:15,880
 

3653
01:26:13,690 --> 01:26:19,450
 bounded by a constant in the in the case

3654
01:26:15,870 --> 01:26:19,450
 

3655
01:26:15,880 --> 01:26:21,400
 of this uniform distribution on the

3656
01:26:19,440 --> 01:26:21,400
 

3657
01:26:19,450 --> 01:26:25,420
 sphere if you take the region in this

3658
01:26:21,390 --> 01:26:25,420
 

3659
01:26:21,400 --> 01:26:28,360
 margin based way as opposed to in

3660
01:26:25,410 --> 01:26:28,360
 

3661
01:26:25,420 --> 01:26:31,300
 comparison the disagreement coefficient

3662
01:26:28,350 --> 01:26:31,300
 

3663
01:26:28,360 --> 01:26:33,040
 if you recall was square root D so we're

3664
01:26:31,290 --> 01:26:33,040
 

3665
01:26:31,300 --> 01:26:36,550
 actually getting an improvement so so

3666
01:26:33,030 --> 01:26:36,550
 

3667
01:26:33,040 --> 01:26:39,179
 that you plug that into the theorems in

3668
01:26:36,540 --> 01:26:39,179
 

3669
01:26:36,550 --> 01:26:41,519
 this case what we're getting out of

3670
01:26:39,169 --> 01:26:41,519
 

3671
01:26:39,179 --> 01:26:44,039
 this sub-region based technique is a

3672
01:26:41,509 --> 01:26:44,039
 

3673
01:26:41,519 --> 01:26:46,499
 number of labels like d log 1 over

3674
01:26:44,029 --> 01:26:46,499
 

3675
01:26:44,039 --> 01:26:48,479
 epsilon was sufficient whereas for the a

3676
01:26:46,489 --> 01:26:48,479
 

3677
01:26:46,499 --> 01:26:50,070
 squared algorithm we were getting that

3678
01:26:48,469 --> 01:26:50,070
 

3679
01:26:48,479 --> 01:26:52,260
 times the disagreement coefficient right

3680
01:26:50,060 --> 01:26:52,260
 

3681
01:26:50,070 --> 01:26:53,030
 so d to the three-halves log 1 over

3682
01:26:52,250 --> 01:26:53,030
 

3683
01:26:52,260 --> 01:26:55,079
 Epsilon

3684
01:26:53,020 --> 01:26:55,079
 

3685
01:26:53,030 --> 01:26:56,459
 both of these are improvements over

3686
01:26:55,069 --> 01:26:56,459
 

3687
01:26:55,079 --> 01:26:58,260
 passive in terms of the dependence on

3688
01:26:56,449 --> 01:26:58,260
 

3689
01:26:56,459 --> 01:27:04,439
 epsilon a passive learning had a deal

3690
01:26:58,250 --> 01:27:04,439
 

3691
01:26:58,260 --> 01:27:06,449
 for epsilon dependence but right so in

3692
01:27:04,429 --> 01:27:06,449
 

3693
01:27:04,439 --> 01:27:08,280
 the case of disagreement based active

3694
01:27:06,439 --> 01:27:08,280
 

3695
01:27:06,449 --> 01:27:11,039
 learning it came at the cost of a square

3696
01:27:08,270 --> 01:27:11,039
 

3697
01:27:08,280 --> 01:27:11,789
 root D factor whereas in the this

3698
01:27:11,029 --> 01:27:11,789
 

3699
01:27:11,039 --> 01:27:14,429
 margin-based

3700
01:27:11,779 --> 01:27:14,429
 

3701
01:27:11,789 --> 01:27:19,880
 the sub-region based approach we're not

3702
01:27:14,419 --> 01:27:19,880
 

3703
01:27:14,429 --> 01:27:23,209
 able oozing any dependence on D okay

3704
01:27:19,870 --> 01:27:23,209
 

3705
01:27:19,880 --> 01:27:25,979
 since we're talking about margin based I

3706
01:27:23,199 --> 01:27:25,979
 

3707
01:27:23,209 --> 01:27:28,050
 should mention there's this as I as I

3708
01:27:25,969 --> 01:27:28,050
 

3709
01:27:25,979 --> 01:27:30,389
 said there's this literature that even

3710
01:27:28,040 --> 01:27:30,389
 

3711
01:27:28,050 --> 01:27:32,519
 predates the the general idea of

3712
01:27:30,379 --> 01:27:32,519
 

3713
01:27:30,389 --> 01:27:34,439
 sub-region based active learning and

3714
01:27:32,509 --> 01:27:34,439
 

3715
01:27:32,519 --> 01:27:35,939
 that's the margin based active learning

3716
01:27:34,429 --> 01:27:35,939
 

3717
01:27:34,439 --> 01:27:38,309
 and there's a simple way to describe

3718
01:27:35,929 --> 01:27:38,309
 

3719
01:27:35,939 --> 01:27:40,739
 margin based active learning that is

3720
01:27:38,299 --> 01:27:40,739
 

3721
01:27:38,309 --> 01:27:48,090
 worth going through just because it's a

3722
01:27:40,729 --> 01:27:48,090
 

3723
01:27:40,739 --> 01:27:51,900
 very nice simple description so that

3724
01:27:48,080 --> 01:27:51,900
 

3725
01:27:48,090 --> 01:27:53,699
 kind of concentration around like within

3726
01:27:51,890 --> 01:27:53,699
 

3727
01:27:51,900 --> 01:27:55,260
 a slab around the separator that wasn't

3728
01:27:53,689 --> 01:27:55,260
 

3729
01:27:53,699 --> 01:27:57,900
 just true for W star that's true for any

3730
01:27:55,250 --> 01:27:57,900
 

3731
01:27:55,260 --> 01:28:01,400
 separator so in particular we could do

3732
01:27:57,890 --> 01:28:01,400
 

3733
01:27:57,900 --> 01:28:05,159
 it for the w hat separator for instance

3734
01:28:01,390 --> 01:28:05,159
 

3735
01:28:01,400 --> 01:28:06,780
 where say on each round we have some W

3736
01:28:05,149 --> 01:28:06,780
 

3737
01:28:05,159 --> 01:28:08,880
 hat and let's say so so here's the

3738
01:28:06,770 --> 01:28:08,880
 

3739
01:28:06,780 --> 01:28:11,189
 algorithm and believe it or not this is

3740
01:28:08,870 --> 01:28:11,189
 

3741
01:28:08,880 --> 01:28:15,119
 kind of roughly a similar it's sort of a

3742
01:28:11,179 --> 01:28:15,119
 

3743
01:28:11,189 --> 01:28:18,030
 relaxation of the exact algorithm that I

3744
01:28:15,109 --> 01:28:18,030
 

3745
01:28:15,119 --> 01:28:19,409
 had on the previous slide so on each

3746
01:28:18,020 --> 01:28:19,409
 

3747
01:28:18,030 --> 01:28:22,349
 round you're taking a number of samples

3748
01:28:19,399 --> 01:28:22,349
 

3749
01:28:19,409 --> 01:28:26,429
 and let's just query the samples that

3750
01:28:22,339 --> 01:28:26,429
 

3751
01:28:22,349 --> 01:28:32,550
 are in a some slab around our current

3752
01:28:26,419 --> 01:28:32,550
 

3753
01:28:26,429 --> 01:28:34,590
 guess a current W hat and then and right

3754
01:28:32,540 --> 01:28:34,590
 

3755
01:28:32,550 --> 01:28:36,780
 and then we update W hat by minimizing

3756
01:28:34,580 --> 01:28:36,780
 

3757
01:28:34,590 --> 01:28:38,039
 the empirical risk and may be subject to

3758
01:28:36,770 --> 01:28:38,039
 

3759
01:28:36,780 --> 01:28:40,349
 a constraint that we don't wander too

3760
01:28:38,029 --> 01:28:40,349
 

3761
01:28:38,039 --> 01:28:42,179
 far so if you in step 3 there's a

3762
01:28:40,339 --> 01:28:42,179
 

3763
01:28:40,349 --> 01:28:43,800
 constraint that we don't move too far on

3764
01:28:42,169 --> 01:28:43,800
 

3765
01:28:42,179 --> 01:28:45,900
 each round but basically we're

3766
01:28:43,790 --> 01:28:45,900
 

3767
01:28:43,800 --> 01:28:48,579
 minimizing the empirical risk on each

3768
01:28:45,890 --> 01:28:48,579
 

3769
01:28:45,900 --> 01:28:51,099
 round ok

3770
01:28:48,569 --> 01:28:51,099
 

3771
01:28:48,579 --> 01:28:54,249
 and that's that's the other then we just

3772
01:28:51,089 --> 01:28:54,249
 

3773
01:28:51,099 --> 01:28:55,900
 repeat that so we have a margin of a

3774
01:28:54,239 --> 01:28:55,900
 

3775
01:28:54,249 --> 01:28:58,030
 certain size to the minus T over square

3776
01:28:55,890 --> 01:28:58,030
 

3777
01:28:55,900 --> 01:29:01,480
 root D and we sample within that margin

3778
01:28:58,020 --> 01:29:01,480
 

3779
01:28:58,030 --> 01:29:04,510
 and do this minimization step to update

3780
01:29:01,470 --> 01:29:04,510
 

3781
01:29:01,480 --> 01:29:08,400
 the separator and under bounded noise we

3782
01:29:04,500 --> 01:29:08,400
 

3783
01:29:04,510 --> 01:29:10,900
 would get this d log 1 over epsilon

3784
01:29:08,390 --> 01:29:10,900
 

3785
01:29:08,400 --> 01:29:13,739
 sample complexity just like that what

3786
01:29:10,890 --> 01:29:13,739
 

3787
01:29:10,900 --> 01:29:16,690
 I've mentioned on the previous slide and

3788
01:29:13,729 --> 01:29:16,690
 

3789
01:29:13,739 --> 01:29:19,989
 so that this is classic work from Vulcan

3790
01:29:16,680 --> 01:29:19,989
 

3791
01:29:16,690 --> 01:29:22,960
 Broder enjoying its since there has been

3792
01:29:19,979 --> 01:29:22,960
 

3793
01:29:19,989 --> 01:29:25,239
 a whole sequence of papers that extend

3794
01:29:22,950 --> 01:29:25,239
 

3795
01:29:22,960 --> 01:29:27,489
 this to other distributions like

3796
01:29:25,229 --> 01:29:27,489
 

3797
01:29:25,239 --> 01:29:30,190
 isotropic long concave distributions and

3798
01:29:27,479 --> 01:29:30,190
 

3799
01:29:27,489 --> 01:29:32,380
 what s concave distributions now so

3800
01:29:30,180 --> 01:29:32,380
 

3801
01:29:30,190 --> 01:29:35,289
 there's a family of distributions where

3802
01:29:32,370 --> 01:29:35,289
 

3803
01:29:32,380 --> 01:29:36,820
 this will work it has to do with when

3804
01:29:35,279 --> 01:29:36,820
 

3805
01:29:35,289 --> 01:29:39,400
 you projected two dimensions do you get

3806
01:29:36,810 --> 01:29:39,400
 

3807
01:29:36,820 --> 01:29:42,760
 that nice concentration of the

3808
01:29:39,390 --> 01:29:42,760
 

3809
01:29:39,400 --> 01:29:44,590
 probability mass inside of a slab so so

3810
01:29:42,750 --> 01:29:44,590
 

3811
01:29:42,760 --> 01:29:46,449
 distributions that have that property

3812
01:29:44,580 --> 01:29:46,449
 

3813
01:29:44,590 --> 01:29:52,530
 what will kind of work with this

3814
01:29:46,439 --> 01:29:52,530
 

3815
01:29:46,449 --> 01:29:54,400
 technique and ok so since I have

3816
01:29:52,520 --> 01:29:54,400
 

3817
01:29:52,530 --> 01:29:55,929
 margin-based active learning up on the

3818
01:29:54,390 --> 01:29:55,929
 

3819
01:29:54,400 --> 01:29:58,329
 slides I have when I mentioned that's a

3820
01:29:55,919 --> 01:29:58,329
 

3821
01:29:55,929 --> 01:30:02,010
 kind of an aside a cool result that came

3822
01:29:58,319 --> 01:30:02,010
 

3823
01:29:58,329 --> 01:30:07,210
 out of this literature this is work from

3824
01:30:02,000 --> 01:30:07,210
 

3825
01:30:02,010 --> 01:30:08,860
 Awasthi Balkan and long basically if you

3826
01:30:07,200 --> 01:30:08,860
 

3827
01:30:07,210 --> 01:30:11,320
 just make a slight change to that

3828
01:30:08,850 --> 01:30:11,320
 

3829
01:30:08,860 --> 01:30:15,969
 algorithm so that in the minimization

3830
01:30:11,310 --> 01:30:15,969
 

3831
01:30:11,320 --> 01:30:18,010
 step you use a surrogate loss you can

3832
01:30:15,959 --> 01:30:18,010
 

3833
01:30:15,969 --> 01:30:19,960
 get a very nice property so in

3834
01:30:18,000 --> 01:30:19,960
 

3835
01:30:18,010 --> 01:30:22,869
 particular what they'll use is the hinge

3836
01:30:19,950 --> 01:30:22,869
 

3837
01:30:19,960 --> 01:30:24,309
 loss but they actually increase on each

3838
01:30:22,859 --> 01:30:24,309
 

3839
01:30:22,869 --> 01:30:25,480
 round they're actually going to increase

3840
01:30:24,299 --> 01:30:25,480
 

3841
01:30:24,309 --> 01:30:28,749
 the slope a little bit

3842
01:30:25,470 --> 01:30:28,749
 

3843
01:30:25,480 --> 01:30:31,030
 well exponentially so on each round they

3844
01:30:28,739 --> 01:30:31,030
 

3845
01:30:28,749 --> 01:30:32,650
 increase the slope of the hinge loss and

3846
01:30:31,020 --> 01:30:32,650
 

3847
01:30:31,030 --> 01:30:34,329
 just by making that chain right so we

3848
01:30:32,640 --> 01:30:34,329
 

3849
01:30:32,650 --> 01:30:35,619
 talked about surrogate losses before and

3850
01:30:34,319 --> 01:30:35,619
 

3851
01:30:34,329 --> 01:30:36,760
 you could do that do all of that but

3852
01:30:35,609 --> 01:30:36,760
 

3853
01:30:35,619 --> 01:30:38,110
 here they're actually changing the

3854
01:30:36,750 --> 01:30:38,110
 

3855
01:30:36,760 --> 01:30:40,749
 surrogate loss from one round to the

3856
01:30:38,100 --> 01:30:40,749
 

3857
01:30:38,110 --> 01:30:43,449
 next and what this does is really cool

3858
01:30:40,739 --> 01:30:43,449
 

3859
01:30:40,749 --> 01:30:45,429
 because you end up with you're getting

3860
01:30:43,439 --> 01:30:45,429
 

3861
01:30:43,449 --> 01:30:47,619
 these label complexity guarantees this

3862
01:30:45,419 --> 01:30:47,619
 

3863
01:30:45,429 --> 01:30:49,690
 algorithm still works without any

3864
01:30:47,609 --> 01:30:49,690
 

3865
01:30:47,619 --> 01:30:51,369
 additional assumptions just the same

3866
01:30:49,680 --> 01:30:51,369
 

3867
01:30:49,690 --> 01:30:53,079
 assumptions that we had right so that

3868
01:30:51,359 --> 01:30:53,079
 

3869
01:30:51,369 --> 01:30:54,639
 unlike the what I mentioned about

3870
01:30:53,069 --> 01:30:54,639
 

3871
01:30:53,079 --> 01:30:56,469
 surrogate losses before where you had to

3872
01:30:54,629 --> 01:30:56,469
 

3873
01:30:54,639 --> 01:31:00,039
 make additional assumptions here there's

3874
01:30:56,459 --> 01:31:00,039
 

3875
01:30:56,469 --> 01:31:02,140
 no additional assumptions with this use

3876
01:31:00,029 --> 01:31:02,140
 

3877
01:31:00,039 --> 01:31:04,870
 of the surrogate loss

3878
01:31:02,130 --> 01:31:04,870
 

3879
01:31:02,140 --> 01:31:06,820
 and it's because it's a convex loss your

3880
01:31:04,860 --> 01:31:06,820
 

3881
01:31:04,870 --> 01:31:08,610
 running time is polynomial so this is

3882
01:31:06,810 --> 01:31:08,610
 

3883
01:31:06,820 --> 01:31:11,650
 actually an efficient algorithm for

3884
01:31:08,600 --> 01:31:11,650
 

3885
01:31:08,610 --> 01:31:16,120
 learning under bounded noise with these

3886
01:31:11,640 --> 01:31:16,120
 

3887
01:31:11,650 --> 01:31:17,710
 distributional constraints and in the

3888
01:31:16,110 --> 01:31:17,710
 

3889
01:31:16,120 --> 01:31:19,150
 Gnostic case you can also get a result

3890
01:31:17,700 --> 01:31:19,150
 

3891
01:31:17,710 --> 01:31:21,550
 it's also important meal time but a

3892
01:31:19,140 --> 01:31:21,550
 

3893
01:31:19,150 --> 01:31:22,870
 little bit weaker because now you're you

3894
01:31:21,540 --> 01:31:22,870
 

3895
01:31:21,550 --> 01:31:29,260
 can only get just a multiplicative

3896
01:31:22,860 --> 01:31:29,260
 

3897
01:31:22,870 --> 01:31:30,610
 factor of the optimal risk but what's

3898
01:31:29,250 --> 01:31:30,610
 

3899
01:31:29,260 --> 01:31:33,520
 remarkable here is this is actually the

3900
01:31:30,600 --> 01:31:33,520
 

3901
01:31:30,610 --> 01:31:35,850
 first algorithm a passive or active to

3902
01:31:33,510 --> 01:31:35,850
 

3903
01:31:33,520 --> 01:31:38,710
 be able to achieve these kinds of

3904
01:31:35,840 --> 01:31:38,710
 

3905
01:31:35,850 --> 01:31:41,170
 polynomial time learning guarantees for

3906
01:31:38,700 --> 01:31:41,170
 

3907
01:31:38,710 --> 01:31:44,230
 these these kinds of noise models and

3908
01:31:41,160 --> 01:31:44,230
 

3909
01:31:41,170 --> 01:31:46,060
 that's it's interesting that the first

3910
01:31:44,220 --> 01:31:46,060
 

3911
01:31:44,230 --> 01:31:49,449
 album that has that guarantee came out

3912
01:31:46,050 --> 01:31:49,449
 

3913
01:31:46,060 --> 01:31:51,030
 of the active learning thinking about

3914
01:31:49,439 --> 01:31:51,030
 

3915
01:31:49,449 --> 01:31:52,870
 things in terms of active learning and

3916
01:31:51,020 --> 01:31:52,870
 

3917
01:31:51,030 --> 01:31:54,690
 this will also work for other

3918
01:31:52,860 --> 01:31:54,690
 

3919
01:31:52,870 --> 01:31:59,679
 distributions isotropic or concave

3920
01:31:54,680 --> 01:31:59,679
 

3921
01:31:54,690 --> 01:32:02,140
 distributions for instance okay for the

3922
01:31:59,669 --> 01:32:02,140
 

3923
01:31:59,679 --> 01:32:07,750
 sake of time I'm going to skip over the

3924
01:32:02,130 --> 01:32:07,750
 

3925
01:32:02,140 --> 01:32:11,580
 next part and just get into the next

3926
01:32:07,740 --> 01:32:11,580
 

3927
01:32:07,750 --> 01:32:15,420
 topic here distribution free analysis

3928
01:32:11,570 --> 01:32:15,420
 

3929
01:32:11,580 --> 01:32:17,230
 okay so everything everything that we've

3930
01:32:15,410 --> 01:32:17,230
 

3931
01:32:15,420 --> 01:32:19,210
 talked about with the disagreement

3932
01:32:17,220 --> 01:32:19,210
 

3933
01:32:17,230 --> 01:32:20,679
 coefficient and the sub-region and the

3934
01:32:19,200 --> 01:32:20,679
 

3935
01:32:19,210 --> 01:32:22,360
 complexity measure of the sub-region and

3936
01:32:20,669 --> 01:32:22,360
 

3937
01:32:20,679 --> 01:32:26,410
 the thing that I skipped over is

3938
01:32:22,350 --> 01:32:26,410
 

3939
01:32:22,360 --> 01:32:27,640
 shattering based active learning these

3940
01:32:26,400 --> 01:32:27,640
 

3941
01:32:26,410 --> 01:32:29,920
 are distribution dependent

3942
01:32:27,630 --> 01:32:29,920
 

3943
01:32:27,640 --> 01:32:30,969
 quantification so XI remember we're

3944
01:32:29,910 --> 01:32:30,969
 

3945
01:32:29,920 --> 01:32:32,170
 talking we were for the linear

3946
01:32:30,959 --> 01:32:32,170
 

3947
01:32:30,969 --> 01:32:35,520
 separators we were talking about uniform

3948
01:32:32,160 --> 01:32:35,520
 

3949
01:32:32,170 --> 01:32:37,480
 distributions and things like that okay

3950
01:32:35,510 --> 01:32:37,480
 

3951
01:32:35,520 --> 01:32:38,650
 you could ask can we do a sample

3952
01:32:37,470 --> 01:32:38,650
 

3953
01:32:37,480 --> 01:32:43,469
 complexity analysis without a

3954
01:32:38,640 --> 01:32:43,469
 

3955
01:32:38,650 --> 01:32:46,060
 distribution dependence and we can and

3956
01:32:43,459 --> 01:32:46,060
 

3957
01:32:43,469 --> 01:32:48,370
 the complexity measure here is actually

3958
01:32:46,050 --> 01:32:48,370
 

3959
01:32:46,060 --> 01:32:49,840
 quite simple this is the the key

3960
01:32:48,360 --> 01:32:49,840
 

3961
01:32:48,370 --> 01:32:50,340
 complexity measure is called the star

3962
01:32:49,830 --> 01:32:50,340
 

3963
01:32:49,840 --> 01:32:53,350
 number

3964
01:32:50,330 --> 01:32:53,350
 

3965
01:32:50,340 --> 01:32:55,840
 it's the largest integer K such that

3966
01:32:53,340 --> 01:32:55,840
 

3967
01:32:53,350 --> 01:32:58,420
 there are k plus 1 hypotheses it's

3968
01:32:55,830 --> 01:32:58,420
 

3969
01:32:55,840 --> 01:33:00,489
 called an h 0 through H K there in the

3970
01:32:58,410 --> 01:33:00,489
 

3971
01:32:58,420 --> 01:33:03,969
 hypothesis class H and there are K

3972
01:33:00,479 --> 01:33:03,969
 

3973
01:33:00,489 --> 01:33:08,890
 points it's called an X 1 to XK such

3974
01:33:03,959 --> 01:33:08,890
 

3975
01:33:03,969 --> 01:33:15,100
 that for any i h i disagrees with h 0

3976
01:33:08,880 --> 01:33:15,100
 

3977
01:33:08,890 --> 01:33:18,160
 only on X I so to say that sort of

3978
01:33:15,090 --> 01:33:18,160
 

3979
01:33:15,100 --> 01:33:21,190
 in a hand-wavy way what we're talking

3980
01:33:18,150 --> 01:33:21,190
 

3981
01:33:18,160 --> 01:33:23,320
 about is having K points such that

3982
01:33:21,180 --> 01:33:23,320
 

3983
01:33:21,190 --> 01:33:25,930
 there's some labeling of those points

3984
01:33:23,310 --> 01:33:25,930
 

3985
01:33:23,320 --> 01:33:31,650
 where I can flip any of their labels

3986
01:33:25,920 --> 01:33:31,650
 

3987
01:33:25,930 --> 01:33:37,180
 while holding the other ones fixed okay

3988
01:33:31,640 --> 01:33:37,180
 

3989
01:33:31,650 --> 01:33:41,320
 so that's the start number and it's

3990
01:33:37,170 --> 01:33:41,320
 

3991
01:33:37,180 --> 01:33:47,530
 never smaller than the VC dimension but

3992
01:33:41,310 --> 01:33:47,530
 

3993
01:33:41,320 --> 01:33:49,150
 it can be larger arbitrarily larger okay

3994
01:33:47,520 --> 01:33:49,150
 

3995
01:33:47,530 --> 01:33:52,090
 so I'll just do a couple of examples

3996
01:33:49,140 --> 01:33:52,090
 

3997
01:33:49,150 --> 01:33:55,300
 just so everyone's clear on the

3998
01:33:52,080 --> 01:33:55,300
 

3999
01:33:52,090 --> 01:33:56,890
 definition let's talk about our simplest

4000
01:33:55,290 --> 01:33:56,890
 

4001
01:33:55,300 --> 01:34:01,840
 problem again the threshold classifiers

4002
01:33:56,880 --> 01:34:01,840
 

4003
01:33:56,890 --> 01:34:04,030
 so here again some everything to the

4004
01:34:01,830 --> 01:34:04,030
 

4005
01:34:01,840 --> 01:34:05,740
 right of some T is is plus and then I

4006
01:34:04,020 --> 01:34:05,740
 

4007
01:34:04,030 --> 01:34:07,210
 can choose T and that tells me which

4008
01:34:05,730 --> 01:34:07,210
 

4009
01:34:05,740 --> 01:34:09,400
 class where I'm using so here the star

4010
01:34:07,200 --> 01:34:09,400
 

4011
01:34:07,210 --> 01:34:12,160
 number is 2 ok so if you if you took two

4012
01:34:09,390 --> 01:34:12,160
 

4013
01:34:09,400 --> 01:34:14,170
 points you can have an H zero threshold

4014
01:34:12,150 --> 01:34:14,170
 

4015
01:34:12,160 --> 01:34:15,490
 between the two and then I could flip I

4016
01:34:14,160 --> 01:34:15,490
 

4017
01:34:14,170 --> 01:34:16,990
 can move to the right and flip that

4018
01:34:15,480 --> 01:34:16,990
 

4019
01:34:15,490 --> 01:34:18,640
 label without changing the other one I

4020
01:34:16,980 --> 01:34:18,640
 

4021
01:34:16,990 --> 01:34:21,280
 can move to the left and flip that label

4022
01:34:18,630 --> 01:34:21,280
 

4023
01:34:18,640 --> 01:34:23,170
 without changing the other one so you

4024
01:34:21,270 --> 01:34:23,170
 

4025
01:34:21,280 --> 01:34:24,370
 can the start number so you can convince

4026
01:34:23,160 --> 01:34:24,370
 

4027
01:34:23,170 --> 01:34:29,740
 yourself that you wouldn't be able to do

4028
01:34:24,360 --> 01:34:29,740
 

4029
01:34:24,370 --> 01:34:33,100
 this with three points okay for linear

4030
01:34:29,730 --> 01:34:33,100
 

4031
01:34:29,740 --> 01:34:35,160
 separators and two dimensions are higher

4032
01:34:33,090 --> 01:34:35,160
 

4033
01:34:33,100 --> 01:34:38,410
 already it's infinite

4034
01:34:35,150 --> 01:34:38,410
 

4035
01:34:35,160 --> 01:34:41,530
 okay so here what you could imagine is

4036
01:34:38,400 --> 01:34:41,530
 

4037
01:34:38,410 --> 01:34:44,260
 having points on a circle h0 is some

4038
01:34:41,520 --> 01:34:44,260
 

4039
01:34:41,530 --> 01:34:46,570
 separator that doesn't interest it just

4040
01:34:44,250 --> 01:34:46,570
 

4041
01:34:44,260 --> 01:34:50,950
 labels all of those points - let's say

4042
01:34:46,560 --> 01:34:50,950
 

4043
01:34:46,570 --> 01:34:53,530
 and then each H I just chops off that X

4044
01:34:50,940 --> 01:34:53,530
 

4045
01:34:50,950 --> 01:34:57,520
 I point just that one point off the

4046
01:34:53,520 --> 01:34:57,520
 

4047
01:34:53,530 --> 01:35:00,940
 circle okay so I can flip any one of the

4048
01:34:57,510 --> 01:35:00,940
 

4049
01:34:57,520 --> 01:35:02,800
 labels H 0 labels them all - I can label

4050
01:35:00,930 --> 01:35:02,800
 

4051
01:35:00,940 --> 01:35:05,680
 any one of them plus while keeping the

4052
01:35:02,790 --> 01:35:05,680
 

4053
01:35:02,800 --> 01:35:06,880
 other ones all - ok and you can do that

4054
01:35:05,670 --> 01:35:06,880
 

4055
01:35:05,680 --> 01:35:11,400
 with any number of points of the star

4056
01:35:06,870 --> 01:35:11,400
 

4057
01:35:06,880 --> 01:35:11,400
 number for linear separators is infinite

4058
01:35:12,080 --> 01:35:12,080
 

4059
01:35:12,090 --> 01:35:16,330
 for interval classifiers we're right

4060
01:35:14,880 --> 01:35:16,330
 

4061
01:35:14,890 --> 01:35:19,690
 it's like a segment it's infinite

4062
01:35:16,320 --> 01:35:19,690
 

4063
01:35:16,330 --> 01:35:20,620
 they're also very similar reason if

4064
01:35:19,680 --> 01:35:20,620
 

4065
01:35:19,690 --> 01:35:23,440
 you're talking about interval

4066
01:35:20,610 --> 01:35:23,440
 

4067
01:35:20,620 --> 01:35:25,600
 classifiers between 0 & 1 and you

4068
01:35:23,430 --> 01:35:25,600
 

4069
01:35:23,440 --> 01:35:26,890
 constrain their width to be W turns out

4070
01:35:25,590 --> 01:35:26,890
 

4071
01:35:25,600 --> 01:35:27,590
 that's an intermediate case where the

4072
01:35:26,880 --> 01:35:27,590
 

4073
01:35:26,890 --> 01:35:35,920
 star number is like

4074
01:35:27,580 --> 01:35:35,920
 

4075
01:35:27,590 --> 01:35:38,000
 1 over W ok so this is the star number

4076
01:35:35,910 --> 01:35:38,000
 

4077
01:35:35,920 --> 01:35:40,040
 why do we care about it well one reason

4078
01:35:37,990 --> 01:35:40,040
 

4079
01:35:38,000 --> 01:35:42,409
 we care about it is the complexity

4080
01:35:40,030 --> 01:35:42,409
 

4081
01:35:40,040 --> 01:35:45,980
 measures that I mentioned in the

4082
01:35:42,399 --> 01:35:45,980
 

4083
01:35:42,409 --> 01:35:49,400
 previous parts if you maximize them over

4084
01:35:45,970 --> 01:35:49,400
 

4085
01:35:45,980 --> 01:35:52,670
 the choice of the distribution on X and

4086
01:35:49,390 --> 01:35:52,670
 

4087
01:35:49,400 --> 01:35:54,110
 the F star function you get exactly the

4088
01:35:52,660 --> 01:35:54,110
 

4089
01:35:52,670 --> 01:35:55,880
 star number actually what you get is the

4090
01:35:54,100 --> 01:35:55,880
 

4091
01:35:54,110 --> 01:35:56,540
 minimum of the star number and 1 over

4092
01:35:55,870 --> 01:35:56,540
 

4093
01:35:55,880 --> 01:35:58,280
 Epsilon

4094
01:35:56,530 --> 01:35:58,280
 

4095
01:35:56,540 --> 01:36:00,290
 so let's just denote that by S sub

4096
01:35:58,270 --> 01:36:00,290
 

4097
01:35:58,280 --> 01:36:04,520
 epsilon took then simplify some

4098
01:36:00,280 --> 01:36:04,520
 

4099
01:36:00,290 --> 01:36:06,290
 expressions okay and and this is

4100
01:36:04,510 --> 01:36:06,290
 

4101
01:36:04,520 --> 01:36:08,290
 actually not unique to those complexity

4102
01:36:06,280 --> 01:36:08,290
 

4103
01:36:06,290 --> 01:36:10,400
 measures that there have been many other

4104
01:36:08,280 --> 01:36:10,400
 

4105
01:36:08,290 --> 01:36:12,380
 techniques in active learning proposed

4106
01:36:10,390 --> 01:36:12,380
 

4107
01:36:10,400 --> 01:36:14,119
 over the years and I won't have time to

4108
01:36:12,370 --> 01:36:14,119
 

4109
01:36:12,380 --> 01:36:15,650
 talk about all of them but each one

4110
01:36:14,109 --> 01:36:15,650
 

4111
01:36:14,119 --> 01:36:17,540
 comes with some interesting complexity

4112
01:36:15,640 --> 01:36:17,540
 

4113
01:36:15,650 --> 01:36:19,010
 measure and it turns out in every case

4114
01:36:17,530 --> 01:36:19,010
 

4115
01:36:17,540 --> 01:36:21,349
 if you maximize them over the

4116
01:36:19,000 --> 01:36:21,349
 

4117
01:36:19,010 --> 01:36:24,380
 distribution and start functioning you

4118
01:36:21,339 --> 01:36:24,380
 

4119
01:36:21,349 --> 01:36:27,860
 you get the start number so it's kind of

4120
01:36:24,370 --> 01:36:27,860
 

4121
01:36:24,380 --> 01:36:30,580
 an interesting unification of all these

4122
01:36:27,850 --> 01:36:30,580
 

4123
01:36:27,860 --> 01:36:35,270
 various distribution dependent theories

4124
01:36:30,570 --> 01:36:35,270
 

4125
01:36:30,580 --> 01:36:38,599
 so for the meaning immediate corollary

4126
01:36:35,260 --> 01:36:38,599
 

4127
01:36:35,270 --> 01:36:39,889
 this is that under bounded noise you can

4128
01:36:38,589 --> 01:36:39,889
 

4129
01:36:38,599 --> 01:36:44,000
 get by with a number of labels that's

4130
01:36:39,879 --> 01:36:44,000
 

4131
01:36:39,889 --> 01:36:45,469
 like SD log 1 over epsilon or in the

4132
01:36:43,990 --> 01:36:45,469
 

4133
01:36:44,000 --> 01:36:47,510
 agnostic case you can get by with the

4134
01:36:45,459 --> 01:36:47,510
 

4135
01:36:45,469 --> 01:36:50,510
 number of labels that's like SD beta

4136
01:36:47,500 --> 01:36:50,510
 

4137
01:36:47,510 --> 01:36:53,360
 squared over epsilon squared actually

4138
01:36:50,500 --> 01:36:53,360
 

4139
01:36:50,510 --> 01:36:57,010
 with the beta in this s that we didn't

4140
01:36:53,350 --> 01:36:57,010
 

4141
01:36:53,360 --> 01:36:58,969
 show that but that you can also do that

4142
01:36:57,000 --> 01:36:58,969
 

4143
01:36:57,010 --> 01:37:02,929
 this S sub beta

4144
01:36:58,959 --> 01:37:02,929
 

4145
01:36:58,969 --> 01:37:05,869
 instead of F sub excellent ok and these

4146
01:37:02,919 --> 01:37:05,869
 

4147
01:37:02,929 --> 01:37:07,040
 are achieved by a squared then I'll say

4148
01:37:05,859 --> 01:37:07,040
 

4149
01:37:05,869 --> 01:37:08,719
 that there's actually a different

4150
01:37:07,030 --> 01:37:08,719
 

4151
01:37:07,040 --> 01:37:12,199
 algorithm that gets a little bit better

4152
01:37:08,709 --> 01:37:12,199
 

4153
01:37:08,719 --> 01:37:14,030
 than this and so this is us this is sort

4154
01:37:12,189 --> 01:37:14,030
 

4155
01:37:12,199 --> 01:37:16,070
 of moving beyond disagreement based

4156
01:37:14,020 --> 01:37:16,070
 

4157
01:37:14,030 --> 01:37:18,949
 active learning there's an algorithm

4158
01:37:16,060 --> 01:37:18,949
 

4159
01:37:16,070 --> 01:37:21,679
 that under bounded noise just gets S sub

4160
01:37:18,939 --> 01:37:21,679
 

4161
01:37:18,949 --> 01:37:23,719
 epsilon over D times log 1 over Epsilon

4162
01:37:21,669 --> 01:37:23,719
 

4163
01:37:21,679 --> 01:37:27,020
 so we sort of shave if s is bounded we

4164
01:37:23,709 --> 01:37:27,020
 

4165
01:37:23,719 --> 01:37:30,349
 shaved off a factor of D in that label

4166
01:37:27,010 --> 01:37:30,349
 

4167
01:37:27,020 --> 01:37:32,429
 complexity and this is actually nearly

4168
01:37:30,339 --> 01:37:32,429
 

4169
01:37:30,349 --> 01:37:33,840
 matching a lower bound so that

4170
01:37:32,419 --> 01:37:33,840
 

4171
01:37:32,429 --> 01:37:35,670
 in fact there's a sense in which this is

4172
01:37:33,830 --> 01:37:35,670
 

4173
01:37:33,840 --> 01:37:39,929
 not an improvable guarantee so this is

4174
01:37:35,660 --> 01:37:39,929
 

4175
01:37:35,670 --> 01:37:44,750
 kind of optimal in in the distribution

4176
01:37:39,919 --> 01:37:44,750
 

4177
01:37:39,929 --> 01:37:50,280
 free analysis under bounded noise and

4178
01:37:44,740 --> 01:37:50,280
 

4179
01:37:44,750 --> 01:37:51,929
 for agnostic we don't know so actually

4180
01:37:50,270 --> 01:37:51,929
 

4181
01:37:50,280 --> 01:37:53,400
 the immediate the guarantee that was

4182
01:37:51,919 --> 01:37:53,400
 

4183
01:37:51,929 --> 01:37:56,909
 stated over here for a disagreement

4184
01:37:53,390 --> 01:37:56,909
 

4185
01:37:53,400 --> 01:37:58,380
 based active learning the S times D beta

4186
01:37:56,899 --> 01:37:58,380
 

4187
01:37:56,909 --> 01:38:00,510
 squared over epsilon squared that's

4188
01:37:58,370 --> 01:38:00,510
 

4189
01:37:58,380 --> 01:38:04,590
 still the best that we know for agnostic

4190
01:38:00,500 --> 01:38:04,590
 

4191
01:38:00,510 --> 01:38:06,690
 learning the lower bound is like that

4192
01:38:04,580 --> 01:38:06,690
 

4193
01:38:04,590 --> 01:38:09,960
 but without the S so basically D beta

4194
01:38:06,680 --> 01:38:09,960
 

4195
01:38:06,690 --> 01:38:11,630
 squared over epsilon squared plus a term

4196
01:38:09,950 --> 01:38:11,630
 

4197
01:38:09,960 --> 01:38:14,310
 that's essentially the bounded noise

4198
01:38:11,620 --> 01:38:14,310
 

4199
01:38:11,630 --> 01:38:18,449
 sample complexity that's a special case

4200
01:38:14,300 --> 01:38:18,449
 

4201
01:38:14,310 --> 01:38:19,650
 of agnostic noise it's not known if you

4202
01:38:18,439 --> 01:38:19,650
 

4203
01:38:18,449 --> 01:38:21,030
 can get an upper bound that that

4204
01:38:19,640 --> 01:38:21,030
 

4205
01:38:19,650 --> 01:38:24,000
 essentially matches that in particular

4206
01:38:21,020 --> 01:38:24,000
 

4207
01:38:21,030 --> 01:38:25,800
 what I wrote here this open question can

4208
01:38:23,990 --> 01:38:25,800
 

4209
01:38:24,000 --> 01:38:28,199
 you get D times beta squared over

4210
01:38:25,790 --> 01:38:28,199
 

4211
01:38:25,800 --> 01:38:30,960
 epsilon squared plus the bounded noise

4212
01:38:28,189 --> 01:38:30,960
 

4213
01:38:28,199 --> 01:38:33,199
 sample complexity upper bound that's not

4214
01:38:30,950 --> 01:38:33,199
 

4215
01:38:30,960 --> 01:38:33,199
 known

4216
01:38:33,490 --> 01:38:33,490
 

4217
01:38:33,500 --> 01:38:37,650
 incidentally I have offered a price for

4218
01:38:35,840 --> 01:38:37,650
 

4219
01:38:35,850 --> 01:38:39,449
 this if any of you can can prove this is

4220
01:38:37,640 --> 01:38:39,449
 

4221
01:38:37,650 --> 01:38:48,810
 true I'm willing to give you five

4222
01:38:39,439 --> 01:38:48,810
 

4223
01:38:39,449 --> 01:38:51,270
 hundred dollars great okay so that's

4224
01:38:48,800 --> 01:38:51,270
 

4225
01:38:48,810 --> 01:38:54,449
 some guarantees you can get let's talk

4226
01:38:51,260 --> 01:38:54,449
 

4227
01:38:51,270 --> 01:38:56,190
 about how to get these guarantees in the

4228
01:38:54,439 --> 01:38:56,190
 

4229
01:38:54,449 --> 01:38:57,810
 case of bounded noise so we'll talk

4230
01:38:56,180 --> 01:38:57,810
 

4231
01:38:56,190 --> 01:38:59,820
 about an algorithm that makes this

4232
01:38:57,800 --> 01:38:59,820
 

4233
01:38:57,810 --> 01:39:02,690
 improvement for bounded noise and solves

4234
01:38:59,810 --> 01:39:02,690
 

4235
01:38:59,820 --> 01:39:05,570
 a special case of this agnostic question

4236
01:39:02,680 --> 01:39:05,570
 

4237
01:39:02,690 --> 01:39:07,949
 okay so the way that this happens is

4238
01:39:05,560 --> 01:39:07,949
 

4239
01:39:05,570 --> 01:39:09,929
 basically what what has disagreement

4240
01:39:07,939 --> 01:39:09,929
 

4241
01:39:07,949 --> 01:39:11,460
 based active learning been good at it's

4242
01:39:09,919 --> 01:39:11,460
 

4243
01:39:09,929 --> 01:39:13,020
 been good at sort of finding interesting

4244
01:39:11,450 --> 01:39:13,020
 

4245
01:39:11,460 --> 01:39:15,830
 things happening with the F star

4246
01:39:13,010 --> 01:39:15,830
 

4247
01:39:13,020 --> 01:39:18,449
 function places where there's a lot of

4248
01:39:15,820 --> 01:39:18,449
 

4249
01:39:15,830 --> 01:39:19,920
 interesting stuff like decision

4250
01:39:18,439 --> 01:39:19,920
 

4251
01:39:18,449 --> 01:39:23,940
 boundaries and things like that finding

4252
01:39:19,910 --> 01:39:23,940
 

4253
01:39:19,920 --> 01:39:24,960
 decision boundaries finding places you

4254
01:39:23,930 --> 01:39:24,960
 

4255
01:39:23,940 --> 01:39:26,310
 know where you want to focus your

4256
01:39:24,950 --> 01:39:26,310
 

4257
01:39:24,960 --> 01:39:28,320
 queries because there's a lot of

4258
01:39:26,300 --> 01:39:28,320
 

4259
01:39:26,310 --> 01:39:29,520
 complexity in the F star function but

4260
01:39:28,310 --> 01:39:29,520
 

4261
01:39:28,320 --> 01:39:30,840
 there's another thing that you can do

4262
01:39:29,510 --> 01:39:30,840
 

4263
01:39:29,520 --> 01:39:32,550
 with active learning that disagreement

4264
01:39:30,830 --> 01:39:32,550
 

4265
01:39:30,840 --> 01:39:35,640
 based active learning doesn't do at all

4266
01:39:32,540 --> 01:39:35,640
 

4267
01:39:32,550 --> 01:39:36,989
 and that's adapting to heterogeneity in

4268
01:39:35,630 --> 01:39:36,989
 

4269
01:39:35,640 --> 01:39:38,520
 the noise meaning that there's some

4270
01:39:36,979 --> 01:39:38,520
 

4271
01:39:36,989 --> 01:39:40,500
 places that are very noisy and some

4272
01:39:38,510 --> 01:39:40,500
 

4273
01:39:38,520 --> 01:39:42,330
 places that are less noisy and you might

4274
01:39:40,490 --> 01:39:42,330
 

4275
01:39:40,500 --> 01:39:44,660
 want to adjust how many queries you make

4276
01:39:42,320 --> 01:39:44,660
 

4277
01:39:42,330 --> 01:39:46,070
 in each of those types of regions

4278
01:39:44,650 --> 01:39:46,070
 

4279
01:39:44,660 --> 01:39:48,440
 okay and we're going to talk about here

4280
01:39:46,060 --> 01:39:48,440
 

4281
01:39:46,070 --> 01:39:53,300
 very briefly an algorithm that is able

4282
01:39:48,430 --> 01:39:53,300
 

4283
01:39:48,440 --> 01:39:55,640
 to do that so this is so basically we're

4284
01:39:53,290 --> 01:39:55,640
 

4285
01:39:53,300 --> 01:39:57,170
 gonna imagine having some active

4286
01:39:55,630 --> 01:39:57,170
 

4287
01:39:55,640 --> 01:39:59,270
 learning algorithm that expects to get

4288
01:39:57,160 --> 01:39:59,270
 

4289
01:39:57,170 --> 01:40:01,910
 exactly F star labels it expects to get

4290
01:39:59,260 --> 01:40:01,910
 

4291
01:39:59,270 --> 01:40:03,110
 true labels for the F star function and

4292
01:40:01,900 --> 01:40:03,110
 

4293
01:40:01,910 --> 01:40:04,130
 then what we'll have is a subroutine

4294
01:40:03,100 --> 01:40:04,130
 

4295
01:40:03,110 --> 01:40:07,520
 tik-tok

4296
01:40:04,120 --> 01:40:07,520
 

4297
01:40:04,130 --> 01:40:09,700
 that sort of D noises noisy labels in

4298
01:40:07,510 --> 01:40:09,700
 

4299
01:40:07,520 --> 01:40:14,530
 order to feed into that

4300
01:40:09,690 --> 01:40:14,530
 

4301
01:40:09,700 --> 01:40:17,630
 okay so again very briefly if we have

4302
01:40:14,520 --> 01:40:17,630
 

4303
01:40:14,530 --> 01:40:20,180
 let's denote a des of X is the expected

4304
01:40:17,620 --> 01:40:20,180
 

4305
01:40:17,630 --> 01:40:22,310
 value of y given X so Y here is plus -1

4306
01:40:20,170 --> 01:40:22,310
 

4307
01:40:20,180 --> 01:40:27,860
 labeled so this is a value that goes

4308
01:40:22,300 --> 01:40:27,860
 

4309
01:40:22,310 --> 01:40:30,080
 from minus 1 to plus 1 and if we suppose

4310
01:40:27,850 --> 01:40:30,080
 

4311
01:40:27,860 --> 01:40:32,000
 let's say f star is the global optimal

4312
01:40:30,070 --> 01:40:32,000
 

4313
01:40:30,080 --> 01:40:35,480
 function now and this is actually some

4314
01:40:31,990 --> 01:40:35,480
 

4315
01:40:32,000 --> 01:40:36,530
 additional assumption so instead of just

4316
01:40:35,470 --> 01:40:36,530
 

4317
01:40:35,480 --> 01:40:38,720
 being the best function in the class

4318
01:40:36,520 --> 01:40:38,720
 

4319
01:40:36,530 --> 01:40:40,040
 which we'll assume it's the best

4320
01:40:38,710 --> 01:40:40,040
 

4321
01:40:38,720 --> 01:40:41,840
 function the class but we'll also assume

4322
01:40:40,030 --> 01:40:41,840
 

4323
01:40:40,040 --> 01:40:43,400
 that it's equal to the sign of the

4324
01:40:41,830 --> 01:40:43,400
 

4325
01:40:41,840 --> 01:40:46,160
 regression for the sign of expected

4326
01:40:43,390 --> 01:40:46,160
 

4327
01:40:43,400 --> 01:40:49,900
 value of y given x which is the best

4328
01:40:46,150 --> 01:40:49,900
 

4329
01:40:46,160 --> 01:40:52,460
 that's the Bayes optimal function so now

4330
01:40:49,890 --> 01:40:52,460
 

4331
01:40:49,900 --> 01:40:54,130
 what this tick-tock subroutine this sort

4332
01:40:52,450 --> 01:40:54,130
 

4333
01:40:52,460 --> 01:40:56,900
 of denoising routine is going to do is

4334
01:40:54,120 --> 01:40:56,900
 

4335
01:40:54,130 --> 01:40:59,000
 essentially query if you give it a point

4336
01:40:56,890 --> 01:40:59,000
 

4337
01:40:56,900 --> 01:41:01,790
 X you say I want to know f stars label

4338
01:40:58,990 --> 01:41:01,790
 

4339
01:40:59,000 --> 01:41:03,590
 at X so now it's gonna query let's say

4340
01:41:01,780 --> 01:41:03,590
 

4341
01:41:01,790 --> 01:41:05,930
 it can either query at X or maybe very

4342
01:41:03,580 --> 01:41:05,930
 

4343
01:41:03,590 --> 01:41:07,280
 nearby and you can there's some analysis

4344
01:41:05,920 --> 01:41:07,280
 

4345
01:41:05,930 --> 01:41:09,980
 you can do to show that there's a sense

4346
01:41:07,270 --> 01:41:09,980
 

4347
01:41:07,280 --> 01:41:12,520
 of metric there's a kind of a metric

4348
01:41:09,970 --> 01:41:12,520
 

4349
01:41:09,980 --> 01:41:15,950
 where very nearby has an importance in

4350
01:41:12,510 --> 01:41:15,950
 

4351
01:41:12,520 --> 01:41:19,100
 VC classes so if you query very nearby

4352
01:41:15,940 --> 01:41:19,100
 

4353
01:41:15,950 --> 01:41:20,840
 to this point X you can get labels that

4354
01:41:19,090 --> 01:41:20,840
 

4355
01:41:19,100 --> 01:41:23,570
 are essentially like independent draws

4356
01:41:20,830 --> 01:41:23,570
 

4357
01:41:20,840 --> 01:41:25,730
 from Y given X and you can use them to

4358
01:41:23,560 --> 01:41:25,730
 

4359
01:41:23,570 --> 01:41:27,530
 try to figure out you're sort of doing a

4360
01:41:25,720 --> 01:41:27,530
 

4361
01:41:25,730 --> 01:41:30,740
 sequential hypothesis test to sort of

4362
01:41:27,520 --> 01:41:30,740
 

4363
01:41:27,530 --> 01:41:32,270
 figure out maybe with some reasoning

4364
01:41:30,730 --> 01:41:32,270
 

4365
01:41:30,740 --> 01:41:36,260
 involving the Hosting's inequality for

4366
01:41:32,260 --> 01:41:36,260
 

4367
01:41:32,270 --> 01:41:37,280
 instance to figure out what is whether

4368
01:41:36,250 --> 01:41:37,280
 

4369
01:41:36,260 --> 01:41:39,260
 the AIDA

4370
01:41:37,270 --> 01:41:39,260
 

4371
01:41:37,280 --> 01:41:41,780
 value the expected value of y given X is

4372
01:41:39,250 --> 01:41:41,780
 

4373
01:41:39,260 --> 01:41:43,610
 bigger than 0 or less than 0 if it's

4374
01:41:41,770 --> 01:41:43,610
 

4375
01:41:41,780 --> 01:41:45,920
 bigger than 0 I'm going to infer that

4376
01:41:43,600 --> 01:41:45,920
 

4377
01:41:43,610 --> 01:41:46,600
 this is a positive if it's less than 0

4378
01:41:45,910 --> 01:41:46,600
 

4379
01:41:45,920 --> 01:41:48,610
 I'm going to end

4380
01:41:46,590 --> 01:41:48,610
 

4381
01:41:46,600 --> 01:41:51,190
 further this is a negative and return

4382
01:41:48,600 --> 01:41:51,190
 

4383
01:41:48,610 --> 01:41:57,150
 that label but and here's the important

4384
01:41:51,180 --> 01:41:57,150
 

4385
01:41:51,190 --> 01:42:00,070
 thing if I won I'm gonna set a cut-off

4386
01:41:57,140 --> 01:42:00,070
 

4387
01:41:57,150 --> 01:42:02,820
 this towel and if I have made towel

4388
01:42:00,060 --> 01:42:02,820
 

4389
01:42:00,070 --> 01:42:07,090
 queries and I still haven't figured out

4390
01:42:02,810 --> 01:42:07,090
 

4391
01:42:02,820 --> 01:42:09,490
 using hosting arguments whether the sign

4392
01:42:07,080 --> 01:42:09,490
 

4393
01:42:07,090 --> 01:42:12,820
 whether the sign of expected value of y

4394
01:42:09,480 --> 01:42:12,820
 

4395
01:42:09,490 --> 01:42:15,190
 given x is plus or minus I'm gonna give

4396
01:42:12,810 --> 01:42:15,190
 

4397
01:42:12,820 --> 01:42:17,530
 up I don't return anything so in the end

4398
01:42:15,180 --> 01:42:17,530
 

4399
01:42:15,190 --> 01:42:20,530
 we're gonna be training and an algorithm

4400
01:42:17,520 --> 01:42:20,530
 

4401
01:42:17,530 --> 01:42:24,570
 on just labels that were returned from

4402
01:42:20,520 --> 01:42:24,570
 

4403
01:42:20,530 --> 01:42:27,430
 this just the examples that this tik-tok

4404
01:42:24,560 --> 01:42:27,430
 

4405
01:42:24,570 --> 01:42:29,920
 tiered cutoff in a test for the optimal

4406
01:42:27,420 --> 01:42:29,920
 

4407
01:42:27,430 --> 01:42:32,980
 classification but what's the acronym if

4408
01:42:29,910 --> 01:42:32,980
 

4409
01:42:29,920 --> 01:42:34,300
 just the labels that tik-tok returns ok

4410
01:42:32,970 --> 01:42:34,300
 

4411
01:42:32,980 --> 01:42:38,290
 so if we think about what's the effect

4412
01:42:34,290 --> 01:42:38,290
 

4413
01:42:34,300 --> 01:42:39,730
 of that the one that we don't return

4414
01:42:38,280 --> 01:42:39,730
 

4415
01:42:38,290 --> 01:42:41,710
 those are the ones where it's hard to

4416
01:42:39,720 --> 01:42:41,710
 

4417
01:42:39,730 --> 01:42:44,050
 figure out whether it's above zero or

4418
01:42:41,700 --> 01:42:44,050
 

4419
01:42:41,710 --> 01:42:47,020
 minus or below zero those are the very

4420
01:42:44,040 --> 01:42:47,020
 

4421
01:42:44,050 --> 01:42:50,080
 noisy points so this is essentially

4422
01:42:47,010 --> 01:42:50,080
 

4423
01:42:47,020 --> 01:42:52,270
 biasing our queries toward point less

4424
01:42:50,070 --> 01:42:52,270
 

4425
01:42:50,080 --> 01:42:55,180
 noisy points and this has a double

4426
01:42:52,260 --> 01:42:55,180
 

4427
01:42:52,270 --> 01:42:56,350
 advantage one the less noisy points are

4428
01:42:55,170 --> 01:42:56,350
 

4429
01:42:55,180 --> 01:42:59,560
 the points that we actually care about

4430
01:42:56,340 --> 01:42:59,560
 

4431
01:42:56,350 --> 01:43:01,390
 if you write down what is the expected

4432
01:42:59,550 --> 01:43:01,390
 

4433
01:42:59,560 --> 01:43:04,590
 if you write down what is the excess

4434
01:43:01,380 --> 01:43:04,590
 

4435
01:43:01,390 --> 01:43:07,150
 risk at a point condition on a point

4436
01:43:04,580 --> 01:43:07,150
 

4437
01:43:04,590 --> 01:43:09,400
 it's exactly it's the indicator that the

4438
01:43:07,140 --> 01:43:09,400
 

4439
01:43:07,150 --> 01:43:12,820
 two functions disagree times the

4440
01:43:09,390 --> 01:43:12,820
 

4441
01:43:09,400 --> 01:43:15,040
 absolute value of ADA okay so a large

4442
01:43:12,810 --> 01:43:15,040
 

4443
01:43:12,820 --> 01:43:17,560
 ADA that's a less noisy point is a point

4444
01:43:15,030 --> 01:43:17,560
 

4445
01:43:15,040 --> 01:43:18,930
 that we care about more in the excess

4446
01:43:17,550 --> 01:43:18,930
 

4447
01:43:17,560 --> 01:43:22,180
 risk it's a point is a point that

4448
01:43:18,920 --> 01:43:22,180
 

4449
01:43:18,930 --> 01:43:27,960
 labeling an opposite F star matters more

4450
01:43:22,170 --> 01:43:27,960
 

4451
01:43:22,180 --> 01:43:32,380
 to the risk and the other thing is if

4452
01:43:27,950 --> 01:43:32,380
 

4453
01:43:27,960 --> 01:43:34,630
 the points that have lower value of ADA

4454
01:43:32,370 --> 01:43:34,630
 

4455
01:43:32,380 --> 01:43:36,610
 are also easier to determine the F star

4456
01:43:34,620 --> 01:43:36,610
 

4457
01:43:34,630 --> 01:43:38,380
 value right so the number of queries

4458
01:43:36,600 --> 01:43:38,380
 

4459
01:43:36,610 --> 01:43:40,270
 needed to determine f star using this

4460
01:43:38,370 --> 01:43:40,270
 

4461
01:43:38,380 --> 01:43:42,940
 hoeffding argument is like 1 over a 2

4462
01:43:40,260 --> 01:43:42,940
 

4463
01:43:40,270 --> 01:43:45,310
 squared okay so we get this double

4464
01:43:42,930 --> 01:43:45,310
 

4465
01:43:42,940 --> 01:43:48,390
 advantage of we use less queries and we

4466
01:43:45,300 --> 01:43:48,390
 

4467
01:43:45,310 --> 01:43:50,890
 focus on the points that matter more

4468
01:43:48,380 --> 01:43:50,890
 

4469
01:43:48,390 --> 01:43:52,240
 okay and the output of this is that

4470
01:43:50,880 --> 01:43:52,240
 

4471
01:43:50,890 --> 01:43:55,900
 guarantee that I mentioned for bounded

4472
01:43:52,230 --> 01:43:55,900
 

4473
01:43:52,240 --> 01:43:56,540
 noise this algorithm gets that s log 1

4474
01:43:55,890 --> 01:43:56,540
 

4475
01:43:55,900 --> 01:43:59,570
 over Epsilon

4476
01:43:56,530 --> 01:43:59,570
 

4477
01:43:56,540 --> 01:44:01,280
 and in the agnostic case with the one

4478
01:43:59,560 --> 01:44:01,280
 

4479
01:43:59,570 --> 01:44:03,980
 additional assumption that F star is

4480
01:44:01,270 --> 01:44:03,980
 

4481
01:44:01,280 --> 01:44:08,210
 this global best function then we're

4482
01:44:03,970 --> 01:44:08,210
 

4483
01:44:03,980 --> 01:44:10,070
 getting exactly the the conjecture that

4484
01:44:08,200 --> 01:44:10,070
 

4485
01:44:08,210 --> 01:44:11,450
 I mentioned on the previous slide that D

4486
01:44:10,060 --> 01:44:11,450
 

4487
01:44:10,070 --> 01:44:14,720
 times beta squared over epsilon squared

4488
01:44:11,440 --> 01:44:14,720
 

4489
01:44:11,450 --> 01:44:16,460
 plus the bounded noise complexity okay

4490
01:44:14,710 --> 01:44:16,460
 

4491
01:44:14,720 --> 01:44:20,810
 so it's au and it and this is optimal

4492
01:44:16,450 --> 01:44:20,810
 

4493
01:44:16,460 --> 01:44:22,970
 you unimprovable okay and this gives us

4494
01:44:20,800 --> 01:44:22,970
 

4495
01:44:20,810 --> 01:44:24,770
 some motivation of a couple of

4496
01:44:22,960 --> 01:44:24,770
 

4497
01:44:22,970 --> 01:44:27,650
 principles for active learning we want a

4498
01:44:24,760 --> 01:44:27,650
 

4499
01:44:24,770 --> 01:44:29,420
 query in dense regions where f hat could

4500
01:44:27,640 --> 01:44:29,420
 

4501
01:44:27,650 --> 01:44:30,950
 disagree a lot with that star that's the

4502
01:44:29,410 --> 01:44:30,950
 

4503
01:44:29,420 --> 01:44:32,780
 kind of thing that disagreement based

4504
01:44:30,940 --> 01:44:32,780
 

4505
01:44:30,950 --> 01:44:34,550
 active learning does and then the second

4506
01:44:32,770 --> 01:44:34,550
 

4507
01:44:32,780 --> 01:44:36,230
 part we want to query in regions that

4508
01:44:34,540 --> 01:44:36,230
 

4509
01:44:34,550 --> 01:44:39,730
 have low noise because that's what those

4510
01:44:36,220 --> 01:44:39,730
 

4511
01:44:36,230 --> 01:44:42,590
 points matter more and they're easier

4512
01:44:39,720 --> 01:44:42,590
 

4513
01:44:39,730 --> 01:44:46,190
 okay and I will skip over this for the

4514
01:44:42,580 --> 01:44:46,190
 

4515
01:44:42,590 --> 01:44:47,870
 sake of time so the conclusion there are

4516
01:44:46,180 --> 01:44:47,870
 

4517
01:44:46,190 --> 01:44:49,430
 many proposals for going beyond

4518
01:44:47,860 --> 01:44:49,430
 

4519
01:44:47,870 --> 01:44:51,350
 disagreement based active learning in

4520
01:44:49,420 --> 01:44:51,350
 

4521
01:44:49,430 --> 01:44:54,290
 each exhibits some improvements in

4522
01:44:51,340 --> 01:44:54,290
 

4523
01:44:51,350 --> 01:44:56,510
 certain cases but yet we still don't

4524
01:44:54,280 --> 01:44:56,510
 

4525
01:44:54,290 --> 01:44:57,830
 know what is the optimal agnostic active

4526
01:44:56,500 --> 01:44:57,830
 

4527
01:44:56,510 --> 01:45:00,170
 learning algorithm and in particular we

4528
01:44:57,820 --> 01:45:00,170
 

4529
01:44:57,830 --> 01:45:02,240
 don't know how to get this kind of a

4530
01:45:00,160 --> 01:45:02,240
 

4531
01:45:00,170 --> 01:45:03,500
 guarantee on the the sample complexity

4532
01:45:02,230 --> 01:45:03,500
 

4533
01:45:02,240 --> 01:45:07,430
 for agnostic learning without any

4534
01:45:03,490 --> 01:45:07,430
 

4535
01:45:03,500 --> 01:45:09,320
 additional constraint assumptions okay

4536
01:45:07,420 --> 01:45:09,320
 

4537
01:45:07,430 --> 01:45:13,610
 with that I will conclude the second

4538
01:45:09,310 --> 01:45:13,610
 

4539
01:45:09,320 --> 01:45:15,610
 part happy to take questions if we have

4540
01:45:13,600 --> 01:45:15,610
 

4541
01:45:13,610 --> 01:45:22,189
 just a brief moment

4542
01:45:15,600 --> 01:45:22,189
 

4543
01:45:15,610 --> 01:45:22,189
[Applause]

4544
01:46:09,850 --> 01:46:09,850
 

4545
01:46:09,860 --> 01:46:16,140
 okay great well thanks Steve for a

4546
01:46:13,580 --> 01:46:16,140
 

4547
01:46:13,590 --> 01:46:18,870
 really great deep dive into some of the

4548
01:46:16,130 --> 01:46:18,870
 

4549
01:46:16,140 --> 01:46:20,910
 theory of active learning what I'm going

4550
01:46:18,860 --> 01:46:20,910
 

4551
01:46:18,870 --> 01:46:24,030
 to suggest is I'm gonna give this last

4552
01:46:20,900 --> 01:46:24,030
 

4553
01:46:20,910 --> 01:46:26,460
 part for it's going to be a little bit

4554
01:46:24,020 --> 01:46:26,460
 

4555
01:46:24,030 --> 01:46:28,320
 more kind of open-ended a little less

4556
01:46:26,450 --> 01:46:28,320
 

4557
01:46:26,460 --> 01:46:29,790
 Theory a little bit more about maybe

4558
01:46:28,310 --> 01:46:29,790
 

4559
01:46:28,320 --> 01:46:31,980
 some of the important problems that

4560
01:46:29,780 --> 01:46:31,980
 

4561
01:46:29,790 --> 01:46:35,310
 could be out there really meaningful to

4562
01:46:31,970 --> 01:46:35,310
 

4563
01:46:31,980 --> 01:46:37,140
 applications too and then when I wrap up

4564
01:46:35,300 --> 01:46:37,140
 

4565
01:46:35,310 --> 01:46:40,110
 with that but I thought we could do is

4566
01:46:37,130 --> 01:46:40,110
 

4567
01:46:37,140 --> 01:46:42,060
 Steve can also come back up we can field

4568
01:46:40,100 --> 01:46:42,060
 

4569
01:46:40,110 --> 01:46:44,460
 all sorts of questions at that point

4570
01:46:42,050 --> 01:46:44,460
 

4571
01:46:42,060 --> 01:46:46,020
 people can also head off to the coffee

4572
01:46:44,450 --> 01:46:46,020
 

4573
01:46:44,460 --> 01:46:47,250
 break but Steve and I are going to hang

4574
01:46:46,010 --> 01:46:47,250
 

4575
01:46:46,020 --> 01:46:49,350
 around people want to come up and

4576
01:46:47,240 --> 01:46:49,350
 

4577
01:46:47,250 --> 01:46:51,540
 discuss things as well

4578
01:46:49,340 --> 01:46:51,540
 

4579
01:46:49,350 --> 01:46:54,780
 so what I want to talk about in this

4580
01:46:51,530 --> 01:46:54,780
 

4581
01:46:51,540 --> 01:46:56,850
 last will segment here is what you might

4582
01:46:54,770 --> 01:46:56,850
 

4583
01:46:54,780 --> 01:46:59,100
 call nonparametric active learning and

4584
01:46:56,840 --> 01:46:59,100
 

4585
01:46:56,850 --> 01:47:05,070
 I'll say a bit about what I mean by that

4586
01:46:59,090 --> 01:47:05,070
 

4587
01:46:59,100 --> 01:47:06,840
 is we we go here so a lot of the active

4588
01:47:05,060 --> 01:47:06,840
 

4589
01:47:05,070 --> 01:47:09,780
 learning fewer methods we've heard about

4590
01:47:06,830 --> 01:47:09,780
 

4591
01:47:06,840 --> 01:47:12,720
 so far are based on taking some class of

4592
01:47:09,770 --> 01:47:12,720
 

4593
01:47:09,780 --> 01:47:16,200
 models usually something like a finite

4594
01:47:12,710 --> 01:47:16,200
 

4595
01:47:12,720 --> 01:47:18,090
 VC class and then using the structure

4596
01:47:16,190 --> 01:47:18,090
 

4597
01:47:16,200 --> 01:47:19,710
 and geometry of that model class and the

4598
01:47:18,080 --> 01:47:19,710
 

4599
01:47:18,090 --> 01:47:23,840
 geometry and structure of the data set

4600
01:47:19,700 --> 01:47:23,840
 

4601
01:47:19,710 --> 01:47:26,280
 to kind of adaptively and selectively

4602
01:47:23,830 --> 01:47:26,280
 

4603
01:47:23,840 --> 01:47:29,940
 decide which labels that take to narrow

4604
01:47:26,270 --> 01:47:29,940
 

4605
01:47:26,280 --> 01:47:31,140
 down the pool of viable models and so

4606
01:47:29,930 --> 01:47:31,140
 

4607
01:47:29,940 --> 01:47:32,550
 let's just think about how that would

4608
01:47:31,130 --> 01:47:32,550
 

4609
01:47:31,140 --> 01:47:35,430
 work if we were just learning a linear

4610
01:47:32,540 --> 01:47:35,430
 

4611
01:47:32,550 --> 01:47:37,260
 classifier again this is the EHR type of

4612
01:47:35,420 --> 01:47:37,260
 

4613
01:47:35,430 --> 01:47:39,180
 problem I had up before in this

4614
01:47:37,250 --> 01:47:39,180
 

4615
01:47:37,260 --> 01:47:41,220
 particular case the best linear

4616
01:47:39,170 --> 01:47:41,220
 

4617
01:47:39,180 --> 01:47:44,150
 classifier for this set of labeled data

4618
01:47:41,210 --> 01:47:44,150
 

4619
01:47:41,220 --> 01:47:46,650
 would be the Green Line I show you here

4620
01:47:44,140 --> 01:47:46,650
 

4621
01:47:44,150 --> 01:47:49,050
 it's not a perfect linear classifier

4622
01:47:46,640 --> 01:47:49,050
 

4623
01:47:46,650 --> 01:47:51,450
 it's missing that little clump of three

4624
01:47:49,040 --> 01:47:51,450
 

4625
01:47:49,050 --> 01:47:53,550
 blue labeled examples up in the top but

4626
01:47:51,440 --> 01:47:53,550
 

4627
01:47:51,450 --> 01:47:55,950
 we can't really accommodate those with a

4628
01:47:53,540 --> 01:47:55,950
 

4629
01:47:53,550 --> 01:47:57,630
 linear classifier right so this is the

4630
01:47:55,940 --> 01:47:57,630
 

4631
01:47:55,950 --> 01:47:59,370
 kind of situation that's almost going to

4632
01:47:57,620 --> 01:47:59,370
 

4633
01:47:57,630 --> 01:48:01,800
 always happen in a real-world problem

4634
01:47:59,360 --> 01:48:01,800
 

4635
01:47:59,370 --> 01:48:03,900
 your models probably aren't going to be

4636
01:48:01,790 --> 01:48:03,900
 

4637
01:48:01,800 --> 01:48:06,600
 rich enough and flexible enough to cover

4638
01:48:03,890 --> 01:48:06,600
 

4639
01:48:03,900 --> 01:48:08,790
 everything you might encounter so what

4640
01:48:06,590 --> 01:48:08,790
 

4641
01:48:06,600 --> 01:48:11,280
 can happen what might go wrong with

4642
01:48:08,780 --> 01:48:11,280
 

4643
01:48:08,790 --> 01:48:13,710
 active learning here well it could be if

4644
01:48:11,270 --> 01:48:13,710
 

4645
01:48:11,280 --> 01:48:16,380
 you're not using one of the very careful

4646
01:48:13,700 --> 01:48:16,380
 

4647
01:48:13,710 --> 01:48:18,330
 algorithms that Steve described you

4648
01:48:16,370 --> 01:48:18,330
 

4649
01:48:16,380 --> 01:48:19,720
 might actually end up converging to a

4650
01:48:18,320 --> 01:48:19,720
 

4651
01:48:18,330 --> 01:48:21,430
 suboptimal

4652
01:48:19,710 --> 01:48:21,430
 

4653
01:48:19,720 --> 01:48:22,750
 solution and this can happen especially

4654
01:48:21,420 --> 01:48:22,750
 

4655
01:48:21,430 --> 01:48:24,820
 if you're doing one of these very

4656
01:48:22,740 --> 01:48:24,820
 

4657
01:48:22,750 --> 01:48:27,160
 aggressive greedy types of active

4658
01:48:24,810 --> 01:48:27,160
 

4659
01:48:24,820 --> 01:48:31,210
 learning algorithms that we talked about

4660
01:48:27,150 --> 01:48:31,210
 

4661
01:48:27,160 --> 01:48:34,120
 earlier in the first part and so the the

4662
01:48:31,200 --> 01:48:34,120
 

4663
01:48:31,210 --> 01:48:35,230
 inductive bias or model bias associated

4664
01:48:34,110 --> 01:48:35,230
 

4665
01:48:34,120 --> 01:48:37,630
 with the class of models were

4666
01:48:35,220 --> 01:48:37,630
 

4667
01:48:35,230 --> 01:48:39,640
 considering means that in situations

4668
01:48:37,620 --> 01:48:39,640
 

4669
01:48:37,630 --> 01:48:41,800
 where the data are really messy and the

4670
01:48:39,630 --> 01:48:41,800
 

4671
01:48:39,640 --> 01:48:44,140
 optimal Bayes classifier is not in that

4672
01:48:41,790 --> 01:48:44,140
 

4673
01:48:41,800 --> 01:48:45,880
 class then the standard active learning

4674
01:48:44,130 --> 01:48:45,880
 

4675
01:48:44,140 --> 01:48:47,560
 algorithms might not really work much

4676
01:48:45,870 --> 01:48:47,560
 

4677
01:48:45,880 --> 01:48:49,420
 better than passive learning and worse

4678
01:48:47,550 --> 01:48:49,420
 

4679
01:48:47,560 --> 01:48:50,920
 yet if you're not being really careful

4680
01:48:49,410 --> 01:48:50,920
 

4681
01:48:49,420 --> 01:48:52,570
 they could actually converge to

4682
01:48:50,910 --> 01:48:52,570
 

4683
01:48:50,920 --> 01:48:55,600
 suboptimal solutions and do worse than

4684
01:48:52,560 --> 01:48:55,600
 

4685
01:48:52,570 --> 01:48:58,030
 passive learning in the long run and so

4686
01:48:55,590 --> 01:48:58,030
 

4687
01:48:55,600 --> 01:49:00,490
 that's a concern why is it a concern

4688
01:48:58,020 --> 01:49:00,490
 

4689
01:48:58,030 --> 01:49:04,060
 well if you listen to some of the

4690
01:49:00,480 --> 01:49:04,060
 

4691
01:49:00,490 --> 01:49:07,570
 futurists we may be worshipping an AI

4692
01:49:04,050 --> 01:49:07,570
 

4693
01:49:04,060 --> 01:49:09,730
 god by I guess 2042 and we certainly

4694
01:49:07,560 --> 01:49:09,730
 

4695
01:49:07,570 --> 01:49:11,920
 want to make sure that this AI God is

4696
01:49:09,720 --> 01:49:11,920
 

4697
01:49:09,730 --> 01:49:13,990
 not converging to a suboptimal position

4698
01:49:11,910 --> 01:49:13,990
 

4699
01:49:11,920 --> 01:49:16,330
 so I don't think I can underscore the

4700
01:49:13,980 --> 01:49:16,330
 

4701
01:49:13,990 --> 01:49:21,910
 importance of getting this right more

4702
01:49:16,320 --> 01:49:21,910
 

4703
01:49:16,330 --> 01:49:23,620
 strongly okay so this is a term it was

4704
01:49:21,900 --> 01:49:23,620
 

4705
01:49:21,910 --> 01:49:25,450
 actually the title of a paper by Sandra

4706
01:49:23,610 --> 01:49:25,450
 

4707
01:49:23,620 --> 01:49:28,240
 desc Gupta they really liked two phases

4708
01:49:25,440 --> 01:49:28,240
 

4709
01:49:25,450 --> 01:49:29,950
 of active learning and basically he was

4710
01:49:28,230 --> 01:49:29,950
 

4711
01:49:28,240 --> 01:49:31,660
 saying like here are the two things that

4712
01:49:29,940 --> 01:49:31,660
 

4713
01:49:29,950 --> 01:49:33,310
 were generally trying to do either we

4714
01:49:31,650 --> 01:49:33,310
 

4715
01:49:31,660 --> 01:49:34,780
 want a label examples close to the

4716
01:49:33,300 --> 01:49:34,780
 

4717
01:49:33,310 --> 01:49:36,610
 decision boundary we've talked a lot

4718
01:49:34,770 --> 01:49:36,610
 

4719
01:49:34,780 --> 01:49:38,950
 about that and sure so a lot of pictures

4720
01:49:36,600 --> 01:49:38,950
 

4721
01:49:36,610 --> 01:49:41,290
 about that or you might want to find

4722
01:49:38,940 --> 01:49:41,290
 

4723
01:49:38,950 --> 01:49:43,590
 clusters in your unlabeled data set and

4724
01:49:41,280 --> 01:49:43,590
 

4725
01:49:41,290 --> 01:49:46,090
 label representatives from each cluster

4726
01:49:43,580 --> 01:49:46,090
 

4727
01:49:43,590 --> 01:49:49,480
 and that's sort of kind of riffing on

4728
01:49:46,080 --> 01:49:49,480
 

4729
01:49:46,090 --> 01:49:51,700
 the heuristic or the possibility that

4730
01:49:49,470 --> 01:49:51,700
 

4731
01:49:49,480 --> 01:49:54,700
 maybe clusters of data reflect

4732
01:49:51,690 --> 01:49:54,700
 

4733
01:49:51,700 --> 01:49:56,950
 homogeneous sets of labeled examples and

4734
01:49:54,690 --> 01:49:56,950
 

4735
01:49:54,700 --> 01:49:58,150
 so our goal in what I'll talk about now

4736
01:49:56,940 --> 01:49:58,150
 

4737
01:49:56,950 --> 01:50:00,820
 is to think about how we could use

4738
01:49:58,140 --> 01:50:00,820
 

4739
01:49:58,150 --> 01:50:04,120
 nonparametric or more gentler are also

4740
01:50:00,810 --> 01:50:04,120
 

4741
01:50:00,820 --> 01:50:06,520
 over parametrized models to avoid bias

4742
01:50:04,110 --> 01:50:06,520
 

4743
01:50:04,120 --> 01:50:08,680
 and design active learning algorithms

4744
01:50:06,510 --> 01:50:08,680
 

4745
01:50:06,520 --> 01:50:12,880
 that exploit intrinsic structure in the

4746
01:50:08,670 --> 01:50:12,880
 

4747
01:50:08,680 --> 01:50:14,680
 data like what I'm showing you here so I

4748
01:50:12,870 --> 01:50:14,680
 

4749
01:50:12,880 --> 01:50:17,200
 mean I first just talked about to

4750
01:50:14,670 --> 01:50:17,200
 

4751
01:50:14,680 --> 01:50:21,010
 motivational papers that I really like

4752
01:50:17,190 --> 01:50:21,010
 

4753
01:50:17,200 --> 01:50:23,740
 the first one is also by dust Gupta and

4754
01:50:21,000 --> 01:50:23,740
 

4755
01:50:21,010 --> 01:50:26,130
 true and this is a paper on hierarchical

4756
01:50:23,730 --> 01:50:26,130
 

4757
01:50:23,740 --> 01:50:29,460
 active clustering for

4758
01:50:26,120 --> 01:50:29,460
 

4759
01:50:26,130 --> 01:50:31,050
 to learn so the idea works like this so

4760
01:50:29,450 --> 01:50:31,050
 

4761
01:50:29,460 --> 01:50:33,659
 we're gonna first of all take our large

4762
01:50:31,040 --> 01:50:33,659
 

4763
01:50:31,050 --> 01:50:35,850
 pool of unlabeled data and we're going

4764
01:50:33,649 --> 01:50:35,850
 

4765
01:50:33,659 --> 01:50:38,190
 to hierarchically cluster it using some

4766
01:50:35,840 --> 01:50:38,190
 

4767
01:50:35,850 --> 01:50:39,840
 clustering criterion and I'm not going

4768
01:50:38,180 --> 01:50:39,840
 

4769
01:50:38,190 --> 01:50:41,100
 to go into what that criterion could be

4770
01:50:39,830 --> 01:50:41,100
 

4771
01:50:39,840 --> 01:50:44,219
 you could just be your favorite

4772
01:50:41,090 --> 01:50:44,219
 

4773
01:50:41,100 --> 01:50:46,199
 unsupervised clustering algorithm so

4774
01:50:44,209 --> 01:50:46,199
 

4775
01:50:44,219 --> 01:50:48,330
 let's say we do that we keep kind of

4776
01:50:46,189 --> 01:50:48,330
 

4777
01:50:46,199 --> 01:50:50,159
 splitting up our data doing this sort of

4778
01:50:48,320 --> 01:50:50,159
 

4779
01:50:48,330 --> 01:50:52,530
 hierarchical clustering so we end up

4780
01:50:50,149 --> 01:50:52,530
 

4781
01:50:50,159 --> 01:50:55,409
 with a cluster tree like this so that's

4782
01:50:52,520 --> 01:50:55,409
 

4783
01:50:52,530 --> 01:50:56,550
 step one now step two what we're going

4784
01:50:55,399 --> 01:50:56,550
 

4785
01:50:55,409 --> 01:50:58,290
 to do is we're going to go back up to

4786
01:50:56,540 --> 01:50:58,290
 

4787
01:50:56,550 --> 01:51:00,810
 the root of this tree first we're going

4788
01:50:58,280 --> 01:51:00,810
 

4789
01:50:58,290 --> 01:51:02,909
 to label some examples and we see that

4790
01:51:00,800 --> 01:51:02,909
 

4791
01:51:00,810 --> 01:51:05,730
 if we label this subset some are red

4792
01:51:02,899 --> 01:51:05,730
 

4793
01:51:02,909 --> 01:51:07,590
 some are blue so this cluster itself is

4794
01:51:05,720 --> 01:51:07,590
 

4795
01:51:05,730 --> 01:51:09,570
 in homogeneous it has some Reds it has

4796
01:51:07,580 --> 01:51:09,570
 

4797
01:51:07,590 --> 01:51:11,639
 some blues so we're going to have to go

4798
01:51:09,560 --> 01:51:11,639
 

4799
01:51:09,570 --> 01:51:15,000
 down further in the tree we're gonna go

4800
01:51:11,629 --> 01:51:15,000
 

4801
01:51:11,639 --> 01:51:17,010
 at the next level and I guess I'm not

4802
01:51:14,990 --> 01:51:17,010
 

4803
01:51:15,000 --> 01:51:19,710
 sure which yeah on the left there you

4804
01:51:17,000 --> 01:51:19,710
 

4805
01:51:17,010 --> 01:51:21,179
 see we have a kind of a mixed bag we

4806
01:51:19,700 --> 01:51:21,179
 

4807
01:51:19,710 --> 01:51:23,670
 have some red some blues again but over

4808
01:51:21,169 --> 01:51:23,670
 

4809
01:51:21,179 --> 01:51:25,260
 on the right we have the samples that we

4810
01:51:23,660 --> 01:51:25,260
 

4811
01:51:23,670 --> 01:51:26,909
 label they're all blue and so we use

4812
01:51:25,250 --> 01:51:26,909
 

4813
01:51:25,260 --> 01:51:30,420
 this or of the tests are these clusters

4814
01:51:26,899 --> 01:51:30,420
 

4815
01:51:26,909 --> 01:51:32,900
 looking like all the labels might be the

4816
01:51:30,410 --> 01:51:32,900
 

4817
01:51:30,420 --> 01:51:35,580
 same in within the cluster and so on the

4818
01:51:32,890 --> 01:51:35,580
 

4819
01:51:32,900 --> 01:51:37,199
 right there we think yeah probably not

4820
01:51:35,570 --> 01:51:37,199
 

4821
01:51:35,580 --> 01:51:39,179
 so we'll stop there that looks fairly

4822
01:51:37,189 --> 01:51:39,179
 

4823
01:51:37,199 --> 01:51:41,790
 homogeneous based on that sample and we

4824
01:51:39,169 --> 01:51:41,790
 

4825
01:51:39,179 --> 01:51:43,409
 can make this into a formal test but on

4826
01:51:41,780 --> 01:51:43,409
 

4827
01:51:41,790 --> 01:51:45,150
 the Left we have to keep going we're

4828
01:51:43,399 --> 01:51:45,150
 

4829
01:51:43,409 --> 01:51:47,219
 going to sample again there and so

4830
01:51:45,140 --> 01:51:47,219
 

4831
01:51:45,150 --> 01:51:49,560
 that's the idea you build up this tree

4832
01:51:47,209 --> 01:51:49,560
 

4833
01:51:47,219 --> 01:51:51,630
 in an unsupervised fashion and then you

4834
01:51:49,550 --> 01:51:51,630
 

4835
01:51:49,560 --> 01:51:53,280
 proceed down through the tree repeating

4836
01:51:51,620 --> 01:51:53,280
 

4837
01:51:51,630 --> 01:51:55,020
 this kind of test for homogeneity and

4838
01:51:53,270 --> 01:51:55,020
 

4839
01:51:53,280 --> 01:51:57,030
 the labels within each cluster and you

4840
01:51:55,010 --> 01:51:57,030
 

4841
01:51:55,020 --> 01:51:59,070
 stop when you get to someplace where you

4842
01:51:57,020 --> 01:51:59,070
 

4843
01:51:57,030 --> 01:52:00,989
 think yes I'm confident that it's fairly

4844
01:51:59,060 --> 01:52:00,989
 

4845
01:51:59,070 --> 01:52:03,480
 homogeneous and the cool thing about

4846
01:52:00,979 --> 01:52:03,480
 

4847
01:52:00,989 --> 01:52:06,210
 this is that if you had a tree like this

4848
01:52:03,470 --> 01:52:06,210
 

4849
01:52:03,480 --> 01:52:08,880
 and if it were possible to prune the

4850
01:52:06,200 --> 01:52:08,880
 

4851
01:52:06,210 --> 01:52:11,510
 cluster tree two M leaves so that the

4852
01:52:08,870 --> 01:52:11,510
 

4853
01:52:08,880 --> 01:52:15,540
 labels and each of those M clusters are

4854
01:52:11,500 --> 01:52:15,540
 

4855
01:52:11,510 --> 01:52:17,370
 relatively pure then our only order M

4856
01:52:15,530 --> 01:52:17,370
 

4857
01:52:15,540 --> 01:52:20,070
 labeled examples will suffice to

4858
01:52:17,360 --> 01:52:20,070
 

4859
01:52:17,370 --> 01:52:21,630
 accurately label the entire data set so

4860
01:52:20,060 --> 01:52:21,630
 

4861
01:52:20,070 --> 01:52:23,760
 you could almost think of this is kind

4862
01:52:21,620 --> 01:52:23,760
 

4863
01:52:21,630 --> 01:52:25,409
 of like an active annotation problem we

4864
01:52:23,750 --> 01:52:25,409
 

4865
01:52:23,760 --> 01:52:27,690
 have a huge pool of unlabeled examples

4866
01:52:25,399 --> 01:52:27,690
 

4867
01:52:25,409 --> 01:52:29,429
 we'd love to just have it fully labeled

4868
01:52:27,680 --> 01:52:29,429
 

4869
01:52:27,690 --> 01:52:30,690
 that would be fantastic we could use

4870
01:52:29,419 --> 01:52:30,690
 

4871
01:52:29,429 --> 01:52:32,699
 that for whatever machine learning

4872
01:52:30,680 --> 01:52:32,699
 

4873
01:52:30,690 --> 01:52:34,949
 purpose we'd like and so this gives you

4874
01:52:32,689 --> 01:52:34,949
 

4875
01:52:32,699 --> 01:52:37,679
 a kind of interactive adaptive way to

4876
01:52:34,939 --> 01:52:37,679
 

4877
01:52:34,949 --> 01:52:39,090
 try to annotate a very large data set

4878
01:52:37,669 --> 01:52:39,090
 

4879
01:52:37,679 --> 01:52:39,570
 and if there's a lot of structure in

4880
01:52:39,080 --> 01:52:39,570
 

4881
01:52:39,090 --> 01:52:41,280
 that data

4882
01:52:39,560 --> 01:52:41,280
 

4883
01:52:39,570 --> 01:52:43,349
 in this case in the form of clusters

4884
01:52:41,270 --> 01:52:43,349
 

4885
01:52:41,280 --> 01:52:45,090
 you're going to be able to do a good job

4886
01:52:43,339 --> 01:52:45,090
 

4887
01:52:43,349 --> 01:52:48,570
 with far fewer labels than labeling

4888
01:52:45,080 --> 01:52:48,570
 

4889
01:52:45,090 --> 01:52:51,179
 everything so another paper that was

4890
01:52:48,560 --> 01:52:51,179
 

4891
01:52:48,570 --> 01:52:54,750
 really inspiring to me is this paper by

4892
01:52:51,169 --> 01:52:54,750
 

4893
01:52:51,179 --> 01:52:56,670
 jus Lafferty and Garr money they called

4894
01:52:54,740 --> 01:52:56,670
 

4895
01:52:54,750 --> 01:52:58,800
 it combining active and semi-supervised

4896
01:52:56,660 --> 01:52:58,800
 

4897
01:52:56,670 --> 01:53:00,719
 learning and it sort of riffs on a

4898
01:52:58,790 --> 01:53:00,719
 

4899
01:52:58,800 --> 01:53:02,489
 similar idea we're going to use kind of

4900
01:53:00,709 --> 01:53:02,489
 

4901
01:53:00,719 --> 01:53:03,869
 a graphical notion and try to exploit

4902
01:53:02,479 --> 01:53:03,869
 

4903
01:53:02,489 --> 01:53:06,329
 the structure in geometry of the

4904
01:53:03,859 --> 01:53:06,329
 

4905
01:53:03,869 --> 01:53:08,550
 unlabeled data set so what where you'd

4906
01:53:06,319 --> 01:53:08,550
 

4907
01:53:06,329 --> 01:53:11,429
 start is you build nearest neighbor

4908
01:53:08,540 --> 01:53:11,429
 

4909
01:53:08,550 --> 01:53:13,260
 graph say of your unlabeled data set and

4910
01:53:11,419 --> 01:53:13,260
 

4911
01:53:11,429 --> 01:53:15,540
 then what you're going to do is use a

4912
01:53:13,250 --> 01:53:15,540
 

4913
01:53:13,260 --> 01:53:17,099
 prior probability model based on the

4914
01:53:15,530 --> 01:53:17,099
 

4915
01:53:15,540 --> 01:53:19,739
 graph structure of the graph laplacian

4916
01:53:17,089 --> 01:53:19,739
 

4917
01:53:17,099 --> 01:53:22,380
 and select examples in a sequential

4918
01:53:19,729 --> 01:53:22,380
 

4919
01:53:19,739 --> 01:53:25,800
 fashion to minimize the predicted risk

4920
01:53:22,370 --> 01:53:25,800
 

4921
01:53:22,380 --> 01:53:27,690
 under that prior probability model and

4922
01:53:25,790 --> 01:53:27,690
 

4923
01:53:25,800 --> 01:53:29,159
 then once you've labeled some subset of

4924
01:53:27,680 --> 01:53:29,159
 

4925
01:53:27,690 --> 01:53:30,900
 them like this what you can do is then

4926
01:53:29,149 --> 01:53:30,900
 

4927
01:53:29,159 --> 01:53:32,849
 propagate the labels to the rest of the

4928
01:53:30,890 --> 01:53:32,849
 

4929
01:53:30,900 --> 01:53:34,650
 graph using nearest neighbors of the

4930
01:53:32,839 --> 01:53:34,650
 

4931
01:53:32,849 --> 01:53:37,650
 graph laplacian extension or whatever

4932
01:53:34,640 --> 01:53:37,650
 

4933
01:53:34,650 --> 01:53:39,659
 and so this kind of graph theoretic

4934
01:53:37,640 --> 01:53:39,659
 

4935
01:53:37,650 --> 01:53:41,520
 semi-supervised learning time can

4936
01:53:39,649 --> 01:53:41,520
 

4937
01:53:39,659 --> 01:53:44,060
 exploit cluster structures but it can

4938
01:53:41,510 --> 01:53:44,060
 

4939
01:53:41,520 --> 01:53:46,739
 also detect boundaries and Serve has

4940
01:53:44,050 --> 01:53:46,739
 

4941
01:53:44,060 --> 01:53:48,659
 both of those phases of active learning

4942
01:53:46,729 --> 01:53:48,659
 

4943
01:53:46,739 --> 01:53:50,579
 are being exercised in a procedure like

4944
01:53:48,649 --> 01:53:50,579
 

4945
01:53:48,659 --> 01:53:52,560
 this and it works pretty well in

4946
01:53:50,569 --> 01:53:52,560
 

4947
01:53:50,579 --> 01:53:56,150
 practice this has some pretty impressive

4948
01:53:52,550 --> 01:53:56,150
 

4949
01:53:52,560 --> 01:53:58,980
 empirical results but there's not a very

4950
01:53:56,140 --> 01:53:58,980
 

4951
01:53:56,150 --> 01:54:00,810
 well there's no real theory to support

4952
01:53:58,970 --> 01:54:00,810
 

4953
01:53:58,980 --> 01:54:02,340
 how it's working we don't have sample

4954
01:54:00,800 --> 01:54:02,340
 

4955
01:54:00,810 --> 01:54:05,849
 complexity bounds and things like that

4956
01:54:02,330 --> 01:54:05,849
 

4957
01:54:02,340 --> 01:54:07,530
 so Jerry and I one of my students went

4958
01:54:05,839 --> 01:54:07,530
 

4959
01:54:05,849 --> 01:54:09,360
 back to the drawing board a few years

4960
01:54:07,520 --> 01:54:09,360
 

4961
01:54:07,530 --> 01:54:11,250
 ago and tried to develop a little bit of

4962
01:54:09,350 --> 01:54:11,250
 

4963
01:54:09,360 --> 01:54:12,659
 theory to help explain the good

4964
01:54:11,240 --> 01:54:12,659
 

4965
01:54:11,250 --> 01:54:16,409
 performance of these types of methods

4966
01:54:12,649 --> 01:54:16,409
 

4967
01:54:12,659 --> 01:54:18,030
 and we kind of developed a method you

4968
01:54:16,399 --> 01:54:18,030
 

4969
01:54:16,409 --> 01:54:19,860
 think of it sort of like a binary search

4970
01:54:18,020 --> 01:54:19,860
 

4971
01:54:18,030 --> 01:54:22,320
 on the graph and so the basic idea is

4972
01:54:19,850 --> 01:54:22,320
 

4973
01:54:19,860 --> 01:54:24,119
 you start off with that unlabeled graph

4974
01:54:22,310 --> 01:54:24,119
 

4975
01:54:22,320 --> 01:54:26,460
 over your the a graph over your

4976
01:54:24,109 --> 01:54:26,460
 

4977
01:54:24,119 --> 01:54:29,010
 unlabeled data set you start randomly

4978
01:54:26,450 --> 01:54:29,010
 

4979
01:54:26,460 --> 01:54:31,139
 selecting nodes for labeling until you

4980
01:54:29,000 --> 01:54:31,139
 

4981
01:54:29,010 --> 01:54:32,969
 find at least one red example in one

4982
01:54:31,129 --> 01:54:32,969
 

4983
01:54:31,139 --> 01:54:35,880
 blue example so for example here now I

4984
01:54:32,959 --> 01:54:35,880
 

4985
01:54:32,969 --> 01:54:37,020
 have one red for our three blue and what

4986
01:54:35,870 --> 01:54:37,020
 

4987
01:54:35,880 --> 01:54:38,780
 you're going to do at this point is

4988
01:54:37,010 --> 01:54:38,780
 

4989
01:54:37,020 --> 01:54:41,880
 you're going to look at all the paths

4990
01:54:38,770 --> 01:54:41,880
 

4991
01:54:38,780 --> 01:54:43,889
 through this graph between oppositely

4992
01:54:41,870 --> 01:54:43,889
 

4993
01:54:41,880 --> 01:54:46,469
 labeled examples so in this case there

4994
01:54:43,879 --> 01:54:46,469
 

4995
01:54:43,889 --> 01:54:48,449
 are three paths one from each of the

4996
01:54:46,459 --> 01:54:48,449
 

4997
01:54:46,469 --> 01:54:49,679
 blue to the red and then among those

4998
01:54:48,439 --> 01:54:49,679
 

4999
01:54:48,449 --> 01:54:51,989
 paths you're going to select the

5000
01:54:49,669 --> 01:54:51,989
 

5001
01:54:49,679 --> 01:54:52,670
 shortest shortest path so this is the

5002
01:54:51,979 --> 01:54:52,670
 

5003
01:54:51,989 --> 01:54:54,350
 shortest

5004
01:54:52,660 --> 01:54:54,350
 

5005
01:54:52,670 --> 01:54:56,390
 his bath this one and you're going to

5006
01:54:54,340 --> 01:54:56,390
 

5007
01:54:54,350 --> 01:54:58,640
 ask for a label that roughly bisects

5008
01:54:56,380 --> 01:54:58,640
 

5009
01:54:56,390 --> 01:55:00,800
 that path between the red and blue

5010
01:54:58,630 --> 01:55:00,800
 

5011
01:54:58,640 --> 01:55:03,530
 example you're going to repeat this

5012
01:55:00,790 --> 01:55:03,530
 

5013
01:55:00,800 --> 01:55:05,270
 process just keep refining and bisecting

5014
01:55:03,520 --> 01:55:05,270
 

5015
01:55:03,530 --> 01:55:07,460
 the shortest shortest path as you go and

5016
01:55:05,260 --> 01:55:07,460
 

5017
01:55:05,270 --> 01:55:09,410
 eventually end up like with a picture

5018
01:55:07,450 --> 01:55:09,410
 

5019
01:55:07,460 --> 01:55:10,760
 like this where you sort of disconnected

5020
01:55:09,400 --> 01:55:10,760
 

5021
01:55:09,410 --> 01:55:13,070
 parts of the graph from each other

5022
01:55:10,750 --> 01:55:13,070
 

5023
01:55:10,760 --> 01:55:14,990
 because once you have two oppositely

5024
01:55:13,060 --> 01:55:14,990
 

5025
01:55:13,070 --> 01:55:17,330
 labeled guys that are adjacent connected

5026
01:55:14,980 --> 01:55:17,330
 

5027
01:55:14,990 --> 01:55:18,770
 by an edge you remove that edge so you

5028
01:55:17,320 --> 01:55:18,770
 

5029
01:55:17,330 --> 01:55:20,660
 get a picture like this then you can

5030
01:55:18,760 --> 01:55:20,660
 

5031
01:55:18,770 --> 01:55:23,210
 propagate the labels again using nearest

5032
01:55:20,650 --> 01:55:23,210
 

5033
01:55:20,660 --> 01:55:24,320
 neighbor or some other extension and

5034
01:55:23,200 --> 01:55:24,320
 

5035
01:55:23,210 --> 01:55:27,590
 this have some really nice properties

5036
01:55:24,310 --> 01:55:27,590
 

5037
01:55:24,320 --> 01:55:29,540
 too it turns out that if the cut-set so

5038
01:55:27,580 --> 01:55:29,540
 

5039
01:55:27,590 --> 01:55:31,580
 if you had a graph that had homogeneous

5040
01:55:29,530 --> 01:55:31,580
 

5041
01:55:29,540 --> 01:55:33,980
 with labeled pieces you look at the cut

5042
01:55:31,570 --> 01:55:33,980
 

5043
01:55:31,580 --> 01:55:35,960
 set if that cut set has M edges then

5044
01:55:33,970 --> 01:55:35,960
 

5045
01:55:33,980 --> 01:55:38,030
 about order M labeled examples suffice

5046
01:55:35,950 --> 01:55:38,030
 

5047
01:55:35,960 --> 01:55:40,340
 to accurately label the entire data set

5048
01:55:38,020 --> 01:55:40,340
 

5049
01:55:38,030 --> 01:55:42,770
 so this again is sort of like this idea

5050
01:55:40,330 --> 01:55:42,770
 

5051
01:55:40,340 --> 01:55:44,990
 of active annotation you have a very

5052
01:55:42,760 --> 01:55:44,990
 

5053
01:55:42,770 --> 01:55:46,340
 large unlabeled data set you would love

5054
01:55:44,980 --> 01:55:46,340
 

5055
01:55:44,990 --> 01:55:47,690
 to have it totally labeled but that's

5056
01:55:46,330 --> 01:55:47,690
 

5057
01:55:46,340 --> 01:55:49,520
 going to cost you a lot so can I

5058
01:55:47,680 --> 01:55:49,520
 

5059
01:55:47,690 --> 01:55:52,520
 interactively go through that large

5060
01:55:49,510 --> 01:55:52,520
 

5061
01:55:49,520 --> 01:55:57,280
 labeled data set in a way that speeds up

5062
01:55:52,510 --> 01:55:57,280
 

5063
01:55:52,520 --> 01:56:01,820
 the ability to label it all accurately

5064
01:55:57,270 --> 01:56:01,820
 

5065
01:55:57,280 --> 01:56:03,860
 so the last part of the talk in the

5066
01:56:01,810 --> 01:56:03,860
 

5067
01:56:01,820 --> 01:56:05,450
 tutorial here I want to talk a little

5068
01:56:03,850 --> 01:56:05,450
 

5069
01:56:03,860 --> 01:56:07,550
 bit about how now we can take some of

5070
01:56:05,440 --> 01:56:07,550
 

5071
01:56:05,450 --> 01:56:09,980
 these ideas and move into you know

5072
01:56:07,540 --> 01:56:09,980
 

5073
01:56:07,550 --> 01:56:11,540
 really contemporary machine learning

5074
01:56:09,970 --> 01:56:11,540
 

5075
01:56:09,980 --> 01:56:13,760
 frameworks like kernel methods and

5076
01:56:11,530 --> 01:56:13,760
 

5077
01:56:11,540 --> 01:56:15,860
 neural networks the active learning

5078
01:56:13,750 --> 01:56:15,860
 

5079
01:56:13,760 --> 01:56:17,060
 based on the graphs and clusters are

5080
01:56:15,850 --> 01:56:17,060
 

5081
01:56:15,860 --> 01:56:18,740
 effective but they kind of had this

5082
01:56:17,050 --> 01:56:18,740
 

5083
01:56:17,060 --> 01:56:20,180
 two-stage process where first you build

5084
01:56:18,730 --> 01:56:20,180
 

5085
01:56:18,740 --> 01:56:22,070
 a graph or a partition over the

5086
01:56:20,170 --> 01:56:22,070
 

5087
01:56:20,180 --> 01:56:23,510
 unlabeled data set and then you exploit

5088
01:56:22,060 --> 01:56:23,510
 

5089
01:56:22,070 --> 01:56:24,890
 the graph or cluster structure for

5090
01:56:23,500 --> 01:56:24,890
 

5091
01:56:23,510 --> 01:56:26,270
 active learning and that doesn't really

5092
01:56:24,880 --> 01:56:26,270
 

5093
01:56:24,890 --> 01:56:27,410
 play well with things like neural

5094
01:56:26,260 --> 01:56:27,410
 

5095
01:56:26,270 --> 01:56:29,540
 networks and so forth so we're

5096
01:56:27,400 --> 01:56:29,540
 

5097
01:56:27,410 --> 01:56:31,070
 interested in can we develop similar

5098
01:56:29,530 --> 01:56:31,070
 

5099
01:56:29,540 --> 01:56:33,140
 procedures that can be applied directly

5100
01:56:31,060 --> 01:56:33,140
 

5101
01:56:31,070 --> 01:56:36,410
 to popular classifiers like kernel

5102
01:56:33,130 --> 01:56:36,410
 

5103
01:56:33,140 --> 01:56:39,200
 methods and neural networks so this is

5104
01:56:36,400 --> 01:56:39,200
 

5105
01:56:36,410 --> 01:56:41,450
 really kind of just ongoing work and it

5106
01:56:39,190 --> 01:56:41,450
 

5107
01:56:39,200 --> 01:56:43,250
 I'll show you a little bit of our

5108
01:56:41,440 --> 01:56:43,250
 

5109
01:56:41,450 --> 01:56:44,960
 understanding of the problem but I want

5110
01:56:43,240 --> 01:56:44,960
 

5111
01:56:43,250 --> 01:56:49,250
 to warn you that this is not a

5112
01:56:44,950 --> 01:56:49,250
 

5113
01:56:44,960 --> 01:56:50,600
 completely polished set of theory so to

5114
01:56:49,240 --> 01:56:50,600
 

5115
01:56:49,250 --> 01:56:52,220
 begin with I want to just remind

5116
01:56:50,590 --> 01:56:52,220
 

5117
01:56:50,600 --> 01:56:55,700
 everybody here I think people have seen

5118
01:56:52,210 --> 01:56:55,700
 

5119
01:56:52,220 --> 01:56:57,380
 this because it's been controversial and

5120
01:56:55,690 --> 01:56:57,380
 

5121
01:56:55,700 --> 01:56:58,550
 also really stimulated a lot of

5122
01:56:57,370 --> 01:56:58,550
 

5123
01:56:57,380 --> 01:57:01,310
 rethinking in the machine learning

5124
01:56:58,540 --> 01:57:01,310
 

5125
01:56:58,550 --> 01:57:03,050
 committee and that is this conventional

5126
01:57:01,300 --> 01:57:03,050
 

5127
01:57:01,310 --> 01:57:05,369
 wisdom of the so-called bias-variance

5128
01:57:03,040 --> 01:57:05,369
 

5129
01:57:03,050 --> 01:57:07,199
 tradeoff and the idea is that

5130
01:57:05,359 --> 01:57:07,199
 

5131
01:57:05,369 --> 01:57:09,179
 yes if we use very very complicated

5132
01:57:07,189 --> 01:57:09,179
 

5133
01:57:07,199 --> 01:57:11,130
 models we can eventually drive that

5134
01:57:09,169 --> 01:57:11,130
 

5135
01:57:09,179 --> 01:57:13,050
 training year to zero we can fit the

5136
01:57:11,120 --> 01:57:13,050
 

5137
01:57:11,130 --> 01:57:14,909
 training data exactly but then that

5138
01:57:13,040 --> 01:57:14,909
 

5139
01:57:13,050 --> 01:57:16,800
 might generalize very poorly and so

5140
01:57:14,899 --> 01:57:16,800
 

5141
01:57:14,909 --> 01:57:21,330
 that's what this kind of figure

5142
01:57:16,790 --> 01:57:21,330
 

5143
01:57:16,800 --> 01:57:23,250
 demonstrates the classic u-shaped form

5144
01:57:21,320 --> 01:57:23,250
 

5145
01:57:21,330 --> 01:57:24,810
 of the test error so if we start

5146
01:57:23,240 --> 01:57:24,810
 

5147
01:57:23,250 --> 01:57:27,449
 overfitting the training data we might

5148
01:57:24,800 --> 01:57:27,449
 

5149
01:57:24,810 --> 01:57:29,460
 actually generalize very poorly so this

5150
01:57:27,439 --> 01:57:29,460
 

5151
01:57:27,449 --> 01:57:31,409
 was basically how we thought about

5152
01:57:29,450 --> 01:57:31,409
 

5153
01:57:29,460 --> 01:57:33,810
 machine learning for decades this is

5154
01:57:31,399 --> 01:57:33,810
 

5155
01:57:31,409 --> 01:57:35,760
 sort of at the root of a lot of

5156
01:57:33,800 --> 01:57:35,760
 

5157
01:57:33,810 --> 01:57:38,099
 statistical learning theory including

5158
01:57:35,750 --> 01:57:38,099
 

5159
01:57:35,760 --> 01:57:39,989
 the VC analysis and all that but what

5160
01:57:38,089 --> 01:57:39,989
 

5161
01:57:38,099 --> 01:57:41,969
 happened is people started experimenting

5162
01:57:39,979 --> 01:57:41,969
 

5163
01:57:39,989 --> 01:57:43,590
 especially with deep neural networks and

5164
01:57:41,959 --> 01:57:43,590
 

5165
01:57:41,969 --> 01:57:45,989
 they found kind of surprising

5166
01:57:43,580 --> 01:57:45,989
 

5167
01:57:43,590 --> 01:57:47,580
 contradictions to this picture so for

5168
01:57:45,979 --> 01:57:47,580
 

5169
01:57:45,989 --> 01:57:50,969
 example this is some work using

5170
01:57:47,570 --> 01:57:50,969
 

5171
01:57:47,580 --> 01:57:54,239
 inception and see far ten and basically

5172
01:57:50,959 --> 01:57:54,239
 

5173
01:57:50,969 --> 01:57:55,980
 this neural network has more parameters

5174
01:57:54,229 --> 01:57:55,980
 

5175
01:57:54,239 --> 01:57:57,659
 than we have training examples so we can

5176
01:57:55,970 --> 01:57:57,659
 

5177
01:57:55,980 --> 01:57:59,699
 perfectly interpolate the training data

5178
01:57:57,649 --> 01:57:59,699
 

5179
01:57:57,659 --> 01:58:01,800
 that's why you see the train year going

5180
01:57:59,689 --> 01:58:01,800
 

5181
01:57:59,699 --> 01:58:04,590
 to zero so there's no bias we fit

5182
01:58:01,790 --> 01:58:04,590
 

5183
01:58:01,800 --> 01:58:05,969
 exactly the training data but also it

5184
01:58:04,580 --> 01:58:05,969
 

5185
01:58:04,590 --> 01:58:08,550
 doesn't kind of blow up the

5186
01:58:05,959 --> 01:58:08,550
 

5187
01:58:05,969 --> 01:58:10,949
 generalization is also very good or at

5188
01:58:08,540 --> 01:58:10,949
 

5189
01:58:08,550 --> 01:58:14,580
 least not getting worse as we over fit

5190
01:58:10,939 --> 01:58:14,580
 

5191
01:58:10,949 --> 01:58:16,020
 to the training data and so deep nets

5192
01:58:14,570 --> 01:58:16,020
 

5193
01:58:14,580 --> 01:58:17,820
 are trained to perfectly fit training

5194
01:58:16,010 --> 01:58:17,820
 

5195
01:58:16,020 --> 01:58:20,699
 data yet they still generalize well and

5196
01:58:17,810 --> 01:58:20,699
 

5197
01:58:17,820 --> 01:58:22,770
 this is been an observation that people

5198
01:58:20,689 --> 01:58:22,770
 

5199
01:58:20,699 --> 01:58:26,000
 are starting to understand theoretically

5200
01:58:22,760 --> 01:58:26,000
 

5201
01:58:22,770 --> 01:58:29,580
 and it really has been a big source of

5202
01:58:25,990 --> 01:58:29,580
 

5203
01:58:26,000 --> 01:58:31,670
 new research and machine learning so

5204
01:58:29,570 --> 01:58:31,670
 

5205
01:58:29,580 --> 01:58:35,550
 this picture I like a lot this is due to

5206
01:58:31,660 --> 01:58:35,550
 

5207
01:58:31,670 --> 01:58:36,840
 Belkin show and MA and Mandel it's what

5208
01:58:35,540 --> 01:58:36,840
 

5209
01:58:35,550 --> 01:58:38,940
 they call the double descent other

5210
01:58:36,830 --> 01:58:38,940
 

5211
01:58:36,840 --> 01:58:41,520
 people have also discussed this kind of

5212
01:58:38,930 --> 01:58:41,520
 

5213
01:58:38,940 --> 01:58:44,040
 behavior so to the left you see the

5214
01:58:41,510 --> 01:58:44,040
 

5215
01:58:41,520 --> 01:58:46,530
 conventional classical setting where we

5216
01:58:44,030 --> 01:58:46,530
 

5217
01:58:44,040 --> 01:58:48,420
 have fewer parameters than we have

5218
01:58:46,520 --> 01:58:48,420
 

5219
01:58:46,530 --> 01:58:50,250
 training examples and we do see that you

5220
01:58:48,410 --> 01:58:50,250
 

5221
01:58:48,420 --> 01:58:52,650
 shape test error that's what I'll call

5222
01:58:50,240 --> 01:58:52,650
 

5223
01:58:50,250 --> 01:58:54,900
 the classical regime but if we move over

5224
01:58:52,640 --> 01:58:54,900
 

5225
01:58:52,650 --> 01:58:56,400
 if we start looking at models that have

5226
01:58:54,890 --> 01:58:56,400
 

5227
01:58:54,900 --> 01:58:58,230
 more parameters than we have trading

5228
01:58:56,390 --> 01:58:58,230
 

5229
01:58:56,400 --> 01:59:00,210
 points then we move into this modern

5230
01:58:58,220 --> 01:59:00,210
 

5231
01:58:58,230 --> 01:59:02,880
 what we could call the interpolate in

5232
01:59:00,200 --> 01:59:02,880
 

5233
01:59:00,210 --> 01:59:05,040
 regime and what can happen is that as we

5234
01:59:02,870 --> 01:59:05,040
 

5235
01:59:02,880 --> 01:59:06,449
 push beyond that sort of critical

5236
01:59:05,030 --> 01:59:06,449
 

5237
01:59:05,040 --> 01:59:08,340
 sampling level where number of

5238
01:59:06,439 --> 01:59:08,340
 

5239
01:59:06,449 --> 01:59:10,800
 parameters equals number of training

5240
01:59:08,330 --> 01:59:10,800
 

5241
01:59:08,340 --> 01:59:13,949
 examples we can actually see the test

5242
01:59:10,790 --> 01:59:13,949
 

5243
01:59:10,800 --> 01:59:15,540
 error decreasing so even though we're

5244
01:59:13,939 --> 01:59:15,540
 

5245
01:59:13,949 --> 01:59:17,460
 interpreting perfectly fitting the

5246
01:59:15,530 --> 01:59:17,460
 

5247
01:59:15,540 --> 01:59:18,539
 training data which we can always do in

5248
01:59:17,450 --> 01:59:18,539
 

5249
01:59:17,460 --> 01:59:20,489
 principle as soon as we

5250
01:59:18,529 --> 01:59:20,489
 

5251
01:59:18,539 --> 01:59:23,129
 as many parameters as we have training

5252
01:59:20,479 --> 01:59:23,129
 

5253
01:59:20,489 --> 01:59:26,010
 examples if we keep pushing along on

5254
01:59:23,119 --> 01:59:26,010
 

5255
01:59:23,129 --> 01:59:28,289
 that complexity access we can drive the

5256
01:59:26,000 --> 01:59:28,289
 

5257
01:59:26,010 --> 01:59:30,899
 test error down so that's kind of crazy

5258
01:59:28,279 --> 01:59:30,899
 

5259
01:59:28,289 --> 01:59:32,339
 and very surprising but the idea is that

5260
01:59:30,889 --> 01:59:32,339
 

5261
01:59:30,899 --> 01:59:34,019
 we're not just looking at any

5262
01:59:32,329 --> 01:59:34,019
 

5263
01:59:32,339 --> 01:59:36,030
 interpolating model we're looking at

5264
01:59:34,009 --> 01:59:36,030
 

5265
01:59:34,019 --> 01:59:38,039
 interpolators there in some sense the

5266
01:59:36,020 --> 01:59:38,039
 

5267
01:59:36,030 --> 01:59:40,619
 smoothest of all functions that could

5268
01:59:38,029 --> 01:59:40,619
 

5269
01:59:38,039 --> 01:59:42,239
 interpolate a given set of training data

5270
01:59:40,609 --> 01:59:42,239
 

5271
01:59:40,619 --> 01:59:44,669
 and so smoothness subject to

5272
01:59:42,229 --> 01:59:44,669
 

5273
01:59:42,239 --> 01:59:46,949
 interpolation constraints is a form of

5274
01:59:44,659 --> 01:59:46,949
 

5275
01:59:44,669 --> 01:59:50,129
 regularization or a form of Occam's

5276
01:59:46,939 --> 01:59:50,129
 

5277
01:59:46,949 --> 01:59:52,469
 razor so this isn't just a kind of

5278
01:59:50,119 --> 01:59:52,469
 

5279
01:59:50,129 --> 01:59:55,919
 theory these are some experiments that

5280
01:59:52,459 --> 01:59:55,919
 

5281
01:59:52,469 --> 01:59:59,510
 Belkin and his collaborators did this is

5282
01:59:55,909 --> 01:59:59,510
 

5283
01:59:55,919 --> 02:00:02,069
 a neural network example but similar

5284
01:59:59,500 --> 02:00:02,069
 

5285
01:59:59,510 --> 02:00:03,989
 real data examples happen with kernels

5286
02:00:02,059 --> 02:00:03,989
 

5287
02:00:02,069 --> 02:00:06,389
 and random features as well but you see

5288
02:00:03,979 --> 02:00:06,389
 

5289
02:00:03,989 --> 02:00:08,189
 distinctly that W characteristic and the

5290
02:00:06,379 --> 02:00:08,189
 

5291
02:00:06,389 --> 02:00:10,079
 blue curve which is the test error here

5292
02:00:08,179 --> 02:00:10,079
 

5293
02:00:08,189 --> 02:00:12,899
 so it's definitely a real thing is

5294
02:00:10,069 --> 02:00:12,899
 

5295
02:00:10,079 --> 02:00:14,549
 definitely happening and we're starting

5296
02:00:12,889 --> 02:00:14,549
 

5297
02:00:12,899 --> 02:00:16,109
 to understand why that is and again the

5298
02:00:14,539 --> 02:00:16,109
 

5299
02:00:14,549 --> 02:00:17,699
 ideas that we're going to interpret

5300
02:00:16,099 --> 02:00:17,699
 

5301
02:00:16,109 --> 02:00:19,589
 we're going to perfectly fit training

5302
02:00:17,689 --> 02:00:19,589
 

5303
02:00:17,699 --> 02:00:21,719
 data but we're going to do it in a way

5304
02:00:19,579 --> 02:00:21,719
 

5305
02:00:19,589 --> 02:00:23,969
 using the smoothest possible function

5306
02:00:21,709 --> 02:00:23,969
 

5307
02:00:21,719 --> 02:00:28,619
 whatever within whatever architecture

5308
02:00:23,959 --> 02:00:28,619
 

5309
02:00:23,969 --> 02:00:30,839
 we're using so so now how do we use this

5310
02:00:28,609 --> 02:00:30,839
 

5311
02:00:28,619 --> 02:00:32,189
 what does this say we should be thinking

5312
02:00:30,829 --> 02:00:32,189
 

5313
02:00:30,839 --> 02:00:34,739
 about in terms of active learning and

5314
02:00:32,179 --> 02:00:34,739
 

5315
02:00:32,189 --> 02:00:37,109
 this again is somewhat speculation and

5316
02:00:34,729 --> 02:00:37,109
 

5317
02:00:34,739 --> 02:00:38,309
 kind of my initial thoughts on it but I

5318
02:00:37,099 --> 02:00:38,309
 

5319
02:00:37,109 --> 02:00:40,679
 think it's kind of interesting so I

5320
02:00:38,299 --> 02:00:40,679
 

5321
02:00:38,309 --> 02:00:42,329
 wanted to share it with you so standard

5322
02:00:40,669 --> 02:00:42,329
 

5323
02:00:40,679 --> 02:00:44,549
 active learning theory methods are best

5324
02:00:42,319 --> 02:00:44,549
 

5325
02:00:42,329 --> 02:00:46,679
 based on bounding test error in terms of

5326
02:00:44,539 --> 02:00:46,679
 

5327
02:00:44,549 --> 02:00:48,510
 training error right and that's what VC

5328
02:00:46,669 --> 02:00:48,510
 

5329
02:00:46,679 --> 02:00:51,109
 theory is all about but if we're

5330
02:00:48,500 --> 02:00:51,109
 

5331
02:00:48,510 --> 02:00:54,449
 interpolating then the training error is

5332
02:00:51,099 --> 02:00:54,449
 

5333
02:00:51,109 --> 02:00:56,459
 identically zero and so whatever model

5334
02:00:54,439 --> 02:00:56,459
 

5335
02:00:54,449 --> 02:00:58,979
 we have if we have multiple different

5336
02:00:56,449 --> 02:00:58,979
 

5337
02:00:56,459 --> 02:01:00,209
 deep neural networks or multiple kernel

5338
02:00:58,969 --> 02:01:00,209
 

5339
02:00:58,979 --> 02:01:02,099
 methods we're always going to train them

5340
02:01:00,199 --> 02:01:02,099
 

5341
02:01:00,209 --> 02:01:03,780
 to fit the training data so we need to

5342
02:01:02,089 --> 02:01:03,780
 

5343
02:01:02,099 --> 02:01:06,419
 kind of think about something other than

5344
02:01:03,770 --> 02:01:06,419
 

5345
02:01:03,780 --> 02:01:10,079
 just looking at that training error as a

5346
02:01:06,409 --> 02:01:10,079
 

5347
02:01:06,419 --> 02:01:11,999
 means of selecting good models so what

5348
02:01:10,069 --> 02:01:11,999
 

5349
02:01:10,079 --> 02:01:14,489
 I'm going to talk about now are some

5350
02:01:11,989 --> 02:01:14,489
 

5351
02:01:11,999 --> 02:01:16,249
 preliminary results in experiments with

5352
02:01:14,479 --> 02:01:16,249
 

5353
02:01:14,489 --> 02:01:18,869
 kernel machines and neural networks

5354
02:01:16,239 --> 02:01:18,869
 

5355
02:01:16,249 --> 02:01:20,909
 kernels kernel machines are single aided

5356
02:01:18,859 --> 02:01:20,909
 

5357
02:01:18,869 --> 02:01:23,820
 or hidden they're a single layer hidden

5358
02:01:20,899 --> 02:01:23,820
 

5359
02:01:20,909 --> 02:01:26,010
 single hidden layer neural networks

5360
02:01:23,810 --> 02:01:26,010
 

5361
02:01:23,820 --> 02:01:27,800
 and I'm going to be focusing on that

5362
02:01:26,000 --> 02:01:27,800
 

5363
02:01:26,010 --> 02:01:29,670
 just a single hidden layer case

5364
02:01:27,790 --> 02:01:29,670
 

5365
02:01:27,800 --> 02:01:31,380
 interpolation is going to be possible

5366
02:01:29,660 --> 02:01:31,380
 

5367
02:01:29,670 --> 02:01:33,120
 here if we're using say an infinite

5368
02:01:31,370 --> 02:01:33,120
 

5369
02:01:31,380 --> 02:01:34,590
 dimensional reproducing kernel Hilbert

5370
02:01:33,110 --> 02:01:34,590
 

5371
02:01:33,120 --> 02:01:36,690
 space or if we have an over

5372
02:01:34,580 --> 02:01:36,690
 

5373
02:01:34,590 --> 02:01:39,300
 parameterised neural network and so I'm

5374
02:01:36,680 --> 02:01:39,300
 

5375
02:01:36,690 --> 02:01:41,070
 kind of showing you that here by giving

5376
02:01:39,290 --> 02:01:41,070
 

5377
02:01:39,300 --> 02:01:43,320
 you a sense that this hidden layer is

5378
02:01:41,060 --> 02:01:43,320
 

5379
02:01:41,070 --> 02:01:45,690
 very very wide and so the number of

5380
02:01:43,310 --> 02:01:45,690
 

5381
02:01:43,320 --> 02:01:46,890
 units in that hidden layer might be many

5382
02:01:45,680 --> 02:01:46,890
 

5383
02:01:45,690 --> 02:01:48,480
 more than the number of training

5384
02:01:46,880 --> 02:01:48,480
 

5385
02:01:46,890 --> 02:01:49,710
 examples for example so we have more

5386
02:01:48,470 --> 02:01:49,710
 

5387
02:01:48,480 --> 02:01:53,820
 weights in our model than we have

5388
02:01:49,700 --> 02:01:53,820
 

5389
02:01:49,710 --> 02:01:55,260
 training examples so let's think about

5390
02:01:53,810 --> 02:01:55,260
 

5391
02:01:53,820 --> 02:01:57,720
 this for a second I'm just going to show

5392
02:01:55,250 --> 02:01:57,720
 

5393
02:01:55,260 --> 02:02:00,450
 you some pictures in 1d to build up a

5394
02:01:57,710 --> 02:02:00,450
 

5395
02:01:57,720 --> 02:02:03,690
 little intuition so suppose that I have

5396
02:02:00,440 --> 02:02:03,690
 

5397
02:02:00,450 --> 02:02:05,760
 these six labeled training examples the

5398
02:02:03,680 --> 02:02:05,760
 

5399
02:02:03,690 --> 02:02:07,530
 Reds and the blues they're labeled plus

5400
02:02:05,750 --> 02:02:07,530
 

5401
02:02:05,760 --> 02:02:09,390
 one or minus one and then I'm showing

5402
02:02:07,520 --> 02:02:09,390
 

5403
02:02:07,530 --> 02:02:11,550
 you as those open circles those are all

5404
02:02:09,380 --> 02:02:11,550
 

5405
02:02:09,390 --> 02:02:15,090
 the remaining unlabeled samples that I

5406
02:02:11,540 --> 02:02:15,090
 

5407
02:02:11,550 --> 02:02:17,280
 haven't received a label yet for the

5408
02:02:15,080 --> 02:02:17,280
 

5409
02:02:15,090 --> 02:02:20,760
 black curve is this minimum norm

5410
02:02:17,270 --> 02:02:20,760
 

5411
02:02:17,280 --> 02:02:22,440
 interpolator of the given six initial

5412
02:02:20,750 --> 02:02:22,440
 

5413
02:02:20,760 --> 02:02:25,530
 training points and now the question is

5414
02:02:22,430 --> 02:02:25,530
 

5415
02:02:22,440 --> 02:02:28,230
 well which point should we label next so

5416
02:02:25,520 --> 02:02:28,230
 

5417
02:02:25,530 --> 02:02:30,990
 if we label any given point I'm kind of

5418
02:02:28,220 --> 02:02:30,990
 

5419
02:02:28,230 --> 02:02:33,270
 sliding it around there we have two

5420
02:02:30,980 --> 02:02:33,270
 

5421
02:02:30,990 --> 02:02:36,510
 possibilities that new point will be

5422
02:02:33,260 --> 02:02:36,510
 

5423
02:02:33,270 --> 02:02:39,080
 labeled blue plus one or red minus one

5424
02:02:36,500 --> 02:02:39,080
 

5425
02:02:36,510 --> 02:02:43,410
 okay so which one should we take

5426
02:02:39,070 --> 02:02:43,410
 

5427
02:02:39,080 --> 02:02:45,360
 so suppose we took an example here I'm

5428
02:02:43,400 --> 02:02:45,360
 

5429
02:02:43,410 --> 02:02:48,570
 showing you in in this kind of magenta

5430
02:02:45,350 --> 02:02:48,570
 

5431
02:02:45,360 --> 02:02:51,330
 color a point you this is a new example

5432
02:02:48,560 --> 02:02:51,330
 

5433
02:02:48,570 --> 02:02:54,210
 that's in between two identically

5434
02:02:51,320 --> 02:02:54,210
 

5435
02:02:51,330 --> 02:02:56,760
 labeled examples so you can see that

5436
02:02:54,200 --> 02:02:56,760
 

5437
02:02:54,210 --> 02:03:00,030
 this new point is selected to be between

5438
02:02:56,750 --> 02:03:00,030
 

5439
02:02:56,760 --> 02:03:02,490
 two of the red label points and if I

5440
02:03:00,020 --> 02:03:02,490
 

5441
02:03:00,030 --> 02:03:04,770
 added that new point in either I'm going

5442
02:03:02,480 --> 02:03:04,770
 

5443
02:03:02,490 --> 02:03:07,500
 to get a label of plus one and if I do

5444
02:03:04,760 --> 02:03:07,500
 

5445
02:03:04,770 --> 02:03:09,540
 then I get the blue interpolation curve

5446
02:03:07,490 --> 02:03:09,540
 

5447
02:03:07,500 --> 02:03:12,300
 there show on the top or it will be

5448
02:03:09,530 --> 02:03:12,300
 

5449
02:03:09,540 --> 02:03:14,640
 labeled minus one red and I get the red

5450
02:03:12,290 --> 02:03:14,640
 

5451
02:03:12,300 --> 02:03:16,980
 interpolation curve that you shouldn't

5452
02:03:14,630 --> 02:03:16,980
 

5453
02:03:14,640 --> 02:03:19,260
 see below and what you can kind of see

5454
02:03:16,970 --> 02:03:19,260
 

5455
02:03:16,980 --> 02:03:22,380
 here is that because that new point lies

5456
02:03:19,250 --> 02:03:22,380
 

5457
02:03:19,260 --> 02:03:23,400
 right in between two red label points

5458
02:03:22,370 --> 02:03:23,400
 

5459
02:03:22,380 --> 02:03:24,960
 it's going to be pretty easy to

5460
02:03:23,390 --> 02:03:24,960
 

5461
02:03:23,400 --> 02:03:26,730
 interpolate so it's easy to interpolate

5462
02:03:24,950 --> 02:03:26,730
 

5463
02:03:24,960 --> 02:03:29,369
 that's what the red curve is showing if

5464
02:03:26,720 --> 02:03:29,369
 

5465
02:03:26,730 --> 02:03:30,420
 I switch the label to blue

5466
02:03:29,359 --> 02:03:30,420
 

5467
02:03:29,369 --> 02:03:32,159
 then it's going to be much more

5468
02:03:30,410 --> 02:03:32,159
 

5469
02:03:30,420 --> 02:03:34,829
 difficult to interpolate that's what you

5470
02:03:32,149 --> 02:03:34,829
 

5471
02:03:32,159 --> 02:03:36,780
 see on top so basically that that the

5472
02:03:34,819 --> 02:03:36,780
 

5473
02:03:34,829 --> 02:03:38,579
 wig leanness of the blue curve is saying

5474
02:03:36,770 --> 02:03:38,579
 

5475
02:03:36,780 --> 02:03:41,039
 it's difficult to interpret it-it's less

5476
02:03:38,569 --> 02:03:41,039
 

5477
02:03:38,579 --> 02:03:42,630
 smooth whereas the red curve is very

5478
02:03:41,029 --> 02:03:42,630
 

5479
02:03:41,039 --> 02:03:44,309
 smooth not much different than the

5480
02:03:42,620 --> 02:03:44,309
 

5481
02:03:42,630 --> 02:03:46,980
 original black curve and it's much

5482
02:03:44,299 --> 02:03:46,980
 

5483
02:03:44,309 --> 02:03:49,050
 easier to interpolate and so we can

5484
02:03:46,970 --> 02:03:49,050
 

5485
02:03:46,980 --> 02:03:52,710
 measure this in terms of a norm the norm

5486
02:03:49,040 --> 02:03:52,710
 

5487
02:03:49,050 --> 02:03:55,349
 could be the norm in the RK HS space or

5488
02:03:52,700 --> 02:03:55,349
 

5489
02:03:52,710 --> 02:04:00,360
 it could be the norm of the neural

5490
02:03:55,339 --> 02:04:00,360
 

5491
02:03:55,349 --> 02:04:02,039
 network weights for example if we

5492
02:04:00,350 --> 02:04:02,039
 

5493
02:04:00,360 --> 02:04:04,800
 instead look at a slightly different

5494
02:04:02,029 --> 02:04:04,800
 

5495
02:04:02,039 --> 02:04:07,500
 point say we take a point that's in

5496
02:04:04,790 --> 02:04:07,500
 

5497
02:04:04,800 --> 02:04:10,170
 between two oppositely labeled examples

5498
02:04:07,490 --> 02:04:10,170
 

5499
02:04:07,500 --> 02:04:12,119
 so this new point you that I'm showing

5500
02:04:10,160 --> 02:04:12,119
 

5501
02:04:10,170 --> 02:04:14,760
 you here now is in between a blue and

5502
02:04:12,109 --> 02:04:14,760
 

5503
02:04:12,119 --> 02:04:18,750
 red labeled example now what happens is

5504
02:04:14,750 --> 02:04:18,750
 

5505
02:04:14,760 --> 02:04:20,820
 no matter what we choose to label that

5506
02:04:18,740 --> 02:04:20,820
 

5507
02:04:18,750 --> 02:04:22,110
 guy either blue or red it's going to be

5508
02:04:20,810 --> 02:04:22,110
 

5509
02:04:20,820 --> 02:04:23,639
 difficult to interpolate because you're

5510
02:04:22,100 --> 02:04:23,639
 

5511
02:04:22,110 --> 02:04:25,770
 really kind of trying to squeeze them in

5512
02:04:23,629 --> 02:04:25,770
 

5513
02:04:23,639 --> 02:04:28,230
 between two oppositely labeled examples

5514
02:04:25,760 --> 02:04:28,230
 

5515
02:04:25,770 --> 02:04:30,059
 so in either case it's difficult to

5516
02:04:28,220 --> 02:04:30,059
 

5517
02:04:28,230 --> 02:04:32,130
 interpolate the new interpolation

5518
02:04:30,049 --> 02:04:32,130
 

5519
02:04:30,059 --> 02:04:33,989
 function will be much more Wiggly less

5520
02:04:32,120 --> 02:04:33,989
 

5521
02:04:32,130 --> 02:04:37,260
 smooth and we can measure this again in

5522
02:04:33,979 --> 02:04:37,260
 

5523
02:04:33,989 --> 02:04:38,760
 terms of the norm so here is a heuristic

5524
02:04:37,250 --> 02:04:38,760
 

5525
02:04:37,260 --> 02:04:41,460
 that I'm just going to propose a

5526
02:04:38,750 --> 02:04:41,460
 

5527
02:04:38,760 --> 02:04:44,010
 heuristic for how you might decide to

5528
02:04:41,450 --> 02:04:44,010
 

5529
02:04:41,460 --> 02:04:46,440
 select a new example the selection of

5530
02:04:44,000 --> 02:04:46,440
 

5531
02:04:44,010 --> 02:04:49,110
 the next example the label is a u star

5532
02:04:46,430 --> 02:04:49,110
 

5533
02:04:46,440 --> 02:04:51,539
 that is the solution to this max min

5534
02:04:49,100 --> 02:04:51,539
 

5535
02:04:49,110 --> 02:04:53,610
 problem for any given point u you're

5536
02:04:51,529 --> 02:04:53,610
 

5537
02:04:51,539 --> 02:04:55,739
 going to look at how Wiggly would the

5538
02:04:53,600 --> 02:04:55,739
 

5539
02:04:53,610 --> 02:04:57,840
 new function be if I label it minus one

5540
02:04:55,729 --> 02:04:57,840
 

5541
02:04:55,739 --> 02:05:00,300
 and how label it how Wiggly would it be

5542
02:04:57,830 --> 02:05:00,300
 

5543
02:04:57,840 --> 02:05:01,739
 if I label it plus one and so I can

5544
02:05:00,290 --> 02:05:01,739
 

5545
02:05:00,300 --> 02:05:03,659
 measure that again with those norms and

5546
02:05:01,729 --> 02:05:03,659
 

5547
02:05:01,739 --> 02:05:05,550
 I just pick the more convenient of those

5548
02:05:03,649 --> 02:05:05,550
 

5549
02:05:03,659 --> 02:05:08,280
 two labels whichever label seems to be

5550
02:05:05,540 --> 02:05:08,280
 

5551
02:05:05,550 --> 02:05:10,139
 easier to accommodate and interpolate so

5552
02:05:08,270 --> 02:05:10,139
 

5553
02:05:08,280 --> 02:05:12,329
 that's what that min is saying and then

5554
02:05:10,129 --> 02:05:12,329
 

5555
02:05:10,139 --> 02:05:15,389
 I max over all possible unlabeled

5556
02:05:12,319 --> 02:05:15,389
 

5557
02:05:12,329 --> 02:05:16,650
 examples and so this is basically the

5558
02:05:15,379 --> 02:05:16,650
 

5559
02:05:15,389 --> 02:05:18,329
 intuition here is we're going to try to

5560
02:05:16,640 --> 02:05:18,329
 

5561
02:05:16,650 --> 02:05:20,699
 attack the most challenging points in

5562
02:05:18,319 --> 02:05:20,699
 

5563
02:05:18,329 --> 02:05:23,190
 the input space first to eliminate

5564
02:05:20,689 --> 02:05:23,190
 

5565
02:05:20,699 --> 02:05:25,980
 hopefully the need to label other easier

5566
02:05:23,180 --> 02:05:25,980
 

5567
02:05:23,190 --> 02:05:27,809
 examples later and this is a very recent

5568
02:05:25,970 --> 02:05:27,809
 

5569
02:05:25,980 --> 02:05:30,719
 kind of tech report that's up on archive

5570
02:05:27,799 --> 02:05:30,719
 

5571
02:05:27,809 --> 02:05:32,219
 if you want to go to look at it so what

5572
02:05:30,709 --> 02:05:32,219
 

5573
02:05:30,719 --> 02:05:33,539
 are the properties of this s just a

5574
02:05:32,209 --> 02:05:33,539
 

5575
02:05:32,219 --> 02:05:35,159
 heuristic it kind of seems like well

5576
02:05:33,529 --> 02:05:35,159
 

5577
02:05:33,539 --> 02:05:37,829
 maybe Rob that seems kind of reasonable

5578
02:05:35,149 --> 02:05:37,829
 

5579
02:05:35,159 --> 02:05:39,420
 what does it do so here is that

5580
02:05:37,819 --> 02:05:39,420
 

5581
02:05:37,829 --> 02:05:41,070
 criterion again and it kind of has some

5582
02:05:39,410 --> 02:05:41,070
 

5583
02:05:39,420 --> 02:05:43,860
 nice properties so you can ask

5584
02:05:41,060 --> 02:05:43,860
 

5585
02:05:41,070 --> 02:05:45,570
 verify analytically first of all the

5586
02:05:43,850 --> 02:05:45,570
 

5587
02:05:43,860 --> 02:05:48,210
 minimum norm labeling of a new example

5588
02:05:45,560 --> 02:05:48,210
 

5589
02:05:45,570 --> 02:05:49,980
 you is given by the sign of the current

5590
02:05:48,200 --> 02:05:49,980
 

5591
02:05:48,210 --> 02:05:52,380
 interpolator so you really don't you all

5592
02:05:49,970 --> 02:05:52,380
 

5593
02:05:49,980 --> 02:05:54,000
 you have to do is say well does the

5594
02:05:52,370 --> 02:05:54,000
 

5595
02:05:52,380 --> 02:05:55,409
 current prediction say that should be

5596
02:05:53,990 --> 02:05:55,409
 

5597
02:05:54,000 --> 02:05:57,389
 labeled plus or minus one and that's

5598
02:05:55,399 --> 02:05:57,389
 

5599
02:05:55,409 --> 02:05:59,820
 that's the the right label to take them

5600
02:05:57,379 --> 02:05:59,820
 

5601
02:05:57,389 --> 02:06:01,500
 for that new example it tends to select

5602
02:05:59,810 --> 02:06:01,500
 

5603
02:05:59,820 --> 02:06:03,510
 samples near the current decision

5604
02:06:01,490 --> 02:06:03,510
 

5605
02:06:01,500 --> 02:06:06,119
 boundary and closest to oppositely

5606
02:06:03,500 --> 02:06:06,119
 

5607
02:06:03,510 --> 02:06:08,520
 labeled examples and what this together

5608
02:06:06,109 --> 02:06:08,520
 

5609
02:06:06,119 --> 02:06:10,679
 means is that in the case of a simple

5610
02:06:08,510 --> 02:06:10,679
 

5611
02:06:08,520 --> 02:06:12,210
 one-dimensional problem it yields an

5612
02:06:10,669 --> 02:06:12,210
 

5613
02:06:10,679 --> 02:06:13,829
 optimal binary search so it sort of

5614
02:06:12,200 --> 02:06:13,829
 

5615
02:06:12,210 --> 02:06:15,239
 reduces to things that we know are

5616
02:06:13,819 --> 02:06:15,239
 

5617
02:06:13,829 --> 02:06:17,280
 optimal I'll just show you that with

5618
02:06:15,229 --> 02:06:17,280
 

5619
02:06:15,239 --> 02:06:19,440
 this little movie so this is an example

5620
02:06:17,270 --> 02:06:19,440
 

5621
02:06:17,280 --> 02:06:21,150
 where I have a multiple threshold

5622
02:06:19,430 --> 02:06:21,150
 

5623
02:06:19,440 --> 02:06:23,070
 problem in one dimension it's just a

5624
02:06:21,140 --> 02:06:23,070
 

5625
02:06:21,150 --> 02:06:26,099
 synthetic example but what happens is

5626
02:06:23,060 --> 02:06:26,099
 

5627
02:06:23,070 --> 02:06:28,320
 this min map maximum sampling criterion

5628
02:06:26,089 --> 02:06:28,320
 

5629
02:06:26,099 --> 02:06:30,060
 the daffodility store goes in locates

5630
02:06:28,310 --> 02:06:30,060
 

5631
02:06:28,320 --> 02:06:31,860
 the decision boundary that moves on and

5632
02:06:30,050 --> 02:06:31,860
 

5633
02:06:30,060 --> 02:06:33,690
 finds another one and what you can prove

5634
02:06:31,850 --> 02:06:33,690
 

5635
02:06:31,860 --> 02:06:35,369
 about it is that if I have endpoints

5636
02:06:33,680 --> 02:06:35,369
 

5637
02:06:33,690 --> 02:06:37,560
 more or less uniformly distributed on

5638
02:06:35,359 --> 02:06:37,560
 

5639
02:06:35,369 --> 02:06:39,480
 the interval and there's a piecewise

5640
02:06:37,550 --> 02:06:39,480
 

5641
02:06:37,560 --> 02:06:41,400
 constant sort of labeling function like

5642
02:06:39,470 --> 02:06:41,400
 

5643
02:06:39,480 --> 02:06:42,719
 the blue thing I'm showing you here then

5644
02:06:41,390 --> 02:06:42,719
 

5645
02:06:41,400 --> 02:06:45,000
 this kernel active learner will

5646
02:06:42,709 --> 02:06:45,000
 

5647
02:06:42,719 --> 02:06:47,909
 perfectly label all endpoints after only

5648
02:06:44,990 --> 02:06:47,909
 

5649
02:06:45,000 --> 02:06:49,800
 labeling K log n of them so again this

5650
02:06:47,899 --> 02:06:49,800
 

5651
02:06:47,909 --> 02:06:51,659
 is kind of like this idea of active or

5652
02:06:49,790 --> 02:06:51,659
 

5653
02:06:49,800 --> 02:06:52,920
 interactive annotation we're going to

5654
02:06:51,649 --> 02:06:52,920
 

5655
02:06:51,659 --> 02:06:54,659
 start marching through our unlabeled

5656
02:06:52,910 --> 02:06:54,659
 

5657
02:06:52,920 --> 02:06:56,369
 data set but we hope that we can

5658
02:06:54,649 --> 02:06:56,369
 

5659
02:06:54,659 --> 02:06:58,710
 accurately predict all the labels before

5660
02:06:56,359 --> 02:06:58,710
 

5661
02:06:56,369 --> 02:07:02,130
 we had to go and get a label for every

5662
02:06:58,700 --> 02:07:02,130
 

5663
02:06:58,710 --> 02:07:03,960
 single example so here's what it looks

5664
02:07:02,120 --> 02:07:03,960
 

5665
02:07:02,130 --> 02:07:06,389
 like in 2d it gets more difficult to

5666
02:07:03,950 --> 02:07:06,389
 

5667
02:07:03,960 --> 02:07:08,550
 analyze this kind of procedure in 2d but

5668
02:07:06,379 --> 02:07:08,550
 

5669
02:07:06,389 --> 02:07:10,199
 if you run it for example in this case

5670
02:07:08,540 --> 02:07:10,199
 

5671
02:07:08,550 --> 02:07:12,929
 where I have just a two-dimensional

5672
02:07:10,189 --> 02:07:12,929
 

5673
02:07:10,199 --> 02:07:14,099
 decision boundary what it does is it

5674
02:07:12,919 --> 02:07:14,099
 

5675
02:07:12,929 --> 02:07:16,650
 sort of initially just some random

5676
02:07:14,089 --> 02:07:16,650
 

5677
02:07:14,099 --> 02:07:18,360
 sampling it finds a decision boundary

5678
02:07:16,640 --> 02:07:18,360
 

5679
02:07:16,650 --> 02:07:20,340
 and that kind of stitches along and

5680
02:07:18,350 --> 02:07:20,340
 

5681
02:07:18,360 --> 02:07:22,469
 traces it out so it's really focusing on

5682
02:07:20,330 --> 02:07:22,469
 

5683
02:07:20,340 --> 02:07:24,960
 that decision boundary and eventually it

5684
02:07:22,459 --> 02:07:24,960
 

5685
02:07:22,469 --> 02:07:27,659
 will learn it perfectly and that will be

5686
02:07:24,950 --> 02:07:27,659
 

5687
02:07:24,960 --> 02:07:29,699
 the end of the process so here's how it

5688
02:07:27,649 --> 02:07:29,699
 

5689
02:07:27,659 --> 02:07:32,070
 works I'm just showing you now a few

5690
02:07:29,689 --> 02:07:32,070
 

5691
02:07:29,699 --> 02:07:34,860
 snapshots of the process through time

5692
02:07:32,060 --> 02:07:34,860
 

5693
02:07:32,070 --> 02:07:36,690
 there on the left but it has its

5694
02:07:34,850 --> 02:07:36,690
 

5695
02:07:34,860 --> 02:07:38,340
 strengths and weaknesses so what's the

5696
02:07:36,680 --> 02:07:38,340
 

5697
02:07:36,690 --> 02:07:40,349
 strength the strength is if you look

5698
02:07:38,330 --> 02:07:40,349
 

5699
02:07:38,340 --> 02:07:43,500
 over at this graph comparing the

5700
02:07:40,339 --> 02:07:43,500
 

5701
02:07:40,349 --> 02:07:46,079
 learning rates you know we basically get

5702
02:07:43,490 --> 02:07:46,079
 

5703
02:07:43,500 --> 02:07:48,270
 to a zero error very quickly the

5704
02:07:46,069 --> 02:07:48,270
 

5705
02:07:46,079 --> 02:07:50,340
 training error goes to zero as soon as

5706
02:07:48,260 --> 02:07:50,340
 

5707
02:07:48,270 --> 02:07:52,860
 we found that full decision boundary and

5708
02:07:50,330 --> 02:07:52,860
 

5709
02:07:50,340 --> 02:07:53,450
 so the red curves you drop it way below

5710
02:07:52,850 --> 02:07:53,450
 

5711
02:07:52,860 --> 02:07:55,670
 the blue curve

5712
02:07:53,440 --> 02:07:55,670
 

5713
02:07:53,450 --> 02:07:57,950
 here unfortunately though it looks a

5714
02:07:55,660 --> 02:07:57,950
 

5715
02:07:55,670 --> 02:07:59,720
 little weird over here initially random

5716
02:07:57,940 --> 02:07:59,720
 

5717
02:07:57,950 --> 02:08:01,010
 sampling passive learning is doing a

5718
02:07:59,710 --> 02:08:01,010
 

5719
02:07:59,720 --> 02:08:02,360
 little bit better than this active

5720
02:08:01,000 --> 02:08:02,360
 

5721
02:08:01,010 --> 02:08:04,640
 learning procedure and what's happening

5722
02:08:02,350 --> 02:08:04,640
 

5723
02:08:02,360 --> 02:08:06,620
 there is that the active the max min

5724
02:08:04,630 --> 02:08:06,620
 

5725
02:08:04,640 --> 02:08:08,360
 criterion is really nailing just a

5726
02:08:06,610 --> 02:08:08,360
 

5727
02:08:06,620 --> 02:08:10,130
 fragment of the decision boundary but

5728
02:08:08,350 --> 02:08:10,130
 

5729
02:08:08,360 --> 02:08:12,620
 not the entire decision boundary and so

5730
02:08:10,120 --> 02:08:12,620
 

5731
02:08:10,130 --> 02:08:13,940
 that leaves a lot of uncovered space so

5732
02:08:12,610 --> 02:08:13,940
 

5733
02:08:12,620 --> 02:08:15,890
 it sort of has this almost like phase

5734
02:08:13,930 --> 02:08:15,890
 

5735
02:08:13,940 --> 02:08:17,780
 change characteristic where once you've

5736
02:08:15,880 --> 02:08:17,780
 

5737
02:08:15,890 --> 02:08:19,880
 found the decision boundary the air

5738
02:08:17,770 --> 02:08:19,880
 

5739
02:08:17,780 --> 02:08:22,910
 drops to 0 but before that you're kind

5740
02:08:19,870 --> 02:08:22,910
 

5741
02:08:19,880 --> 02:08:24,860
 of missing a lot of the big picture so

5742
02:08:22,900 --> 02:08:24,860
 

5743
02:08:22,910 --> 02:08:27,470
 that's the limitation it can be a little

5744
02:08:24,850 --> 02:08:27,470
 

5745
02:08:24,860 --> 02:08:29,150
 bit myopically focused on the learning

5746
02:08:27,460 --> 02:08:29,150
 

5747
02:08:27,470 --> 02:08:30,710
 the decision boundary and it's not

5748
02:08:29,140 --> 02:08:30,710
 

5749
02:08:29,150 --> 02:08:32,660
 really sensitive to the distribution of

5750
02:08:30,700 --> 02:08:32,660
 

5751
02:08:30,710 --> 02:08:35,870
 the data and that's another kind of

5752
02:08:32,650 --> 02:08:35,870
 

5753
02:08:32,660 --> 02:08:37,730
 issue so we also study a different type

5754
02:08:35,860 --> 02:08:37,730
 

5755
02:08:35,870 --> 02:08:40,010
 of norm just a variation rather than

5756
02:08:37,720 --> 02:08:40,010
 

5757
02:08:37,730 --> 02:08:41,240
 using the kernel norm or the neural

5758
02:08:40,000 --> 02:08:41,240
 

5759
02:08:40,010 --> 02:08:44,030
 network weight norm we could use a

5760
02:08:41,230 --> 02:08:44,030
 

5761
02:08:41,240 --> 02:08:46,900
 database norm so here's the idea so

5762
02:08:44,020 --> 02:08:46,900
 

5763
02:08:44,030 --> 02:08:49,790
 again we have F we'll just a note our

5764
02:08:46,890 --> 02:08:49,790
 

5765
02:08:46,900 --> 02:08:53,150
 minimum norm interpolator of a given set

5766
02:08:49,780 --> 02:08:53,150
 

5767
02:08:49,790 --> 02:08:55,100
 of initial training examples f you will

5768
02:08:53,140 --> 02:08:55,100
 

5769
02:08:53,150 --> 02:08:58,450
 be the yeah sorry about that

5770
02:08:55,090 --> 02:08:58,450
 

5771
02:08:55,100 --> 02:09:00,800
 F superscript you will be the men norm

5772
02:08:58,440 --> 02:09:00,800
 

5773
02:08:58,450 --> 02:09:02,690
 interpolator if I add the point you and

5774
02:09:00,790 --> 02:09:02,690
 

5775
02:09:00,800 --> 02:09:04,900
 then what we'll do is to choose which

5776
02:09:02,680 --> 02:09:04,900
 

5777
02:09:02,690 --> 02:09:10,520
 point that label next we're going to

5778
02:09:04,890 --> 02:09:10,520
 

5779
02:09:04,900 --> 02:09:12,320
 look at which point and resulting

5780
02:09:10,510 --> 02:09:12,320
 

5781
02:09:10,520 --> 02:09:13,610
 interpolator sort of leads to the

5782
02:09:12,310 --> 02:09:13,610
 

5783
02:09:12,320 --> 02:09:15,560
 greatest change from the previous

5784
02:09:13,600 --> 02:09:15,560
 

5785
02:09:13,610 --> 02:09:17,450
 interpolator but we're measuring that

5786
02:09:15,550 --> 02:09:17,450
 

5787
02:09:15,560 --> 02:09:18,950
 greatest change or the most significant

5788
02:09:17,440 --> 02:09:18,950
 

5789
02:09:17,450 --> 02:09:22,210
 amount of change with respect to the

5790
02:09:18,940 --> 02:09:22,210
 

5791
02:09:18,950 --> 02:09:25,160
 full pool of unlabeled training examples

5792
02:09:22,200 --> 02:09:25,160
 

5793
02:09:22,210 --> 02:09:27,560
 so this has a lot of nice properties too

5794
02:09:25,150 --> 02:09:27,560
 

5795
02:09:25,160 --> 02:09:29,840
 I'm showing you a comparison of the

5796
02:09:27,550 --> 02:09:29,840
 

5797
02:09:27,560 --> 02:09:33,920
 movies in this two-dimensional setting

5798
02:09:29,830 --> 02:09:33,920
 

5799
02:09:29,840 --> 02:09:35,510
 using the rkh net our KHS norm and this

5800
02:09:33,910 --> 02:09:35,510
 

5801
02:09:33,920 --> 02:09:38,780
 database norm and what you can see is

5802
02:09:35,500 --> 02:09:38,780
 

5803
02:09:35,510 --> 02:09:40,670
 that the database norm strikes a balance

5804
02:09:38,770 --> 02:09:40,670
 

5805
02:09:38,780 --> 02:09:43,100
 between focusing on that boundary and

5806
02:09:40,660 --> 02:09:43,100
 

5807
02:09:40,670 --> 02:09:44,570
 then exploring more globally eventually

5808
02:09:43,090 --> 02:09:44,570
 

5809
02:09:43,100 --> 02:09:46,640
 it really starts homing in on the

5810
02:09:44,560 --> 02:09:46,640
 

5811
02:09:44,570 --> 02:09:50,120
 decision boundary but it's getting kind

5812
02:09:46,630 --> 02:09:50,120
 

5813
02:09:46,640 --> 02:09:53,570
 of a more accurate large-scale picture

5814
02:09:50,110 --> 02:09:53,570
 

5815
02:09:50,120 --> 02:09:56,090
 of the problem at the expense of maybe

5816
02:09:53,560 --> 02:09:56,090
 

5817
02:09:53,570 --> 02:09:58,850
 not perfectly nailing down exactly where

5818
02:09:56,080 --> 02:09:58,850
 

5819
02:09:56,090 --> 02:10:00,620
 the decision boundary is so here are

5820
02:09:58,840 --> 02:10:00,620
 

5821
02:09:58,850 --> 02:10:02,960
 just some snapshots over a number of

5822
02:10:00,610 --> 02:10:02,960
 

5823
02:10:00,620 --> 02:10:05,890
 labeled examples in this case here of

5824
02:10:02,950 --> 02:10:05,890
 

5825
02:10:02,960 --> 02:10:08,770
 the two different norm criteria

5826
02:10:05,880 --> 02:10:08,770
 

5827
02:10:05,890 --> 02:10:10,660
 and I get what you see is roots maybe I

5828
02:10:08,760 --> 02:10:10,660
 

5829
02:10:08,770 --> 02:10:12,730
 didn't sorry I guess I mean what you can

5830
02:10:10,650 --> 02:10:12,730
 

5831
02:10:10,660 --> 02:10:14,410
 see is that basically both of these

5832
02:10:12,720 --> 02:10:14,410
 

5833
02:10:12,730 --> 02:10:17,590
 active methods are doing better than

5834
02:10:14,400 --> 02:10:17,590
 

5835
02:10:14,410 --> 02:10:19,300
 passive but the error decay of the

5836
02:10:17,580 --> 02:10:19,300
 

5837
02:10:17,590 --> 02:10:21,310
 database criteria is much more graceful

5838
02:10:19,290 --> 02:10:21,310
 

5839
02:10:19,300 --> 02:10:23,470
 it doesn't have this like sharp phase

5840
02:10:21,300 --> 02:10:23,470
 

5841
02:10:21,310 --> 02:10:25,570
 change it starts improving on active

5842
02:10:23,460 --> 02:10:25,570
 

5843
02:10:23,470 --> 02:10:27,340
 learning right off the bat so in

5844
02:10:25,560 --> 02:10:27,340
 

5845
02:10:25,570 --> 02:10:30,700
 practice we've observed this kind of

5846
02:10:27,330 --> 02:10:30,700
 

5847
02:10:27,340 --> 02:10:34,240
 behavior works a bit better in

5848
02:10:30,690 --> 02:10:34,240
 

5849
02:10:30,700 --> 02:10:37,090
 application here's another cool feature

5850
02:10:34,230 --> 02:10:37,090
 

5851
02:10:34,240 --> 02:10:38,860
 of the database criterion actually has

5852
02:10:37,080 --> 02:10:38,860
 

5853
02:10:37,090 --> 02:10:41,170
 this sort of cluster seeking nature so

5854
02:10:38,850 --> 02:10:41,170
 

5855
02:10:38,860 --> 02:10:44,950
 this is again how it might cover both

5856
02:10:41,160 --> 02:10:44,950
 

5857
02:10:41,170 --> 02:10:47,200
 phases of active learning the the are

5858
02:10:44,940 --> 02:10:47,200
 

5859
02:10:44,950 --> 02:10:49,480
 KHS norm our neural network weight norm

5860
02:10:47,190 --> 02:10:49,480
 

5861
02:10:47,200 --> 02:10:51,250
 really is aiming to nail down decision

5862
02:10:49,470 --> 02:10:51,250
 

5863
02:10:49,480 --> 02:10:53,280
 boundaries whereas the database

5864
02:10:51,240 --> 02:10:53,280
 

5865
02:10:51,250 --> 02:10:55,720
 criterion first make sure it covers

5866
02:10:53,270 --> 02:10:55,720
 

5867
02:10:53,280 --> 02:10:57,820
 probability mass and covers the problem

5868
02:10:55,710 --> 02:10:57,820
 

5869
02:10:55,720 --> 02:10:59,260
 well so it sort of finds roughly one

5870
02:10:57,810 --> 02:10:59,260
 

5871
02:10:57,820 --> 02:11:02,980
 representative from each of the clusters

5872
02:10:59,250 --> 02:11:02,980
 

5873
02:10:59,260 --> 02:11:06,190
 first and I'm just gonna wrap up with

5874
02:11:02,970 --> 02:11:06,190
 

5875
02:11:02,980 --> 02:11:07,870
 again I just want to caution you this is

5876
02:11:06,180 --> 02:11:07,870
 

5877
02:11:06,190 --> 02:11:09,760
 all very preliminary but we have been

5878
02:11:07,860 --> 02:11:09,760
 

5879
02:11:07,870 --> 02:11:11,650
 running experiments with eminent other

5880
02:11:09,750 --> 02:11:11,650
 

5881
02:11:09,760 --> 02:11:14,230
 things and and we see essentially the

5882
02:11:11,640 --> 02:11:14,230
 

5883
02:11:11,650 --> 02:11:17,710
 same story being borne out with real

5884
02:11:14,220 --> 02:11:17,710
 

5885
02:11:14,230 --> 02:11:19,690
 data so here is an example comparing

5886
02:11:17,700 --> 02:11:19,690
 

5887
02:11:17,710 --> 02:11:21,970
 passive learning to the max min

5888
02:11:19,680 --> 02:11:21,970
 

5889
02:11:19,690 --> 02:11:25,150
 criterion the database criterion and you

5890
02:11:21,960 --> 02:11:25,150
 

5891
02:11:21,970 --> 02:11:27,040
 see that we're doing driving the trainer

5892
02:11:25,140 --> 02:11:27,040
 

5893
02:11:25,150 --> 02:11:28,570
 to zero very quickly using active

5894
02:11:27,030 --> 02:11:28,570
 

5895
02:11:27,040 --> 02:11:31,570
 learning compared to passive learning

5896
02:11:28,560 --> 02:11:31,570
 

5897
02:11:28,570 --> 02:11:33,670
 that generalization or test error is

5898
02:11:31,560 --> 02:11:33,670
 

5899
02:11:31,570 --> 02:11:35,860
 stabilized we're not really too worried

5900
02:11:33,660 --> 02:11:35,860
 

5901
02:11:33,670 --> 02:11:38,470
 that we're possibly overfitting just the

5902
02:11:35,850 --> 02:11:38,470
 

5903
02:11:35,860 --> 02:11:41,140
 the new story that's emerging emerging

5904
02:11:38,460 --> 02:11:41,140
 

5905
02:11:38,470 --> 02:11:43,270
 about this interpolation regime and then

5906
02:11:41,130 --> 02:11:43,270
 

5907
02:11:41,140 --> 02:11:45,220
 we also see that the max min criterion

5908
02:11:43,260 --> 02:11:45,220
 

5909
02:11:43,270 --> 02:11:47,130
 initially does maybe a little bit worse

5910
02:11:45,210 --> 02:11:47,130
 

5911
02:11:45,220 --> 02:11:50,980
 than passive learning but eventually

5912
02:11:47,120 --> 02:11:50,980
 

5913
02:11:47,130 --> 02:11:52,330
 catches up and so I'm gonna stop that

5914
02:11:50,970 --> 02:11:52,330
 

5915
02:11:50,980 --> 02:11:55,300
 there I'm just going to say a couple of

5916
02:11:52,320 --> 02:11:55,300
 

5917
02:11:52,330 --> 02:11:56,860
 conclusions so the theory methods of

5918
02:11:55,290 --> 02:11:56,860
 

5919
02:11:55,300 --> 02:11:58,780
 active learning are really well

5920
02:11:56,850 --> 02:11:58,780
 

5921
02:11:56,860 --> 02:12:02,560
 developed in the classical statistical

5922
02:11:58,770 --> 02:12:02,560
 

5923
02:11:58,780 --> 02:12:04,210
 learning framework the problem is that

5924
02:12:02,550 --> 02:12:04,210
 

5925
02:12:02,560 --> 02:12:05,710
 that is that the classical theory may

5926
02:12:04,200 --> 02:12:05,710
 

5927
02:12:04,210 --> 02:12:07,420
 not be applicable in this over

5928
02:12:05,700 --> 02:12:07,420
 

5929
02:12:05,710 --> 02:12:09,040
 parameterised regime that I was just

5930
02:12:07,410 --> 02:12:09,040
 

5931
02:12:07,420 --> 02:12:11,560
 talking about and so we have this new

5932
02:12:09,030 --> 02:12:11,560
 

5933
02:12:09,040 --> 02:12:14,500
 framework based on minimum norm

5934
02:12:11,550 --> 02:12:14,500
 

5935
02:12:11,560 --> 02:12:17,140
 interpolation that shows some promise in

5936
02:12:14,490 --> 02:12:17,140
 

5937
02:12:14,500 --> 02:12:18,739
 theory in practice and what I think is

5938
02:12:17,130 --> 02:12:18,739
 

5939
02:12:17,140 --> 02:12:20,239
 really exciting is there

5940
02:12:18,729 --> 02:12:20,239
 

5941
02:12:18,739 --> 02:12:22,940
 are a lot of opportunities for

5942
02:12:20,229 --> 02:12:22,940
 

5943
02:12:20,239 --> 02:12:24,650
 developing new theory especially in

5944
02:12:22,930 --> 02:12:24,650
 

5945
02:12:22,940 --> 02:12:26,780
 conjunction with modern deep learning

5946
02:12:24,640 --> 02:12:26,780
 

5947
02:12:24,650 --> 02:12:28,850
 methods and there's a big space for

5948
02:12:26,770 --> 02:12:28,850
 

5949
02:12:26,780 --> 02:12:30,950
 improvement in terms of developing new

5950
02:12:28,840 --> 02:12:30,950
 

5951
02:12:28,850 --> 02:12:32,420
 computationally efficient active

5952
02:12:30,940 --> 02:12:32,420
 

5953
02:12:30,950 --> 02:12:34,460
 learning algorithms and so I think it's

5954
02:12:32,410 --> 02:12:34,460
 

5955
02:12:32,420 --> 02:12:37,280
 a great time to try to jump into the

5956
02:12:34,450 --> 02:12:37,280
 

5957
02:12:34,460 --> 02:12:39,230
 field and I just have the note of the

5958
02:12:37,270 --> 02:12:39,230
 

5959
02:12:37,280 --> 02:12:41,000
 slides there again and I forgot to tell

5960
02:12:39,220 --> 02:12:41,000
 

5961
02:12:39,230 --> 02:12:43,640
 you before and Steve was good to show

5962
02:12:40,990 --> 02:12:43,640
 

5963
02:12:41,000 --> 02:12:45,200
 these but with each of these slides we

5964
02:12:43,630 --> 02:12:45,200
 

5965
02:12:43,640 --> 02:12:47,540
 also have some recommended readings

5966
02:12:45,190 --> 02:12:47,540
 

5967
02:12:45,200 --> 02:12:48,739
 pointing you to some of our favorites

5968
02:12:47,530 --> 02:12:48,739
 

5969
02:12:47,540 --> 02:12:50,540
 and the literature they're not

5970
02:12:48,729 --> 02:12:50,540
 

5971
02:12:48,739 --> 02:12:52,910
 comprehensive but it certainly will give

5972
02:12:50,530 --> 02:12:52,910
 

5973
02:12:50,540 --> 02:12:55,310
 you a good set of starting points if you

5974
02:12:52,900 --> 02:12:55,310
 

5975
02:12:52,910 --> 02:12:56,930
 want to explore more on your own so with

5976
02:12:55,300 --> 02:12:56,930
 

5977
02:12:55,310 --> 02:12:58,550
 that I'll thank you all for listening

5978
02:12:56,920 --> 02:12:58,550
 

5979
02:12:56,930 --> 02:13:00,560
 we'll take more questions I'm going to

5980
02:12:58,540 --> 02:13:00,560
 

5981
02:12:58,550 --> 02:13:02,000
 invite Steve back up here but it's been

5982
02:13:00,550 --> 02:13:02,000
 

5983
02:13:00,560 --> 02:13:03,380
 a real pleasure and an honor to address

5984
02:13:01,990 --> 02:13:03,380
 

5985
02:13:02,000 --> 02:13:04,840
 you today and I hope this was valuable

5986
02:13:03,370 --> 02:13:04,840
 

5987
02:13:03,380 --> 02:13:14,350
 to you thanks

5988
02:13:04,830 --> 02:13:14,350
 

5989
02:13:04,840 --> 02:13:14,350
[Applause]

5990
02:13:30,090 --> 02:13:30,090
 

5991
02:13:30,100 --> 02:13:35,060
 hey um so for those clustering based

5992
02:13:33,160 --> 02:13:35,060
 

5993
02:13:33,170 --> 02:13:36,500
 methods for up from the active learning

5994
02:13:35,050 --> 02:13:36,500
 

5995
02:13:35,060 --> 02:13:37,970
 where you kind of automatically

5996
02:13:36,490 --> 02:13:37,970
 

5997
02:13:36,500 --> 02:13:39,020
 annotating the data based on the

5998
02:13:37,960 --> 02:13:39,020
 

5999
02:13:37,970 --> 02:13:42,290
 clusters

6000
02:13:39,010 --> 02:13:42,290
 

6001
02:13:39,020 --> 02:13:44,330
 how does using those data actually train

6002
02:13:42,280 --> 02:13:44,330
 

6003
02:13:42,290 --> 02:13:45,950
 a classifier compared to just using some

6004
02:13:44,320 --> 02:13:45,950
 

6005
02:13:44,330 --> 02:13:48,470
 sort of K nearest neighbors method and

6006
02:13:45,940 --> 02:13:48,470
 

6007
02:13:45,950 --> 02:13:57,050
 just you know using those labels

6008
02:13:48,460 --> 02:13:57,050
 

6009
02:13:48,470 --> 02:13:59,120
 automatically on like a test set so the

6010
02:13:57,040 --> 02:13:59,120
 

6011
02:13:57,050 --> 02:14:01,700
 idea and the clustering methods again is

6012
02:13:59,110 --> 02:14:01,700
 

6013
02:13:59,120 --> 02:14:04,220
 to use some unsupervised learning to try

6014
02:14:01,690 --> 02:14:04,220
 

6015
02:14:01,700 --> 02:14:06,170
 to figure out is there some interesting

6016
02:14:04,210 --> 02:14:06,170
 

6017
02:14:04,220 --> 02:14:09,200
 geometrical structure in your data and

6018
02:14:06,160 --> 02:14:09,200
 

6019
02:14:06,170 --> 02:14:11,000
 that it sort of follows both both that

6020
02:14:09,190 --> 02:14:11,000
 

6021
02:14:09,200 --> 02:14:13,790
 in the graph based learning methods

6022
02:14:10,990 --> 02:14:13,790
 

6023
02:14:11,000 --> 02:14:16,310
 follow the sort of semi supervised idea

6024
02:14:13,780 --> 02:14:16,310
 

6025
02:14:13,790 --> 02:14:17,900
 in that something about the structure

6026
02:14:16,300 --> 02:14:17,900
 

6027
02:14:16,310 --> 02:14:20,390
 and distribution of the unlabeled

6028
02:14:17,890 --> 02:14:20,390
 

6029
02:14:17,900 --> 02:14:22,610
 examples is giving you a little bit of a

6030
02:14:20,380 --> 02:14:22,610
 

6031
02:14:20,390 --> 02:14:24,250
 clue as to what the labeling might be or

6032
02:14:22,600 --> 02:14:24,250
 

6033
02:14:22,610 --> 02:14:26,840
 at least where the labels might be

6034
02:14:24,240 --> 02:14:26,840
 

6035
02:14:24,250 --> 02:14:29,510
 similar to each other so if I see an

6036
02:14:26,830 --> 02:14:29,510
 

6037
02:14:26,840 --> 02:14:32,860
 isolated cluster of examples maybe those

6038
02:14:29,500 --> 02:14:32,860
 

6039
02:14:29,510 --> 02:14:35,450
 are all cats for all dogs and so that's

6040
02:14:32,850 --> 02:14:35,450
 

6041
02:14:32,860 --> 02:14:38,320
 the kind of intuition behind those

6042
02:14:35,440 --> 02:14:38,320
 

6043
02:14:35,450 --> 02:14:41,300
 methods so I wasn't quite clear exactly

6044
02:14:38,310 --> 02:14:41,300
 

6045
02:14:38,320 --> 02:14:43,160
 I'm not sure if that's answering your

6046
02:14:41,290 --> 02:14:43,160
 

6047
02:14:41,300 --> 02:14:45,590
 question or if you yep is that and so

6048
02:14:43,150 --> 02:14:45,590
 

6049
02:14:43,160 --> 02:14:49,250
 you do really want to kind of look at

6050
02:14:45,580 --> 02:14:49,250
 

6051
02:14:45,590 --> 02:14:50,900
 the geometry may be that those data are

6052
02:14:49,240 --> 02:14:50,900
 

6053
02:14:49,250 --> 02:14:52,580
 clustered or maybe they're on some sort

6054
02:14:50,890 --> 02:14:52,580
 

6055
02:14:50,900 --> 02:14:54,200
 of low dimensional manifold and a high

6056
02:14:52,570 --> 02:14:54,200
 

6057
02:14:52,580 --> 02:14:56,270
 dimensional feature space and so that's

6058
02:14:54,190 --> 02:14:56,270
 

6059
02:14:54,200 --> 02:14:59,240
 sort of what we're trying to exploit in

6060
02:14:56,260 --> 02:14:59,240
 

6061
02:14:56,270 --> 02:15:03,710
 those methods and and they do work you

6062
02:14:59,230 --> 02:15:03,710
 

6063
02:14:59,240 --> 02:15:06,110
 know well in many cases in your last

6064
02:15:03,700 --> 02:15:06,110
 

6065
02:15:03,710 --> 02:15:08,240
 example you had that classifier fu that

6066
02:15:06,100 --> 02:15:08,240
 

6067
02:15:06,110 --> 02:15:10,070
 depends on the new data point you but

6068
02:15:08,230 --> 02:15:10,070
 

6069
02:15:08,240 --> 02:15:14,200
 how do you compute those without knowing

6070
02:15:10,060 --> 02:15:14,200
 

6071
02:15:10,070 --> 02:15:17,240
 the label of you yeah so if I go back to

6072
02:15:14,190 --> 02:15:17,240
 

6073
02:15:14,200 --> 02:15:21,740
 for example this new and maybe I'll say

6074
02:15:17,230 --> 02:15:21,740
 

6075
02:15:17,240 --> 02:15:23,810
 here so f superscript 2 U is the minimum

6076
02:15:21,730 --> 02:15:23,810
 

6077
02:15:21,740 --> 02:15:25,730
 normal area in the new point u so

6078
02:15:23,800 --> 02:15:25,730
 

6079
02:15:23,810 --> 02:15:27,650
 there's in binary classification there'd

6080
02:15:25,720 --> 02:15:27,650
 

6081
02:15:25,730 --> 02:15:29,870
 be two options for that new data point

6082
02:15:27,640 --> 02:15:29,870
 

6083
02:15:27,650 --> 02:15:32,630
 plus one or minus one we actually try to

6084
02:15:29,860 --> 02:15:32,630
 

6085
02:15:29,870 --> 02:15:34,520
 interpolate both cases so we say what if

6086
02:15:32,620 --> 02:15:34,520
 

6087
02:15:32,630 --> 02:15:36,260
 we made it plus what if we made it - and

6088
02:15:34,510 --> 02:15:36,260
 

6089
02:15:34,520 --> 02:15:38,510
 then we say which one of those two is

6090
02:15:36,250 --> 02:15:38,510
 

6091
02:15:36,260 --> 02:15:40,159
 the simpler function and then we're

6092
02:15:38,500 --> 02:15:40,159
 

6093
02:15:38,510 --> 02:15:42,230
 going to go with that one is our sort of

6094
02:15:40,149 --> 02:15:42,230
 

6095
02:15:40,159 --> 02:15:44,150
 prediction of what the label should be

6096
02:15:42,220 --> 02:15:44,150
 

6097
02:15:42,230 --> 02:15:47,210
 it's going to be the easiest one to

6098
02:15:44,140 --> 02:15:47,210
 

6099
02:15:44,150 --> 02:15:49,730
 interpolate right and so that's what F

6100
02:15:47,200 --> 02:15:49,730
 

6101
02:15:47,210 --> 02:15:51,770
 superscript you here denotes it denotes

6102
02:15:49,720 --> 02:15:51,770
 

6103
02:15:49,730 --> 02:15:54,500
 with this sort of most the easiest

6104
02:15:51,760 --> 02:15:54,500
 

6105
02:15:51,770 --> 02:15:56,570
 choice for what that label would be yeah

6106
02:15:54,490 --> 02:15:56,570
 

6107
02:15:54,500 --> 02:15:58,159
 so you kind of guess ahead and take the

6108
02:15:56,560 --> 02:15:58,159
 

6109
02:15:56,570 --> 02:15:59,840
 easier of the two cases in binary

6110
02:15:58,149 --> 02:15:59,840
 

6111
02:15:58,159 --> 02:16:05,780
 classification you get general it's a

6112
02:15:59,830 --> 02:16:05,780
 

6113
02:15:59,840 --> 02:16:08,810
 multi class - thanks thank you for

6114
02:16:05,770 --> 02:16:08,810
 

6115
02:16:05,780 --> 02:16:12,050
 greater and I think per dubbed the

6116
02:16:08,800 --> 02:16:12,050
 

6117
02:16:08,810 --> 02:16:14,510
 mini-map the Artemus max based sampling

6118
02:16:12,040 --> 02:16:14,510
 

6119
02:16:12,050 --> 02:16:18,199
 algorithm Porter over para master lies

6120
02:16:14,500 --> 02:16:18,199
 

6121
02:16:14,510 --> 02:16:21,560
 scheme I think every examples about no

6122
02:16:18,189 --> 02:16:21,560
 

6123
02:16:18,199 --> 02:16:25,130
 Noi setting so where no label no in

6124
02:16:21,550 --> 02:16:25,130
 

6125
02:16:21,560 --> 02:16:27,650
 setting and I think maybe that Mac max

6126
02:16:25,120 --> 02:16:27,650
 

6127
02:16:25,130 --> 02:16:30,050
 based scheme can be easily fooled by the

6128
02:16:27,640 --> 02:16:30,050
 

6129
02:16:27,650 --> 02:16:33,910
 natal noise because it is somehow greedy

6130
02:16:30,040 --> 02:16:33,910
 

6131
02:16:30,050 --> 02:16:37,700
 and so do you have any experiment about

6132
02:16:33,900 --> 02:16:37,700
 

6133
02:16:33,910 --> 02:16:39,200
 undergone that label or setting yeah

6134
02:16:37,690 --> 02:16:39,200
 

6135
02:16:37,700 --> 02:16:42,950
 that's a great question so the question

6136
02:16:39,190 --> 02:16:42,950
 

6137
02:16:39,200 --> 02:16:45,559
 is this kind of max min scheme is quite

6138
02:16:42,940 --> 02:16:45,559
 

6139
02:16:42,950 --> 02:16:47,689
 greedy and if there's label noise it

6140
02:16:45,549 --> 02:16:47,689
 

6141
02:16:45,559 --> 02:16:51,080
 could be noise seeking it could easily

6142
02:16:47,679 --> 02:16:51,080
 

6143
02:16:47,689 --> 02:16:53,210
 get fooled and so forth and so that's a

6144
02:16:51,070 --> 02:16:53,210
 

6145
02:16:51,080 --> 02:16:55,550
 great point but I'll just say if we go

6146
02:16:53,200 --> 02:16:55,550
 

6147
02:16:53,210 --> 02:16:58,939
 all the way back to this idea of over

6148
02:16:55,540 --> 02:16:58,939
 

6149
02:16:55,550 --> 02:17:01,040
 parameterize interpolating models even

6150
02:16:58,929 --> 02:17:01,040
 

6151
02:16:58,939 --> 02:17:02,840
 if you have label noise sometimes these

6152
02:17:01,030 --> 02:17:02,840
 

6153
02:17:01,040 --> 02:17:04,309
 can still be very very good I mean if

6154
02:17:02,830 --> 02:17:04,309
 

6155
02:17:02,840 --> 02:17:07,309
 there's label noise you can't predict

6156
02:17:04,299 --> 02:17:07,309
 

6157
02:17:04,309 --> 02:17:09,530
 noise but maybe the best interpolator

6158
02:17:07,299 --> 02:17:09,530
 

6159
02:17:07,309 --> 02:17:11,719
 still generalizes about as well as

6160
02:17:09,520 --> 02:17:11,719
 

6161
02:17:09,530 --> 02:17:13,490
 something that tried to not interpolate

6162
02:17:11,709 --> 02:17:13,490
 

6163
02:17:11,719 --> 02:17:16,040
 the data that's sort of what those

6164
02:17:13,480 --> 02:17:16,040
 

6165
02:17:13,490 --> 02:17:18,559
 double descent curves are telling us and

6166
02:17:16,030 --> 02:17:18,559
 

6167
02:17:16,040 --> 02:17:20,929
 there's a you know not a lot but there's

6168
02:17:18,549 --> 02:17:20,929
 

6169
02:17:18,559 --> 02:17:22,399
 some theoretical work out there to help

6170
02:17:20,919 --> 02:17:22,399
 

6171
02:17:20,929 --> 02:17:23,149
 us understand and explain this

6172
02:17:22,389 --> 02:17:23,149
 

6173
02:17:22,399 --> 02:17:26,389
 phenomenon

6174
02:17:23,139 --> 02:17:26,389
 

6175
02:17:23,149 --> 02:17:28,460
 so in what I proposed for this new

6176
02:17:26,379 --> 02:17:28,460
 

6177
02:17:26,389 --> 02:17:30,229
 active learning framework I'm just going

6178
02:17:28,450 --> 02:17:30,229
 

6179
02:17:28,460 --> 02:17:32,240
 to say let's run with that idea of being

6180
02:17:30,219 --> 02:17:32,240
 

6181
02:17:30,229 --> 02:17:34,639
 over parameterize and interpolating the

6182
02:17:32,230 --> 02:17:34,639
 

6183
02:17:32,240 --> 02:17:36,649
 training data we know that is sometimes

6184
02:17:34,629 --> 02:17:36,649
 

6185
02:17:34,639 --> 02:17:39,610
 not such a bad thing to do even if there

6186
02:17:36,639 --> 02:17:39,610
 

6187
02:17:36,649 --> 02:17:42,860
 is label noise and so our goal is to

6188
02:17:39,600 --> 02:17:42,860
 

6189
02:17:39,610 --> 02:17:44,780
 essentially try to label everything the

6190
02:17:42,850 --> 02:17:44,780
 

6191
02:17:42,860 --> 02:17:46,179
 training set perfectly fit our model to

6192
02:17:44,770 --> 02:17:46,179
 

6193
02:17:44,780 --> 02:17:47,979
 the training data

6194
02:17:46,169 --> 02:17:47,979
 

6195
02:17:46,179 --> 02:17:51,010
 and that's what we're doing in a lot of

6196
02:17:47,969 --> 02:17:51,010
 

6197
02:17:47,979 --> 02:17:52,479
 modern applications and if you're even

6198
02:17:51,000 --> 02:17:52,479
 

6199
02:17:51,010 --> 02:17:54,250
 that might not always be the best thing

6200
02:17:52,469 --> 02:17:54,250
 

6201
02:17:52,479 --> 02:17:56,439
 is that to say that regularization and

6202
02:17:54,240 --> 02:17:56,439
 

6203
02:17:54,250 --> 02:17:58,210
 and and so forth can't help but if we

6204
02:17:56,429 --> 02:17:58,210
 

6205
02:17:56,439 --> 02:18:00,010
 are going to try to operate in that

6206
02:17:58,200 --> 02:18:00,010
 

6207
02:17:58,210 --> 02:18:02,500
 interpolating over parameterize regime

6208
02:18:00,000 --> 02:18:02,500
 

6209
02:18:00,010 --> 02:18:08,920
 then maybe this is a reasonable active

6210
02:18:02,490 --> 02:18:08,920
 

6211
02:18:02,500 --> 02:18:10,660
 learning strategy Thanks so it's similar

6212
02:18:08,910 --> 02:18:10,660
 

6213
02:18:08,920 --> 02:18:12,819
 to the question asked before like if you

6214
02:18:10,650 --> 02:18:12,819
 

6215
02:18:10,660 --> 02:18:16,179
 have noisy data and over parameterised

6216
02:18:12,809 --> 02:18:16,179
 

6217
02:18:12,819 --> 02:18:17,800
 networks like deep networks if we want

6218
02:18:16,169 --> 02:18:17,800
 

6219
02:18:16,179 --> 02:18:19,960
 to try something like disagreement based

6220
02:18:17,790 --> 02:18:19,960
 

6221
02:18:17,800 --> 02:18:21,340
 learning how are we going to like what

6222
02:18:19,950 --> 02:18:21,340
 

6223
02:18:19,960 --> 02:18:23,769
 are some good ways to come up with good

6224
02:18:21,330 --> 02:18:23,769
 

6225
02:18:21,340 --> 02:18:26,559
 models like on the data we already have

6226
02:18:23,759 --> 02:18:26,559
 

6227
02:18:23,769 --> 02:18:32,099
 which happen of diversity to have some

6228
02:18:26,549 --> 02:18:32,099
 

6229
02:18:26,559 --> 02:18:34,149
 disagreement right yeah the question is

6230
02:18:32,089 --> 02:18:34,149
 

6231
02:18:32,099 --> 02:18:38,050
 about running disagreement based

6232
02:18:34,139 --> 02:18:38,050
 

6233
02:18:34,149 --> 02:18:42,519
 learning with over parametrized models

6234
02:18:38,040 --> 02:18:42,519
 

6235
02:18:38,050 --> 02:18:46,120
 that can interpolate right I'm not sure

6236
02:18:42,509 --> 02:18:46,120
 

6237
02:18:42,519 --> 02:18:48,099
 that's wise in fact you would just query

6238
02:18:46,110 --> 02:18:48,099
 

6239
02:18:46,120 --> 02:18:50,050
 everything right because you could label

6240
02:18:48,089 --> 02:18:50,050
 

6241
02:18:48,099 --> 02:18:51,729
 it in either way and it wouldn't really

6242
02:18:50,040 --> 02:18:51,729
 

6243
02:18:50,050 --> 02:18:54,969
 affect your ability to fit the other

6244
02:18:51,719 --> 02:18:54,969
 

6245
02:18:51,729 --> 02:18:59,109
 data so yeah so disagreement based

6246
02:18:54,959 --> 02:18:59,109
 

6247
02:18:54,969 --> 02:19:02,410
 learning in in these over parametrized

6248
02:18:59,099 --> 02:19:02,410
 

6249
02:18:59,109 --> 02:19:05,679
 cases hasn't really that theory hasn't

6250
02:19:02,400 --> 02:19:05,679
 

6251
02:19:02,410 --> 02:19:07,979
 really been developed as to what's the

6252
02:19:05,669 --> 02:19:07,979
 

6253
02:19:05,679 --> 02:19:11,260
 right way to do it I think that I mean

6254
02:19:07,969 --> 02:19:11,260
 

6255
02:19:07,979 --> 02:19:13,239
 and this has come out a lot you know the

6256
02:19:11,250 --> 02:19:13,239
 

6257
02:19:11,260 --> 02:19:16,000
 nature of the algorithm really matters

6258
02:19:13,229 --> 02:19:16,000
 

6259
02:19:13,239 --> 02:19:19,510
 in those cases where you have a lot of

6260
02:19:15,990 --> 02:19:19,510
 

6261
02:19:16,000 --> 02:19:21,250
 parametrized a lot of parameters what

6262
02:19:19,500 --> 02:19:21,250
 

6263
02:19:19,510 --> 02:19:24,489
 algorithm you use and so maybe there's

6264
02:19:21,240 --> 02:19:24,489
 

6265
02:19:21,250 --> 02:19:26,380
 some theory that could come out of if

6266
02:19:24,479 --> 02:19:26,380
 

6267
02:19:24,489 --> 02:19:28,779
 you're using a certain algorithm to

6268
02:19:26,370 --> 02:19:28,779
 

6269
02:19:26,380 --> 02:19:30,189
 minimize the empirical risk in it maybe

6270
02:19:28,769 --> 02:19:30,189
 

6271
02:19:28,779 --> 02:19:31,890
 there's some way that you can still get

6272
02:19:30,179 --> 02:19:31,890
 

6273
02:19:30,189 --> 02:19:34,260
 something like disagreement facing

6274
02:19:31,880 --> 02:19:34,260
 

6275
02:19:31,890 --> 02:19:42,890
 to work but that that theory has not

6276
02:19:34,250 --> 02:19:42,890
 

6277
02:19:34,260 --> 02:19:46,051
 been developed yet thank I so for the

6278
02:19:42,880 --> 02:19:46,051
 

6279
02:19:42,890 --> 02:19:47,881
 for the kind of algorithm that Steve

6280
02:19:46,041 --> 02:19:47,881
 

6281
02:19:46,051 --> 02:19:49,740
 talked about like this a squared and

6282
02:19:47,871 --> 02:19:49,740
 

6283
02:19:47,881 --> 02:19:51,811
 Martian based learning site based

6284
02:19:49,730 --> 02:19:51,811
 

6285
02:19:49,740 --> 02:19:54,780
 learning I was just wondering if there

6286
02:19:51,801 --> 02:19:54,780
 

6287
02:19:51,811 --> 02:19:56,400
 is any software available so I have a

6288
02:19:54,770 --> 02:19:56,400
 

6289
02:19:54,780 --> 02:20:00,181
 bird would play with it and build

6290
02:19:56,390 --> 02:20:00,181
 

6291
02:19:56,400 --> 02:20:02,190
 cementation yes so I if you get the

6292
02:20:00,171 --> 02:20:02,190
 

6293
02:20:00,181 --> 02:20:06,061
 slides there's a name and there the

6294
02:20:02,180 --> 02:20:06,061
 

6295
02:20:02,190 --> 02:20:10,320
 vocal webbot library which is available

6296
02:20:06,051 --> 02:20:10,320
 

6297
02:20:06,061 --> 02:20:11,791
 online I think on github somewhere if

6298
02:20:10,310 --> 02:20:11,791
 

6299
02:20:10,320 --> 02:20:14,070
 you search for vocal a bit you'll find

6300
02:20:11,781 --> 02:20:14,070
 

6301
02:20:11,791 --> 02:20:17,610
 it it's maintained by John Lankford in

6302
02:20:14,060 --> 02:20:17,610
 

6303
02:20:14,070 --> 02:20:20,671
 his group ok thanks so it has it has a

6304
02:20:17,600 --> 02:20:20,671
 

6305
02:20:17,610 --> 02:20:22,291
 kind of a with a bit of a heuristic in

6306
02:20:20,661 --> 02:20:22,291
 

6307
02:20:20,671 --> 02:20:25,950
 it but it has a sort of an approximate

6308
02:20:22,281 --> 02:20:25,950
 

6309
02:20:22,291 --> 02:20:35,730
 version of a disagreement based learning

6310
02:20:25,940 --> 02:20:35,730
 

6311
02:20:25,950 --> 02:20:38,101
 algorithm a new father tutorial so I'm

6312
02:20:35,720 --> 02:20:38,101
 

6313
02:20:35,730 --> 02:20:39,931
 wondering about a minimum norm framework

6314
02:20:38,091 --> 02:20:39,931
 

6315
02:20:38,101 --> 02:20:42,690
 and the margin algorithm earlier

6316
02:20:39,921 --> 02:20:42,690
 

6317
02:20:39,931 --> 02:20:45,001
 omission looks like it could be some

6318
02:20:42,680 --> 02:20:45,001
 

6319
02:20:42,690 --> 02:20:47,400
 connections between them but I'm not

6320
02:20:44,991 --> 02:20:47,400
 

6321
02:20:45,001 --> 02:20:53,671
 quite sure would you manage sharing some

6322
02:20:47,390 --> 02:20:53,671
 

6323
02:20:47,400 --> 02:20:55,291
 insights there yeah I think your your

6324
02:20:53,661 --> 02:20:55,291
 

6325
02:20:53,671 --> 02:20:58,021
 intuition is right I think there are

6326
02:20:55,281 --> 02:20:58,021
 

6327
02:20:55,291 --> 02:21:00,181
 connections you're the first one that we

6328
02:20:58,011 --> 02:21:00,181
 

6329
02:20:58,021 --> 02:21:03,230
 were able to rigorously establish is the

6330
02:21:00,171 --> 02:21:03,230
 

6331
02:21:00,181 --> 02:21:05,881
 connection between that criterion and

6332
02:21:03,220 --> 02:21:05,881
 

6333
02:21:03,230 --> 02:21:08,551
 basically an optimal bisection procedure

6334
02:21:05,871 --> 02:21:08,551
 

6335
02:21:05,881 --> 02:21:12,091
 in one dimension and how it relates to

6336
02:21:08,541 --> 02:21:12,091
 

6337
02:21:08,551 --> 02:21:14,400
 margin-based types of ideas in higher

6338
02:21:12,081 --> 02:21:14,400
 

6339
02:21:12,091 --> 02:21:15,931
 dimensional spaces is something we're

6340
02:21:14,390 --> 02:21:15,931
 

6341
02:21:14,400 --> 02:21:17,851
 trying to explore I don't know that it's

6342
02:21:15,921 --> 02:21:17,851
 

6343
02:21:15,931 --> 02:21:20,881
 going to be super easy to come up with a

6344
02:21:17,841 --> 02:21:20,881
 

6345
02:21:17,851 --> 02:21:23,271
 nice clean theory because it's such a

6346
02:21:20,871 --> 02:21:23,271
 

6347
02:21:20,881 --> 02:21:26,131
 general flexible kind of approach but

6348
02:21:23,261 --> 02:21:26,131
 

6349
02:21:23,271 --> 02:21:28,320
 certainly experimentally we see some

6350
02:21:26,121 --> 02:21:28,320
 

6351
02:21:26,131 --> 02:21:30,121
 correspondence between what would

6352
02:21:28,310 --> 02:21:30,121
 

6353
02:21:28,320 --> 02:21:31,771
 observe in a kind of margin based

6354
02:21:30,111 --> 02:21:31,771
 

6355
02:21:30,121 --> 02:21:33,900
 sampling scheme and that maximum

6356
02:21:31,761 --> 02:21:33,900
 

6357
02:21:31,771 --> 02:21:35,610
 criteria and I should also mention that

6358
02:21:33,890 --> 02:21:35,610
 

6359
02:21:33,900 --> 02:21:37,550
 the behavior I showed you in those

6360
02:21:35,600 --> 02:21:37,550
 

6361
02:21:35,610 --> 02:21:40,920
 two-dimensional types of

6362
02:21:37,540 --> 02:21:40,920
 

6363
02:21:37,550 --> 02:21:42,900
 examples is very very reminiscent of

6364
02:21:40,910 --> 02:21:42,900
 

6365
02:21:40,920 --> 02:21:44,689
 what we see when we run the graph base

6366
02:21:42,890 --> 02:21:44,689
 

6367
02:21:42,900 --> 02:21:47,310
 kind of active semi-supervised learning

6368
02:21:44,679 --> 02:21:47,310
 

6369
02:21:44,689 --> 02:21:50,010
 procedures have a very similar kind of

6370
02:21:47,300 --> 02:21:50,010
 

6371
02:21:47,310 --> 02:21:52,320
 behavior so I think an interesting area

6372
02:21:50,000 --> 02:21:52,320
 

6373
02:21:50,010 --> 02:21:54,320
 that I'd like to explore more are these

6374
02:21:52,310 --> 02:21:54,320
 

6375
02:21:52,320 --> 02:21:55,620
 connections between some of the

6376
02:21:54,310 --> 02:21:55,620
 

6377
02:21:54,320 --> 02:21:57,270
 well-developed

6378
02:21:55,610 --> 02:21:57,270
 

6379
02:21:55,620 --> 02:21:59,040
 theory methods for active learning and

6380
02:21:57,260 --> 02:21:59,040
 

6381
02:21:57,270 --> 02:22:01,410
 something that you might be able to pour

6382
02:21:59,030 --> 02:22:01,410
 

6383
02:21:59,040 --> 02:22:04,110
 it over to deep learning systems and so

6384
02:22:01,400 --> 02:22:04,110
 

6385
02:22:01,410 --> 02:22:13,170
 forth like the maximum criterion thank

6386
02:22:04,100 --> 02:22:13,170
 

6387
02:22:04,110 --> 02:22:15,930
 you hi I have a very naive question in

6388
02:22:13,160 --> 02:22:15,930
 

6389
02:22:13,170 --> 02:22:19,170
 active learning we say that if we want

6390
02:22:15,920 --> 02:22:19,170
 

6391
02:22:15,930 --> 02:22:21,600
 to add more samples we want to take the

6392
02:22:19,160 --> 02:22:21,600
 

6393
02:22:19,170 --> 02:22:24,390
 more complex samples the one lying at

6394
02:22:21,590 --> 02:22:24,390
 

6395
02:22:21,600 --> 02:22:26,490
 the boundary now in curriculum learning

6396
02:22:24,380 --> 02:22:26,490
 

6397
02:22:24,390 --> 02:22:28,439
 we're saying that if we want to learn

6398
02:22:26,480 --> 02:22:28,439
 

6399
02:22:26,490 --> 02:22:31,560
 the better model we should start only

6400
02:22:28,429 --> 02:22:31,560
 

6401
02:22:28,439 --> 02:22:33,930
 with the simple samples and then add

6402
02:22:31,550 --> 02:22:33,930
 

6403
02:22:31,560 --> 02:22:35,910
 progressively more complex samples so at

6404
02:22:33,920 --> 02:22:35,910
 

6405
02:22:33,930 --> 02:22:38,340
 first sites it seems a little bit

6406
02:22:35,900 --> 02:22:38,340
 

6407
02:22:35,910 --> 02:22:41,960
 contradictory I would I was wondering

6408
02:22:38,330 --> 02:22:41,960
 

6409
02:22:38,340 --> 02:22:44,400
 how you women do make sense out of that

6410
02:22:41,950 --> 02:22:44,400
 

6411
02:22:41,960 --> 02:22:46,979
 maybe I'll take a quick stab and then

6412
02:22:44,390 --> 02:22:46,979
 

6413
02:22:44,400 --> 02:22:50,640
 Steve can probably say more to but there

6414
02:22:46,969 --> 02:22:50,640
 

6415
02:22:46,979 --> 02:22:53,340
 is a an area called machine teaching

6416
02:22:50,630 --> 02:22:53,340
 

6417
02:22:50,640 --> 02:22:55,800
 which is a little bit more in line with

6418
02:22:53,330 --> 02:22:55,800
 

6419
02:22:53,340 --> 02:23:00,270
 the idea of teaching the students so if

6420
02:22:55,790 --> 02:23:00,270
 

6421
02:22:55,800 --> 02:23:02,250
 I have a learner and I know the model or

6422
02:23:00,260 --> 02:23:02,250
 

6423
02:23:00,270 --> 02:23:05,939
 an algorithm that they're using to learn

6424
02:23:02,240 --> 02:23:05,939
 

6425
02:23:02,250 --> 02:23:09,090
 with then I could strategically develop

6426
02:23:05,929 --> 02:23:09,090
 

6427
02:23:05,939 --> 02:23:11,820
 a curriculum to help them learn as in as

6428
02:23:09,080 --> 02:23:11,820
 

6429
02:23:09,090 --> 02:23:14,400
 few examples as possible so the idea

6430
02:23:11,810 --> 02:23:14,400
 

6431
02:23:11,820 --> 02:23:16,439
 there is that the teacher the supervisor

6432
02:23:14,390 --> 02:23:16,439
 

6433
02:23:14,400 --> 02:23:18,990
 knows the algorithm of the learner is

6434
02:23:16,429 --> 02:23:18,990
 

6435
02:23:16,439 --> 02:23:20,820
 ready and then knowing the algorithm

6436
02:23:18,980 --> 02:23:20,820
 

6437
02:23:18,990 --> 02:23:23,550
 that the learners using tries to pick

6438
02:23:20,810 --> 02:23:23,550
 

6439
02:23:20,820 --> 02:23:25,410
 out good teaching examples that's a

6440
02:23:23,540 --> 02:23:25,410
 

6441
02:23:23,550 --> 02:23:27,570
 little different than active learning

6442
02:23:25,400 --> 02:23:27,570
 

6443
02:23:25,410 --> 02:23:29,280
 where the learner itself doesn't really

6444
02:23:27,560 --> 02:23:29,280
 

6445
02:23:27,570 --> 02:23:31,350
 know what it's trying to learn but it's

6446
02:23:29,270 --> 02:23:31,350
 

6447
02:23:29,280 --> 02:23:34,500
 still trying to somehow adaptively

6448
02:23:31,340 --> 02:23:34,500
 

6449
02:23:31,350 --> 02:23:35,850
 request help from a supervisor so I

6450
02:23:34,490 --> 02:23:35,850
 

6451
02:23:34,500 --> 02:23:37,950
 don't know if you want to add anything

6452
02:23:35,840 --> 02:23:37,950
 

6453
02:23:35,850 --> 02:23:39,750
 to that but machine teaching is sort of

6454
02:23:37,940 --> 02:23:39,750
 

6455
02:23:37,950 --> 02:23:42,750
 more similar to the problem you

6456
02:23:39,740 --> 02:23:42,750
 

6457
02:23:39,750 --> 02:23:43,450
 mentioned which is a related to but a

6458
02:23:42,740 --> 02:23:43,450
 

6459
02:23:42,750 --> 02:23:45,641
 little bit

6460
02:23:43,440 --> 02:23:45,641
 

6461
02:23:43,450 --> 02:23:48,040
 from active learning because essentially

6462
02:23:45,631 --> 02:23:48,040
 

6463
02:23:45,641 --> 02:23:51,160
 machine teaching means that the

6464
02:23:48,030 --> 02:23:51,160
 

6465
02:23:48,040 --> 02:23:52,780
 supervisor the teacher knows the

6466
02:23:51,150 --> 02:23:52,780
 

6467
02:23:51,160 --> 02:23:55,570
 learning algorithm that the learner is

6468
02:23:52,770 --> 02:23:55,570
 

6469
02:23:52,780 --> 02:24:01,030
 using and that teacher is in charge of

6470
02:23:55,560 --> 02:24:01,030
 

6471
02:23:55,570 --> 02:24:03,340
 picking examples right in a sort of

6472
02:24:01,020 --> 02:24:03,340
 

6473
02:24:01,030 --> 02:24:06,311
 orthogonal direction and possibly

6474
02:24:03,330 --> 02:24:06,311
 

6475
02:24:03,340 --> 02:24:09,730
 related to this is that a lot of the

6476
02:24:06,301 --> 02:24:09,730
 

6477
02:24:06,311 --> 02:24:11,440
 techniques that I've mentioned also that

6478
02:24:09,720 --> 02:24:11,440
 

6479
02:24:09,730 --> 02:24:13,181
 both of us mentioned can could also be

6480
02:24:11,430 --> 02:24:13,181
 

6481
02:24:11,440 --> 02:24:14,530
 used in conjunction with some model

6482
02:24:13,171 --> 02:24:14,530
 

6483
02:24:13,181 --> 02:24:19,090
 selection so you could for instance

6484
02:24:14,520 --> 02:24:19,090
 

6485
02:24:14,530 --> 02:24:21,250
 learn the best linear separator using

6486
02:24:19,080 --> 02:24:21,250
 

6487
02:24:19,090 --> 02:24:24,250
 some agnostic active learning techniques

6488
02:24:21,240 --> 02:24:24,250
 

6489
02:24:21,250 --> 02:24:28,630
 and then say okay if that's not a good

6490
02:24:24,240 --> 02:24:28,630
 

6491
02:24:24,250 --> 02:24:31,090
 fit yet I can then get a larger model

6492
02:24:28,620 --> 02:24:31,090
 

6493
02:24:28,630 --> 02:24:33,101
 class quadratic curves maybe some kind

6494
02:24:31,080 --> 02:24:33,101
 

6495
02:24:31,090 --> 02:24:34,931
 of neural net or something so you can

6496
02:24:33,091 --> 02:24:34,931
 

6497
02:24:33,101 --> 02:24:38,230
 increase the complexity and you can

6498
02:24:34,921 --> 02:24:38,230
 

6499
02:24:34,931 --> 02:24:40,300
 actually decide how complex of a model

6500
02:24:38,220 --> 02:24:40,300
 

6501
02:24:38,230 --> 02:24:42,040
 you need while doing active learning it

6502
02:24:40,290 --> 02:24:42,040
 

6503
02:24:40,300 --> 02:24:44,620
 and you can do it while sort of still

6504
02:24:42,030 --> 02:24:44,620
 

6505
02:24:42,040 --> 02:24:46,450
 being better than passive learning for

6506
02:24:44,610 --> 02:24:46,450
 

6507
02:24:44,620 --> 02:24:48,070
 the similar kind of model selection type

6508
02:24:46,440 --> 02:24:48,070
 

6509
02:24:46,450 --> 02:24:50,641
 of tasks so there's some literature on

6510
02:24:48,060 --> 02:24:50,641
 

6511
02:24:48,070 --> 02:24:50,641
 that as well

6512
02:24:54,610 --> 02:24:54,610
 

6513
02:24:54,620 --> 02:25:01,389
[Applause]

6514
02:25:12,730 --> 02:25:12,730
 

6515
02:25:12,740 --> 02:25:23,870
[Music]

6516
02:25:28,750 --> 02:25:28,750
 

6517
02:25:28,760 --> 02:25:43,760
[Music]

6518
02:25:46,000 --> 02:25:46,000
 

6519
02:25:46,010 --> 02:26:45,750
[Music]