1
00:00:00,359 --> 00:00:06,240
 this is a mockery Leo will talk after me

2
00:00:03,169 --> 00:00:06,240
 

3
00:00:03,179 --> 00:00:09,300
 and and well I won't go through his kind

4
00:00:06,230 --> 00:00:09,300
 

5
00:00:06,240 --> 00:00:10,320
 of outline he'll talk you through that

6
00:00:09,290 --> 00:00:10,320
 

7
00:00:09,300 --> 00:00:11,969
 but there's going to be at the end

8
00:00:10,310 --> 00:00:11,969
 

9
00:00:10,320 --> 00:00:15,269
 there's going to be another 10 to 15

10
00:00:11,959 --> 00:00:15,269
 

11
00:00:11,969 --> 00:00:17,160
 minutes sort of QA period when both of

12
00:00:15,259 --> 00:00:17,160
 

13
00:00:15,269 --> 00:00:18,779
 us will come up together so you can ask

14
00:00:17,150 --> 00:00:18,779
 

15
00:00:17,160 --> 00:00:23,960
 questions about you know any part of the

16
00:00:18,769 --> 00:00:23,960
 

17
00:00:18,779 --> 00:00:27,390
 of the presentation up till then okay so

18
00:00:23,950 --> 00:00:27,390
 

19
00:00:23,960 --> 00:00:29,640
 first of all at a very general level you

20
00:00:27,380 --> 00:00:29,640
 

21
00:00:27,390 --> 00:00:31,679
 know where do we locate unsupervised

22
00:00:29,630 --> 00:00:31,679
 

23
00:00:29,640 --> 00:00:33,059
 learning with in machine learning within

24
00:00:31,669 --> 00:00:33,059
 

25
00:00:31,679 --> 00:00:36,030
 artificial intelligence and deep

26
00:00:33,049 --> 00:00:36,030
 

27
00:00:33,059 --> 00:00:38,190
 learning and so forth and people often

28
00:00:36,020 --> 00:00:38,190
 

29
00:00:36,030 --> 00:00:39,410
 talk about kind of three different types

30
00:00:38,180 --> 00:00:39,410
 

31
00:00:38,190 --> 00:00:41,160
 of learning supervised learning

32
00:00:39,400 --> 00:00:41,160
 

33
00:00:39,410 --> 00:00:44,059
 unsupervised learning and reinforcement

34
00:00:41,150 --> 00:00:44,059
 

35
00:00:41,160 --> 00:00:46,289
 learning and I find this categorization

36
00:00:44,049 --> 00:00:46,289
 

37
00:00:44,059 --> 00:00:48,660
 slightly unhelpful I think it's better

38
00:00:46,279 --> 00:00:48,660
 

39
00:00:46,289 --> 00:00:50,570
 to think along two dimensions of

40
00:00:48,650 --> 00:00:50,570
 

41
00:00:48,660 --> 00:00:53,430
 variability here so one of them is

42
00:00:50,560 --> 00:00:53,430
 

43
00:00:50,570 --> 00:00:55,079
 active versus passive so in the

44
00:00:53,420 --> 00:00:55,079
 

45
00:00:53,430 --> 00:00:57,690
 supervised or in the unsupervised

46
00:00:55,069 --> 00:00:57,690
 

47
00:00:55,079 --> 00:00:59,609
 setting the learner the agent if you

48
00:00:57,680 --> 00:00:59,609
 

49
00:00:57,690 --> 00:01:02,489
 want is passive it's just receiving data

50
00:00:59,599 --> 00:01:02,489
 

51
00:00:59,609 --> 00:01:04,860
 it's not acting in any way so as to

52
00:01:02,479 --> 00:01:04,860
 

53
00:01:02,489 --> 00:01:06,900
 modify the observations of the data it's

54
00:01:04,850 --> 00:01:06,900
 

55
00:01:04,860 --> 00:01:08,549
 going to receive in the future and then

56
00:01:06,890 --> 00:01:08,549
 

57
00:01:06,900 --> 00:01:09,750
 we have the active setting where we have

58
00:01:08,539 --> 00:01:09,750
 

59
00:01:08,549 --> 00:01:12,600
 what we would normally think of as an

60
00:01:09,740 --> 00:01:12,600
 

61
00:01:09,750 --> 00:01:15,360
 agent that actually interacts with in an

62
00:01:12,590 --> 00:01:15,360
 

63
00:01:12,600 --> 00:01:17,189
 environment and thereby influences the

64
00:01:15,350 --> 00:01:17,189
 

65
00:01:15,360 --> 00:01:20,490
 information that is going to receive and

66
00:01:17,179 --> 00:01:20,490
 

67
00:01:17,189 --> 00:01:23,369
 and the state that is going to be in and

68
00:01:20,480 --> 00:01:23,369
 

69
00:01:20,490 --> 00:01:26,430
 the other kind of degree of freedom here

70
00:01:23,359 --> 00:01:26,430
 

71
00:01:23,369 --> 00:01:29,400
 is whether or not there's a signal

72
00:01:26,420 --> 00:01:29,400
 

73
00:01:26,430 --> 00:01:30,930
 coming from a teacher so obviously in

74
00:01:29,390 --> 00:01:30,930
 

75
00:01:29,400 --> 00:01:33,360
 supervised learning you've got targets

76
00:01:30,920 --> 00:01:33,360
 

77
00:01:30,930 --> 00:01:34,500
 you've got predefined training targets

78
00:01:33,350 --> 00:01:34,500
 

79
00:01:33,360 --> 00:01:38,490
 that have been given to you by

80
00:01:34,490 --> 00:01:38,490
 

81
00:01:34,500 --> 00:01:40,200
 presumably a human teacher unsupervised

82
00:01:38,480 --> 00:01:40,200
 

83
00:01:38,490 --> 00:01:41,310
 learning you take those targets away and

84
00:01:40,190 --> 00:01:41,310
 

85
00:01:40,200 --> 00:01:44,310
 you're just learning directly from the

86
00:01:41,300 --> 00:01:44,310
 

87
00:01:41,310 --> 00:01:45,630
 data and so the analog in reinforcement

88
00:01:44,300 --> 00:01:45,630
 

89
00:01:44,310 --> 00:01:46,860
 learning would be well normal you know

90
00:01:45,620 --> 00:01:46,860
 

91
00:01:45,630 --> 00:01:48,360
 normal reinforcement learning

92
00:01:46,850 --> 00:01:48,360
 

93
00:01:46,860 --> 00:01:51,180
 reinforcement learning with extrinsic

94
00:01:48,350 --> 00:01:51,180
 

95
00:01:48,360 --> 00:01:52,619
 reward you have a teacher when you take

96
00:01:51,170 --> 00:01:52,619
 

97
00:01:51,180 --> 00:01:54,840
 the teacher away you're in this setting

98
00:01:52,609 --> 00:01:54,840
 

99
00:01:52,619 --> 00:01:56,790
 of we call intrinsic motivation or

100
00:01:54,830 --> 00:01:56,790
 

101
00:01:54,840 --> 00:01:58,560
 exploration where you're just finding

102
00:01:56,780 --> 00:01:58,560
 

103
00:01:56,790 --> 00:02:00,930
 something out about the environment in

104
00:01:58,550 --> 00:02:00,930
 

105
00:01:58,560 --> 00:02:02,549
 some sense for the sake of finding it

106
00:02:00,920 --> 00:02:02,549
 

107
00:02:00,930 --> 00:02:04,500
 out rather than for optimizing a

108
00:02:02,539 --> 00:02:04,500
 

109
00:02:02,549 --> 00:02:06,619
 particular reward and it should be

110
00:02:04,490 --> 00:02:06,619
 

111
00:02:04,500 --> 00:02:08,330
 should make it clear that you know

112
00:02:06,609 --> 00:02:08,330
 

113
00:02:06,619 --> 00:02:09,470
 neither of these categorizations are

114
00:02:08,320 --> 00:02:09,470
 

115
00:02:08,330 --> 00:02:11,360
 hard and fast there's kind of a

116
00:02:09,460 --> 00:02:11,360
 

117
00:02:09,470 --> 00:02:12,560
 continuum between having a teacher not

118
00:02:11,350 --> 00:02:12,560
 

119
00:02:11,360 --> 00:02:13,970
 having a teacher there are varying

120
00:02:12,550 --> 00:02:13,970
 

121
00:02:12,560 --> 00:02:16,069
 degrees of supervision you can have

122
00:02:13,960 --> 00:02:16,069
 

123
00:02:13,970 --> 00:02:19,190
 semi-supervised learning you can have

124
00:02:16,059 --> 00:02:19,190
 

125
00:02:16,069 --> 00:02:21,650
 you know very sparse extrinsic rewards

126
00:02:19,180 --> 00:02:21,650
 

127
00:02:19,190 --> 00:02:22,550
 coupled with with intrinsic rewards and

128
00:02:21,640 --> 00:02:22,550
 

129
00:02:21,650 --> 00:02:23,750
 reinforcement learning

130
00:02:22,540 --> 00:02:23,750
 

131
00:02:22,550 --> 00:02:26,120
 you can also there's also a kind of

132
00:02:23,740 --> 00:02:26,120
 

133
00:02:23,750 --> 00:02:27,260
 continuum between active and passive so

134
00:02:26,110 --> 00:02:27,260
 

135
00:02:26,120 --> 00:02:29,120
 there's you know a fully active

136
00:02:27,250 --> 00:02:29,120
 

137
00:02:27,260 --> 00:02:30,440
 reinforcement learning agent at one

138
00:02:29,110 --> 00:02:30,440
 

139
00:02:29,120 --> 00:02:31,819
 extreme and then there's something like

140
00:02:30,430 --> 00:02:31,819
 

141
00:02:30,440 --> 00:02:33,440
 active learning where you have what is

142
00:02:31,809 --> 00:02:33,440
 

143
00:02:31,819 --> 00:02:35,900
 essentially supervised learning except

144
00:02:33,430 --> 00:02:35,900
 

145
00:02:33,440 --> 00:02:38,480
 that you're making a decision about what

146
00:02:35,890 --> 00:02:38,480
 

147
00:02:35,900 --> 00:02:40,519
 kind of data to look at yet curriculum

148
00:02:38,470 --> 00:02:40,519
 

149
00:02:38,480 --> 00:02:43,340
 learning is another example of that so

150
00:02:40,509 --> 00:02:43,340
 

151
00:02:40,519 --> 00:02:45,920
 anyway this talk will mostly be focused

152
00:02:43,330 --> 00:02:45,920
 

153
00:02:43,340 --> 00:02:47,599
 on the unsupervised learning setting but

154
00:02:45,910 --> 00:02:47,599
 

155
00:02:45,920 --> 00:02:49,190
 I will also try to talk a little bit

156
00:02:47,589 --> 00:02:49,190
 

157
00:02:47,599 --> 00:02:51,019
 about intrinsic motivation and

158
00:02:49,180 --> 00:02:51,019
 

159
00:02:49,190 --> 00:02:54,980
 reinforcement learning without without a

160
00:02:51,009 --> 00:02:54,980
 

161
00:02:51,019 --> 00:02:57,140
 teacher signal and hopefully hopefully I

162
00:02:54,970 --> 00:02:57,140
 

163
00:02:54,980 --> 00:02:58,250
 can get across the idea that all of all

164
00:02:57,130 --> 00:02:58,250
 

165
00:02:57,140 --> 00:03:00,170
 of the stuff we're talking about here

166
00:02:58,240 --> 00:03:00,170
 

167
00:02:58,250 --> 00:03:01,700
 should be relevant to both settings so

168
00:03:00,160 --> 00:03:01,700
 

169
00:03:00,170 --> 00:03:03,920
 we're really talking about learning

170
00:03:01,690 --> 00:03:03,920
 

171
00:03:01,700 --> 00:03:05,840
 without a teacher in general rather than

172
00:03:03,910 --> 00:03:05,840
 

173
00:03:03,920 --> 00:03:06,769
 specifically learning for you know

174
00:03:05,830 --> 00:03:06,769
 

175
00:03:05,840 --> 00:03:08,209
 what's usually thought of as

176
00:03:06,759 --> 00:03:08,209
 

177
00:03:06,769 --> 00:03:09,290
 unsupervised learning or for

178
00:03:08,199 --> 00:03:09,290
 

179
00:03:08,209 --> 00:03:13,370
 unsupervised reinforcement learning

180
00:03:09,280 --> 00:03:13,370
 

181
00:03:09,290 --> 00:03:15,680
 anyway so following that preamble you

182
00:03:13,360 --> 00:03:15,680
 

183
00:03:13,370 --> 00:03:16,730
 know the first question is so if what we

184
00:03:15,670 --> 00:03:16,730
 

185
00:03:15,680 --> 00:03:18,590
 want to do here is talk about how to

186
00:03:16,720 --> 00:03:18,590
 

187
00:03:16,730 --> 00:03:19,910
 learn without a teacher maybe the first

188
00:03:18,580 --> 00:03:19,910
 

189
00:03:18,590 --> 00:03:22,280
 question is you know why would you want

190
00:03:19,900 --> 00:03:22,280
 

191
00:03:19,910 --> 00:03:24,349
 to do that so if your ultimate goal is

192
00:03:22,270 --> 00:03:24,349
 

193
00:03:22,280 --> 00:03:26,750
 to create intelligent systems that can

194
00:03:24,339 --> 00:03:26,750
 

195
00:03:24,349 --> 00:03:28,609
 perform a wide variety of tasks so

196
00:03:26,740 --> 00:03:28,609
 

197
00:03:26,750 --> 00:03:31,790
 whether these are reinforcement learning

198
00:03:28,599 --> 00:03:31,790
 

199
00:03:28,609 --> 00:03:33,019
 active tasks or supervised tasks you

200
00:03:31,780 --> 00:03:33,019
 

201
00:03:31,790 --> 00:03:34,930
 might ask well why don't we just teach

202
00:03:33,009 --> 00:03:34,930
 

203
00:03:33,019 --> 00:03:36,980
 them how to do those tasks directly

204
00:03:34,920 --> 00:03:36,980
 

205
00:03:34,930 --> 00:03:38,450
 gather lots of data and teach them how

206
00:03:36,970 --> 00:03:38,450
 

207
00:03:36,980 --> 00:03:39,650
 to do it well you know the first one the

208
00:03:38,440 --> 00:03:39,650
 

209
00:03:38,450 --> 00:03:41,480
 first question the first sort of

210
00:03:39,640 --> 00:03:41,480
 

211
00:03:39,650 --> 00:03:42,950
 complaint that everyone brings up here

212
00:03:41,470 --> 00:03:42,950
 

213
00:03:41,480 --> 00:03:45,530
 is well it can be difficult to obtain

214
00:03:42,940 --> 00:03:45,530
 

215
00:03:42,950 --> 00:03:46,970
 all of this kind of training signal all

216
00:03:45,520 --> 00:03:46,970
 

217
00:03:45,530 --> 00:03:48,109
 of this teaching signal so in the case

218
00:03:46,960 --> 00:03:48,109
 

219
00:03:46,970 --> 00:03:50,599
 of supervised learning you have to get

220
00:03:48,099 --> 00:03:50,599
 

221
00:03:48,109 --> 00:03:52,069
 huge quantities of labelled data which

222
00:03:50,589 --> 00:03:52,069
 

223
00:03:50,599 --> 00:03:54,380
 can be difficult to obtain in the

224
00:03:52,059 --> 00:03:54,380
 

225
00:03:52,069 --> 00:03:55,760
 reinforcement learning case the active

226
00:03:54,370 --> 00:03:55,760
 

227
00:03:54,380 --> 00:03:58,910
 case is a little bit you know more

228
00:03:55,750 --> 00:03:58,910
 

229
00:03:55,760 --> 00:04:00,620
 subtle than that in the the the reward

230
00:03:58,900 --> 00:04:00,620
 

231
00:03:58,910 --> 00:04:02,209
 signals are generally defined in a more

232
00:04:00,610 --> 00:04:02,209
 

233
00:04:00,620 --> 00:04:03,980
 like programmatic way they're not kind

234
00:04:02,199 --> 00:04:03,980
 

235
00:04:02,209 --> 00:04:06,380
 of hand assigned to through specific

236
00:04:03,970 --> 00:04:06,380
 

237
00:04:03,980 --> 00:04:07,940
 observations but they're still difficult

238
00:04:06,370 --> 00:04:07,940
 

239
00:04:06,380 --> 00:04:09,530
 to define someone still has to think

240
00:04:07,930 --> 00:04:09,530
 

241
00:04:07,940 --> 00:04:10,880
 about what the task should be and what

242
00:04:09,520 --> 00:04:10,880
 

243
00:04:09,530 --> 00:04:12,769
 the reward structure would be and so

244
00:04:10,870 --> 00:04:12,769
 

245
00:04:10,880 --> 00:04:14,120
 anyone who's worked and reinforcement

246
00:04:12,759 --> 00:04:14,120
 

247
00:04:12,769 --> 00:04:15,890
 learning knows that determining the

248
00:04:14,110 --> 00:04:15,890
 

249
00:04:14,120 --> 00:04:17,599
 reward structure is really a big part of

250
00:04:15,880 --> 00:04:17,599
 

251
00:04:15,890 --> 00:04:20,280
 the problem that's where a lot of the

252
00:04:17,589 --> 00:04:20,280
 

253
00:04:17,599 --> 00:04:23,400
 work goes well the second

254
00:04:20,270 --> 00:04:23,400
 

255
00:04:20,280 --> 00:04:24,780
 here is a little bit more more fuzzy you

256
00:04:23,390 --> 00:04:24,780
 

257
00:04:23,400 --> 00:04:26,970
 know it's it's a somewhat subjective

258
00:04:24,770 --> 00:04:26,970
 

259
00:04:24,780 --> 00:04:28,980
 point but I think there's a general

260
00:04:26,960 --> 00:04:28,980
 

261
00:04:26,970 --> 00:04:31,380
 consensus that unsupervised learning

262
00:04:28,970 --> 00:04:31,380
 

263
00:04:28,980 --> 00:04:33,510
 feels a little bit more like the way a

264
00:04:31,370 --> 00:04:33,510
 

265
00:04:31,380 --> 00:04:35,130
 human learns right babies are I mean to

266
00:04:33,500 --> 00:04:35,130
 

267
00:04:33,510 --> 00:04:37,139
 some extent we we teach our children

268
00:04:35,120 --> 00:04:37,139
 

269
00:04:35,130 --> 00:04:39,060
 things we tell them things but for the

270
00:04:37,129 --> 00:04:39,060
 

271
00:04:37,139 --> 00:04:40,470
 most part they explore and interact with

272
00:04:39,050 --> 00:04:40,470
 

273
00:04:39,060 --> 00:04:42,990
 the world and find things out about the

274
00:04:40,460 --> 00:04:42,990
 

275
00:04:40,470 --> 00:04:44,340
 world without being told how to do it

276
00:04:42,980 --> 00:04:44,340
 

277
00:04:42,990 --> 00:04:46,169
 and so there's just the general feeling

278
00:04:44,330 --> 00:04:46,169
 

279
00:04:44,340 --> 00:04:48,060
 that if all we're doing is giving our

280
00:04:46,159 --> 00:04:48,060
 

281
00:04:46,169 --> 00:04:50,100
 agents signals and asking them to

282
00:04:48,050 --> 00:04:50,100
 

283
00:04:48,060 --> 00:04:51,870
 exactly optimize and match those signals

284
00:04:50,090 --> 00:04:51,870
 

285
00:04:50,100 --> 00:04:54,650
 they're never really learning in a way

286
00:04:51,860 --> 00:04:54,650
 

287
00:04:51,870 --> 00:04:57,120
 that we think of as human intelligence

288
00:04:54,640 --> 00:04:57,120
 

289
00:04:54,650 --> 00:04:59,520
 but I think the last point is the really

290
00:04:57,110 --> 00:04:59,520
 

291
00:04:57,120 --> 00:05:02,580
 critical one which is that we don't just

292
00:04:59,510 --> 00:05:02,580
 

293
00:04:59,520 --> 00:05:04,830
 want to solve a particular set of tasks

294
00:05:02,570 --> 00:05:04,830
 

295
00:05:02,580 --> 00:05:07,830
 we want to rapidly generalize the new

296
00:05:04,820 --> 00:05:07,830
 

297
00:05:04,830 --> 00:05:10,020
 tasks and situations and this is you

298
00:05:07,820 --> 00:05:10,020
 

299
00:05:07,830 --> 00:05:11,310
 know evident already in the supervised

300
00:05:10,010 --> 00:05:11,310
 

301
00:05:10,020 --> 00:05:13,260
 learning setting but it's much more

302
00:05:11,300 --> 00:05:13,260
 

303
00:05:11,310 --> 00:05:15,630
 evident in the reinforcement setting so

304
00:05:13,250 --> 00:05:15,630
 

305
00:05:13,260 --> 00:05:17,580
 it's you know there's no way we can hope

306
00:05:15,620 --> 00:05:17,580
 

307
00:05:15,630 --> 00:05:20,280
 to build a kind of generally intelligent

308
00:05:17,570 --> 00:05:20,280
 

309
00:05:17,580 --> 00:05:21,900
 agent by exposing it to all of the

310
00:05:20,270 --> 00:05:21,900
 

311
00:05:20,280 --> 00:05:24,419
 possible scenarios that it will ever

312
00:05:21,890 --> 00:05:24,419
 

313
00:05:21,900 --> 00:05:25,290
 encounter in its lifetime the world

314
00:05:24,409 --> 00:05:25,290
 

315
00:05:24,419 --> 00:05:28,050
 doesn't work like that

316
00:05:25,280 --> 00:05:28,050
 

317
00:05:25,290 --> 00:05:29,640
 things change novel situations arise the

318
00:05:28,040 --> 00:05:29,640
 

319
00:05:28,050 --> 00:05:31,979
 sort of the key element of general

320
00:05:29,630 --> 00:05:31,979
 

321
00:05:29,640 --> 00:05:33,419
 intelligence I think is actually not the

322
00:05:31,969 --> 00:05:33,419
 

323
00:05:31,979 --> 00:05:35,940
 ability to learn lots of things but

324
00:05:33,409 --> 00:05:35,940
 

325
00:05:33,419 --> 00:05:37,620
 rather the ability to rapidly adapt to

326
00:05:35,930 --> 00:05:37,620
 

327
00:05:35,940 --> 00:05:39,090
 new things and new situations and I

328
00:05:37,610 --> 00:05:39,090
 

329
00:05:37,620 --> 00:05:40,410
 think this is something that there's a

330
00:05:39,080 --> 00:05:40,410
 

331
00:05:39,090 --> 00:05:43,530
 general feeling that this is just not

332
00:05:40,400 --> 00:05:43,530
 

333
00:05:40,410 --> 00:05:46,700
 happening with just with you know

334
00:05:43,520 --> 00:05:46,700
 

335
00:05:43,530 --> 00:05:49,830
 supervised learning or extrinsic rewards

336
00:05:46,690 --> 00:05:49,830
 

337
00:05:46,700 --> 00:05:50,820
 and so I say it's not happening of

338
00:05:49,820 --> 00:05:50,820
 

339
00:05:49,830 --> 00:05:52,410
 course there is this you know there's a

340
00:05:50,810 --> 00:05:52,410
 

341
00:05:50,820 --> 00:05:54,210
 whole field of you know transfer

342
00:05:52,400 --> 00:05:54,210
 

343
00:05:52,410 --> 00:05:55,650
 learning right teach on one task and

344
00:05:54,200 --> 00:05:55,650
 

345
00:05:54,210 --> 00:05:57,690
 transfer to another so multitask

346
00:05:55,640 --> 00:05:57,690
 

347
00:05:55,650 --> 00:05:59,280
 learning one-shot learning these sorts

348
00:05:57,680 --> 00:05:59,280
 

349
00:05:57,690 --> 00:06:01,110
 of things and without meaning any

350
00:05:59,270 --> 00:06:01,110
 

351
00:05:59,280 --> 00:06:03,720
 disrespect to people working in this

352
00:06:01,100 --> 00:06:03,720
 

353
00:06:01,110 --> 00:06:05,520
 field I think it's fair to say that it

354
00:06:03,710 --> 00:06:05,520
 

355
00:06:03,720 --> 00:06:07,470
 does kind of work so you know an example

356
00:06:05,510 --> 00:06:07,470
 

357
00:06:05,520 --> 00:06:08,520
 that I remember hearing about not long

358
00:06:07,460 --> 00:06:08,520
 

359
00:06:07,470 --> 00:06:10,650
 ago is that you know for speech

360
00:06:08,510 --> 00:06:10,650
 

361
00:06:08,520 --> 00:06:11,780
 recognition systems and this is for you

362
00:06:10,640 --> 00:06:11,780
 

363
00:06:10,650 --> 00:06:14,100
 know commercial speech recognition

364
00:06:11,770 --> 00:06:14,100
 

365
00:06:11,780 --> 00:06:15,270
 systems as I use the Google and places

366
00:06:14,090 --> 00:06:15,270
 

367
00:06:14,100 --> 00:06:17,610
 like that

368
00:06:15,260 --> 00:06:17,610
 

369
00:06:15,270 --> 00:06:19,770
 when you've got a language for which you

370
00:06:17,600 --> 00:06:19,770
 

371
00:06:17,610 --> 00:06:21,539
 don't have very much data you can

372
00:06:19,760 --> 00:06:21,539
 

373
00:06:19,770 --> 00:06:23,430
 sometimes kind of bootstrap from another

374
00:06:21,529 --> 00:06:23,430
 

375
00:06:21,539 --> 00:06:24,990
 language that's quite similar and you do

376
00:06:23,420 --> 00:06:24,990
 

377
00:06:23,430 --> 00:06:26,460
 have a lot of data so you're getting an

378
00:06:24,980 --> 00:06:26,460
 

379
00:06:24,990 --> 00:06:28,650
 actual sort of practical you know

380
00:06:26,450 --> 00:06:28,650
 

381
00:06:26,460 --> 00:06:31,260
 commercial improvement by transferring

382
00:06:28,640 --> 00:06:31,260
 

383
00:06:28,650 --> 00:06:32,800
 from a similar class but you're not

384
00:06:31,250 --> 00:06:32,800
 

385
00:06:31,260 --> 00:06:34,360
 transferring very far typically

386
00:06:32,790 --> 00:06:34,360
 

387
00:06:32,800 --> 00:06:36,550
 going from you know one maybe one

388
00:06:34,350 --> 00:06:36,550
 

389
00:06:34,360 --> 00:06:38,590
 dialect of one letter one one language

390
00:06:36,540 --> 00:06:38,590
 

391
00:06:36,550 --> 00:06:41,740
 one closely related language to another

392
00:06:38,580 --> 00:06:41,740
 

393
00:06:38,590 --> 00:06:43,360
 when in general there's a sort of gut

394
00:06:41,730 --> 00:06:43,360
 

395
00:06:41,740 --> 00:06:45,430
 feeling that this kind of transfer

396
00:06:43,350 --> 00:06:45,430
 

397
00:06:43,360 --> 00:06:48,509
 doesn't seem to go it never generalizes

398
00:06:45,420 --> 00:06:48,509
 

399
00:06:45,430 --> 00:06:51,699
 as far or as fast as we want it to and

400
00:06:48,499 --> 00:06:51,699
 

401
00:06:48,509 --> 00:06:52,990
 one kind of hypothesis hypothesis for

402
00:06:51,689 --> 00:06:52,990
 

403
00:06:51,699 --> 00:06:55,150
 this is that there just isn't enough

404
00:06:52,980 --> 00:06:55,150
 

405
00:06:52,990 --> 00:06:57,009
 information in the targets and the

406
00:06:55,140 --> 00:06:57,009
 

407
00:06:55,150 --> 00:06:58,599
 rewards that teachers can provide for

408
00:06:56,999 --> 00:06:58,599
 

409
00:06:57,009 --> 00:07:00,849
 these agents for them to be able to

410
00:06:58,589 --> 00:07:00,849
 

411
00:06:58,599 --> 00:07:02,620
 learn transferable skills and so this is

412
00:07:00,839 --> 00:07:02,620
 

413
00:07:00,849 --> 00:07:04,659
 a highlight this word skills

414
00:07:02,610 --> 00:07:04,659
 

415
00:07:02,620 --> 00:07:05,710
 I saw talked recently by senator Singh

416
00:07:04,649 --> 00:07:05,710
 

417
00:07:04,659 --> 00:07:07,210
 and he said something that really stuck

418
00:07:05,700 --> 00:07:07,210
 

419
00:07:05,710 --> 00:07:09,460
 in my mind which is we need to stop

420
00:07:07,200 --> 00:07:09,460
 

421
00:07:07,210 --> 00:07:11,319
 thinking about teaching agents to learn

422
00:07:09,450 --> 00:07:11,319
 

423
00:07:09,460 --> 00:07:14,139
 tasks and start teaching them to learn

424
00:07:11,309 --> 00:07:14,139
 

425
00:07:11,319 --> 00:07:16,060
 skills people don't solve a series of

426
00:07:14,129 --> 00:07:16,060
 

427
00:07:14,139 --> 00:07:17,650
 tasks in isolation and then transfer

428
00:07:16,050 --> 00:07:17,650
 

429
00:07:16,060 --> 00:07:20,409
 them rather we pick up skills through

430
00:07:17,640 --> 00:07:20,409
 

431
00:07:17,650 --> 00:07:22,830
 our life that we're able to to reuse and

432
00:07:20,399 --> 00:07:22,830
 

433
00:07:20,409 --> 00:07:26,349
 you know flexibly adapt later on in life

434
00:07:22,820 --> 00:07:26,349
 

435
00:07:22,830 --> 00:07:28,990
 okay so a lot of you have probably read

436
00:07:26,339 --> 00:07:28,990
 

437
00:07:26,349 --> 00:07:30,430
 this quote before from Janna Lacan he

438
00:07:28,980 --> 00:07:30,430
 

439
00:07:28,990 --> 00:07:32,379
 says if intelligence was a cake

440
00:07:30,420 --> 00:07:32,379
 

441
00:07:30,430 --> 00:07:34,629
 unsupervised learning unsupervised

442
00:07:32,369 --> 00:07:34,629
 

443
00:07:32,379 --> 00:07:35,770
 learning would be the cake supervised

444
00:07:34,619 --> 00:07:35,770
 

445
00:07:34,629 --> 00:07:37,180
 learning would be the icing on the cake

446
00:07:35,760 --> 00:07:37,180
 

447
00:07:35,770 --> 00:07:39,460
 and reinforcement learning would be the

448
00:07:37,170 --> 00:07:39,460
 

449
00:07:37,180 --> 00:07:40,719
 cherry on the cake and some of you might

450
00:07:39,450 --> 00:07:40,719
 

451
00:07:39,460 --> 00:07:44,080
 be a little bit surprised to see someone

452
00:07:40,709 --> 00:07:44,080
 

453
00:07:40,719 --> 00:07:47,500
 from deep mind quoting this but I don't

454
00:07:44,070 --> 00:07:47,500
 

455
00:07:44,080 --> 00:07:49,449
 read it as a slide towards reinforcement

456
00:07:47,490 --> 00:07:49,449
 

457
00:07:47,500 --> 00:07:52,270
 learning but rather a statement of this

458
00:07:49,439 --> 00:07:52,270
 

459
00:07:49,449 --> 00:07:54,099
 massive disparity in the kind of volume

460
00:07:52,260 --> 00:07:54,099
 

461
00:07:52,270 --> 00:07:57,460
 of information between these different

462
00:07:54,089 --> 00:07:57,460
 

463
00:07:54,099 --> 00:07:58,930
 types of signals so it's very clear the

464
00:07:57,450 --> 00:07:58,930
 

465
00:07:57,460 --> 00:08:00,520
 targets for supervised learning have a

466
00:07:58,920 --> 00:08:00,520
 

467
00:07:58,930 --> 00:08:01,539
 lot less information than the input data

468
00:08:00,510 --> 00:08:01,539
 

469
00:08:00,520 --> 00:08:03,460
 and I'm actually going to go through a

470
00:08:01,529 --> 00:08:03,460
 

471
00:08:01,539 --> 00:08:04,810
 specific example of that in the next

472
00:08:03,450 --> 00:08:04,810
 

473
00:08:03,460 --> 00:08:06,969
 slide just to kind of make that point

474
00:08:04,800 --> 00:08:06,969
 

475
00:08:04,810 --> 00:08:08,349
 very clear but of course reward signals

476
00:08:06,959 --> 00:08:08,349
 

477
00:08:06,969 --> 00:08:09,580
 from reinforcement learning contain even

478
00:08:08,339 --> 00:08:09,580
 

479
00:08:08,349 --> 00:08:11,830
 less so you might have a very long

480
00:08:09,570 --> 00:08:11,830
 

481
00:08:09,580 --> 00:08:14,319
 sequence of actions interactions with a

482
00:08:11,820 --> 00:08:14,319
 

483
00:08:11,830 --> 00:08:17,590
 you know complicated environment

484
00:08:14,309 --> 00:08:17,590
 

485
00:08:14,319 --> 00:08:18,460
 involving vision sound whatever and then

486
00:08:17,580 --> 00:08:18,460
 

487
00:08:17,590 --> 00:08:20,800
 at the end you're just gonna get a

488
00:08:18,450 --> 00:08:20,800
 

489
00:08:18,460 --> 00:08:22,300
 single bit of reward you know you guys

490
00:08:20,790 --> 00:08:22,300
 

491
00:08:20,800 --> 00:08:23,979
 are succeeded or you didn't a one or a

492
00:08:22,290 --> 00:08:23,979
 

493
00:08:22,300 --> 00:08:25,659
 zero and it's kind of amazing that

494
00:08:23,969 --> 00:08:25,659
 

495
00:08:23,979 --> 00:08:27,069
 agents can learn at all under those

496
00:08:25,649 --> 00:08:27,069
 

497
00:08:25,659 --> 00:08:30,310
 circumstances they're given so little

498
00:08:27,059 --> 00:08:30,310
 

499
00:08:27,069 --> 00:08:32,800
 kind of you know information to latch on

500
00:08:30,300 --> 00:08:32,800
 

501
00:08:30,310 --> 00:08:35,229
 to they think they can do something but

502
00:08:32,790 --> 00:08:35,229
 

503
00:08:32,800 --> 00:08:37,029
 it's also clear that if we turn this

504
00:08:35,219 --> 00:08:37,029
 

505
00:08:35,229 --> 00:08:38,289
 around if we stop thinking about signals

506
00:08:37,019 --> 00:08:38,289
 

507
00:08:37,029 --> 00:08:39,640
 from teachers and start thinking about

508
00:08:38,279 --> 00:08:39,640
 

509
00:08:38,289 --> 00:08:41,680
 the signal that's just inherently

510
00:08:39,630 --> 00:08:41,680
 

511
00:08:39,640 --> 00:08:43,149
 present in the data then we've got this

512
00:08:41,670 --> 00:08:43,149
 

513
00:08:41,680 --> 00:08:43,970
 basically unlimited supply of

514
00:08:43,139 --> 00:08:43,970
 

515
00:08:43,149 --> 00:08:45,680
 information

516
00:08:43,960 --> 00:08:45,680
 

517
00:08:43,970 --> 00:08:47,900
 the world we can we can just go and you

518
00:08:45,670 --> 00:08:47,900
 

519
00:08:45,680 --> 00:08:50,120
 know troll the whole internet we can set

520
00:08:47,890 --> 00:08:50,120
 

521
00:08:47,900 --> 00:08:51,560
 a digital camera running we can you know

522
00:08:50,110 --> 00:08:51,560
 

523
00:08:50,120 --> 00:08:53,000
 we can gather limitless amounts of

524
00:08:51,550 --> 00:08:53,000
 

525
00:08:51,560 --> 00:08:57,650
 information if we don't have to worry

526
00:08:52,990 --> 00:08:57,650
 

527
00:08:53,000 --> 00:08:59,300
 about how we are labeling or assigning

528
00:08:57,640 --> 00:08:59,300
 

529
00:08:57,650 --> 00:09:01,280
 reward to that information it just seems

530
00:08:59,290 --> 00:09:01,280
 

531
00:08:59,300 --> 00:09:02,990
 obvious that we should exploit that and

532
00:09:01,270 --> 00:09:02,990
 

533
00:09:01,280 --> 00:09:04,400
 so just just to go through a little

534
00:09:02,980 --> 00:09:04,400
 

535
00:09:02,990 --> 00:09:06,710
 example here and this is a little bit

536
00:09:04,390 --> 00:09:06,710
 

537
00:09:04,400 --> 00:09:08,210
 you know pedantic in some sense but I

538
00:09:06,700 --> 00:09:08,210
 

539
00:09:06,710 --> 00:09:10,010
 think it's a good point to make so let's

540
00:09:08,200 --> 00:09:10,010
 

541
00:09:08,210 --> 00:09:11,360
 say we go we look at image net and we

542
00:09:10,000 --> 00:09:11,360
 

543
00:09:10,010 --> 00:09:13,190
 look at the training set it's got around

544
00:09:11,350 --> 00:09:13,190
 

545
00:09:11,360 --> 00:09:16,010
 one point to eight million images and

546
00:09:13,180 --> 00:09:16,010
 

547
00:09:13,190 --> 00:09:17,570
 each one is assigned one of 1,000 labels

548
00:09:16,000 --> 00:09:17,570
 

549
00:09:16,010 --> 00:09:18,800
 I think I've got these numbers right but

550
00:09:17,560 --> 00:09:18,800
 

551
00:09:17,570 --> 00:09:22,220
 it's not really important if they're if

552
00:09:18,790 --> 00:09:22,220
 

553
00:09:18,800 --> 00:09:23,870
 they're wrong if those labels are you

554
00:09:22,210 --> 00:09:23,870
 

555
00:09:22,220 --> 00:09:25,640
 know roughly equally probable there's an

556
00:09:23,860 --> 00:09:25,640
 

557
00:09:23,870 --> 00:09:27,440
 equal distribution across classes then

558
00:09:25,630 --> 00:09:27,440
 

559
00:09:25,640 --> 00:09:30,230
 basically the set of labels contain

560
00:09:27,430 --> 00:09:30,230
 

561
00:09:27,440 --> 00:09:32,320
 something like well log of 1,000 times

562
00:09:30,220 --> 00:09:32,320
 

563
00:09:30,230 --> 00:09:35,270
 1.2 mu and so once at twelve point eight

564
00:09:32,310 --> 00:09:35,270
 

565
00:09:32,320 --> 00:09:36,440
 megabits of information and when you

566
00:09:35,260 --> 00:09:36,440
 

567
00:09:35,270 --> 00:09:38,270
 think about it that's not really that

568
00:09:36,430 --> 00:09:38,270
 

569
00:09:36,440 --> 00:09:39,860
 much you know people are often surprised

570
00:09:38,260 --> 00:09:39,860
 

571
00:09:38,270 --> 00:09:41,240
 at beginners you know when people sort

572
00:09:39,850 --> 00:09:41,240
 

573
00:09:39,860 --> 00:09:43,820
 of start working on machine learning

574
00:09:41,230 --> 00:09:43,820
 

575
00:09:41,240 --> 00:09:45,680
 they're surprised that their neural

576
00:09:43,810 --> 00:09:45,680
 

577
00:09:43,820 --> 00:09:47,420
 networks can over fit on such a big data

578
00:09:45,670 --> 00:09:47,420
 

579
00:09:45,680 --> 00:09:48,470
 set but the point is that if you're

580
00:09:47,410 --> 00:09:48,470
 

581
00:09:47,420 --> 00:09:50,720
 doing supervised learning you're not

582
00:09:48,460 --> 00:09:50,720
 

583
00:09:48,470 --> 00:09:52,160
 overfitting on the inputs in the data

584
00:09:50,710 --> 00:09:52,160
 

585
00:09:50,720 --> 00:09:53,780
 you're overfitting on the targets and

586
00:09:52,150 --> 00:09:53,780
 

587
00:09:52,160 --> 00:09:55,640
 there aren't actually that many targets

588
00:09:53,770 --> 00:09:55,640
 

589
00:09:53,780 --> 00:09:58,220
 so by contrast if you were to take the

590
00:09:55,630 --> 00:09:58,220
 

591
00:09:55,640 --> 00:10:00,680
 whole set of the images uncompressed

592
00:09:58,210 --> 00:10:00,680
 

593
00:09:58,220 --> 00:10:02,810
 that 128 528 resolution you'd be looking

594
00:10:00,670 --> 00:10:02,810
 

595
00:10:00,680 --> 00:10:05,930
 at someone by my rough estimate

596
00:10:02,800 --> 00:10:05,930
 

597
00:10:02,810 --> 00:10:08,630
 something like 500 gigabits of data so

598
00:10:05,920 --> 00:10:08,630
 

599
00:10:05,930 --> 00:10:10,190
 for orders or more of magnitude more and

600
00:10:08,620 --> 00:10:10,190
 

601
00:10:08,630 --> 00:10:13,420
 so there's this recent paper that showed

602
00:10:10,180 --> 00:10:13,420
 

603
00:10:10,190 --> 00:10:15,560
 well you know these neural nets can

604
00:10:13,410 --> 00:10:15,560
 

605
00:10:13,420 --> 00:10:17,420
 sometimes have too much capacity right

606
00:10:15,550 --> 00:10:17,420
 

607
00:10:15,560 --> 00:10:19,100
 we can take image net and randomize the

608
00:10:17,410 --> 00:10:19,100
 

609
00:10:17,420 --> 00:10:20,570
 labelings and it can still solve the

610
00:10:19,090 --> 00:10:20,570
 

611
00:10:19,100 --> 00:10:22,460
 training set perfectly which means that

612
00:10:20,560 --> 00:10:22,460
 

613
00:10:20,570 --> 00:10:24,050
 what is learning is gibberish right it's

614
00:10:22,450 --> 00:10:24,050
 

615
00:10:22,460 --> 00:10:25,160
 learning it's just memorizing a bunch of

616
00:10:24,040 --> 00:10:25,160
 

617
00:10:24,050 --> 00:10:27,380
 labels that have no relationship

618
00:10:25,150 --> 00:10:27,380
 

619
00:10:25,160 --> 00:10:28,610
 whatsoever to the the inputs and

620
00:10:27,370 --> 00:10:28,610
 

621
00:10:27,380 --> 00:10:30,470
 actually that's not that surprising

622
00:10:28,600 --> 00:10:30,470
 

623
00:10:28,610 --> 00:10:31,880
 right the convolution that has maybe 30

624
00:10:30,460 --> 00:10:31,880
 

625
00:10:30,470 --> 00:10:34,010
 million weights and you're asking it to

626
00:10:31,870 --> 00:10:34,010
 

627
00:10:31,880 --> 00:10:35,990
 learn twelve point eight million bits

628
00:10:34,000 --> 00:10:35,990
 

629
00:10:34,010 --> 00:10:38,270
 right so you do the math you don't have

630
00:10:35,980 --> 00:10:38,270
 

631
00:10:35,990 --> 00:10:41,150
 to embed that many you know less than

632
00:10:38,260 --> 00:10:41,150
 

633
00:10:38,270 --> 00:10:42,950
 one bit per parameter sorry that's less

634
00:10:41,140 --> 00:10:42,950
 

635
00:10:41,150 --> 00:10:44,750
 than one bit per weight right it's not

636
00:10:42,940 --> 00:10:44,750
 

637
00:10:42,950 --> 00:10:47,030
 that difficult fruit to do but of course

638
00:10:44,740 --> 00:10:47,030
 

639
00:10:44,750 --> 00:10:49,190
 that same Network wouldn't be able to to

640
00:10:47,020 --> 00:10:49,190
 

641
00:10:47,030 --> 00:10:51,320
 memorize the pixels if we randomized the

642
00:10:49,180 --> 00:10:51,320
 

643
00:10:49,190 --> 00:10:53,420
 pixels and asked it to do density

644
00:10:51,310 --> 00:10:53,420
 

645
00:10:51,320 --> 00:10:55,910
 modeling asked it to you know model the

646
00:10:53,410 --> 00:10:55,910
 

647
00:10:53,420 --> 00:10:57,470
 pixels it just wouldn't work at all and

648
00:10:55,900 --> 00:10:57,470
 

649
00:10:55,910 --> 00:11:00,770
 so actually the

650
00:10:57,460 --> 00:11:00,770
 

651
00:10:57,470 --> 00:11:02,630
 best unsupervised or the biggest

652
00:11:00,760 --> 00:11:02,630
 

653
00:11:00,770 --> 00:11:04,820
 unsupervised models for image that I'm

654
00:11:02,620 --> 00:11:04,820
 

655
00:11:02,630 --> 00:11:07,760
 told are now starting to overfit at 32

656
00:11:04,810 --> 00:11:07,760
 

657
00:11:04,820 --> 00:11:08,990
 by 32 resolution on the training set but

658
00:11:07,750 --> 00:11:08,990
 

659
00:11:07,760 --> 00:11:10,400
 you know they're not memorizing it

660
00:11:08,980 --> 00:11:10,400
 

661
00:11:08,990 --> 00:11:12,470
 they're just overfitting someone anyway

662
00:11:10,390 --> 00:11:12,470
 

663
00:11:10,400 --> 00:11:14,630
 this is just to sort of keep this in

664
00:11:12,460 --> 00:11:14,630
 

665
00:11:12,470 --> 00:11:17,150
 mind in terms of like this this the the

666
00:11:14,620 --> 00:11:17,150
 

667
00:11:14,630 --> 00:11:19,130
 the orders of magnitude the degrees of

668
00:11:17,140 --> 00:11:19,130
 

669
00:11:17,150 --> 00:11:21,680
 disparity between the amounts of

670
00:11:19,120 --> 00:11:21,680
 

671
00:11:19,130 --> 00:11:26,060
 information that are available in data

672
00:11:21,670 --> 00:11:26,060
 

673
00:11:21,680 --> 00:11:28,340
 versus training signals or labels okay

674
00:11:26,050 --> 00:11:28,340
 

675
00:11:26,060 --> 00:11:29,870
 so you know one slide what is supervised

676
00:11:28,330 --> 00:11:29,870
 

677
00:11:28,340 --> 00:11:31,340
 learning in one slide well it's really

678
00:11:29,860 --> 00:11:31,340
 

679
00:11:29,870 --> 00:11:33,140
 pretty straightforward honestly you know

680
00:11:31,330 --> 00:11:33,140
 

681
00:11:31,340 --> 00:11:34,970
 you have a data set of inputs X and you

682
00:11:33,130 --> 00:11:34,970
 

683
00:11:33,140 --> 00:11:37,040
 label them the targets Y and your task

684
00:11:34,960 --> 00:11:37,040
 

685
00:11:34,970 --> 00:11:38,210
 is predict Y given X and you know

686
00:11:37,030 --> 00:11:38,210
 

687
00:11:37,040 --> 00:11:40,430
 typically you're just doing this at

688
00:11:38,200 --> 00:11:40,430
 

689
00:11:38,210 --> 00:11:42,500
 maximum likelihood so you have a loss

690
00:11:40,420 --> 00:11:42,500
 

691
00:11:40,430 --> 00:11:44,090
 function that is the sum over these XY

692
00:11:42,490 --> 00:11:44,090
 

693
00:11:42,500 --> 00:11:46,850
 pairs in the data set of the negative

694
00:11:44,080 --> 00:11:46,850
 

695
00:11:44,090 --> 00:11:49,090
 log probability of the targets given the

696
00:11:46,840 --> 00:11:49,090
 

697
00:11:46,850 --> 00:11:51,350
 inputs and this is really you know

698
00:11:49,080 --> 00:11:51,350
 

699
00:11:49,090 --> 00:11:54,920
 really this is still the engine that's

700
00:11:51,340 --> 00:11:54,920
 

701
00:11:51,350 --> 00:11:56,630
 that's powering an awful lot of deep

702
00:11:54,910 --> 00:11:56,630
 

703
00:11:54,920 --> 00:11:58,520
 learning right you know this a lot of

704
00:11:56,620 --> 00:11:58,520
 

705
00:11:56,630 --> 00:12:00,220
 the headline results you know going back

706
00:11:58,510 --> 00:12:00,220
 

707
00:11:58,520 --> 00:12:01,970
 to sort of the the image net

708
00:12:00,210 --> 00:12:01,970
 

709
00:12:00,220 --> 00:12:03,770
 classification results and the speech

710
00:12:01,960 --> 00:12:03,770
 

711
00:12:01,970 --> 00:12:05,210
 recognition results and machine

712
00:12:03,760 --> 00:12:05,210
 

713
00:12:03,770 --> 00:12:08,750
 translation and things like that they're

714
00:12:05,200 --> 00:12:08,750
 

715
00:12:05,210 --> 00:12:10,100
 mostly you know so obviously there's a

716
00:12:08,740 --> 00:12:10,100
 

717
00:12:08,750 --> 00:12:11,570
 lot more to it than that there's there's

718
00:12:10,090 --> 00:12:11,570
 

719
00:12:10,100 --> 00:12:13,160
 the the architecture of the network

720
00:12:11,560 --> 00:12:13,160
 

721
00:12:11,570 --> 00:12:14,390
 there's the way you optimize Network and

722
00:12:13,150 --> 00:12:14,390
 

723
00:12:13,160 --> 00:12:15,590
 things like that but in terms of the

724
00:12:14,380 --> 00:12:15,590
 

725
00:12:14,390 --> 00:12:17,270
 actual loss function that you're running

726
00:12:15,580 --> 00:12:17,270
 

727
00:12:15,590 --> 00:12:22,880
 it's it's very simple it's an you know

728
00:12:17,260 --> 00:12:22,880
 

729
00:12:17,270 --> 00:12:23,990
 ancient statistical technique so then

730
00:12:22,870 --> 00:12:23,990
 

731
00:12:22,880 --> 00:12:25,580
 the question is well what happens when

732
00:12:23,980 --> 00:12:25,580
 

733
00:12:23,990 --> 00:12:27,560
 we take the targets away so now our data

734
00:12:25,570 --> 00:12:27,560
 

735
00:12:25,580 --> 00:12:30,200
 set just consists of a lot of X's and

736
00:12:27,550 --> 00:12:30,200
 

737
00:12:27,560 --> 00:12:31,910
 the Y's what's the loss what's the loss

738
00:12:30,190 --> 00:12:31,910
 

739
00:12:30,200 --> 00:12:33,650
 B and of course this is the fundamental

740
00:12:31,900 --> 00:12:33,650
 

741
00:12:31,910 --> 00:12:35,810
 challenge of unsupervised learning right

742
00:12:33,640 --> 00:12:35,810
 

743
00:12:33,650 --> 00:12:38,120
 the task is undefined by definition we

744
00:12:35,800 --> 00:12:38,120
 

745
00:12:35,810 --> 00:12:39,950
 don't have a task anymore and that's

746
00:12:38,110 --> 00:12:39,950
 

747
00:12:38,120 --> 00:12:41,330
 exactly what makes unsupervised learning

748
00:12:39,940 --> 00:12:41,330
 

749
00:12:39,950 --> 00:12:43,010
 hard but it's also what makes it

750
00:12:41,320 --> 00:12:43,010
 

751
00:12:41,330 --> 00:12:45,020
 interesting because we're not just

752
00:12:43,000 --> 00:12:45,020
 

753
00:12:43,010 --> 00:12:47,270
 optimizing some predefined task so we

754
00:12:45,010 --> 00:12:47,270
 

755
00:12:45,020 --> 00:12:49,280
 have to kind of come up with something

756
00:12:47,260 --> 00:12:49,280
 

757
00:12:47,270 --> 00:12:51,560
 that somehow indirectly gives us what we

758
00:12:49,270 --> 00:12:51,560
 

759
00:12:49,280 --> 00:12:53,750
 think we want from from from the system

760
00:12:51,550 --> 00:12:53,750
 

761
00:12:51,560 --> 00:12:55,820
 and the problem is that even if we do

762
00:12:53,740 --> 00:12:55,820
 

763
00:12:53,750 --> 00:12:57,290
 that so let's say we could we could say

764
00:12:55,810 --> 00:12:57,290
 

765
00:12:55,820 --> 00:12:59,090
 in vague terms or what we really want

766
00:12:57,280 --> 00:12:59,090
 

767
00:12:57,290 --> 00:13:00,470
 from this thing is a single task that

768
00:12:59,080 --> 00:13:00,470
 

769
00:12:59,090 --> 00:13:01,970
 would allow the network to generalize

770
00:13:00,460 --> 00:13:01,970
 

771
00:13:00,470 --> 00:13:03,740
 the many other tasks you want to learn

772
00:13:01,960 --> 00:13:03,740
 

773
00:13:01,970 --> 00:13:06,050
 whatever it is that will allow you to

774
00:13:03,730 --> 00:13:06,050
 

775
00:13:03,740 --> 00:13:07,370
 rapidly generalize to whatever life is

776
00:13:06,040 --> 00:13:07,370
 

777
00:13:06,050 --> 00:13:09,200
 going to throw at you in the future but

778
00:13:07,360 --> 00:13:09,200
 

779
00:13:07,370 --> 00:13:09,390
 then the question is well which tasks

780
00:13:09,190 --> 00:13:09,390
 

781
00:13:09,200 --> 00:13:10,470
 are you

782
00:13:09,380 --> 00:13:10,470
 

783
00:13:09,390 --> 00:13:12,810
 talking about right if you if you

784
00:13:10,460 --> 00:13:12,810
 

785
00:13:10,470 --> 00:13:14,250
 predefined a set of tests then actually

786
00:13:12,800 --> 00:13:14,250
 

787
00:13:12,810 --> 00:13:15,480
 you're just doing supervised learning or

788
00:13:14,240 --> 00:13:15,480
 

789
00:13:14,250 --> 00:13:17,130
 reinforcement learning again you're just

790
00:13:15,470 --> 00:13:17,130
 

791
00:13:15,480 --> 00:13:19,860
 doing it at one step removed you're kind

792
00:13:17,120 --> 00:13:19,860
 

793
00:13:17,130 --> 00:13:21,270
 of saying optimize this thing so as to

794
00:13:19,850 --> 00:13:21,270
 

795
00:13:19,860 --> 00:13:24,570
 get the best possible score on that

796
00:13:21,260 --> 00:13:24,570
 

797
00:13:21,270 --> 00:13:25,950
 thing you know that set of tasks and I'm

798
00:13:24,560 --> 00:13:25,950
 

799
00:13:24,570 --> 00:13:27,180
 not convinced that that will ever give

800
00:13:25,940 --> 00:13:27,180
 

801
00:13:25,950 --> 00:13:28,830
 us this kind of generalization

802
00:13:27,170 --> 00:13:28,830
 

803
00:13:27,180 --> 00:13:30,330
 generalization we want either because

804
00:13:28,820 --> 00:13:30,330
 

805
00:13:28,830 --> 00:13:31,470
 there it really has to be able to

806
00:13:30,320 --> 00:13:31,470
 

807
00:13:30,330 --> 00:13:33,240
 generalize there's something that you

808
00:13:31,460 --> 00:13:33,240
 

809
00:13:31,470 --> 00:13:36,180
 didn't predict at the time when you made

810
00:13:33,230 --> 00:13:36,180
 

811
00:13:33,240 --> 00:13:38,640
 the model okay well the simplest thing

812
00:13:36,170 --> 00:13:38,640
 

813
00:13:36,180 --> 00:13:39,750
 you can do arguably is just basically do

814
00:13:38,630 --> 00:13:39,750
 

815
00:13:38,640 --> 00:13:41,820
 the same thing as you were doing before

816
00:13:39,740 --> 00:13:41,820
 

817
00:13:39,750 --> 00:13:43,230
 only without the target so now you're

818
00:13:41,810 --> 00:13:43,230
 

819
00:13:41,820 --> 00:13:44,850
 doing maximum likelihood on the data

820
00:13:43,220 --> 00:13:44,850
 

821
00:13:43,230 --> 00:13:48,320
 instead of the target so this is just

822
00:13:44,840 --> 00:13:48,320
 

823
00:13:44,850 --> 00:13:50,430
 good old-fashioned density modeling and

824
00:13:48,310 --> 00:13:50,430
 

825
00:13:48,320 --> 00:13:51,840
 you know what's the goal of density

826
00:13:50,420 --> 00:13:51,840
 

827
00:13:50,430 --> 00:13:53,070
 modeling well fundamentally what you're

828
00:13:51,830 --> 00:13:53,070
 

829
00:13:51,840 --> 00:13:54,630
 trying to do is learn the true

830
00:13:53,060 --> 00:13:54,630
 

831
00:13:53,070 --> 00:13:55,920
 distribution from which the data was

832
00:13:54,620 --> 00:13:55,920
 

833
00:13:54,630 --> 00:13:57,300
 drawn and I've got true and kind of

834
00:13:55,910 --> 00:13:57,300
 

835
00:13:55,920 --> 00:13:59,250
 scare quotes here because there's always

836
00:13:57,290 --> 00:13:59,250
 

837
00:13:57,300 --> 00:14:00,750
 this question of well all you actually

838
00:13:59,240 --> 00:14:00,750
 

839
00:13:59,250 --> 00:14:02,040
 have is a data set is it really

840
00:14:00,740 --> 00:14:02,040
 

841
00:14:00,750 --> 00:14:03,690
 meaningful to talk about a true

842
00:14:02,030 --> 00:14:03,690
 

843
00:14:02,040 --> 00:14:05,490
 distribution or should you just try to

844
00:14:03,680 --> 00:14:05,490
 

845
00:14:03,690 --> 00:14:07,200
 learn that data set as well as possible

846
00:14:05,480 --> 00:14:07,200
 

847
00:14:05,490 --> 00:14:08,520
 and I there's an endless sort of

848
00:14:07,190 --> 00:14:08,520
 

849
00:14:07,200 --> 00:14:10,470
 philosophical rabbit hole that I don't

850
00:14:08,510 --> 00:14:10,470
 

851
00:14:08,520 --> 00:14:12,720
 want to get into there but the basic

852
00:14:10,460 --> 00:14:12,720
 

853
00:14:10,470 --> 00:14:14,640
 idea is you're training the system to

854
00:14:12,710 --> 00:14:14,640
 

855
00:14:12,720 --> 00:14:16,200
 try to learn as much as it can about the

856
00:14:14,630 --> 00:14:16,200
 

857
00:14:14,640 --> 00:14:17,790
 data learn everything everything you

858
00:14:16,190 --> 00:14:17,790
 

859
00:14:16,200 --> 00:14:20,270
 every patter and every regularity you

860
00:14:17,780 --> 00:14:20,270
 

861
00:14:17,790 --> 00:14:23,970
 find in the data should in some way

862
00:14:20,260 --> 00:14:23,970
 

863
00:14:20,270 --> 00:14:26,580
 allow you to reduce this this these

864
00:14:23,960 --> 00:14:26,580
 

865
00:14:23,970 --> 00:14:30,870
 negative this log loss right to improve

866
00:14:26,570 --> 00:14:30,870
 

867
00:14:26,580 --> 00:14:32,520
 these log probabilities well I think

868
00:14:30,860 --> 00:14:32,520
 

869
00:14:30,870 --> 00:14:36,150
 before we even get into any of the

870
00:14:32,510 --> 00:14:36,150
 

871
00:14:32,520 --> 00:14:37,560
 details of that we should so and before

872
00:14:36,140 --> 00:14:37,560
 

873
00:14:36,150 --> 00:14:38,970
 yeah before I go any further and one

874
00:14:37,550 --> 00:14:38,970
 

875
00:14:37,560 --> 00:14:40,470
 thing that I want everyone to take away

876
00:14:38,960 --> 00:14:40,470
 

877
00:14:38,970 --> 00:14:42,900
 from this tutorial is that there are no

878
00:14:40,460 --> 00:14:42,900
 

879
00:14:40,470 --> 00:14:44,580
 clear answers about how to do

880
00:14:42,890 --> 00:14:44,580
 

881
00:14:42,900 --> 00:14:46,200
 unsupervised learning right as I said

882
00:14:44,570 --> 00:14:46,200
 

883
00:14:44,580 --> 00:14:47,610
 that's what makes it difficult but also

884
00:14:46,190 --> 00:14:47,610
 

885
00:14:46,200 --> 00:14:50,480
 makes it interesting and you're here

886
00:14:47,600 --> 00:14:50,480
 

887
00:14:47,610 --> 00:14:52,530
 you're going to hear a lot of different

888
00:14:50,470 --> 00:14:52,530
 

889
00:14:50,480 --> 00:14:54,390
 models and different approaches already

890
00:14:52,520 --> 00:14:54,390
 

891
00:14:52,530 --> 00:14:56,040
 from me and Marco aurélio and you'd hear

892
00:14:54,380 --> 00:14:56,040
 

893
00:14:54,390 --> 00:14:57,300
 a lot more if you can uphold people in

894
00:14:56,030 --> 00:14:57,300
 

895
00:14:56,040 --> 00:14:58,710
 the field in general so there really

896
00:14:57,290 --> 00:14:58,710
 

897
00:14:57,300 --> 00:15:00,660
 isn't the consensus and what the best

898
00:14:58,700 --> 00:15:00,660
 

899
00:14:58,710 --> 00:15:02,820
 thing to do is but one thing that you'll

900
00:15:00,650 --> 00:15:02,820
 

901
00:15:00,660 --> 00:15:04,260
 often hear is that well we shouldn't try

902
00:15:02,810 --> 00:15:04,260
 

903
00:15:02,820 --> 00:15:05,610
 to understand everything at once that

904
00:15:04,250 --> 00:15:05,610
 

905
00:15:04,260 --> 00:15:07,650
 doesn't make sense we should focus on

906
00:15:05,600 --> 00:15:07,650
 

907
00:15:05,610 --> 00:15:09,450
 things that we have some idea or some

908
00:15:07,640 --> 00:15:09,450
 

909
00:15:07,650 --> 00:15:11,280
 belief will one day be useful for us and

910
00:15:09,440 --> 00:15:11,280
 

911
00:15:09,450 --> 00:15:14,460
 I was reading this book a death in the

912
00:15:11,270 --> 00:15:14,460
 

913
00:15:11,280 --> 00:15:17,070
 family by karlovy now scored I think you

914
00:15:14,450 --> 00:15:17,070
 

915
00:15:14,460 --> 00:15:18,150
 pronounce it and I won't read out the

916
00:15:17,060 --> 00:15:18,150
 

917
00:15:17,070 --> 00:15:22,580
 code it's kind of long you can look

918
00:15:18,140 --> 00:15:22,580
 

919
00:15:18,150 --> 00:15:22,580
 through it but the basic idea was

920
00:15:24,590 --> 00:15:24,590
 

921
00:15:24,600 --> 00:15:28,900
 the narrator was walking through the

922
00:15:27,450 --> 00:15:28,900
 

923
00:15:27,460 --> 00:15:31,420
 streets of Stockholm and he sees this

924
00:15:28,890 --> 00:15:31,420
 

925
00:15:28,900 --> 00:15:32,770
 you know very striking cloud formation I

926
00:15:31,410 --> 00:15:32,770
 

927
00:15:31,420 --> 00:15:34,420
 think maybe it's a sunset or something

928
00:15:32,760 --> 00:15:34,420
 

929
00:15:32,770 --> 00:15:36,100
 like that and the clouds are colored and

930
00:15:34,410 --> 00:15:36,100
 

931
00:15:34,420 --> 00:15:38,200
 they're beautiful and he looks up and he

932
00:15:36,090 --> 00:15:38,200
 

933
00:15:36,100 --> 00:15:39,820
 realizes that no one else is looking at

934
00:15:38,190 --> 00:15:39,820
 

935
00:15:38,200 --> 00:15:41,110
 the clouds and he wonders why does no

936
00:15:39,810 --> 00:15:41,110
 

937
00:15:39,820 --> 00:15:43,660
 one else pay any attention to this this

938
00:15:41,100 --> 00:15:43,660
 

939
00:15:41,110 --> 00:15:45,460
 is some you know beautiful like an

940
00:15:43,650 --> 00:15:45,460
 

941
00:15:43,660 --> 00:15:46,810
 intricate pattern that you're never

942
00:15:45,450 --> 00:15:46,810
 

943
00:15:45,460 --> 00:15:49,330
 gonna see again in your life and yet no

944
00:15:46,800 --> 00:15:49,330
 

945
00:15:46,810 --> 00:15:50,500
 one's looking at it and obviously the

946
00:15:49,320 --> 00:15:50,500
 

947
00:15:49,330 --> 00:15:51,730
 reason is we're not looking at it

948
00:15:50,490 --> 00:15:51,730
 

949
00:15:50,500 --> 00:15:53,140
 because we don't need to write other

950
00:15:51,720 --> 00:15:53,140
 

951
00:15:51,730 --> 00:15:54,580
 than maybe gauging you know whether or

952
00:15:53,130 --> 00:15:54,580
 

953
00:15:53,140 --> 00:15:56,350
 not it's likely to rain or something

954
00:15:54,570 --> 00:15:56,350
 

955
00:15:54,580 --> 00:15:57,940
 like that we don't really need to look

956
00:15:56,340 --> 00:15:57,940
 

957
00:15:56,350 --> 00:16:01,300
 at the clouds they they're an

958
00:15:57,930 --> 00:16:01,300
 

959
00:15:57,940 --> 00:16:03,640
 inexhaustible store of structure in some

960
00:16:01,290 --> 00:16:03,640
 

961
00:16:01,300 --> 00:16:05,050
 sense of patterns of information but

962
00:16:03,630 --> 00:16:05,050
 

963
00:16:03,640 --> 00:16:06,370
 they're not patterns we can we can

964
00:16:05,040 --> 00:16:06,370
 

965
00:16:05,050 --> 00:16:07,600
 pretty safely say they're not patterns

966
00:16:06,360 --> 00:16:07,600
 

967
00:16:06,370 --> 00:16:08,560
 that are going to be useful for us and

968
00:16:07,590 --> 00:16:08,560
 

969
00:16:07,600 --> 00:16:11,080
 that's maybe your professional

970
00:16:08,550 --> 00:16:11,080
 

971
00:16:08,560 --> 00:16:13,750
 meteorologists and so that's a kind of

972
00:16:11,070 --> 00:16:13,750
 

973
00:16:11,080 --> 00:16:15,190
 ongoing conundrum here is like given

974
00:16:13,740 --> 00:16:15,190
 

975
00:16:13,750 --> 00:16:17,560
 that you can't really hope to learn

976
00:16:15,180 --> 00:16:17,560
 

977
00:16:15,190 --> 00:16:18,820
 everything shouldn't you be a little bit

978
00:16:17,550 --> 00:16:18,820
 

979
00:16:17,560 --> 00:16:21,400
 more focused about what it is that

980
00:16:18,810 --> 00:16:21,400
 

981
00:16:18,820 --> 00:16:22,690
 you're trying to learn ok and actually

982
00:16:21,390 --> 00:16:22,690
 

983
00:16:21,400 --> 00:16:25,089
 I'm gonna kind of repeat that again on

984
00:16:22,680 --> 00:16:25,089
 

985
00:16:22,690 --> 00:16:26,920
 this slide you know when you say oh

986
00:16:25,079 --> 00:16:26,920
 

987
00:16:25,089 --> 00:16:28,540
 let's you know let's do unsupervised

988
00:16:26,910 --> 00:16:28,540
 

989
00:16:26,920 --> 00:16:30,910
 learning just by by density modeling

990
00:16:28,530 --> 00:16:30,910
 

991
00:16:28,540 --> 00:16:32,170
 let's just model everything well one

992
00:16:30,900 --> 00:16:32,170
 

993
00:16:30,910 --> 00:16:34,000
 thing is kind of obvious as density

994
00:16:32,160 --> 00:16:34,000
 

995
00:16:32,170 --> 00:16:35,560
 modeling is hard so you go from one

996
00:16:33,990 --> 00:16:35,560
 

997
00:16:34,000 --> 00:16:37,209
 extreme to another you didn't have back

998
00:16:35,550 --> 00:16:37,209
 

999
00:16:35,560 --> 00:16:38,560
 in the reinforcement learning supervised

1000
00:16:37,199 --> 00:16:38,560
 

1001
00:16:37,209 --> 00:16:40,450
 setting you didn't have enough bits to

1002
00:16:38,550 --> 00:16:40,450
 

1003
00:16:38,560 --> 00:16:41,890
 learn from now you end up with too many

1004
00:16:40,440 --> 00:16:41,890
 

1005
00:16:40,450 --> 00:16:43,510
 just in terms of you know if you're

1006
00:16:41,880 --> 00:16:43,510
 

1007
00:16:41,890 --> 00:16:45,490
 talking about you know high-definition

1008
00:16:43,500 --> 00:16:45,490
 

1009
00:16:43,510 --> 00:16:46,570
 video running at 60 or whatever it is 30

1010
00:16:45,480 --> 00:16:46,570
 

1011
00:16:45,490 --> 00:16:48,760
 frames a second there's too much

1012
00:16:46,560 --> 00:16:48,760
 

1013
00:16:46,570 --> 00:16:50,050
 information flowing into the system even

1014
00:16:48,750 --> 00:16:50,050
 

1015
00:16:48,760 --> 00:16:51,630
 with today's computers it's very

1016
00:16:50,040 --> 00:16:51,630
 

1017
00:16:50,050 --> 00:16:54,220
 difficult to model all of that at once

1018
00:16:51,620 --> 00:16:54,220
 

1019
00:16:51,630 --> 00:16:55,900
 and the second problem too which I would

1020
00:16:54,210 --> 00:16:55,900
 

1021
00:16:54,220 --> 00:16:57,459
 argue that there we have a little bit

1022
00:16:55,890 --> 00:16:57,459
 

1023
00:16:55,900 --> 00:16:58,930
 more of a solution is you know you have

1024
00:16:57,449 --> 00:16:58,930
 

1025
00:16:57,459 --> 00:17:00,160
 to deal with complex interactions

1026
00:16:58,920 --> 00:17:00,160
 

1027
00:16:58,930 --> 00:17:02,200
 between these variables is a kind of

1028
00:17:00,150 --> 00:17:02,200
 

1029
00:17:00,160 --> 00:17:04,420
 curse of dimensionality these are very

1030
00:17:02,190 --> 00:17:04,420
 

1031
00:17:02,200 --> 00:17:08,439
 high dimensional very complex and

1032
00:17:04,410 --> 00:17:08,439
 

1033
00:17:04,420 --> 00:17:10,209
 difficult to model data the second

1034
00:17:08,429 --> 00:17:10,209
 

1035
00:17:08,439 --> 00:17:11,470
 problem is actually more fundamental and

1036
00:17:10,199 --> 00:17:11,470
 

1037
00:17:10,209 --> 00:17:13,900
 it's basically a restatement of what I

1038
00:17:11,460 --> 00:17:13,900
 

1039
00:17:11,470 --> 00:17:15,160
 just said in the last slide you

1040
00:17:13,890 --> 00:17:15,160
 

1041
00:17:13,900 --> 00:17:17,050
 something you'll often hear people say

1042
00:17:15,150 --> 00:17:17,050
 

1043
00:17:15,160 --> 00:17:18,760
 not all bits are created equal this is a

1044
00:17:17,040 --> 00:17:18,760
 

1045
00:17:17,050 --> 00:17:21,550
 discussion like a here over and over

1046
00:17:18,750 --> 00:17:21,550
 

1047
00:17:18,760 --> 00:17:22,720
 again among my colleagues basically the

1048
00:17:21,540 --> 00:17:22,720
 

1049
00:17:21,550 --> 00:17:24,280
 log likely if all you're doing is

1050
00:17:22,710 --> 00:17:24,280
 

1051
00:17:22,720 --> 00:17:25,720
 optimizing log likelihoods they depend

1052
00:17:24,270 --> 00:17:25,720
 

1053
00:17:24,280 --> 00:17:27,520
 much more on these really kind of

1054
00:17:25,710 --> 00:17:27,520
 

1055
00:17:25,720 --> 00:17:29,440
 low-level insignificant details things

1056
00:17:27,510 --> 00:17:29,440
 

1057
00:17:27,520 --> 00:17:32,760
 like exact correlations between

1058
00:17:29,430 --> 00:17:32,760
 

1059
00:17:29,440 --> 00:17:34,260
 neighboring pixels word level engrams

1060
00:17:32,750 --> 00:17:34,260
 

1061
00:17:32,760 --> 00:17:36,710
 and might only be you know the last two

1062
00:17:34,250 --> 00:17:36,710
 

1063
00:17:34,260 --> 00:17:39,660
 or three words but getting those

1064
00:17:36,700 --> 00:17:39,660
 

1065
00:17:36,710 --> 00:17:41,340
 statistics very very precise and those

1066
00:17:39,650 --> 00:17:41,340
 

1067
00:17:39,660 --> 00:17:43,320
 aren't really the things that we want to

1068
00:17:41,330 --> 00:17:43,320
 

1069
00:17:41,340 --> 00:17:44,640
 learn about intuitively right rather we

1070
00:17:43,310 --> 00:17:44,640
 

1071
00:17:43,320 --> 00:17:46,110
 want to learn about high-level structure

1072
00:17:44,630 --> 00:17:46,110
 

1073
00:17:44,640 --> 00:17:47,520
 we want to know what are the contents of

1074
00:17:46,100 --> 00:17:47,520
 

1075
00:17:46,110 --> 00:17:49,830
 the image was it a cat or a dog

1076
00:17:47,510 --> 00:17:49,830
 

1077
00:17:47,520 --> 00:17:53,040
 rather than precisely what value of

1078
00:17:49,820 --> 00:17:53,040
 

1079
00:17:49,830 --> 00:17:54,570
 color each pixel was likely to be we

1080
00:17:53,030 --> 00:17:54,570
 

1081
00:17:53,040 --> 00:17:56,310
 want to know about the semantics of the

1082
00:17:54,560 --> 00:17:56,310
 

1083
00:17:54,570 --> 00:17:58,410
 language rather than you know precise

1084
00:17:56,300 --> 00:17:58,410
 

1085
00:17:56,310 --> 00:18:00,900
 transition probabilities between between

1086
00:17:58,400 --> 00:18:00,900
 

1087
00:17:58,410 --> 00:18:02,790
 engrams and there's another problem

1088
00:18:00,890 --> 00:18:02,790
 

1089
00:18:00,900 --> 00:18:04,440
 which is a more general problem a

1090
00:18:02,780 --> 00:18:04,440
 

1091
00:18:02,790 --> 00:18:07,290
 problem for real unsupervised learning

1092
00:18:04,430 --> 00:18:07,290
 

1093
00:18:04,440 --> 00:18:08,970
 and you know more broadly which even

1094
00:18:07,280 --> 00:18:08,970
 

1095
00:18:07,290 --> 00:18:10,500
 which is even if you do have in some

1096
00:18:08,960 --> 00:18:10,500
 

1097
00:18:08,970 --> 00:18:12,240
 sense the right loss function and it

1098
00:18:10,490 --> 00:18:12,240
 

1099
00:18:10,500 --> 00:18:14,640
 gives you the right underlying structure

1100
00:18:12,230 --> 00:18:14,640
 

1101
00:18:12,240 --> 00:18:17,190
 it's not always clear how to access that

1102
00:18:14,630 --> 00:18:17,190
 

1103
00:18:14,640 --> 00:18:18,600
 that underlying structure for future

1104
00:18:17,180 --> 00:18:18,600
 

1105
00:18:17,190 --> 00:18:20,490
 tasks and this is really where the issue

1106
00:18:18,590 --> 00:18:20,490
 

1107
00:18:18,600 --> 00:18:22,470
 of representation learning comes in so

1108
00:18:20,480 --> 00:18:22,470
 

1109
00:18:20,490 --> 00:18:24,510
 you know in order to solve these kinds

1110
00:18:22,460 --> 00:18:24,510
 

1111
00:18:22,470 --> 00:18:28,430
 of problems neural networks have to

1112
00:18:24,500 --> 00:18:28,430
 

1113
00:18:24,510 --> 00:18:30,500
 learn interesting and you know and

1114
00:18:28,420 --> 00:18:30,500
 

1115
00:18:28,430 --> 00:18:32,790
 essentially what should be useful

1116
00:18:30,490 --> 00:18:32,790
 

1117
00:18:30,500 --> 00:18:34,500
 representations but they tend to be kind

1118
00:18:32,780 --> 00:18:34,500
 

1119
00:18:32,790 --> 00:18:35,880
 of black boxes and it's difficult to

1120
00:18:34,490 --> 00:18:35,880
 

1121
00:18:34,500 --> 00:18:38,460
 access those representations and I'll

1122
00:18:35,870 --> 00:18:38,460
 

1123
00:18:35,880 --> 00:18:40,950
 come back to this later in the talk okay

1124
00:18:38,450 --> 00:18:40,950
 

1125
00:18:38,460 --> 00:18:42,330
 so one nice thing about densities is

1126
00:18:40,940 --> 00:18:42,330
 

1127
00:18:40,950 --> 00:18:44,040
 that you kind of get a generative model

1128
00:18:42,320 --> 00:18:44,040
 

1129
00:18:42,330 --> 00:18:46,230
 for free so if you if you've learned how

1130
00:18:44,030 --> 00:18:46,230
 

1131
00:18:44,040 --> 00:18:48,120
 to do you know model the data this

1132
00:18:46,220 --> 00:18:48,120
 

1133
00:18:46,230 --> 00:18:49,530
 probability distribution of the data as

1134
00:18:48,110 --> 00:18:49,530
 

1135
00:18:48,120 --> 00:18:51,540
 long as you can draw samples from that

1136
00:18:49,520 --> 00:18:51,540
 

1137
00:18:49,530 --> 00:18:54,540
 distribution than you have by definition

1138
00:18:51,530 --> 00:18:54,540
 

1139
00:18:51,540 --> 00:18:57,240
 a generative model and generative models

1140
00:18:54,530 --> 00:18:57,240
 

1141
00:18:54,540 --> 00:18:59,460
 are nice in a lot of ways I think you

1142
00:18:57,230 --> 00:18:59,460
 

1143
00:18:57,240 --> 00:19:00,510
 know fundamentally for me the most

1144
00:18:59,450 --> 00:19:00,510
 

1145
00:18:59,460 --> 00:19:03,300
 important thing is that they allow you

1146
00:19:00,500 --> 00:19:03,300
 

1147
00:19:00,510 --> 00:19:04,770
 to to see what the model has and hasn't

1148
00:19:03,290 --> 00:19:04,770
 

1149
00:19:03,300 --> 00:19:06,750
 learned right you can generate from the

1150
00:19:04,760 --> 00:19:06,750
 

1151
00:19:04,770 --> 00:19:08,700
 distribution so log probabilities

1152
00:19:06,740 --> 00:19:08,700
 

1153
00:19:06,750 --> 00:19:10,290
 metrics of any sort will only tell you

1154
00:19:08,690 --> 00:19:10,290
 

1155
00:19:08,700 --> 00:19:12,390
 so much about how good your model is

1156
00:19:10,280 --> 00:19:12,390
 

1157
00:19:10,290 --> 00:19:14,130
 when you generate from it then that's

1158
00:19:12,380 --> 00:19:14,130
 

1159
00:19:12,390 --> 00:19:15,600
 when you see what kinds of patterns it

1160
00:19:14,120 --> 00:19:15,600
 

1161
00:19:14,130 --> 00:19:17,040
 is actually learned and what kinds of

1162
00:19:15,590 --> 00:19:17,040
 

1163
00:19:15,600 --> 00:19:18,950
 structure it is actually managed to

1164
00:19:17,030 --> 00:19:18,950
 

1165
00:19:17,040 --> 00:19:21,210
 embed so this is kind of like a

1166
00:19:18,940 --> 00:19:21,210
 

1167
00:19:18,950 --> 00:19:22,500
 quantification of Richard Feynman's

1168
00:19:21,200 --> 00:19:22,500
 

1169
00:19:21,210 --> 00:19:24,240
 famous quote you know what I cannot

1170
00:19:22,490 --> 00:19:24,240
 

1171
00:19:22,500 --> 00:19:25,890
 create I do not understand so we're

1172
00:19:24,230 --> 00:19:25,890
 

1173
00:19:24,240 --> 00:19:27,450
 going down this path of right we're

1174
00:19:25,880 --> 00:19:27,450
 

1175
00:19:25,890 --> 00:19:29,520
 building creative models models that

1176
00:19:27,440 --> 00:19:29,520
 

1177
00:19:27,450 --> 00:19:31,320
 have to be able to actually create the

1178
00:19:29,510 --> 00:19:31,320
 

1179
00:19:29,520 --> 00:19:33,870
 data to prove that they've understood it

1180
00:19:31,310 --> 00:19:33,870
 

1181
00:19:31,320 --> 00:19:35,520
 and another use for these things is if

1182
00:19:33,860 --> 00:19:35,520
 

1183
00:19:33,870 --> 00:19:37,470
 you've got a generative model you can

1184
00:19:35,510 --> 00:19:37,470
 

1185
00:19:35,520 --> 00:19:39,030
 use it for example to imagine possible

1186
00:19:37,460 --> 00:19:39,030
 

1187
00:19:37,470 --> 00:19:40,650
 scenarios and that's been something that

1188
00:19:39,020 --> 00:19:40,650
 

1189
00:19:39,030 --> 00:19:42,169
 people have explored in the context of

1190
00:19:40,640 --> 00:19:42,169
 

1191
00:19:40,650 --> 00:19:44,029
 model-based are

1192
00:19:42,159 --> 00:19:44,029
 

1193
00:19:42,169 --> 00:19:46,369
 I'm not going to talk further about that

1194
00:19:44,019 --> 00:19:46,369
 

1195
00:19:44,029 --> 00:19:49,070
 just now okay so then the question is

1196
00:19:46,359 --> 00:19:49,070
 

1197
00:19:46,369 --> 00:19:52,220
 like if we if you you know if you're on

1198
00:19:49,060 --> 00:19:52,220
 

1199
00:19:49,070 --> 00:19:54,200
 board doing density modeling is a good

1200
00:19:52,210 --> 00:19:54,200
 

1201
00:19:52,220 --> 00:19:55,820
 idea and that's a big if already then

1202
00:19:54,190 --> 00:19:55,820
 

1203
00:19:54,200 --> 00:19:58,519
 the next question is well what models

1204
00:19:55,810 --> 00:19:58,519
 

1205
00:19:55,820 --> 00:19:59,989
 should we use to do density modeling and

1206
00:19:58,509 --> 00:19:59,989
 

1207
00:19:58,519 --> 00:20:01,190
 what I'm gonna talk about now is auto

1208
00:19:59,979 --> 00:20:01,190
 

1209
00:19:59,989 --> 00:20:04,039
 regressive models which are a very

1210
00:20:01,180 --> 00:20:04,039
 

1211
00:20:01,190 --> 00:20:06,440
 simple very powerful class of models

1212
00:20:04,029 --> 00:20:06,440
 

1213
00:20:04,039 --> 00:20:09,350
 that I think do a very good job of

1214
00:20:06,430 --> 00:20:09,350
 

1215
00:20:06,440 --> 00:20:12,169
 modeling a surprisingly wide range of

1216
00:20:09,340 --> 00:20:12,169
 

1217
00:20:09,350 --> 00:20:14,090
 data types okay so the one piece of math

1218
00:20:12,159 --> 00:20:14,090
 

1219
00:20:12,169 --> 00:20:16,340
 which is probably I'm sure it's familiar

1220
00:20:14,080 --> 00:20:16,340
 

1221
00:20:14,090 --> 00:20:17,629
 to you all already you know basically

1222
00:20:16,330 --> 00:20:17,629
 

1223
00:20:16,340 --> 00:20:19,789
 the only piece of math you need to know

1224
00:20:17,619 --> 00:20:19,789
 

1225
00:20:17,629 --> 00:20:21,379
 for auto regressive models is the chain

1226
00:20:19,779 --> 00:20:21,379
 

1227
00:20:19,789 --> 00:20:23,690
 rule for probabilities so essentially

1228
00:20:21,369 --> 00:20:23,690
 

1229
00:20:21,379 --> 00:20:25,700
 given any joint probability over a bunch

1230
00:20:23,680 --> 00:20:25,700
 

1231
00:20:23,690 --> 00:20:28,700
 of variables you can always decompose it

1232
00:20:25,690 --> 00:20:28,700
 

1233
00:20:25,700 --> 00:20:31,730
 as a chain of conditional probabilities

1234
00:20:28,690 --> 00:20:31,730
 

1235
00:20:28,700 --> 00:20:33,350
 over all the variables so far and what's

1236
00:20:31,720 --> 00:20:33,350
 

1237
00:20:31,730 --> 00:20:34,580
 important here so in this case here we

1238
00:20:33,340 --> 00:20:34,580
 

1239
00:20:33,350 --> 00:20:35,929
 have a language model so we have the cat

1240
00:20:34,570 --> 00:20:35,929
 

1241
00:20:34,580 --> 00:20:37,369
 sat on the mat you start off with

1242
00:20:35,919 --> 00:20:37,369
 

1243
00:20:35,929 --> 00:20:39,259
 probability of the then you have the

1244
00:20:37,359 --> 00:20:39,259
 

1245
00:20:37,369 --> 00:20:41,659
 probability of cat given the probability

1246
00:20:39,249 --> 00:20:41,659
 

1247
00:20:39,259 --> 00:20:43,759
 of Sat given the cat and so on and

1248
00:20:41,649 --> 00:20:43,759
 

1249
00:20:41,659 --> 00:20:45,409
 there's a very natural ordering here but

1250
00:20:43,749 --> 00:20:45,409
 

1251
00:20:43,759 --> 00:20:46,789
 actually the chain rule still works in

1252
00:20:45,399 --> 00:20:46,789
 

1253
00:20:45,409 --> 00:20:48,679
 whatever order you want so you could

1254
00:20:46,779 --> 00:20:48,679
 

1255
00:20:46,789 --> 00:20:51,619
 scramble this ordering and run the same

1256
00:20:48,669 --> 00:20:51,619
 

1257
00:20:48,679 --> 00:20:52,669
 thing in principle in practice and this

1258
00:20:51,609 --> 00:20:52,669
 

1259
00:20:51,619 --> 00:20:55,100
 is something I'll discuss a bit further

1260
00:20:52,659 --> 00:20:55,100
 

1261
00:20:52,669 --> 00:20:57,830
 on the ordering you choose is very

1262
00:20:55,090 --> 00:20:57,830
 

1263
00:20:55,100 --> 00:20:59,119
 important for like it that is critical

1264
00:20:57,820 --> 00:20:59,119
 

1265
00:20:57,830 --> 00:21:01,009
 to how well you're actually going to be

1266
00:20:59,109 --> 00:21:01,009
 

1267
00:20:59,119 --> 00:21:03,200
 able to learn the system it introduces a

1268
00:21:00,999 --> 00:21:03,200
 

1269
00:21:01,009 --> 00:21:04,429
 kind of inductive bias and that's a

1270
00:21:03,190 --> 00:21:04,429
 

1271
00:21:03,200 --> 00:21:07,330
 whole you know there's a lot of debate

1272
00:21:04,419 --> 00:21:07,330
 

1273
00:21:04,429 --> 00:21:10,609
 around that for auto regressive models

1274
00:21:07,320 --> 00:21:10,609
 

1275
00:21:07,330 --> 00:21:14,210
 actually like a short aside here I once

1276
00:21:10,599 --> 00:21:14,210
 

1277
00:21:10,609 --> 00:21:15,799
 trained a character level neural network

1278
00:21:14,200 --> 00:21:15,799
 

1279
00:21:14,210 --> 00:21:19,730
 language model where I allowed it to

1280
00:21:15,789 --> 00:21:19,730
 

1281
00:21:15,799 --> 00:21:21,019
 choose which which character to predict

1282
00:21:19,720 --> 00:21:21,019
 

1283
00:21:19,730 --> 00:21:22,700
 next so rather than just going from

1284
00:21:21,009 --> 00:21:22,700
 

1285
00:21:21,019 --> 00:21:24,710
 right to left left to right like usual

1286
00:21:22,690 --> 00:21:24,710
 

1287
00:21:22,700 --> 00:21:26,419
 it could choose where to look and what

1288
00:21:24,700 --> 00:21:26,419
 

1289
00:21:24,710 --> 00:21:28,369
 was interesting is that rather than

1290
00:21:26,409 --> 00:21:28,369
 

1291
00:21:26,419 --> 00:21:30,499
 doing it almost did the characters in

1292
00:21:28,359 --> 00:21:30,499
 

1293
00:21:28,369 --> 00:21:32,629
 the normal order except that it would

1294
00:21:30,489 --> 00:21:32,629
 

1295
00:21:30,499 --> 00:21:34,700
 skip over the first letter of every word

1296
00:21:32,619 --> 00:21:34,700
 

1297
00:21:32,629 --> 00:21:36,080
 and go straight to the second one then

1298
00:21:34,690 --> 00:21:36,080
 

1299
00:21:34,700 --> 00:21:38,450
 get to the end of the word and go back

1300
00:21:36,070 --> 00:21:38,450
 

1301
00:21:36,080 --> 00:21:39,950
 to the first letter and my kind of guess

1302
00:21:38,440 --> 00:21:39,950
 

1303
00:21:38,450 --> 00:21:42,470
 for why I did that is that in English

1304
00:21:39,940 --> 00:21:42,470
 

1305
00:21:39,950 --> 00:21:43,999
 the second letter is often a vowel which

1306
00:21:42,460 --> 00:21:43,999
 

1307
00:21:42,470 --> 00:21:47,090
 means it has lower entropy than the

1308
00:21:43,989 --> 00:21:47,090
 

1309
00:21:43,999 --> 00:21:49,190
 first letter and so I think if there's

1310
00:21:47,080 --> 00:21:49,190
 

1311
00:21:47,090 --> 00:21:52,399
 if there's a sort of fundamental rule

1312
00:21:49,180 --> 00:21:52,399
 

1313
00:21:49,190 --> 00:21:53,440
 for how we should order how we should

1314
00:21:52,389 --> 00:21:53,440
 

1315
00:21:52,399 --> 00:21:56,710
 order

1316
00:21:53,430 --> 00:21:56,710
 

1317
00:21:53,440 --> 00:21:58,720
 Otto associative predictions it's

1318
00:21:56,700 --> 00:21:58,720
 

1319
00:21:56,710 --> 00:22:00,730
 probably that the system always wants to

1320
00:21:58,710 --> 00:22:00,730
 

1321
00:21:58,720 --> 00:22:01,960
 make this take the safest bet first that

1322
00:22:00,720 --> 00:22:01,960
 

1323
00:22:00,730 --> 00:22:04,270
 wants to make the prediction that it's

1324
00:22:01,950 --> 00:22:04,270
 

1325
00:22:01,960 --> 00:22:06,210
 surest of given the information so far

1326
00:22:04,260 --> 00:22:06,210
 

1327
00:22:04,270 --> 00:22:09,160
 first and then move on to the next one

1328
00:22:06,200 --> 00:22:09,160
 

1329
00:22:06,210 --> 00:22:10,660
 but typically that's roughly what you

1330
00:22:09,150 --> 00:22:10,660
 

1331
00:22:09,160 --> 00:22:11,920
 get you know if you have a time series

1332
00:22:10,650 --> 00:22:11,920
 

1333
00:22:10,660 --> 00:22:13,060
 and you make the predictions in order of

1334
00:22:11,910 --> 00:22:13,060
 

1335
00:22:11,920 --> 00:22:15,400
 time that's roughly what you're getting

1336
00:22:13,050 --> 00:22:15,400
 

1337
00:22:13,060 --> 00:22:18,100
 so anyway autoregressive network well

1338
00:22:15,390 --> 00:22:18,100
 

1339
00:22:15,400 --> 00:22:20,170
 it's just basically you know applying

1340
00:22:18,090 --> 00:22:20,170
 

1341
00:22:18,100 --> 00:22:21,490
 that that trick to a neural network so

1342
00:22:20,160 --> 00:22:21,490
 

1343
00:22:20,170 --> 00:22:22,960
 you take high dimensional data and you

1344
00:22:21,480 --> 00:22:22,960
 

1345
00:22:21,490 --> 00:22:24,430
 split it up into a sequence of small

1346
00:22:22,950 --> 00:22:24,430
 

1347
00:22:22,960 --> 00:22:26,440
 pieces where you have to choose what the

1348
00:22:24,420 --> 00:22:26,440
 

1349
00:22:24,430 --> 00:22:27,730
 sequence is and you predict each piece

1350
00:22:26,430 --> 00:22:27,730
 

1351
00:22:26,440 --> 00:22:29,920
 from those before and to me this

1352
00:22:27,720 --> 00:22:29,920
 

1353
00:22:27,730 --> 00:22:31,510
 basically answers and our marker earlier

1354
00:22:29,910 --> 00:22:31,510
 

1355
00:22:29,920 --> 00:22:32,920
 might disagree with me on this one we

1356
00:22:31,500 --> 00:22:32,920
 

1357
00:22:31,510 --> 00:22:34,840
 had a long debate about the relative

1358
00:22:32,910 --> 00:22:34,840
 

1359
00:22:32,920 --> 00:22:36,190
 merits of auto regressive models so

1360
00:22:34,830 --> 00:22:36,190
 

1361
00:22:34,840 --> 00:22:37,630
 you'll hear more about that later but I

1362
00:22:36,180 --> 00:22:37,630
 

1363
00:22:36,190 --> 00:22:39,100
 think that this kind of answers the

1364
00:22:37,620 --> 00:22:39,100
 

1365
00:22:37,630 --> 00:22:40,720
 curse of dimensionality part of the

1366
00:22:39,090 --> 00:22:40,720
 

1367
00:22:39,100 --> 00:22:43,000
 problem because you're only predicting a

1368
00:22:40,710 --> 00:22:43,000
 

1369
00:22:40,720 --> 00:22:44,860
 small piece of the system at once you

1370
00:22:42,990 --> 00:22:44,860
 

1371
00:22:43,000 --> 00:22:47,170
 don't no longer have this issue where

1372
00:22:44,850 --> 00:22:47,170
 

1373
00:22:44,860 --> 00:22:49,000
 you you just can't handle the the

1374
00:22:47,160 --> 00:22:49,000
 

1375
00:22:47,170 --> 00:22:53,350
 covariance or the interactions between

1376
00:22:48,990 --> 00:22:53,350
 

1377
00:22:49,000 --> 00:22:55,600
 many variables at once and the

1378
00:22:53,340 --> 00:22:55,600
 

1379
00:22:53,350 --> 00:22:58,540
 conditioning on the past so you know the

1380
00:22:55,590 --> 00:22:58,540
 

1381
00:22:55,600 --> 00:23:00,930
 this this this probability has been spit

1382
00:22:58,530 --> 00:23:00,930
 

1383
00:22:58,540 --> 00:23:03,370
 up for you into a chain a sequence of

1384
00:23:00,920 --> 00:23:03,370
 

1385
00:23:00,930 --> 00:23:05,110
 independent predictions each of which is

1386
00:23:03,360 --> 00:23:05,110
 

1387
00:23:03,370 --> 00:23:06,940
 conditioned on the past state this is

1388
00:23:05,100 --> 00:23:06,940
 

1389
00:23:05,110 --> 00:23:08,860
 done via the state of the neural network

1390
00:23:06,930 --> 00:23:08,860
 

1391
00:23:06,940 --> 00:23:10,900
 and so this this tutorial isn't going to

1392
00:23:08,850 --> 00:23:10,900
 

1393
00:23:08,860 --> 00:23:13,060
 talk very much about hardly can talk at

1394
00:23:10,890 --> 00:23:13,060
 

1395
00:23:10,900 --> 00:23:14,410
 all about deep learning architectures is

1396
00:23:13,050 --> 00:23:14,410
 

1397
00:23:13,060 --> 00:23:16,120
 much more about loss functions and

1398
00:23:14,400 --> 00:23:16,120
 

1399
00:23:14,410 --> 00:23:17,560
 training methods but when it comes to

1400
00:23:16,110 --> 00:23:17,560
 

1401
00:23:16,120 --> 00:23:19,360
 auto regressive networks there's

1402
00:23:17,550 --> 00:23:19,360
 

1403
00:23:17,560 --> 00:23:20,950
 probably like three basic structures

1404
00:23:19,350 --> 00:23:20,950
 

1405
00:23:19,360 --> 00:23:22,840
 have emerged as the winners there's

1406
00:23:20,940 --> 00:23:22,840
 

1407
00:23:20,950 --> 00:23:25,750
 return gated recurrent neural networks

1408
00:23:22,830 --> 00:23:25,750
 

1409
00:23:22,840 --> 00:23:27,520
 like lsdm and GRU there's masked

1410
00:23:25,740 --> 00:23:27,520
 

1411
00:23:25,750 --> 00:23:31,120
 convolutions where the masks are there

1412
00:23:27,510 --> 00:23:31,120
 

1413
00:23:27,520 --> 00:23:33,610
 to ensure that the the future state is

1414
00:23:31,110 --> 00:23:33,610
 

1415
00:23:31,120 --> 00:23:35,980
 not accessible to to the past state when

1416
00:23:33,600 --> 00:23:35,980
 

1417
00:23:33,610 --> 00:23:38,410
 it's making predictions and recently

1418
00:23:35,970 --> 00:23:38,410
 

1419
00:23:35,980 --> 00:23:40,920
 there's been a growing trend towards

1420
00:23:38,400 --> 00:23:40,920
 

1421
00:23:38,410 --> 00:23:42,940
 transformers where this you know

1422
00:23:40,910 --> 00:23:42,940
 

1423
00:23:40,920 --> 00:23:44,800
 convolutional or recursive state is

1424
00:23:42,930 --> 00:23:44,800
 

1425
00:23:42,940 --> 00:23:45,970
 removed completely and conditioning on

1426
00:23:44,790 --> 00:23:45,970
 

1427
00:23:44,800 --> 00:23:49,000
 the past is done entirely through

1428
00:23:45,960 --> 00:23:49,000
 

1429
00:23:45,970 --> 00:23:50,710
 attention mechanisms anyway whatever you

1430
00:23:48,990 --> 00:23:50,710
 

1431
00:23:49,000 --> 00:23:53,050
 choose you end up with the same loss

1432
00:23:50,700 --> 00:23:53,050
 

1433
00:23:50,710 --> 00:23:55,330
 function and broadly similar behavior

1434
00:23:53,040 --> 00:23:55,330
 

1435
00:23:53,050 --> 00:23:58,090
 and then you haven't you know given so

1436
00:23:55,320 --> 00:23:58,090
 

1437
00:23:55,330 --> 00:23:59,860
 given this network that embeds the past

1438
00:23:58,080 --> 00:23:59,860
 

1439
00:23:58,090 --> 00:24:02,200
 as some sort of high dimensional vector

1440
00:23:59,850 --> 00:24:02,200
 

1441
00:23:59,860 --> 00:24:03,610
 you then pass that through the output

1442
00:24:02,190 --> 00:24:03,610
 

1443
00:24:02,200 --> 00:24:04,900
 layer of the network which whose job is

1444
00:24:03,600 --> 00:24:04,900
 

1445
00:24:03,610 --> 00:24:07,030
 essentially to parametrize the

1446
00:24:04,890 --> 00:24:07,030
 

1447
00:24:04,900 --> 00:24:08,710
 prediction so whatever your predictive

1448
00:24:07,020 --> 00:24:08,710
 

1449
00:24:07,030 --> 00:24:10,690
 Dushan is for what comes next and most

1450
00:24:08,700 --> 00:24:10,690
 

1451
00:24:08,710 --> 00:24:12,730
 often it's a softmax as a classification

1452
00:24:10,680 --> 00:24:12,730
 

1453
00:24:10,690 --> 00:24:14,160
 then your output layer is just giving

1454
00:24:12,720 --> 00:24:14,160
 

1455
00:24:12,730 --> 00:24:16,690
 you the parameters of that prediction

1456
00:24:14,150 --> 00:24:16,690
 

1457
00:24:14,160 --> 00:24:18,580
 okay so just to see how that works for a

1458
00:24:16,680 --> 00:24:18,580
 

1459
00:24:16,690 --> 00:24:19,900
 recurrent neural network language model

1460
00:24:18,570 --> 00:24:19,900
 

1461
00:24:18,580 --> 00:24:21,730
 which is maybe one of the oldest sort of

1462
00:24:19,890 --> 00:24:21,730
 

1463
00:24:19,900 --> 00:24:23,200
 neural language models and that we had

1464
00:24:21,720 --> 00:24:23,200
 

1465
00:24:21,730 --> 00:24:26,110
 Jeffrey Ullman doing this back in the

1466
00:24:23,190 --> 00:24:26,110
 

1467
00:24:23,200 --> 00:24:29,590
 early 90s finding structure and time and

1468
00:24:26,100 --> 00:24:29,590
 

1469
00:24:26,110 --> 00:24:32,350
 so forth the system has this recurrent

1470
00:24:29,580 --> 00:24:32,350
 

1471
00:24:29,590 --> 00:24:36,570
 connection it takes in vectors of input

1472
00:24:32,340 --> 00:24:36,570
 

1473
00:24:32,350 --> 00:24:39,160
 at each step and cyclically kind of

1474
00:24:36,560 --> 00:24:39,160
 

1475
00:24:36,570 --> 00:24:42,550
 feeds those in along with its own state

1476
00:24:39,150 --> 00:24:42,550
 

1477
00:24:39,160 --> 00:24:44,290
 in the previous step to build up a state

1478
00:24:42,540 --> 00:24:44,290
 

1479
00:24:42,550 --> 00:24:45,580
 that represents the history at any point

1480
00:24:44,280 --> 00:24:45,580
 

1481
00:24:44,290 --> 00:24:49,630
 in time and then it makes predictions

1482
00:24:45,570 --> 00:24:49,630
 

1483
00:24:45,580 --> 00:24:51,400
 based on that okay so you know why why

1484
00:24:49,620 --> 00:24:51,400
 

1485
00:24:49,630 --> 00:24:54,100
 do I like Auto regressive models so much

1486
00:24:51,390 --> 00:24:54,100
 

1487
00:24:51,400 --> 00:24:55,330
 but one one point which I think is

1488
00:24:54,090 --> 00:24:55,330
 

1489
00:24:54,100 --> 00:24:56,500
 already clear is they're quite simple

1490
00:24:55,320 --> 00:24:56,500
 

1491
00:24:55,330 --> 00:24:58,030
 right you just once you've picked an

1492
00:24:56,490 --> 00:24:58,030
 

1493
00:24:56,500 --> 00:24:59,500
 ordering everything you just sort of

1494
00:24:58,020 --> 00:24:59,500
 

1495
00:24:58,030 --> 00:25:00,580
 turn the handle right the loss function

1496
00:24:59,490 --> 00:25:00,580
 

1497
00:24:59,500 --> 00:25:03,760
 is just done for you and you don't have

1498
00:25:00,570 --> 00:25:03,760
 

1499
00:25:00,580 --> 00:25:05,950
 to think about it and it's all it's you

1500
00:25:03,750 --> 00:25:05,950
 

1501
00:25:03,760 --> 00:25:08,670
 know it's all normalized and and you

1502
00:25:05,940 --> 00:25:08,670
 

1503
00:25:05,950 --> 00:25:11,110
 don't have to do you know to worry about

1504
00:25:08,660 --> 00:25:11,110
 

1505
00:25:08,670 --> 00:25:12,520
 about having properly defined

1506
00:25:11,100 --> 00:25:12,520
 

1507
00:25:11,110 --> 00:25:14,410
 probability distributions and things

1508
00:25:12,510 --> 00:25:14,410
 

1509
00:25:12,520 --> 00:25:15,370
 like that and and a kind of related

1510
00:25:14,400 --> 00:25:15,370
 

1511
00:25:14,410 --> 00:25:16,870
 point there is it's very easy to

1512
00:25:15,360 --> 00:25:16,870
 

1513
00:25:15,370 --> 00:25:19,360
 generate samples so basically how do you

1514
00:25:16,860 --> 00:25:19,360
 

1515
00:25:16,870 --> 00:25:22,600
 generate samples from an RNN from a auto

1516
00:25:19,350 --> 00:25:22,600
 

1517
00:25:19,360 --> 00:25:24,280
 regressive model you just predict pick a

1518
00:25:22,590 --> 00:25:24,280
 

1519
00:25:22,600 --> 00:25:26,620
 sample from the predictive distribution

1520
00:25:24,270 --> 00:25:26,620
 

1521
00:25:24,280 --> 00:25:28,240
 at each step and then feed that in as if

1522
00:25:26,610 --> 00:25:28,240
 

1523
00:25:26,620 --> 00:25:29,530
 it's the input at the next step so I

1524
00:25:28,230 --> 00:25:29,530
 

1525
00:25:28,240 --> 00:25:31,240
 always think of this as sort of like the

1526
00:25:29,520 --> 00:25:31,240
 

1527
00:25:29,530 --> 00:25:32,650
 equivalent of dreaming for a neural

1528
00:25:31,230 --> 00:25:32,650
 

1529
00:25:31,240 --> 00:25:34,450
 network for a computer when we dream

1530
00:25:32,640 --> 00:25:34,450
 

1531
00:25:32,650 --> 00:25:35,950
 were imagining things that might happen

1532
00:25:34,440 --> 00:25:35,950
 

1533
00:25:34,450 --> 00:25:38,560
 and then kind of treating them as if

1534
00:25:35,940 --> 00:25:38,560
 

1535
00:25:35,950 --> 00:25:39,790
 they actually have happened but really

1536
00:25:38,550 --> 00:25:39,790
 

1537
00:25:38,560 --> 00:25:41,530
 the main reason to use auto regressive

1538
00:25:39,780 --> 00:25:41,530
 

1539
00:25:39,790 --> 00:25:42,880
 models is you know less poetic than that

1540
00:25:41,520 --> 00:25:42,880
 

1541
00:25:41,530 --> 00:25:44,350
 it's just that they give the best log

1542
00:25:42,870 --> 00:25:44,350
 

1543
00:25:42,880 --> 00:25:46,690
 likelihoods for lots of types of data

1544
00:25:44,340 --> 00:25:46,690
 

1545
00:25:44,350 --> 00:25:48,850
 and in particular the kinds of data that

1546
00:25:46,680 --> 00:25:48,850
 

1547
00:25:46,690 --> 00:25:50,410
 I think we're most focused on when it

1548
00:25:48,840 --> 00:25:50,410
 

1549
00:25:48,850 --> 00:25:52,360
 comes to you know general intelligence

1550
00:25:50,400 --> 00:25:52,360
 

1551
00:25:50,410 --> 00:25:55,690
 things like images audio video text

1552
00:25:52,350 --> 00:25:55,690
 

1553
00:25:52,360 --> 00:25:57,190
 these are something like the four senses

1554
00:25:55,680 --> 00:25:57,190
 

1555
00:25:55,690 --> 00:25:58,630
 of the digital world right these are the

1556
00:25:57,180 --> 00:25:58,630
 

1557
00:25:57,190 --> 00:26:01,750
 things that you find on the internet and

1558
00:25:58,620 --> 00:26:01,750
 

1559
00:25:58,630 --> 00:26:05,680
 that you find in huge amounts digitized

1560
00:26:01,740 --> 00:26:05,680
 

1561
00:26:01,750 --> 00:26:07,780
 but of course there are disadvantages of

1562
00:26:05,670 --> 00:26:07,780
 

1563
00:26:05,680 --> 00:26:09,370
 these types of models and the first one

1564
00:26:07,770 --> 00:26:09,370
 

1565
00:26:07,780 --> 00:26:12,220
 it's just that they're very expensive as

1566
00:26:09,360 --> 00:26:12,220
 

1567
00:26:09,370 --> 00:26:13,960
 that you know as the data basically you

1568
00:26:12,210 --> 00:26:13,960
 

1569
00:26:12,220 --> 00:26:15,610
 know you have as many predictions as you

1570
00:26:13,950 --> 00:26:15,610
 

1571
00:26:13,960 --> 00:26:17,980
 have pieces in the data and so if you're

1572
00:26:15,600 --> 00:26:17,980
 

1573
00:26:15,610 --> 00:26:19,690
 splitting your high dimensional video

1574
00:26:17,970 --> 00:26:19,690
 

1575
00:26:17,980 --> 00:26:21,340
 your high-density video

1576
00:26:19,680 --> 00:26:21,340
 

1577
00:26:19,690 --> 00:26:22,389
 up into individual pixels or individual

1578
00:26:21,330 --> 00:26:22,389
 

1579
00:26:21,340 --> 00:26:24,159
 color channels you're talking about

1580
00:26:22,379 --> 00:26:24,159
 

1581
00:26:22,389 --> 00:26:26,230
 millions of predictions per second and

1582
00:26:24,149 --> 00:26:26,230
 

1583
00:26:24,159 --> 00:26:27,909
 even with today's computers that becomes

1584
00:26:26,220 --> 00:26:27,909
 

1585
00:26:26,230 --> 00:26:29,860
 extremely expensive and you can mitigate

1586
00:26:27,899 --> 00:26:29,860
 

1587
00:26:27,909 --> 00:26:31,720
 this to some extent by parallelizing

1588
00:26:29,850 --> 00:26:31,720
 

1589
00:26:29,860 --> 00:26:33,100
 things during training but that doesn't

1590
00:26:31,710 --> 00:26:33,100
 

1591
00:26:31,720 --> 00:26:34,840
 work during generating because during

1592
00:26:33,090 --> 00:26:34,840
 

1593
00:26:33,100 --> 00:26:37,629
 generating it has to be serial you have

1594
00:26:34,830 --> 00:26:37,629
 

1595
00:26:34,840 --> 00:26:39,309
 to pick something from the predictive

1596
00:26:37,619 --> 00:26:39,309
 

1597
00:26:37,629 --> 00:26:41,980
 distribution and then feed it in and

1598
00:26:39,299 --> 00:26:41,980
 

1599
00:26:39,309 --> 00:26:44,289
 then pick something again there's this

1600
00:26:41,970 --> 00:26:44,289
 

1601
00:26:41,980 --> 00:26:46,480
 issue of order dependence so as I said

1602
00:26:44,279 --> 00:26:46,480
 

1603
00:26:44,289 --> 00:26:48,429
 it really does matter what order you

1604
00:26:46,470 --> 00:26:48,429
 

1605
00:26:46,480 --> 00:26:53,259
 make the predictions in and and a kind

1606
00:26:48,419 --> 00:26:53,259
 

1607
00:26:48,429 --> 00:26:54,279
 of following on I think that follows on

1608
00:26:53,249 --> 00:26:54,279
 

1609
00:26:53,259 --> 00:26:57,370
 from that is that you can't easily

1610
00:26:54,269 --> 00:26:57,370
 

1611
00:26:54,279 --> 00:26:58,450
 impute missing data out of order I think

1612
00:26:57,360 --> 00:26:58,450
 

1613
00:26:57,370 --> 00:26:59,860
 mark Aurelio is going to talk a little

1614
00:26:58,440 --> 00:26:59,860
 

1615
00:26:58,450 --> 00:27:01,990
 bit about that so by impute I mean for

1616
00:26:59,850 --> 00:27:01,990
 

1617
00:26:59,860 --> 00:27:03,850
 example given you know a sentence and

1618
00:27:01,980 --> 00:27:03,850
 

1619
00:27:01,990 --> 00:27:06,429
 some missing words fill in the missing

1620
00:27:03,840 --> 00:27:06,429
 

1621
00:27:03,850 --> 00:27:08,919
 words auto regressive models are kind of

1622
00:27:06,419 --> 00:27:08,919
 

1623
00:27:06,429 --> 00:27:10,269
 good at filling in possible futures or

1624
00:27:08,909 --> 00:27:10,269
 

1625
00:27:08,919 --> 00:27:11,889
 guessing possible futures but not at

1626
00:27:10,259 --> 00:27:11,889
 

1627
00:27:10,269 --> 00:27:13,450
 filling in some part of the past now in

1628
00:27:11,879 --> 00:27:13,450
 

1629
00:27:11,889 --> 00:27:15,909
 principle you could do something like a

1630
00:27:13,440 --> 00:27:15,909
 

1631
00:27:13,450 --> 00:27:17,350
 beam search and try to infer what those

1632
00:27:15,899 --> 00:27:17,350
 

1633
00:27:15,909 --> 00:27:18,460
 words must be even with an

1634
00:27:17,340 --> 00:27:18,460
 

1635
00:27:17,350 --> 00:27:19,899
 autoregressive model but it's very

1636
00:27:18,450 --> 00:27:19,899
 

1637
00:27:18,460 --> 00:27:22,389
 awkward

1638
00:27:19,889 --> 00:27:22,389
 

1639
00:27:19,899 --> 00:27:23,500
 another issue that a lot of people worry

1640
00:27:22,379 --> 00:27:23,500
 

1641
00:27:22,389 --> 00:27:25,059
 about with auto regressive models is

1642
00:27:23,490 --> 00:27:25,059
 

1643
00:27:23,500 --> 00:27:26,559
 this idea of teacher forcing so

1644
00:27:25,049 --> 00:27:26,559
 

1645
00:27:25,059 --> 00:27:28,389
 basically because the system is only

1646
00:27:26,549 --> 00:27:28,389
 

1647
00:27:26,559 --> 00:27:30,879
 ever being trained to predict one step

1648
00:27:28,379 --> 00:27:30,879
 

1649
00:27:28,389 --> 00:27:33,070
 ahead it's never learning how to

1650
00:27:30,869 --> 00:27:33,070
 

1651
00:27:30,879 --> 00:27:35,529
 generate many steps ahead I think this

1652
00:27:33,060 --> 00:27:35,529
 

1653
00:27:33,070 --> 00:27:37,360
 is usually counted as a demerit for Auto

1654
00:27:35,519 --> 00:27:37,360
 

1655
00:27:35,529 --> 00:27:38,679
 regressive models in the context of

1656
00:27:37,350 --> 00:27:38,679
 

1657
00:27:37,360 --> 00:27:41,110
 generative modeling and people say oh

1658
00:27:38,669 --> 00:27:41,110
 

1659
00:27:38,679 --> 00:27:43,210
 they can't generate long sequences my

1660
00:27:41,100 --> 00:27:43,210
 

1661
00:27:41,110 --> 00:27:44,649
 experience is that modern auto

1662
00:27:43,200 --> 00:27:44,649
 

1663
00:27:43,210 --> 00:27:45,940
 regressive neural that's actually can

1664
00:27:44,639 --> 00:27:45,940
 

1665
00:27:44,649 --> 00:27:48,639
 generate very convincingly long

1666
00:27:45,930 --> 00:27:48,639
 

1667
00:27:45,940 --> 00:27:51,009
 sequences in either case that's not

1668
00:27:48,629 --> 00:27:51,009
 

1669
00:27:48,639 --> 00:27:52,179
 really the issue for from the context of

1670
00:27:50,999 --> 00:27:52,179
 

1671
00:27:51,009 --> 00:27:53,919
 unsupervised learning that's not the

1672
00:27:52,169 --> 00:27:53,919
 

1673
00:27:52,179 --> 00:27:56,080
 issue the the worry here would be more

1674
00:27:53,909 --> 00:27:56,080
 

1675
00:27:53,919 --> 00:27:57,490
 that by only predicting one step ahead

1676
00:27:56,070 --> 00:27:57,490
 

1677
00:27:56,080 --> 00:27:59,440
 they're learning a kind of myopic

1678
00:27:57,480 --> 00:27:59,440
 

1679
00:27:57,490 --> 00:28:01,120
 representation we're back to this bits

1680
00:27:59,430 --> 00:28:01,120
 

1681
00:27:59,440 --> 00:28:02,799
 not being equal they're focusing on just

1682
00:28:01,110 --> 00:28:02,799
 

1683
00:28:01,120 --> 00:28:05,889
 the next bit the next prediction and not

1684
00:28:02,789 --> 00:28:05,889
 

1685
00:28:02,799 --> 00:28:07,840
 on the large-scale structure of the data

1686
00:28:05,879 --> 00:28:07,840
 

1687
00:28:05,889 --> 00:28:10,450
 which is what you really want okay so

1688
00:28:07,830 --> 00:28:10,450
 

1689
00:28:07,840 --> 00:28:13,059
 just a quick like rundown of you know

1690
00:28:10,440 --> 00:28:13,059
 

1691
00:28:10,450 --> 00:28:14,740
 why you know state how where language

1692
00:28:13,049 --> 00:28:14,740
 

1693
00:28:13,059 --> 00:28:17,590
 modeling is that sorry we're addressing

1694
00:28:14,730 --> 00:28:17,590
 

1695
00:28:14,740 --> 00:28:19,149
 models are at in terms of data you know

1696
00:28:17,580 --> 00:28:19,149
 

1697
00:28:17,590 --> 00:28:20,710
 in the language modeling I think it's

1698
00:28:19,139 --> 00:28:20,710
 

1699
00:28:19,149 --> 00:28:21,850
 clear they really dominate in some sense

1700
00:28:20,700 --> 00:28:21,850
 

1701
00:28:20,710 --> 00:28:23,019
 language models have always been auto

1702
00:28:21,840 --> 00:28:23,019
 

1703
00:28:21,850 --> 00:28:24,639
 regressive this is the very natural

1704
00:28:23,009 --> 00:28:24,639
 

1705
00:28:23,019 --> 00:28:26,799
 thing to do here is to predict the next

1706
00:28:24,629 --> 00:28:26,799
 

1707
00:28:24,639 --> 00:28:28,029
 word given the previous ones and this

1708
00:28:26,789 --> 00:28:28,029
 

1709
00:28:26,799 --> 00:28:30,070
 slide sort of shows you know just how

1710
00:28:28,019 --> 00:28:30,070
 

1711
00:28:28,029 --> 00:28:31,000
 advanced that's become and if you look

1712
00:28:30,060 --> 00:28:31,000
 

1713
00:28:30,070 --> 00:28:31,640
 at these results you can see this

1714
00:28:30,990 --> 00:28:31,640
 

1715
00:28:31,000 --> 00:28:33,620
 perplexing

1716
00:28:31,630 --> 00:28:33,620
 

1717
00:28:31,640 --> 00:28:34,790
 is becoming if you if you know anything

1718
00:28:33,610 --> 00:28:34,790
 

1719
00:28:33,620 --> 00:28:36,170
 about prep Exodus and numbers they're

1720
00:28:34,780 --> 00:28:36,170
 

1721
00:28:34,790 --> 00:28:40,250
 becoming ridiculously low with these

1722
00:28:36,160 --> 00:28:40,250
 

1723
00:28:36,170 --> 00:28:41,450
 giant neural network models and you know

1724
00:28:40,240 --> 00:28:41,450
 

1725
00:28:40,250 --> 00:28:44,570
 as a generative model they're also

1726
00:28:41,440 --> 00:28:44,570
 

1727
00:28:41,450 --> 00:28:46,430
 getting pretty impressive so half of

1728
00:28:44,560 --> 00:28:46,430
 

1729
00:28:44,570 --> 00:28:47,630
 these quotes are generated and half of

1730
00:28:46,420 --> 00:28:47,630
 

1731
00:28:46,430 --> 00:28:49,190
 them are not and they're color-coded

1732
00:28:47,620 --> 00:28:49,190
 

1733
00:28:47,630 --> 00:28:50,180
 according to which is which you know if

1734
00:28:49,180 --> 00:28:50,180
 

1735
00:28:49,190 --> 00:28:51,740
 you have a look at them and try and

1736
00:28:50,170 --> 00:28:51,740
 

1737
00:28:50,180 --> 00:28:54,770
 decide which color is true in which

1738
00:28:51,730 --> 00:28:54,770
 

1739
00:28:51,740 --> 00:28:56,690
 color is fake you know honestly I had to

1740
00:28:54,760 --> 00:28:56,690
 

1741
00:28:54,770 --> 00:28:59,420
 I had to ask Oriole Oriole vini owls who

1742
00:28:56,680 --> 00:28:59,420
 

1743
00:28:56,690 --> 00:29:04,130
 gave me this slide which was which but

1744
00:28:59,410 --> 00:29:04,130
 

1745
00:28:59,420 --> 00:29:07,430
 it is in fact the the red ones are fake

1746
00:29:04,120 --> 00:29:07,430
 

1747
00:29:04,130 --> 00:29:08,540
 and the green ones are real okay so a

1748
00:29:07,420 --> 00:29:08,540
 

1749
00:29:07,430 --> 00:29:10,940
 lot of you probably heard about wave

1750
00:29:08,530 --> 00:29:10,940
 

1751
00:29:08,540 --> 00:29:12,200
 nets you know wave net is it's a

1752
00:29:10,930 --> 00:29:12,200
 

1753
00:29:10,940 --> 00:29:14,030
 slightly different setting here it's not

1754
00:29:12,190 --> 00:29:14,030
 

1755
00:29:12,200 --> 00:29:15,560
 purely unsupervised you've got a

1756
00:29:14,020 --> 00:29:15,560
 

1757
00:29:14,030 --> 00:29:17,750
 conditioning signal right you've got a

1758
00:29:15,550 --> 00:29:17,750
 

1759
00:29:15,560 --> 00:29:19,400
 text signal and from that X signal you

1760
00:29:17,740 --> 00:29:19,400
 

1761
00:29:17,750 --> 00:29:21,110
 generate the data but mostly it is

1762
00:29:19,390 --> 00:29:21,110
 

1763
00:29:19,400 --> 00:29:23,930
 unsupervised so even if you run wave net

1764
00:29:21,100 --> 00:29:23,930
 

1765
00:29:21,110 --> 00:29:25,490
 without the text it already generates

1766
00:29:23,920 --> 00:29:25,490
 

1767
00:29:23,930 --> 00:29:27,770
 very convincing audio generates a kind

1768
00:29:25,480 --> 00:29:27,770
 

1769
00:29:25,490 --> 00:29:30,860
 of babble that contains real words and

1770
00:29:27,760 --> 00:29:30,860
 

1771
00:29:27,770 --> 00:29:32,450
 even individual phrases and to me I was

1772
00:29:30,850 --> 00:29:32,450
 

1773
00:29:30,860 --> 00:29:35,630
 amazed when I saw wave networking just

1774
00:29:32,440 --> 00:29:35,630
 

1775
00:29:32,450 --> 00:29:37,130
 because of the sheer volume of data that

1776
00:29:35,620 --> 00:29:37,130
 

1777
00:29:35,630 --> 00:29:39,650
 it has to generate the frequency of the

1778
00:29:37,120 --> 00:29:39,650
 

1779
00:29:37,130 --> 00:29:41,570
 data so it's you know sixteen thousand

1780
00:29:39,640 --> 00:29:41,570
 

1781
00:29:39,650 --> 00:29:44,270
 predictions per second 16 kilohertz to

1782
00:29:41,560 --> 00:29:44,270
 

1783
00:29:41,570 --> 00:29:46,340
 generate sort of raw audio and a network

1784
00:29:44,260 --> 00:29:46,340
 

1785
00:29:44,270 --> 00:29:48,770
 can do this and it can remain coherent

1786
00:29:46,330 --> 00:29:48,770
 

1787
00:29:46,340 --> 00:29:50,570
 over long spans of time and you know

1788
00:29:48,760 --> 00:29:50,570
 

1789
00:29:48,770 --> 00:29:52,250
 it's now I guess I think it's pretty

1790
00:29:50,560 --> 00:29:52,250
 

1791
00:29:50,570 --> 00:29:55,790
 clear that this is the state of the art

1792
00:29:52,240 --> 00:29:55,790
 

1793
00:29:52,250 --> 00:29:57,410
 as far as audio generation goes so pixel

1794
00:29:55,780 --> 00:29:57,410
 

1795
00:29:55,790 --> 00:29:59,300
 rnns now this is an interesting one

1796
00:29:57,400 --> 00:29:59,300
 

1797
00:29:57,410 --> 00:30:00,680
 which is what happens when you take the

1798
00:29:59,290 --> 00:30:00,680
 

1799
00:29:59,300 --> 00:30:02,300
 auto regressive models and apply them to

1800
00:30:00,670 --> 00:30:02,300
 

1801
00:30:00,680 --> 00:30:04,730
 images well here the ordering problem

1802
00:30:02,290 --> 00:30:04,730
 

1803
00:30:02,300 --> 00:30:05,930
 really becomes an issue right like you

1804
00:30:04,720 --> 00:30:05,930
 

1805
00:30:04,730 --> 00:30:07,820
 want to predict all the pixels in the

1806
00:30:05,920 --> 00:30:07,820
 

1807
00:30:05,930 --> 00:30:10,220
 image but you have to give a specific

1808
00:30:07,810 --> 00:30:10,220
 

1809
00:30:07,820 --> 00:30:12,200
 order they should be in and so in speech

1810
00:30:10,210 --> 00:30:12,200
 

1811
00:30:10,220 --> 00:30:13,220
 you know they anything anything temporal

1812
00:30:12,190 --> 00:30:13,220
 

1813
00:30:12,200 --> 00:30:15,830
 there's a clear ordering like you

1814
00:30:13,210 --> 00:30:15,830
 

1815
00:30:13,220 --> 00:30:17,720
 predict each audio signal predicted on

1816
00:30:15,820 --> 00:30:17,720
 

1817
00:30:15,830 --> 00:30:19,070
 the last predicated on the last one

1818
00:30:17,710 --> 00:30:19,070
 

1819
00:30:17,720 --> 00:30:21,230
 there's that there's a temporal order in

1820
00:30:19,060 --> 00:30:21,230
 

1821
00:30:19,070 --> 00:30:23,210
 pixel RNN it's pretty much random right

1822
00:30:21,220 --> 00:30:23,210
 

1823
00:30:21,230 --> 00:30:24,680
 you're not random but arbitrary right

1824
00:30:23,200 --> 00:30:24,680
 

1825
00:30:23,210 --> 00:30:26,270
 you you just have to pick some ordering

1826
00:30:24,670 --> 00:30:26,270
 

1827
00:30:24,680 --> 00:30:28,220
 and so there's this kind of raster scan

1828
00:30:26,260 --> 00:30:28,220
 

1829
00:30:26,270 --> 00:30:31,490
 ordering is often used where you know

1830
00:30:28,210 --> 00:30:31,490
 

1831
00:30:28,220 --> 00:30:33,110
 pixel X I here is condition it's

1832
00:30:31,480 --> 00:30:33,110
 

1833
00:30:31,490 --> 00:30:35,080
 predicted condition and all the other

1834
00:30:33,100 --> 00:30:35,080
 

1835
00:30:33,110 --> 00:30:39,080
 pixels that have been uncovered so far

1836
00:30:35,070 --> 00:30:39,080
 

1837
00:30:35,080 --> 00:30:40,850
 and another important sort of surprising

1838
00:30:39,070 --> 00:30:40,850
 

1839
00:30:39,080 --> 00:30:41,990
 and important thing about pixel Aaron

1840
00:30:40,840 --> 00:30:41,990
 

1841
00:30:40,850 --> 00:30:44,360
 and it's actually true of wavenet as

1842
00:30:41,980 --> 00:30:44,360
 

1843
00:30:41,990 --> 00:30:46,840
 well is that even though it's modeling

1844
00:30:44,350 --> 00:30:46,840
 

1845
00:30:44,360 --> 00:30:49,400
 what is essentially continuous data

1846
00:30:46,830 --> 00:30:49,400
 

1847
00:30:46,840 --> 00:30:52,340
 although it's discretized continuous

1848
00:30:49,390 --> 00:30:52,340
 

1849
00:30:49,400 --> 00:30:54,650
 data is doing so with a softmax output

1850
00:30:52,330 --> 00:30:54,650
 

1851
00:30:52,340 --> 00:30:56,179
 there so it's treating these these it's

1852
00:30:54,640 --> 00:30:56,179
 

1853
00:30:54,650 --> 00:30:57,920
 actually I should be more specific here

1854
00:30:56,169 --> 00:30:57,920
 

1855
00:30:56,179 --> 00:30:59,870
 what actually models are the individual

1856
00:30:57,910 --> 00:30:59,870
 

1857
00:30:57,920 --> 00:31:01,790
 color channels in each pixel so the

1858
00:30:59,860 --> 00:31:01,790
 

1859
00:30:59,870 --> 00:31:03,080
 individual RGB numbers in each pixels

1860
00:31:01,780 --> 00:31:03,080
 

1861
00:31:01,790 --> 00:31:05,210
 there's three predictions for every

1862
00:31:03,070 --> 00:31:05,210
 

1863
00:31:03,080 --> 00:31:07,220
 pixel each conditioned on the previous

1864
00:31:05,200 --> 00:31:07,220
 

1865
00:31:05,210 --> 00:31:11,960
 ones like blue is conditioned on red and

1866
00:31:07,210 --> 00:31:11,960
 

1867
00:31:07,220 --> 00:31:14,059
 green and so on and it's treating those

1868
00:31:11,950 --> 00:31:14,059
 

1869
00:31:11,960 --> 00:31:16,130
 numbers those numbers which are you know

1870
00:31:14,049 --> 00:31:16,130
 

1871
00:31:14,059 --> 00:31:20,390
 a number between 0 and 255 as if they're

1872
00:31:16,120 --> 00:31:20,390
 

1873
00:31:16,130 --> 00:31:22,250
 just 256 on like completely distinct

1874
00:31:20,380 --> 00:31:22,250
 

1875
00:31:20,390 --> 00:31:23,660
 classes which is seems at first sight a

1876
00:31:22,240 --> 00:31:23,660
 

1877
00:31:22,250 --> 00:31:26,450
 bizarre thing to do because it throws

1878
00:31:23,650 --> 00:31:26,450
 

1879
00:31:23,660 --> 00:31:27,919
 away this information we have namely the

1880
00:31:26,440 --> 00:31:27,919
 

1881
00:31:26,450 --> 00:31:29,750
 numbers that are close to each other

1882
00:31:27,909 --> 00:31:29,750
 

1883
00:31:27,919 --> 00:31:31,520
 correspond to colors that are similar to

1884
00:31:29,740 --> 00:31:31,520
 

1885
00:31:29,750 --> 00:31:33,679
 one another one to eight is closer to

1886
00:31:31,510 --> 00:31:33,679
 

1887
00:31:31,520 --> 00:31:36,559
 one to nine as a color channel than it

1888
00:31:33,669 --> 00:31:36,559
 

1889
00:31:33,679 --> 00:31:38,150
 is the 255 but somehow just ignoring

1890
00:31:36,549 --> 00:31:38,150
 

1891
00:31:36,559 --> 00:31:41,600
 that and training it as if the separate

1892
00:31:38,140 --> 00:31:41,600
 

1893
00:31:38,150 --> 00:31:43,640
 classes works better than treating it as

1894
00:31:41,590 --> 00:31:43,640
 

1895
00:31:41,600 --> 00:31:45,380
 if it's continuous and so you can run

1896
00:31:43,630 --> 00:31:45,380
 

1897
00:31:43,640 --> 00:31:46,760
 this kind of model with a mixture model

1898
00:31:45,370 --> 00:31:46,760
 

1899
00:31:45,380 --> 00:31:50,150
 instead of a soft max but it works

1900
00:31:46,750 --> 00:31:50,150
 

1901
00:31:46,760 --> 00:31:52,340
 better with a soft max anyway it gives

1902
00:31:50,140 --> 00:31:52,340
 

1903
00:31:50,150 --> 00:31:53,900
 samples trained on imagenet the original

1904
00:31:52,330 --> 00:31:53,900
 

1905
00:31:52,340 --> 00:31:55,400
 pixel RNN and these are all like in

1906
00:31:53,890 --> 00:31:55,400
 

1907
00:31:53,900 --> 00:31:58,600
 terms of image generation you know these

1908
00:31:55,390 --> 00:31:58,600
 

1909
00:31:55,400 --> 00:32:00,650
 are two years old they're very dated and

1910
00:31:58,590 --> 00:32:00,650
 

1911
00:31:58,600 --> 00:32:04,549
 I'll show you some hits and better

1912
00:32:00,640 --> 00:32:04,549
 

1913
00:32:00,650 --> 00:32:06,530
 images in a second but it it gives

1914
00:32:04,539 --> 00:32:06,530
 

1915
00:32:04,549 --> 00:32:08,870
 textures and sort of like you can see

1916
00:32:06,520 --> 00:32:08,870
 

1917
00:32:06,530 --> 00:32:11,000
 something of the the flavor of these

1918
00:32:08,860 --> 00:32:11,000
 

1919
00:32:08,870 --> 00:32:12,410
 real images but they there's nothing

1920
00:32:10,990 --> 00:32:12,410
 

1921
00:32:11,000 --> 00:32:14,179
 there's there's local structure not

1922
00:32:12,400 --> 00:32:14,179
 

1923
00:32:12,410 --> 00:32:15,200
 global structure basically so they at

1924
00:32:14,169 --> 00:32:15,200
 

1925
00:32:14,179 --> 00:32:16,730
 first sight they appeared to be

1926
00:32:15,190 --> 00:32:16,730
 

1927
00:32:15,200 --> 00:32:17,900
 confirming what everyone said about auto

1928
00:32:16,720 --> 00:32:17,900
 

1929
00:32:16,730 --> 00:32:20,929
 regressive models or they don't capture

1930
00:32:17,890 --> 00:32:20,929
 

1931
00:32:17,900 --> 00:32:23,299
 global structure interestingly when you

1932
00:32:20,919 --> 00:32:23,299
 

1933
00:32:20,929 --> 00:32:25,850
 start conditioning them on the labels

1934
00:32:23,289 --> 00:32:25,850
 

1935
00:32:23,299 --> 00:32:27,140
 given in the data then immediately you

1936
00:32:25,840 --> 00:32:27,140
 

1937
00:32:25,850 --> 00:32:28,730
 get much more global structure and this

1938
00:32:27,130 --> 00:32:28,730
 

1939
00:32:27,140 --> 00:32:30,049
 isn't a specific thing a pixel RN and

1940
00:32:28,720 --> 00:32:30,049
 

1941
00:32:28,730 --> 00:32:32,240
 this is true of generative models in

1942
00:32:30,039 --> 00:32:32,240
 

1943
00:32:30,049 --> 00:32:34,160
 general and it comes back again to this

1944
00:32:32,230 --> 00:32:34,160
 

1945
00:32:32,240 --> 00:32:36,230
 issue of bits not being equal so it

1946
00:32:34,150 --> 00:32:36,230
 

1947
00:32:34,160 --> 00:32:37,970
 seems weird at first you're conditioning

1948
00:32:36,220 --> 00:32:37,970
 

1949
00:32:36,230 --> 00:32:41,960
 on something that is only you know a

1950
00:32:37,960 --> 00:32:41,960
 

1951
00:32:37,970 --> 00:32:43,549
 best log a thousand bits and suddenly

1952
00:32:41,950 --> 00:32:43,549
 

1953
00:32:41,960 --> 00:32:45,049
 your image looks so much better but of

1954
00:32:43,539 --> 00:32:45,049
 

1955
00:32:43,549 --> 00:32:46,580
 course the log-likelihood isn't going to

1956
00:32:45,039 --> 00:32:46,580
 

1957
00:32:45,049 --> 00:32:47,750
 get much better than log of thousand

1958
00:32:46,570 --> 00:32:47,750
 

1959
00:32:46,580 --> 00:32:49,100
 bits right there's there's only so much

1960
00:32:47,740 --> 00:32:49,100
 

1961
00:32:47,750 --> 00:32:50,150
 you know the information in is the

1962
00:32:49,090 --> 00:32:50,150
 

1963
00:32:49,100 --> 00:32:52,550
 information out so the log-likelihood

1964
00:32:50,140 --> 00:32:52,550
 

1965
00:32:50,150 --> 00:32:54,290
 doesn't change much the pics the

1966
00:32:52,540 --> 00:32:54,290
 

1967
00:32:52,550 --> 00:32:55,580
 degeneration the thing that seems like

1968
00:32:54,280 --> 00:32:55,580
 

1969
00:32:54,290 --> 00:32:56,870
 what we care about the high-level

1970
00:32:55,570 --> 00:32:56,870
 

1971
00:32:55,580 --> 00:32:58,040
 structure changes a lot

1972
00:32:56,860 --> 00:32:58,040
 

1973
00:32:56,870 --> 00:33:03,559
 this is often used as an argument

1974
00:32:58,030 --> 00:33:03,559
 

1975
00:32:58,040 --> 00:33:06,800
 against density modeling okay but we can

1976
00:33:03,549 --> 00:33:06,800
 

1977
00:33:03,559 --> 00:33:08,330
 go further than that and say well what

1978
00:33:06,790 --> 00:33:08,330
 

1979
00:33:06,800 --> 00:33:10,370
 happens if we show we've said there's

1980
00:33:08,320 --> 00:33:10,370
 

1981
00:33:08,330 --> 00:33:12,410
 this this this ordering is arbitrary

1982
00:33:10,360 --> 00:33:12,410
 

1983
00:33:10,370 --> 00:33:14,090
 this raster scan ordering what happens

1984
00:33:12,400 --> 00:33:14,090
 

1985
00:33:12,410 --> 00:33:15,440
 if we change it around right we were

1986
00:33:14,080 --> 00:33:15,440
 

1987
00:33:14,090 --> 00:33:17,110
 free to do this any way we want so

1988
00:33:15,430 --> 00:33:17,110
 

1989
00:33:15,440 --> 00:33:20,510
 there's a recent paper very recent paper

1990
00:33:17,100 --> 00:33:20,510
 

1991
00:33:17,110 --> 00:33:22,220
 on we're called subsample pixel networks

1992
00:33:20,500 --> 00:33:22,220
 

1993
00:33:20,510 --> 00:33:24,470
 where the basic idea is to take this

1994
00:33:22,210 --> 00:33:24,470
 

1995
00:33:22,220 --> 00:33:26,809
 this this Auto regressive image

1996
00:33:24,460 --> 00:33:26,809
 

1997
00:33:24,470 --> 00:33:28,550
 generator and change the order of the

1998
00:33:26,799 --> 00:33:28,550
 

1999
00:33:26,809 --> 00:33:30,500
 the auto regressive prediction so it's

2000
00:33:28,540 --> 00:33:30,500
 

2001
00:33:28,550 --> 00:33:33,860
 now you have this image is sliced up

2002
00:33:30,490 --> 00:33:33,860
 

2003
00:33:30,500 --> 00:33:34,850
 into kind of smaller like say if it's a

2004
00:33:33,850 --> 00:33:34,850
 

2005
00:33:33,860 --> 00:33:37,040
 hundred twenty eight hundred twenty

2006
00:33:34,840 --> 00:33:37,040
 

2007
00:33:34,850 --> 00:33:39,170
 image you cut it up into four 32 by 32

2008
00:33:37,030 --> 00:33:39,170
 

2009
00:33:37,040 --> 00:33:40,370
 slices you model each slice other

2010
00:33:39,160 --> 00:33:40,370
 

2011
00:33:39,170 --> 00:33:41,660
 aggressively and then you model the next

2012
00:33:40,360 --> 00:33:41,660
 

2013
00:33:40,370 --> 00:33:43,970
 slice conditioned on the previous one

2014
00:33:41,650 --> 00:33:43,970
 

2015
00:33:41,660 --> 00:33:45,350
 and so on and there are other tricks

2016
00:33:43,960 --> 00:33:45,350
 

2017
00:33:43,970 --> 00:33:48,170
 going on in here as well it's not just

2018
00:33:45,340 --> 00:33:48,170
 

2019
00:33:45,350 --> 00:33:50,360
 about reordering for example the the

2020
00:33:48,160 --> 00:33:50,360
 

2021
00:33:48,170 --> 00:33:52,640
 actual bits within each channel are also

2022
00:33:50,350 --> 00:33:52,640
 

2023
00:33:50,360 --> 00:33:54,890
 reordered so that the lower five bits

2024
00:33:52,630 --> 00:33:54,890
 

2025
00:33:52,640 --> 00:33:56,870
 are predicted first and then the the

2026
00:33:54,880 --> 00:33:56,870
 

2027
00:33:54,890 --> 00:33:58,490
 kind of high frequency bits are

2028
00:33:56,860 --> 00:33:58,490
 

2029
00:33:56,870 --> 00:34:00,410
 predicted at the end so there's a lot of

2030
00:33:58,480 --> 00:34:00,410
 

2031
00:33:58,490 --> 00:34:02,390
 clever kind of restructuring going on

2032
00:34:00,400 --> 00:34:02,390
 

2033
00:34:00,410 --> 00:34:04,580
 and there's a real jump in image quality

2034
00:34:02,380 --> 00:34:04,580
 

2035
00:34:02,390 --> 00:34:05,990
 so for example and also a jump in the

2036
00:34:04,570 --> 00:34:05,990
 

2037
00:34:04,580 --> 00:34:08,360
 size of images that you can feasibly

2038
00:34:05,980 --> 00:34:08,360
 

2039
00:34:05,990 --> 00:34:09,679
 model with current hardware so the

2040
00:34:08,350 --> 00:34:09,679
 

2041
00:34:08,360 --> 00:34:11,720
 original so has a kind of practical

2042
00:34:09,669 --> 00:34:11,720
 

2043
00:34:09,679 --> 00:34:14,090
 aspect so if we look at these generated

2044
00:34:11,710 --> 00:34:14,090
 

2045
00:34:11,720 --> 00:34:17,929
 faces from celeb a they're not at the

2046
00:34:14,080 --> 00:34:17,929
 

2047
00:34:14,090 --> 00:34:19,159
 level of Gann generated faces but

2048
00:34:17,919 --> 00:34:19,159
 

2049
00:34:17,929 --> 00:34:20,840
 they're getting to be pretty pretty

2050
00:34:19,149 --> 00:34:20,840
 

2051
00:34:19,159 --> 00:34:23,060
 convincing right I mean you can see that

2052
00:34:20,830 --> 00:34:23,060
 

2053
00:34:20,840 --> 00:34:26,750
 a few more iterations of this and we

2054
00:34:23,050 --> 00:34:26,750
 

2055
00:34:23,060 --> 00:34:28,340
 might have you know auto regressive you

2056
00:34:26,740 --> 00:34:28,340
 

2057
00:34:26,750 --> 00:34:30,139
 know just purely log-likelihood models

2058
00:34:28,330 --> 00:34:30,139
 

2059
00:34:28,340 --> 00:34:33,200
 generating things that images that we

2060
00:34:30,129 --> 00:34:33,200
 

2061
00:34:30,139 --> 00:34:35,300
 can't tell are not real and on image net

2062
00:34:33,190 --> 00:34:35,300
 

2063
00:34:33,200 --> 00:34:36,409
 its image that as a hardier database to

2064
00:34:35,290 --> 00:34:36,409
 

2065
00:34:35,300 --> 00:34:37,490
 generate from because it's much more

2066
00:34:36,399 --> 00:34:37,490
 

2067
00:34:36,409 --> 00:34:39,050
 general but you can really see that

2068
00:34:37,480 --> 00:34:39,050
 

2069
00:34:37,490 --> 00:34:41,560
 there's a lot more global structure

2070
00:34:39,040 --> 00:34:41,560
 

2071
00:34:39,050 --> 00:34:43,700
 going on here like this is clearly a dog

2072
00:34:41,550 --> 00:34:43,700
 

2073
00:34:41,560 --> 00:34:47,000
 you know in the middle at the top and so

2074
00:34:43,690 --> 00:34:47,000
 

2075
00:34:43,700 --> 00:34:48,710
 forth and you know even even video you

2076
00:34:46,990 --> 00:34:48,710
 

2077
00:34:47,000 --> 00:34:50,840
 can apply like this fully auto

2078
00:34:48,700 --> 00:34:50,840
 

2079
00:34:48,710 --> 00:34:51,679
 regressive approach to typically you're

2080
00:34:50,830 --> 00:34:51,679
 

2081
00:34:50,840 --> 00:34:53,659
 going to be looking at at the moment

2082
00:34:51,669 --> 00:34:53,659
 

2083
00:34:51,679 --> 00:34:55,250
 relatively low resolution videos because

2084
00:34:53,649 --> 00:34:55,250
 

2085
00:34:53,659 --> 00:34:58,820
 it you know it's pretty hard to model

2086
00:34:55,240 --> 00:34:58,820
 

2087
00:34:55,250 --> 00:35:00,440
 that much data okay so going back a bit

2088
00:34:58,810 --> 00:35:00,440
 

2089
00:34:58,820 --> 00:35:02,090
 in time you know an auto regressive

2090
00:35:00,430 --> 00:35:02,090
 

2091
00:35:00,440 --> 00:35:03,950
 model that's quite you know nostalgic

2092
00:35:02,080 --> 00:35:03,950
 

2093
00:35:02,090 --> 00:35:05,220
 for me was my own handwriting synthesis

2094
00:35:03,940 --> 00:35:05,220
 

2095
00:35:03,950 --> 00:35:07,050
 network where it

2096
00:35:05,210 --> 00:35:07,050
 

2097
00:35:05,220 --> 00:35:09,599
 the network was trained to generate

2098
00:35:07,040 --> 00:35:09,599
 

2099
00:35:07,050 --> 00:35:11,430
 people's handwriting one step at a time

2100
00:35:09,589 --> 00:35:11,430
 

2101
00:35:09,599 --> 00:35:12,990
 and what's kind of interesting about

2102
00:35:11,420 --> 00:35:12,990
 

2103
00:35:11,430 --> 00:35:15,180
 that is it going back to this debate

2104
00:35:12,980 --> 00:35:15,180
 

2105
00:35:12,990 --> 00:35:16,740
 about continuous versus discrete this

2106
00:35:15,170 --> 00:35:16,740
 

2107
00:35:15,180 --> 00:35:19,020
 really was a fully continuous model

2108
00:35:16,730 --> 00:35:19,020
 

2109
00:35:16,740 --> 00:35:21,960
 things weren't discretized rather there

2110
00:35:19,010 --> 00:35:21,960
 

2111
00:35:19,020 --> 00:35:24,660
 was a mixture density model at each time

2112
00:35:21,950 --> 00:35:24,660
 

2113
00:35:21,960 --> 00:35:26,910
 step that predict predicted the

2114
00:35:24,650 --> 00:35:26,910
 

2115
00:35:24,660 --> 00:35:30,569
 two-dimensional density like the XY

2116
00:35:26,900 --> 00:35:30,569
 

2117
00:35:26,910 --> 00:35:32,069
 coordinate of the next the neck where

2118
00:35:30,559 --> 00:35:32,069
 

2119
00:35:30,569 --> 00:35:33,720
 the next point would appear in the

2120
00:35:32,059 --> 00:35:33,720
 

2121
00:35:32,069 --> 00:35:36,000
 sequence so this data is recorded as a

2122
00:35:33,710 --> 00:35:36,000
 

2123
00:35:33,720 --> 00:35:37,859
 series of XY coordinates XY points as

2124
00:35:35,990 --> 00:35:37,859
 

2125
00:35:36,000 --> 00:35:39,240
 someone is writing with a pen and this

2126
00:35:37,849 --> 00:35:39,240
 

2127
00:35:37,859 --> 00:35:40,680
 thing shows what the density map looks

2128
00:35:39,230 --> 00:35:40,680
 

2129
00:35:39,240 --> 00:35:42,329
 like and you can see that it's very rich

2130
00:35:40,670 --> 00:35:42,329
 

2131
00:35:40,680 --> 00:35:44,310
 there's a lot of structure there there's

2132
00:35:42,319 --> 00:35:44,310
 

2133
00:35:42,329 --> 00:35:46,619
 a lot of covariance it's not just sort

2134
00:35:44,300 --> 00:35:46,619
 

2135
00:35:44,310 --> 00:35:48,450
 of dropping a you know finding the mean

2136
00:35:46,609 --> 00:35:48,450
 

2137
00:35:46,619 --> 00:35:50,010
 of the next thing it's really shaping

2138
00:35:48,440 --> 00:35:50,010
 

2139
00:35:48,450 --> 00:35:51,960
 the predictions and this bit at the

2140
00:35:50,000 --> 00:35:51,960
 

2141
00:35:50,010 --> 00:35:53,819
 bottom shows how it switches between

2142
00:35:51,950 --> 00:35:53,819
 

2143
00:35:51,960 --> 00:35:56,940
 different weights and in the component

2144
00:35:53,809 --> 00:35:56,940
 

2145
00:35:53,819 --> 00:35:58,530
 as it predicts them okay so moving on I

2146
00:35:56,930 --> 00:35:58,530
 

2147
00:35:56,940 --> 00:35:59,760
 mean that's all very well you know what

2148
00:35:58,520 --> 00:35:59,760
 

2149
00:35:58,530 --> 00:36:01,680
 we talked about so far is just like

2150
00:35:59,750 --> 00:36:01,680
 

2151
00:35:59,760 --> 00:36:03,450
 these auto regressive models at least

2152
00:36:01,670 --> 00:36:03,450
 

2153
00:36:01,680 --> 00:36:06,810
 give us a fighting chance of actually

2154
00:36:03,440 --> 00:36:06,810
 

2155
00:36:03,450 --> 00:36:09,660
 modeling unsupervised data you know in a

2156
00:36:06,800 --> 00:36:09,660
 

2157
00:36:06,810 --> 00:36:11,970
 reasonably coherent way but ultimately

2158
00:36:09,650 --> 00:36:11,970
 

2159
00:36:09,660 --> 00:36:13,560
 it's not about modeling the data it's

2160
00:36:11,960 --> 00:36:13,560
 

2161
00:36:11,970 --> 00:36:14,700
 about in some sense representing the

2162
00:36:13,550 --> 00:36:14,700
 

2163
00:36:13,560 --> 00:36:16,170
 data that's what we really care about

2164
00:36:14,690 --> 00:36:16,170
 

2165
00:36:14,700 --> 00:36:17,790
 before we care about is using

2166
00:36:16,160 --> 00:36:17,790
 

2167
00:36:16,170 --> 00:36:19,740
 unsupervised learning to generalize the

2168
00:36:17,780 --> 00:36:19,740
 

2169
00:36:17,790 --> 00:36:22,440
 new tasks its representations that we

2170
00:36:19,730 --> 00:36:22,440
 

2171
00:36:19,740 --> 00:36:24,180
 really care about and so the way I think

2172
00:36:22,430 --> 00:36:24,180
 

2173
00:36:22,440 --> 00:36:26,460
 about representation learning is as a

2174
00:36:24,170 --> 00:36:26,460
 

2175
00:36:24,180 --> 00:36:28,950
 language deep networks the way they work

2176
00:36:26,450 --> 00:36:28,950
 

2177
00:36:26,460 --> 00:36:31,410
 is that they learn complex and often

2178
00:36:28,940 --> 00:36:31,410
 

2179
00:36:28,950 --> 00:36:33,180
 hierarchical representations of the

2180
00:36:31,400 --> 00:36:33,180
 

2181
00:36:31,410 --> 00:36:35,369
 input data so each layer kind of Ari

2182
00:36:33,170 --> 00:36:35,369
 

2183
00:36:33,180 --> 00:36:36,900
 represents layer below and the way I

2184
00:36:35,359 --> 00:36:36,900
 

2185
00:36:35,369 --> 00:36:38,550
 think about this is a kind of internal

2186
00:36:36,890 --> 00:36:38,550
 

2187
00:36:36,900 --> 00:36:40,200
 language that the network is using to

2188
00:36:38,540 --> 00:36:40,200
 

2189
00:36:38,550 --> 00:36:41,940
 describe the data and this is what I

2190
00:36:40,190 --> 00:36:41,940
 

2191
00:36:40,200 --> 00:36:43,920
 think is really key here there's this

2192
00:36:41,930 --> 00:36:43,920
 

2193
00:36:41,940 --> 00:36:45,930
 sort of description going on here

2194
00:36:43,910 --> 00:36:45,930
 

2195
00:36:43,920 --> 00:36:47,849
 and this means that you can get a kind

2196
00:36:45,920 --> 00:36:47,849
 

2197
00:36:45,930 --> 00:36:49,800
 of functional language emerging from a

2198
00:36:47,839 --> 00:36:49,800
 

2199
00:36:47,849 --> 00:36:51,630
 relatively simple task like object

2200
00:36:49,790 --> 00:36:51,630
 

2201
00:36:49,800 --> 00:36:53,520
 recognition so if you have to recognize

2202
00:36:51,620 --> 00:36:53,520
 

2203
00:36:51,630 --> 00:36:55,619
 hats then you might have a language that

2204
00:36:53,510 --> 00:36:55,619
 

2205
00:36:53,520 --> 00:36:57,660
 does something like identify pointy ears

2206
00:36:55,609 --> 00:36:57,660
 

2207
00:36:55,619 --> 00:36:58,890
 and whiskers and a tail and if you put

2208
00:36:57,650 --> 00:36:58,890
 

2209
00:36:57,660 --> 00:37:00,599
 all those together then maybe you can

2210
00:36:58,880 --> 00:37:00,599
 

2211
00:36:58,890 --> 00:37:01,619
 say this is a cat and this is I think

2212
00:37:00,589 --> 00:37:01,619
 

2213
00:37:00,599 --> 00:37:05,670
 this is really interesting because it

2214
00:37:01,609 --> 00:37:05,670
 

2215
00:37:01,619 --> 00:37:07,589
 relates back to Vidkun Stein's kind of

2216
00:37:05,660 --> 00:37:07,589
 

2217
00:37:05,670 --> 00:37:08,790
 ideas about language games to be put

2218
00:37:07,579 --> 00:37:08,790
 

2219
00:37:07,589 --> 00:37:10,440
 forward in his philosophical

2220
00:37:08,780 --> 00:37:10,440
 

2221
00:37:08,790 --> 00:37:12,300
 investigations and you know a long time

2222
00:37:10,430 --> 00:37:12,300
 

2223
00:37:10,440 --> 00:37:13,859
 ago when was this 1930 something like

2224
00:37:12,290 --> 00:37:13,859
 

2225
00:37:12,300 --> 00:37:15,510
 that where you know he talked about

2226
00:37:13,849 --> 00:37:15,510
 

2227
00:37:13,859 --> 00:37:16,020
 language what he was kind of getting at

2228
00:37:15,500 --> 00:37:16,020
 

2229
00:37:15,510 --> 00:37:17,190
 was

2230
00:37:16,010 --> 00:37:17,190
 

2231
00:37:16,020 --> 00:37:18,540
 how do we have how can we have a

2232
00:37:17,180 --> 00:37:18,540
 

2233
00:37:17,190 --> 00:37:19,890
 language without the finding of

2234
00:37:18,530 --> 00:37:19,890
 

2235
00:37:18,540 --> 00:37:22,230
 vocabulary how did you get a language

2236
00:37:19,880 --> 00:37:22,230
 

2237
00:37:19,890 --> 00:37:24,210
 that emerges before there is a language

2238
00:37:22,220 --> 00:37:24,210
 

2239
00:37:22,230 --> 00:37:25,890
 with which to express it and what he

2240
00:37:24,200 --> 00:37:25,890
 

2241
00:37:24,210 --> 00:37:28,020
 kind of talked about was a you know just

2242
00:37:25,880 --> 00:37:28,020
 

2243
00:37:25,890 --> 00:37:29,640
 a language game being played between two

2244
00:37:28,010 --> 00:37:29,640
 

2245
00:37:28,020 --> 00:37:32,040
 people for some functional purpose so

2246
00:37:29,630 --> 00:37:32,040
 

2247
00:37:29,640 --> 00:37:33,390
 his example he had was two builders and

2248
00:37:32,030 --> 00:37:33,390
 

2249
00:37:32,040 --> 00:37:35,040
 they're collaborating on building and

2250
00:37:33,380 --> 00:37:35,040
 

2251
00:37:33,390 --> 00:37:37,620
 constructing a building they're gonna

2252
00:37:35,030 --> 00:37:37,620
 

2253
00:37:35,040 --> 00:37:39,840
 learn a language of bricks and blocks

2254
00:37:37,610 --> 00:37:39,840
 

2255
00:37:37,620 --> 00:37:41,190
 and pass me this beam and put that block

2256
00:37:39,830 --> 00:37:41,190
 

2257
00:37:39,840 --> 00:37:42,600
 down there because that's the language

2258
00:37:41,180 --> 00:37:42,600
 

2259
00:37:41,190 --> 00:37:45,300
 they need in order to carry out their

2260
00:37:42,590 --> 00:37:45,300
 

2261
00:37:42,600 --> 00:37:47,670
 tasks and so some of you may have seen

2262
00:37:45,290 --> 00:37:47,670
 

2263
00:37:45,300 --> 00:37:49,140
 these papers on distill by Christopher

2264
00:37:47,660 --> 00:37:49,140
 

2265
00:37:47,670 --> 00:37:50,460
 Ola and others if you haven't I really

2266
00:37:49,130 --> 00:37:50,460
 

2267
00:37:49,140 --> 00:37:52,860
 recommend looking they're quite

2268
00:37:50,450 --> 00:37:52,860
 

2269
00:37:50,460 --> 00:37:54,600
 beautiful and they do this amazingly

2270
00:37:52,850 --> 00:37:54,600
 

2271
00:37:52,860 --> 00:37:56,610
 detailed analysis of what is what is

2272
00:37:54,590 --> 00:37:56,610
 

2273
00:37:54,600 --> 00:37:58,500
 actually going on inside one of these

2274
00:37:56,600 --> 00:37:58,500
 

2275
00:37:56,610 --> 00:38:01,680
 great big convolutional networks that

2276
00:37:58,490 --> 00:38:01,680
 

2277
00:37:58,500 --> 00:38:03,960
 can classify imagenet or similar data

2278
00:38:01,670 --> 00:38:03,960
 

2279
00:38:01,680 --> 00:38:05,790
 sets and they find this kind of visual

2280
00:38:03,950 --> 00:38:05,790
 

2281
00:38:03,960 --> 00:38:07,080
 vocabulary is built up inside the

2282
00:38:05,780 --> 00:38:07,080
 

2283
00:38:05,790 --> 00:38:09,660
 network that starts at the bottom with

2284
00:38:07,070 --> 00:38:09,660
 

2285
00:38:07,080 --> 00:38:11,670
 really low-level things like edges and

2286
00:38:09,650 --> 00:38:11,670
 

2287
00:38:09,660 --> 00:38:12,930
 and orientations and builds up to more

2288
00:38:11,660 --> 00:38:12,930
 

2289
00:38:11,670 --> 00:38:15,120
 like what look more like textures

2290
00:38:12,920 --> 00:38:15,120
 

2291
00:38:12,930 --> 00:38:18,150
 patterns and then these composite things

2292
00:38:15,110 --> 00:38:18,150
 

2293
00:38:15,120 --> 00:38:19,830
 like eyes bookshelves texts rivets birds

2294
00:38:18,140 --> 00:38:19,830
 

2295
00:38:18,150 --> 00:38:21,930
 builds them up even further into

2296
00:38:19,820 --> 00:38:21,930
 

2297
00:38:19,830 --> 00:38:26,460
 complete objects houses dogs wheels so

2298
00:38:21,920 --> 00:38:26,460
 

2299
00:38:21,930 --> 00:38:30,030
 forth and so I mean one thing to take

2300
00:38:26,450 --> 00:38:30,030
 

2301
00:38:26,460 --> 00:38:31,290
 from that is actually you know even

2302
00:38:30,020 --> 00:38:31,290
 

2303
00:38:30,030 --> 00:38:32,970
 though even though supervised learning

2304
00:38:31,280 --> 00:38:32,970
 

2305
00:38:31,290 --> 00:38:35,910
 is very limited in the number of bits

2306
00:38:32,960 --> 00:38:35,910
 

2307
00:38:32,970 --> 00:38:37,470
 we're feeding into the system it's

2308
00:38:35,900 --> 00:38:37,470
 

2309
00:38:35,910 --> 00:38:39,120
 learning something very rich from that

2310
00:38:37,460 --> 00:38:39,120
 

2311
00:38:37,470 --> 00:38:41,520
 right it's learning quite a

2312
00:38:39,110 --> 00:38:41,520
 

2313
00:38:39,120 --> 00:38:43,380
 sophisticated representation then the

2314
00:38:41,510 --> 00:38:43,380
 

2315
00:38:41,520 --> 00:38:44,820
 question is well do we expect on

2316
00:38:43,370 --> 00:38:44,820
 

2317
00:38:43,380 --> 00:38:46,710
 supervised representations to do better

2318
00:38:44,810 --> 00:38:46,710
 

2319
00:38:44,820 --> 00:38:48,960
 than that and you know the answer has to

2320
00:38:46,700 --> 00:38:48,960
 

2321
00:38:46,710 --> 00:38:51,300
 be yes we do hope they would be so you

2322
00:38:48,950 --> 00:38:51,300
 

2323
00:38:48,960 --> 00:38:52,620
 know ultimately you can get a lot out of

2324
00:38:51,290 --> 00:38:52,620
 

2325
00:38:51,300 --> 00:38:54,180
 just image recognition but of course

2326
00:38:52,610 --> 00:38:54,180
 

2327
00:38:52,620 --> 00:38:55,950
 it's always limited by the requirements

2328
00:38:54,170 --> 00:38:55,950
 

2329
00:38:54,180 --> 00:38:57,300
 of the task so there's there's no need

2330
00:38:55,940 --> 00:38:57,300
 

2331
00:38:55,950 --> 00:38:59,220
 if all you ever have to do is recognize

2332
00:38:57,290 --> 00:38:59,220
 

2333
00:38:57,300 --> 00:39:00,960
 objects you probably don't need to

2334
00:38:59,210 --> 00:39:00,960
 

2335
00:38:59,220 --> 00:39:02,700
 internalize the laws of physics for

2336
00:39:00,950 --> 00:39:02,700
 

2337
00:39:00,960 --> 00:39:04,290
 example you don't need to learn all that

2338
00:39:02,690 --> 00:39:04,290
 

2339
00:39:02,700 --> 00:39:06,240
 much about the world you just need to

2340
00:39:04,280 --> 00:39:06,240
 

2341
00:39:04,290 --> 00:39:08,700
 learn you know whatever it is that will

2342
00:39:06,230 --> 00:39:08,700
 

2343
00:39:06,240 --> 00:39:10,860
 help you to solve a specific problem and

2344
00:39:08,690 --> 00:39:10,860
 

2345
00:39:08,700 --> 00:39:12,030
 that the sort of guiding principle I

2346
00:39:10,850 --> 00:39:12,030
 

2347
00:39:10,860 --> 00:39:13,080
 guess was behind unsupervised

2348
00:39:12,020 --> 00:39:13,080
 

2349
00:39:12,030 --> 00:39:14,820
 representation learning is that the

2350
00:39:13,070 --> 00:39:14,820
 

2351
00:39:13,080 --> 00:39:16,950
 representation should be more general

2352
00:39:14,810 --> 00:39:16,950
 

2353
00:39:14,820 --> 00:39:19,410
 right anything that helps you to model

2354
00:39:16,940 --> 00:39:19,410
 

2355
00:39:16,950 --> 00:39:22,180
 or predict something about the world

2356
00:39:19,400 --> 00:39:22,180
 

2357
00:39:19,410 --> 00:39:24,250
 should be worth representing in your

2358
00:39:22,170 --> 00:39:24,250
 

2359
00:39:22,180 --> 00:39:25,990
 in your unsupervised system and that

2360
00:39:24,240 --> 00:39:25,990
 

2361
00:39:24,250 --> 00:39:29,380
 should help you generalize that's the

2362
00:39:25,980 --> 00:39:29,380
 

2363
00:39:25,990 --> 00:39:31,059
 theory and so as I said what we really

2364
00:39:29,370 --> 00:39:31,059
 

2365
00:39:29,380 --> 00:39:32,589
 want is a neural network that describes

2366
00:39:31,049 --> 00:39:32,589
 

2367
00:39:31,059 --> 00:39:34,380
 data to us so you think about these you

2368
00:39:32,579 --> 00:39:34,380
 

2369
00:39:32,589 --> 00:39:37,420
 know image captioning experiments where

2370
00:39:34,370 --> 00:39:37,420
 

2371
00:39:34,380 --> 00:39:39,130
 people took images and and automatically

2372
00:39:37,410 --> 00:39:39,130
 

2373
00:39:37,420 --> 00:39:41,500
 you know train them to map from an image

2374
00:39:39,120 --> 00:39:41,500
 

2375
00:39:39,130 --> 00:39:43,839
 to a caption like a man is standing on a

2376
00:39:41,490 --> 00:39:43,839
 

2377
00:39:41,500 --> 00:39:45,220
 train platform or something like that we

2378
00:39:43,829 --> 00:39:45,220
 

2379
00:39:43,839 --> 00:39:48,430
 kind of want the network to be able to

2380
00:39:45,210 --> 00:39:48,430
 

2381
00:39:45,220 --> 00:39:49,930
 do that for us for a wide range of data

2382
00:39:48,420 --> 00:39:49,930
 

2383
00:39:48,430 --> 00:39:51,849
 and we want to get these descriptions

2384
00:39:49,920 --> 00:39:51,849
 

2385
00:39:49,930 --> 00:39:53,289
 out of the network in some sense or at

2386
00:39:51,839 --> 00:39:53,289
 

2387
00:39:51,849 --> 00:39:55,809
 least oh really

2388
00:39:53,279 --> 00:39:55,809
 

2389
00:39:53,289 --> 00:39:58,289
 sorry I saw I've taken a lot longer than

2390
00:39:55,799 --> 00:39:58,289
 

2391
00:39:55,809 --> 00:40:02,339
 I thought I'm gonna have to cut my

2392
00:39:58,279 --> 00:40:02,339
 

2393
00:39:58,289 --> 00:40:02,339
 presentation a little bit short here

2394
00:40:03,230 --> 00:40:03,230
 

2395
00:40:03,240 --> 00:40:10,240
 maybe we'll we'll run over a little bit

2396
00:40:06,059 --> 00:40:10,240
 

2397
00:40:06,069 --> 00:40:11,799
 into the question time okay so we want

2398
00:40:10,230 --> 00:40:11,799
 

2399
00:40:10,240 --> 00:40:14,920
 to get these descriptions out how can we

2400
00:40:11,789 --> 00:40:14,920
 

2401
00:40:11,799 --> 00:40:17,440
 do that one way that's been explored in

2402
00:40:14,910 --> 00:40:17,440
 

2403
00:40:14,920 --> 00:40:19,089
 the past is an auto encoder so we encode

2404
00:40:17,430 --> 00:40:19,089
 

2405
00:40:17,440 --> 00:40:20,980
 the data we put it into some compressed

2406
00:40:19,079 --> 00:40:20,980
 

2407
00:40:19,089 --> 00:40:23,319
 latent representation and then we decode

2408
00:40:20,970 --> 00:40:23,319
 

2409
00:40:20,980 --> 00:40:25,240
 and the idea is that the act of kind of

2410
00:40:23,309 --> 00:40:25,240
 

2411
00:40:23,319 --> 00:40:31,240
 squeezing this latent representation

2412
00:40:25,230 --> 00:40:31,240
 

2413
00:40:25,240 --> 00:40:33,789
 will enforce usable and kind of

2414
00:40:31,230 --> 00:40:33,789
 

2415
00:40:31,240 --> 00:40:37,720
 accessible representations that you can

2416
00:40:33,779 --> 00:40:37,720
 

2417
00:40:33,789 --> 00:40:39,430
 reuse elsewhere now a sort of

2418
00:40:37,710 --> 00:40:39,430
 

2419
00:40:37,720 --> 00:40:41,170
 generalization of that is to instead of

2420
00:40:39,420 --> 00:40:41,170
 

2421
00:40:39,430 --> 00:40:42,670
 just you know compressing this

2422
00:40:41,160 --> 00:40:42,670
 

2423
00:40:41,170 --> 00:40:46,500
 bottleneck for example by making it

2424
00:40:42,660 --> 00:40:46,500
 

2425
00:40:42,670 --> 00:40:49,150
 smaller using fewer hidden units you can

2426
00:40:46,490 --> 00:40:49,150
 

2427
00:40:46,500 --> 00:40:50,859
 do take a sort of information theoretic

2428
00:40:49,140 --> 00:40:50,859
 

2429
00:40:49,150 --> 00:40:54,039
 approach and look at what the coding

2430
00:40:50,849 --> 00:40:54,039
 

2431
00:40:50,859 --> 00:40:58,329
 cost is of representing the latent

2432
00:40:54,029 --> 00:40:58,329
 

2433
00:40:54,039 --> 00:40:59,799
 information and I think I'm really

2434
00:40:58,319 --> 00:40:59,799
 

2435
00:40:58,329 --> 00:41:05,529
 unfortunate I'm gonna have to skip

2436
00:40:59,789 --> 00:41:05,529
 

2437
00:40:59,799 --> 00:41:06,849
 through a lot of this maybe I can give

2438
00:41:05,519 --> 00:41:06,849
 

2439
00:41:05,529 --> 00:41:08,770
 you just a little taste there here of

2440
00:41:06,839 --> 00:41:08,770
 

2441
00:41:06,849 --> 00:41:10,319
 some of some of the so associative

2442
00:41:08,760 --> 00:41:10,319
 

2443
00:41:08,770 --> 00:41:12,400
 compression networks are a type of

2444
00:41:10,309 --> 00:41:12,400
 

2445
00:41:10,319 --> 00:41:15,279
 network that I've been working on

2446
00:41:12,390 --> 00:41:15,279
 

2447
00:41:12,400 --> 00:41:17,410
 recently and they're a way of kind of

2448
00:41:15,269 --> 00:41:17,410
 

2449
00:41:15,279 --> 00:41:19,000
 escaping from a well known problem with

2450
00:41:17,400 --> 00:41:19,000
 

2451
00:41:17,410 --> 00:41:20,410
 variation with variational encoders

2452
00:41:18,990 --> 00:41:20,410
 

2453
00:41:19,000 --> 00:41:22,990
 which is that the codes when you make

2454
00:41:20,400 --> 00:41:22,990
 

2455
00:41:20,410 --> 00:41:25,299
 the decoders powerful the decodes the

2456
00:41:22,980 --> 00:41:25,299
 

2457
00:41:22,990 --> 00:41:27,779
 codes tend to get ignored the decoder

2458
00:41:25,289 --> 00:41:27,779
 

2459
00:41:25,299 --> 00:41:29,950
 just tends to take over and very

2460
00:41:27,769 --> 00:41:29,950
 

2461
00:41:27,779 --> 00:41:32,710
 associative compression networks get

2462
00:41:29,940 --> 00:41:32,710
 

2463
00:41:29,950 --> 00:41:33,819
 away from that by allowing the priors

2464
00:41:32,700 --> 00:41:33,819
 

2465
00:41:32,710 --> 00:41:35,739
 for the codes to be

2466
00:41:33,809 --> 00:41:35,739
 

2467
00:41:33,819 --> 00:41:37,509
 conditioned on similar codes and things

2468
00:41:35,729 --> 00:41:37,509
 

2469
00:41:35,739 --> 00:41:38,949
 that are nearby and latent space you get

2470
00:41:37,499 --> 00:41:38,949
 

2471
00:41:37,509 --> 00:41:41,349
 this kind of natural clustering and

2472
00:41:38,939 --> 00:41:41,349
 

2473
00:41:38,949 --> 00:41:43,809
 laking space of similar codes and the

2474
00:41:41,339 --> 00:41:43,809
 

2475
00:41:41,349 --> 00:41:47,890
 you know the high-level idea behind this

2476
00:41:43,799 --> 00:41:47,890
 

2477
00:41:43,809 --> 00:41:49,449
 is that instead of learning and instead

2478
00:41:47,880 --> 00:41:49,449
 

2479
00:41:47,890 --> 00:41:50,739
 of learning individual data points what

2480
00:41:49,439 --> 00:41:50,739
 

2481
00:41:49,449 --> 00:41:52,089
 you're doing here is you're learning the

2482
00:41:50,729 --> 00:41:52,089
 

2483
00:41:50,739 --> 00:41:53,829
 entire data set you're looking

2484
00:41:52,079 --> 00:41:53,829
 

2485
00:41:52,089 --> 00:41:56,289
 holistically at the structure of the

2486
00:41:53,819 --> 00:41:56,289
 

2487
00:41:53,829 --> 00:41:58,299
 data set and saying how can I represent

2488
00:41:56,279 --> 00:41:58,299
 

2489
00:41:56,289 --> 00:41:59,799
 what's in there and so the idea is that

2490
00:41:58,289 --> 00:41:59,799
 

2491
00:41:58,299 --> 00:42:01,719
 you the high level information that you

2492
00:41:59,789 --> 00:42:01,719
 

2493
00:41:59,799 --> 00:42:03,309
 get out should organize the low-level

2494
00:42:01,709 --> 00:42:03,309
 

2495
00:42:01,719 --> 00:42:06,430
 data rather than just individually

2496
00:42:03,299 --> 00:42:06,430
 

2497
00:42:03,309 --> 00:42:08,920
 annotating it and you'll have to look at

2498
00:42:06,420 --> 00:42:08,920
 

2499
00:42:06,430 --> 00:42:12,459
 the paper for the details but if we look

2500
00:42:08,910 --> 00:42:12,459
 

2501
00:42:08,920 --> 00:42:14,049
 say this is the set of reconstructions

2502
00:42:12,449 --> 00:42:14,049
 

2503
00:42:12,459 --> 00:42:16,900
 given by an associative compression

2504
00:42:14,039 --> 00:42:16,900
 

2505
00:42:14,049 --> 00:42:19,420
 network so obviously the quality here

2506
00:42:16,890 --> 00:42:19,420
 

2507
00:42:16,900 --> 00:42:21,849
 again is you know is not so high that's

2508
00:42:19,410 --> 00:42:21,849
 

2509
00:42:19,420 --> 00:42:23,739
 mostly a function of the decoder and of

2510
00:42:21,839 --> 00:42:23,739
 

2511
00:42:21,849 --> 00:42:25,599
 the fact that we went down to sort of

2512
00:42:23,729 --> 00:42:25,599
 

2513
00:42:23,739 --> 00:42:28,089
 very small images just for you know

2514
00:42:25,589 --> 00:42:28,089
 

2515
00:42:25,599 --> 00:42:30,279
 computational purposes at 32 by 32 but

2516
00:42:28,079 --> 00:42:30,279
 

2517
00:42:28,089 --> 00:42:31,809
 the idea the real the the point here was

2518
00:42:30,269 --> 00:42:31,809
 

2519
00:42:30,279 --> 00:42:33,969
 about the representations and not about

2520
00:42:31,799 --> 00:42:33,969
 

2521
00:42:31,809 --> 00:42:35,529
 the image quality so what's going on

2522
00:42:33,959 --> 00:42:35,529
 

2523
00:42:33,969 --> 00:42:37,269
 here well if you look let's say these

2524
00:42:35,519 --> 00:42:37,269
 

2525
00:42:35,529 --> 00:42:39,279
 images on the leftmost column are the

2526
00:42:37,259 --> 00:42:39,279
 

2527
00:42:37,269 --> 00:42:40,509
 originals the real images and then the

2528
00:42:39,269 --> 00:42:40,509
 

2529
00:42:39,279 --> 00:42:42,130
 ones to the right of that are the ones

2530
00:42:40,499 --> 00:42:42,130
 

2531
00:42:40,509 --> 00:42:44,739
 regenerated from that image so let's say

2532
00:42:42,120 --> 00:42:44,739
 

2533
00:42:42,130 --> 00:42:46,059
 look at this second one from the top you

2534
00:42:44,729 --> 00:42:46,059
 

2535
00:42:44,739 --> 00:42:48,099
 could say well what's in the latent

2536
00:42:46,049 --> 00:42:48,099
 

2537
00:42:46,059 --> 00:42:50,349
 representation here what it seems is

2538
00:42:48,089 --> 00:42:50,349
 

2539
00:42:48,099 --> 00:42:51,640
 that the network has describes I just

2540
00:42:50,339 --> 00:42:51,640
 

2541
00:42:50,349 --> 00:42:53,650
 said we want this kind of caption or a

2542
00:42:51,630 --> 00:42:53,650
 

2543
00:42:51,640 --> 00:42:56,319
 description of the image and the caption

2544
00:42:53,640 --> 00:42:56,319
 

2545
00:42:53,650 --> 00:42:57,699
 here could be a dark-haired man with

2546
00:42:56,309 --> 00:42:57,699
 

2547
00:42:56,319 --> 00:42:59,440
 stubble is looking straight at the

2548
00:42:57,689 --> 00:42:59,440
 

2549
00:42:57,699 --> 00:43:01,449
 camera and smiling on a light-colored

2550
00:42:59,430 --> 00:43:01,449
 

2551
00:42:59,440 --> 00:43:02,890
 background and then it's handed that

2552
00:43:01,439 --> 00:43:02,890
 

2553
00:43:01,449 --> 00:43:04,779
 description off to the decoder and said

2554
00:43:02,880 --> 00:43:04,779
 

2555
00:43:02,890 --> 00:43:06,459
 okay now create a bunch of images

2556
00:43:04,769 --> 00:43:06,459
 

2557
00:43:04,779 --> 00:43:07,569
 matching that description and it's more

2558
00:43:06,449 --> 00:43:07,569
 

2559
00:43:06,459 --> 00:43:09,069
 or less than that I mean you know

2560
00:43:07,559 --> 00:43:09,069
 

2561
00:43:07,569 --> 00:43:10,479
 they're not perfect but it's generally

2562
00:43:09,059 --> 00:43:10,479
 

2563
00:43:09,069 --> 00:43:12,880
 capturing that they all look male

2564
00:43:10,469 --> 00:43:12,880
 

2565
00:43:10,479 --> 00:43:14,380
 they're generally smiling the pose is

2566
00:43:12,870 --> 00:43:14,380
 

2567
00:43:12,880 --> 00:43:17,279
 kind of straight on and so forth so it's

2568
00:43:14,370 --> 00:43:17,279
 

2569
00:43:14,380 --> 00:43:19,539
 clearly capturing these high level

2570
00:43:17,269 --> 00:43:19,539
 

2571
00:43:17,279 --> 00:43:20,799
 properties of the data that we wanted to

2572
00:43:19,529 --> 00:43:20,799
 

2573
00:43:19,539 --> 00:43:22,509
 capture and you can see this also from

2574
00:43:20,789 --> 00:43:22,509
 

2575
00:43:20,799 --> 00:43:24,249
 the reconstructions and actually you can

2576
00:43:22,499 --> 00:43:24,249
 

2577
00:43:22,509 --> 00:43:27,039
 also see it from from classification

2578
00:43:24,239 --> 00:43:27,039
 

2579
00:43:24,249 --> 00:43:28,539
 results so this is you know an ongoing

2580
00:43:27,029 --> 00:43:28,539
 

2581
00:43:27,039 --> 00:43:32,440
 thing with representation learning is

2582
00:43:28,529 --> 00:43:32,440
 

2583
00:43:28,539 --> 00:43:34,539
 how do you how do you evaluate how good

2584
00:43:32,430 --> 00:43:34,539
 

2585
00:43:32,440 --> 00:43:35,559
 your representations are and often what

2586
00:43:34,529 --> 00:43:35,559
 

2587
00:43:34,539 --> 00:43:37,089
 you do is you look at some sort of

2588
00:43:35,549 --> 00:43:37,089
 

2589
00:43:35,559 --> 00:43:39,339
 downstream past if we took this

2590
00:43:37,079 --> 00:43:39,339
 

2591
00:43:37,089 --> 00:43:40,749
 representation fed it to a simple

2592
00:43:39,329 --> 00:43:40,749
 

2593
00:43:39,339 --> 00:43:42,860
 classifier could it tell us for example

2594
00:43:40,739 --> 00:43:42,860
 

2595
00:43:40,749 --> 00:43:45,200
 which M this digit we're looking at and

2596
00:43:42,850 --> 00:43:45,200
 

2597
00:43:42,860 --> 00:43:47,420
 yes it can okay so I'm gonna have to

2598
00:43:45,190 --> 00:43:47,420
 

2599
00:43:45,200 --> 00:43:49,160
 skip a lot of stuff here Oh

2600
00:43:47,410 --> 00:43:49,160
 

2601
00:43:47,420 --> 00:43:51,530
 one thing I'd like to say is that so

2602
00:43:49,150 --> 00:43:51,530
 

2603
00:43:49,160 --> 00:43:53,120
 mutual information is a really vital

2604
00:43:51,520 --> 00:43:53,120
 

2605
00:43:51,530 --> 00:43:54,260
 concept when it comes to representation

2606
00:43:53,110 --> 00:43:54,260
 

2607
00:43:53,120 --> 00:43:56,000
 learning as I've said we want codes that

2608
00:43:54,250 --> 00:43:56,000
 

2609
00:43:54,260 --> 00:43:57,590
 describe the data well and

2610
00:43:55,990 --> 00:43:57,590
 

2611
00:43:56,000 --> 00:43:59,030
 mathematically speaking what that means

2612
00:43:57,580 --> 00:43:59,030
 

2613
00:43:57,590 --> 00:44:00,800
 is we want to maximize the mutual

2614
00:43:59,020 --> 00:44:00,800
 

2615
00:43:59,030 --> 00:44:03,590
 information between the code and the

2616
00:44:00,790 --> 00:44:03,590
 

2617
00:44:00,800 --> 00:44:05,300
 data and so and also encoder a

2618
00:44:03,580 --> 00:44:05,300
 

2619
00:44:03,590 --> 00:44:06,830
 variational encoder or these associated

2620
00:44:05,290 --> 00:44:06,830
 

2621
00:44:05,300 --> 00:44:08,660
 professionals they all actually do this

2622
00:44:06,820 --> 00:44:08,660
 

2623
00:44:06,830 --> 00:44:11,510
 implicitly because the difference

2624
00:44:08,650 --> 00:44:11,510
 

2625
00:44:08,660 --> 00:44:13,700
 between decoding the data using a sample

2626
00:44:11,500 --> 00:44:13,700
 

2627
00:44:11,510 --> 00:44:15,980
 from the latent space a latent code and

2628
00:44:13,690 --> 00:44:15,980
 

2629
00:44:13,700 --> 00:44:18,980
 optimally like the best possible

2630
00:44:15,970 --> 00:44:18,980
 

2631
00:44:15,980 --> 00:44:20,540
 decoding you can do without that latent

2632
00:44:18,970 --> 00:44:20,540
 

2633
00:44:18,980 --> 00:44:21,920
 sample is a lower bound on the mutual

2634
00:44:20,530 --> 00:44:21,920
 

2635
00:44:20,540 --> 00:44:23,360
 information which means that all the

2636
00:44:21,910 --> 00:44:23,360
 

2637
00:44:21,920 --> 00:44:25,400
 time you're minimizing the

2638
00:44:23,350 --> 00:44:25,400
 

2639
00:44:23,360 --> 00:44:27,170
 reconstruction costs you're pushing up

2640
00:44:25,390 --> 00:44:27,170
 

2641
00:44:25,400 --> 00:44:30,140
 this lower bound on the mutual

2642
00:44:27,160 --> 00:44:30,140
 

2643
00:44:27,170 --> 00:44:31,820
 information but it seems like a kind of

2644
00:44:30,130 --> 00:44:31,820
 

2645
00:44:30,140 --> 00:44:33,710
 a sledgehammer approach right they've

2646
00:44:31,810 --> 00:44:33,710
 

2647
00:44:31,820 --> 00:44:35,120
 all we want is the codes then doing all

2648
00:44:33,700 --> 00:44:35,120
 

2649
00:44:33,710 --> 00:44:36,290
 this work the decode seems very

2650
00:44:35,110 --> 00:44:36,290
 

2651
00:44:35,120 --> 00:44:37,580
 expensive all we're trying to do is

2652
00:44:36,280 --> 00:44:37,580
 

2653
00:44:36,290 --> 00:44:40,580
 maximize mutual information and are

2654
00:44:37,570 --> 00:44:40,580
 

2655
00:44:37,580 --> 00:44:42,170
 there other ways to do this and there's

2656
00:44:40,570 --> 00:44:42,170
 

2657
00:44:40,580 --> 00:44:43,580
 a recent paper representation learning

2658
00:44:42,160 --> 00:44:43,580
 

2659
00:44:42,170 --> 00:44:44,930
 with contrast the predictive coding has

2660
00:44:43,570 --> 00:44:44,930
 

2661
00:44:43,580 --> 00:44:46,700
 a very clever way of doing this so

2662
00:44:44,920 --> 00:44:46,700
 

2663
00:44:44,930 --> 00:44:49,460
 instead of in this situation here you

2664
00:44:46,690 --> 00:44:49,460
 

2665
00:44:46,700 --> 00:44:51,290
 got some audio signal a code C from some

2666
00:44:49,450 --> 00:44:51,290
 

2667
00:44:49,460 --> 00:44:52,730
 part of the signal rather than doing

2668
00:44:51,280 --> 00:44:52,730
 

2669
00:44:51,290 --> 00:44:54,950
 what an auto encoder would do in

2670
00:44:52,720 --> 00:44:54,950
 

2671
00:44:52,730 --> 00:44:57,020
 attempting to decode and so I should say

2672
00:44:54,940 --> 00:44:57,020
 

2673
00:44:54,950 --> 00:44:59,330
 unlike an auto encoder here this system

2674
00:44:57,010 --> 00:44:59,330
 

2675
00:44:57,020 --> 00:45:00,680
 is predictive so rather than passing

2676
00:44:59,320 --> 00:45:00,680
 

2677
00:44:59,330 --> 00:45:02,990
 through a bottleneck and reconstructing

2678
00:45:00,670 --> 00:45:02,990
 

2679
00:45:00,680 --> 00:45:05,090
 the same data is taking codes and using

2680
00:45:02,980 --> 00:45:05,090
 

2681
00:45:02,990 --> 00:45:06,950
 them to predict future data but rather

2682
00:45:05,080 --> 00:45:06,950
 

2683
00:45:05,090 --> 00:45:10,340
 than doing this prediction by exactly

2684
00:45:06,940 --> 00:45:10,340
 

2685
00:45:06,950 --> 00:45:12,380
 reconstruction it attempts to find two

2686
00:45:10,330 --> 00:45:12,380
 

2687
00:45:10,340 --> 00:45:14,240
 codes and maximize the mutual

2688
00:45:12,370 --> 00:45:14,240
 

2689
00:45:12,380 --> 00:45:16,190
 information between them in a more

2690
00:45:14,230 --> 00:45:16,190
 

2691
00:45:14,240 --> 00:45:17,810
 direct way and okay I won't go through

2692
00:45:16,180 --> 00:45:17,810
 

2693
00:45:16,190 --> 00:45:19,130
 the details there's no time but it

2694
00:45:17,800 --> 00:45:19,130
 

2695
00:45:17,810 --> 00:45:21,620
 basically uses this idea of noise

2696
00:45:19,120 --> 00:45:21,620
 

2697
00:45:19,130 --> 00:45:23,990
 contrast of estimation which is given

2698
00:45:21,610 --> 00:45:23,990
 

2699
00:45:21,620 --> 00:45:26,570
 this code C and this code said take a

2700
00:45:23,980 --> 00:45:26,570
 

2701
00:45:23,990 --> 00:45:28,790
 whole bunch of other codes that could

2702
00:45:26,560 --> 00:45:28,790
 

2703
00:45:26,570 --> 00:45:30,320
 have been said instead and in practice

2704
00:45:28,780 --> 00:45:30,320
 

2705
00:45:28,790 --> 00:45:32,180
 what you do is you have a big mini batch

2706
00:45:30,310 --> 00:45:32,180
 

2707
00:45:30,320 --> 00:45:33,680
 over training examples and you take

2708
00:45:32,170 --> 00:45:33,680
 

2709
00:45:32,180 --> 00:45:35,140
 those ads from the other elements of the

2710
00:45:33,670 --> 00:45:35,140
 

2711
00:45:33,680 --> 00:45:38,150
 batch like the things that were not

2712
00:45:35,130 --> 00:45:38,150
 

2713
00:45:35,140 --> 00:45:40,220
 correlated with C but rather came from

2714
00:45:38,140 --> 00:45:40,220
 

2715
00:45:38,150 --> 00:45:42,530
 some other audio sequence and you say

2716
00:45:40,210 --> 00:45:42,530
 

2717
00:45:40,220 --> 00:45:43,790
 how well can you distinguish this next

2718
00:45:42,520 --> 00:45:43,790
 

2719
00:45:42,530 --> 00:45:45,230
 code from the previous ones

2720
00:45:43,780 --> 00:45:45,230
 

2721
00:45:43,790 --> 00:45:47,090
 can you can you predict what it is

2722
00:45:45,220 --> 00:45:47,090
 

2723
00:45:45,230 --> 00:45:49,760
 against this background of negative

2724
00:45:47,080 --> 00:45:49,760
 

2725
00:45:47,090 --> 00:45:51,500
 examples and this you have to take it on

2726
00:45:49,750 --> 00:45:51,500
 

2727
00:45:49,760 --> 00:45:52,760
 faith here but this this kind of gives

2728
00:45:51,490 --> 00:45:52,760
 

2729
00:45:51,500 --> 00:45:54,500
 you another bound of mutual information

2730
00:45:52,750 --> 00:45:54,500
 

2731
00:45:52,760 --> 00:45:56,330
 and pushes up this this

2732
00:45:54,490 --> 00:45:56,330
 

2733
00:45:54,500 --> 00:46:00,890
 this descriptiveness of the codes about

2734
00:45:56,320 --> 00:46:00,890
 

2735
00:45:56,330 --> 00:46:02,870
 the data and crucially the further ahead

2736
00:46:00,880 --> 00:46:02,870
 

2737
00:46:00,890 --> 00:46:04,780
 you predict and the larger window you

2738
00:46:02,860 --> 00:46:04,780
 

2739
00:46:02,870 --> 00:46:07,430
 predict with these contrastive

2740
00:46:04,770 --> 00:46:07,430
 

2741
00:46:04,780 --> 00:46:08,870
 predictions the slower varying the

2742
00:46:07,420 --> 00:46:08,870
 

2743
00:46:07,430 --> 00:46:10,280
 higher level features you're looking at

2744
00:46:08,860 --> 00:46:10,280
 

2745
00:46:08,870 --> 00:46:11,780
 you're going from maybe something low

2746
00:46:10,270 --> 00:46:11,780
 

2747
00:46:10,280 --> 00:46:13,550
 level like phonemes to something higher

2748
00:46:11,770 --> 00:46:13,550
 

2749
00:46:11,780 --> 00:46:15,620
 level like words or prosody or something

2750
00:46:13,540 --> 00:46:15,620
 

2751
00:46:13,550 --> 00:46:17,450
 like that and the results are very good

2752
00:46:15,610 --> 00:46:17,450
 

2753
00:46:15,620 --> 00:46:19,040
 in practice you get you know really

2754
00:46:17,440 --> 00:46:19,040
 

2755
00:46:17,450 --> 00:46:21,020
 quite incredible results for things like

2756
00:46:19,030 --> 00:46:21,020
 

2757
00:46:19,040 --> 00:46:22,880
 it's considering that this is completely

2758
00:46:21,010 --> 00:46:22,880
 

2759
00:46:21,020 --> 00:46:24,530
 unsupervised and then there's just a you

2760
00:46:22,870 --> 00:46:24,530
 

2761
00:46:22,880 --> 00:46:25,880
 know a linear classifier dropped on top

2762
00:46:24,520 --> 00:46:25,880
 

2763
00:46:24,530 --> 00:46:27,440
 afterwards and you're getting

2764
00:46:25,870 --> 00:46:27,440
 

2765
00:46:25,880 --> 00:46:29,120
 classification rates that aren't that

2766
00:46:27,430 --> 00:46:29,120
 

2767
00:46:27,440 --> 00:46:31,310
 far off you know a fully supervised

2768
00:46:29,110 --> 00:46:31,310
 

2769
00:46:29,120 --> 00:46:33,860
 approach and likewise when you do an

2770
00:46:31,300 --> 00:46:33,860
 

2771
00:46:31,310 --> 00:46:36,140
 image net is doing better than all of

2772
00:46:33,850 --> 00:46:36,140
 

2773
00:46:33,860 --> 00:46:37,340
 the other unsupervised approaches at

2774
00:46:36,130 --> 00:46:37,340
 

2775
00:46:36,140 --> 00:46:42,380
 least that were they were out there at

2776
00:46:37,330 --> 00:46:42,380
 

2777
00:46:37,340 --> 00:46:43,520
 this point so now and I'm how much time

2778
00:46:42,370 --> 00:46:43,520
 

2779
00:46:42,380 --> 00:46:46,220
 that like how much time have I got left

2780
00:46:43,510 --> 00:46:46,220
 

2781
00:46:43,520 --> 00:46:47,540
 so okay this is gonna have to be a real

2782
00:46:46,210 --> 00:46:47,540
 

2783
00:46:46,220 --> 00:46:48,560
 whirlwind tour unfortunate I should have

2784
00:46:47,530 --> 00:46:48,560
 

2785
00:46:47,540 --> 00:46:51,800
 left more time to talk about

2786
00:46:48,550 --> 00:46:51,800
 

2787
00:46:48,560 --> 00:46:54,320
 unsupervised reinforcement learning you

2788
00:46:51,790 --> 00:46:54,320
 

2789
00:46:51,800 --> 00:46:55,790
 know simplest way you can you can you

2790
00:46:54,310 --> 00:46:55,790
 

2791
00:46:54,320 --> 00:46:56,990
 can sort of leverage unsupervised

2792
00:46:55,780 --> 00:46:56,990
 

2793
00:46:55,790 --> 00:46:58,970
 learning for reinforcement learning is

2794
00:46:56,980 --> 00:46:58,970
 

2795
00:46:56,990 --> 00:47:00,560
 to do treat it's called like an auxilary

2796
00:46:58,960 --> 00:47:00,560
 

2797
00:46:58,970 --> 00:47:02,810
 task so basically you have a single

2798
00:47:00,550 --> 00:47:02,810
 

2799
00:47:00,560 --> 00:47:04,910
 network that's both maximizing reward

2800
00:47:02,800 --> 00:47:04,910
 

2801
00:47:02,810 --> 00:47:06,080
 and minimizing some unsupervised loss

2802
00:47:04,900 --> 00:47:06,080
 

2803
00:47:04,910 --> 00:47:08,060
 and it's crucial that it's the same

2804
00:47:06,070 --> 00:47:08,060
 

2805
00:47:06,080 --> 00:47:09,680
 network because basically what's going

2806
00:47:08,050 --> 00:47:09,680
 

2807
00:47:08,060 --> 00:47:11,390
 on here is if the unsupervised loss is

2808
00:47:09,670 --> 00:47:11,390
 

2809
00:47:09,680 --> 00:47:13,810
 going to affect the representations

2810
00:47:11,380 --> 00:47:13,810
 

2811
00:47:11,390 --> 00:47:15,980
 formed inside that network and those

2812
00:47:13,800 --> 00:47:15,980
 

2813
00:47:13,810 --> 00:47:18,170
 representations should help it perform

2814
00:47:15,970 --> 00:47:18,170
 

2815
00:47:15,980 --> 00:47:19,670
 whatever super whatever reinforcement

2816
00:47:18,160 --> 00:47:19,670
 

2817
00:47:18,170 --> 00:47:21,170
 learning tasks it has to do so this is

2818
00:47:19,660 --> 00:47:21,170
 

2819
00:47:19,670 --> 00:47:23,540
 really like a direct expression of this

2820
00:47:21,160 --> 00:47:23,540
 

2821
00:47:21,170 --> 00:47:24,830
 idea of you know learning structure and

2822
00:47:23,530 --> 00:47:24,830
 

2823
00:47:23,540 --> 00:47:28,010
 representing it makes you better at

2824
00:47:24,820 --> 00:47:28,010
 

2825
00:47:24,830 --> 00:47:29,990
 generalizing and yeah I'm gonna have to

2826
00:47:28,000 --> 00:47:29,990
 

2827
00:47:28,010 --> 00:47:31,490
 just you know skip through highlights

2828
00:47:29,980 --> 00:47:31,490
 

2829
00:47:29,990 --> 00:47:33,530
 here but in practice this really works

2830
00:47:31,480 --> 00:47:33,530
 

2831
00:47:31,490 --> 00:47:35,090
 and it's becoming it's actually you know

2832
00:47:33,520 --> 00:47:35,090
 

2833
00:47:33,530 --> 00:47:36,410
 in the space of maybe you know a couple

2834
00:47:35,080 --> 00:47:36,410
 

2835
00:47:35,090 --> 00:47:38,600
 of years this has now become quite a

2836
00:47:36,400 --> 00:47:38,600
 

2837
00:47:36,410 --> 00:47:40,480
 standard technique for modern RL agents

2838
00:47:38,590 --> 00:47:40,480
 

2839
00:47:38,600 --> 00:47:43,250
 is to add these kinds of auxilary losses

2840
00:47:40,470 --> 00:47:43,250
 

2841
00:47:40,480 --> 00:47:45,800
 and here we show like improvements on

2842
00:47:43,240 --> 00:47:45,800
 

2843
00:47:43,250 --> 00:47:50,000
 performance on some you know RL tasks

2844
00:47:45,790 --> 00:47:50,000
 

2845
00:47:45,800 --> 00:47:51,470
 when you add when you add well first of

2846
00:47:49,990 --> 00:47:51,470
 

2847
00:47:50,000 --> 00:47:52,700
 all they looked at input reconstruction

2848
00:47:51,460 --> 00:47:52,700
 

2849
00:47:51,470 --> 00:47:54,050
 which is a very straight forward just

2850
00:47:52,690 --> 00:47:54,050
 

2851
00:47:52,700 --> 00:47:55,760
 kind of model all the pixels and then

2852
00:47:54,040 --> 00:47:55,760
 

2853
00:47:54,050 --> 00:47:57,470
 they looked at some other you know

2854
00:47:55,750 --> 00:47:57,470
 

2855
00:47:55,760 --> 00:47:59,480
 auxilary losses that were more efficient

2856
00:47:57,460 --> 00:47:59,480
 

2857
00:47:57,470 --> 00:48:03,320
 and really improved the baselines here

2858
00:47:59,470 --> 00:48:03,320
 

2859
00:47:59,480 --> 00:48:04,580
 okay so skipping on like so a more

2860
00:48:03,310 --> 00:48:04,580
 

2861
00:48:03,320 --> 00:48:05,990
 fundamental way of combining

2862
00:48:04,570 --> 00:48:05,990
 

2863
00:48:04,580 --> 00:48:06,430
 unsupervised learning with reinforcement

2864
00:48:05,980 --> 00:48:06,430
 

2865
00:48:05,990 --> 00:48:07,750
 learning though

2866
00:48:06,420 --> 00:48:07,750
 

2867
00:48:06,430 --> 00:48:09,190
 to say that the unser rezoning can

2868
00:48:07,740 --> 00:48:09,190
 

2869
00:48:07,750 --> 00:48:10,599
 actually guide the policies it's not

2870
00:48:09,180 --> 00:48:10,599
 

2871
00:48:09,190 --> 00:48:12,460
 just about shaping representations it

2872
00:48:10,589 --> 00:48:12,460
 

2873
00:48:10,599 --> 00:48:14,109
 guides the policy and at that point what

2874
00:48:12,450 --> 00:48:14,109
 

2875
00:48:12,460 --> 00:48:16,750
 you have is an intrinsically motivated

2876
00:48:14,099 --> 00:48:16,750
 

2877
00:48:14,109 --> 00:48:18,280
 agent the agent is just motivated by

2878
00:48:16,740 --> 00:48:18,280
 

2879
00:48:16,750 --> 00:48:19,630
 what it sees in the environment what it

2880
00:48:18,270 --> 00:48:19,630
 

2881
00:48:18,280 --> 00:48:21,130
 can find out about the environment what

2882
00:48:19,620 --> 00:48:21,130
 

2883
00:48:19,630 --> 00:48:23,200
 it can control in the environment

2884
00:48:21,120 --> 00:48:23,200
 

2885
00:48:21,130 --> 00:48:24,430
 without having an extrinsic reward and

2886
00:48:23,190 --> 00:48:24,430
 

2887
00:48:23,200 --> 00:48:26,140
 as with the rest of unsupervised

2888
00:48:24,420 --> 00:48:26,140
 

2889
00:48:24,430 --> 00:48:27,700
 learning there's lots and lots of ways

2890
00:48:26,130 --> 00:48:27,700
 

2891
00:48:26,140 --> 00:48:29,079
 of doing this and there's no consensus

2892
00:48:27,690 --> 00:48:29,079
 

2893
00:48:27,700 --> 00:48:30,819
 as to which one is right just to very

2894
00:48:29,069 --> 00:48:30,819
 

2895
00:48:29,079 --> 00:48:32,589
 quickly go through it's made two main

2896
00:48:30,809 --> 00:48:32,589
 

2897
00:48:30,819 --> 00:48:34,359
 strands one of them is to reward the

2898
00:48:32,579 --> 00:48:34,359
 

2899
00:48:32,589 --> 00:48:36,099
 agent for being curious by guiding it

2900
00:48:34,349 --> 00:48:36,099
 

2901
00:48:34,359 --> 00:48:38,710
 towards novel observations from which it

2902
00:48:36,089 --> 00:48:38,710
 

2903
00:48:36,099 --> 00:48:39,910
 can rapidly learn and there's lots of

2904
00:48:38,700 --> 00:48:39,910
 

2905
00:48:38,710 --> 00:48:41,619
 different ways that can do that there

2906
00:48:39,900 --> 00:48:41,619
 

2907
00:48:39,910 --> 00:48:43,630
 was a paper curiosity driven exploration

2908
00:48:41,609 --> 00:48:43,630
 

2909
00:48:41,619 --> 00:48:45,849
 myself supervised prediction that came

2910
00:48:43,620 --> 00:48:45,849
 

2911
00:48:43,630 --> 00:48:47,680
 out last years had a big impact here

2912
00:48:45,839 --> 00:48:47,680
 

2913
00:48:45,849 --> 00:48:49,809
 which just looks at making predictions

2914
00:48:47,670 --> 00:48:49,809
 

2915
00:48:47,680 --> 00:48:51,640
 and latent space and looking for places

2916
00:48:49,799 --> 00:48:51,640
 

2917
00:48:49,809 --> 00:48:53,740
 where you find things that give you

2918
00:48:51,630 --> 00:48:53,740
 

2919
00:48:51,640 --> 00:48:55,000
 unpredictable latent values and so

2920
00:48:53,730 --> 00:48:55,000
 

2921
00:48:53,740 --> 00:48:56,619
 that's that's a way of drawing you

2922
00:48:54,990 --> 00:48:56,619
 

2923
00:48:55,000 --> 00:48:58,900
 towards the stuff you don't know and

2924
00:48:56,609 --> 00:48:58,900
 

2925
00:48:56,619 --> 00:49:04,050
 away from the stuff you do know there's

2926
00:48:58,890 --> 00:49:04,050
 

2927
00:48:58,900 --> 00:49:06,819
 lots of other ways of defining curiosity

2928
00:49:04,040 --> 00:49:06,819
 

2929
00:49:04,050 --> 00:49:08,680
 jurgen schmidhuber who may or may not be

2930
00:49:06,809 --> 00:49:08,680
 

2931
00:49:06,819 --> 00:49:11,589
 in the audience here has defined perhaps

2932
00:49:08,670 --> 00:49:11,589
 

2933
00:49:08,680 --> 00:49:14,380
 the sort of the most the most curious

2934
00:49:11,579 --> 00:49:14,380
 

2935
00:49:11,589 --> 00:49:15,670
 agent possible and I've given it the

2936
00:49:14,370 --> 00:49:15,670
 

2937
00:49:14,380 --> 00:49:17,020
 wrong title here sorry about that you're

2938
00:49:15,660 --> 00:49:17,020
 

2939
00:49:15,670 --> 00:49:18,369
 gonna should not say complexity gain

2940
00:49:17,010 --> 00:49:18,369
 

2941
00:49:17,020 --> 00:49:20,049
 there it should rather say compression

2942
00:49:18,359 --> 00:49:20,049
 

2943
00:49:18,369 --> 00:49:21,970
 progress and the idea there is you're

2944
00:49:20,039 --> 00:49:21,970
 

2945
00:49:20,049 --> 00:49:23,980
 seeking out data that doesn't just help

2946
00:49:21,960 --> 00:49:23,980
 

2947
00:49:21,970 --> 00:49:25,270
 you to learn about that data but helps

2948
00:49:23,970 --> 00:49:25,270
 

2949
00:49:23,980 --> 00:49:26,770
 you to learn about everything you've

2950
00:49:25,260 --> 00:49:26,770
 

2951
00:49:25,270 --> 00:49:28,540
 ever observes you if you had like a

2952
00:49:26,760 --> 00:49:28,540
 

2953
00:49:26,770 --> 00:49:31,359
 complete record of you of your past in

2954
00:49:28,530 --> 00:49:31,359
 

2955
00:49:28,540 --> 00:49:32,829
 other words a memory then what you're

2956
00:49:31,349 --> 00:49:32,829
 

2957
00:49:31,359 --> 00:49:35,079
 saying is how much that this new

2958
00:49:32,819 --> 00:49:35,079
 

2959
00:49:32,829 --> 00:49:36,640
 observation teach me about all of this

2960
00:49:35,069 --> 00:49:36,640
 

2961
00:49:35,079 --> 00:49:37,990
 stuff that I'd ever learned in my whole

2962
00:49:36,630 --> 00:49:37,990
 

2963
00:49:36,640 --> 00:49:40,000
 lifetime so it's obviously quite an

2964
00:49:37,980 --> 00:49:40,000
 

2965
00:49:37,990 --> 00:49:43,500
 ambitious thing to actually work with

2966
00:49:39,990 --> 00:49:43,500
 

2967
00:49:40,000 --> 00:49:45,880
 but I think it has the it has it has the

2968
00:49:43,490 --> 00:49:45,880
 

2969
00:49:43,500 --> 00:49:47,140
 it has a very interesting sort of

2970
00:49:45,870 --> 00:49:47,140
 

2971
00:49:45,880 --> 00:49:48,579
 property that's missing from these other

2972
00:49:47,130 --> 00:49:48,579
 

2973
00:49:47,140 --> 00:49:50,559
 curiosity signals and that it's kind of

2974
00:49:48,569 --> 00:49:50,559
 

2975
00:49:48,579 --> 00:49:52,990
 retrospective it tells you that you you

2976
00:49:50,549 --> 00:49:52,990
 

2977
00:49:50,559 --> 00:49:55,180
 want to learn things to to inform you

2978
00:49:52,980 --> 00:49:55,180
 

2979
00:49:52,990 --> 00:49:59,170
 about your past and this last thing last

2980
00:49:55,170 --> 00:49:59,170
 

2981
00:49:55,180 --> 00:50:01,210
 night now is as well as curiosity

2982
00:49:59,160 --> 00:50:01,210
 

2983
00:49:59,170 --> 00:50:03,670
 another sort of information theoretic

2984
00:50:01,200 --> 00:50:03,670
 

2985
00:50:01,210 --> 00:50:04,990
 strand of unsupervised reinforcement

2986
00:50:03,660 --> 00:50:04,990
 

2987
00:50:03,670 --> 00:50:06,460
 learning of intrinsic motivation is

2988
00:50:04,980 --> 00:50:06,460
 

2989
00:50:04,990 --> 00:50:08,290
 what's known as empowerment and here

2990
00:50:06,450 --> 00:50:08,290
 

2991
00:50:06,460 --> 00:50:09,490
 it's again mutual information kind of

2992
00:50:08,280 --> 00:50:09,490
 

2993
00:50:08,290 --> 00:50:11,380
 returns here you're maximizing the

2994
00:50:09,480 --> 00:50:11,380
 

2995
00:50:09,490 --> 00:50:13,150
 mutual information between the agents

2996
00:50:11,370 --> 00:50:13,150
 

2997
00:50:11,380 --> 00:50:15,069
 actions in some sense the consequence of

2998
00:50:13,140 --> 00:50:15,069
 

2999
00:50:13,150 --> 00:50:16,880
 its action so for example the state that

3000
00:50:15,059 --> 00:50:16,880
 

3001
00:50:15,069 --> 00:50:18,080
 those actions will lead to and

3002
00:50:16,870 --> 00:50:18,080
 

3003
00:50:16,880 --> 00:50:20,050
 this really means is the agent wants

3004
00:50:18,070 --> 00:50:20,050
 

3005
00:50:18,080 --> 00:50:23,540
 control wants to be in a position where

3006
00:50:20,040 --> 00:50:23,540
 

3007
00:50:20,050 --> 00:50:25,250
 it is able to manipulate and accurately

3008
00:50:23,530 --> 00:50:25,250
 

3009
00:50:23,540 --> 00:50:26,210
 control the environment as much as

3010
00:50:25,240 --> 00:50:26,210
 

3011
00:50:25,250 --> 00:50:28,070
 possible if you think about it in

3012
00:50:26,200 --> 00:50:28,070
 

3013
00:50:26,210 --> 00:50:29,720
 information terms it wants the

3014
00:50:28,060 --> 00:50:29,720
 

3015
00:50:28,070 --> 00:50:31,370
 environment to be this clear channel

3016
00:50:29,710 --> 00:50:31,370
 

3017
00:50:29,720 --> 00:50:33,050
 there's noiseless channel that will

3018
00:50:31,360 --> 00:50:33,050
 

3019
00:50:31,370 --> 00:50:36,640
 exactly reflect whatever it was that

3020
00:50:33,040 --> 00:50:36,640
 

3021
00:50:33,050 --> 00:50:39,410
 their intention was to operate on that

3022
00:50:36,630 --> 00:50:39,410
 

3023
00:50:36,640 --> 00:50:41,170
 environment and Daniel Palani who was

3024
00:50:39,400 --> 00:50:41,170
 

3025
00:50:39,410 --> 00:50:43,880
 one of the authors of the original

3026
00:50:41,160 --> 00:50:43,880
 

3027
00:50:41,170 --> 00:50:45,470
 empowerment paper characterized it as

3028
00:50:43,870 --> 00:50:45,470
 

3029
00:50:43,880 --> 00:50:47,930
 saying they were curious agent is like

3030
00:50:45,460 --> 00:50:47,930
 

3031
00:50:45,470 --> 00:50:50,360
 this eager young student who wants to

3032
00:50:47,920 --> 00:50:50,360
 

3033
00:50:47,930 --> 00:50:52,490
 learn everything at once an empowered

3034
00:50:50,350 --> 00:50:52,490
 

3035
00:50:50,360 --> 00:50:54,500
 agent is like a wise old man who sits on

3036
00:50:52,480 --> 00:50:54,500
 

3037
00:50:52,490 --> 00:50:56,000
 a hill and just surveys everything but

3038
00:50:54,490 --> 00:50:56,000
 

3039
00:50:54,500 --> 00:50:57,500
 doesn't feel the need to move as long as

3040
00:50:55,990 --> 00:50:57,500
 

3041
00:50:56,000 --> 00:50:59,210
 he's in this position where he can see

3042
00:50:57,490 --> 00:50:59,210
 

3043
00:50:57,500 --> 00:51:02,420
 everything and know what's going to

3044
00:50:59,200 --> 00:51:02,420
 

3045
00:50:59,210 --> 00:51:04,940
 happen ok so what are the conclusions

3046
00:51:02,410 --> 00:51:04,940
 

3047
00:51:02,420 --> 00:51:06,350
 here well obviously I'm to rise learning

3048
00:51:04,930 --> 00:51:06,350
 

3049
00:51:04,940 --> 00:51:08,780
 gives us a lot of more data to look at

3050
00:51:06,340 --> 00:51:08,780
 

3051
00:51:06,350 --> 00:51:09,890
 but it's not clear what you know how we

3052
00:51:08,770 --> 00:51:09,890
 

3053
00:51:08,780 --> 00:51:11,270
 should do that right there's no

3054
00:51:09,880 --> 00:51:11,270
 

3055
00:51:09,890 --> 00:51:13,310
 consensus and what the right objective

3056
00:51:11,260 --> 00:51:13,310
 

3057
00:51:11,270 --> 00:51:15,050
 is density modeling is one option or to

3058
00:51:13,300 --> 00:51:15,050
 

3059
00:51:13,310 --> 00:51:17,180
 regress the models are one option for

3060
00:51:15,040 --> 00:51:17,180
 

3061
00:51:15,050 --> 00:51:18,380
 density modeling I'm not trying to claim

3062
00:51:17,170 --> 00:51:18,380
 

3063
00:51:17,180 --> 00:51:21,950
 that these are the only options by any

3064
00:51:18,370 --> 00:51:21,950
 

3065
00:51:18,380 --> 00:51:23,840
 means what's clear though is that you

3066
00:51:21,940 --> 00:51:23,840
 

3067
00:51:21,950 --> 00:51:26,090
 know these unsupervised methods can

3068
00:51:23,830 --> 00:51:26,090
 

3069
00:51:23,840 --> 00:51:27,800
 yield very useful latent representations

3070
00:51:26,080 --> 00:51:27,800
 

3071
00:51:26,090 --> 00:51:30,100
 the representations can directly benefit

3072
00:51:27,790 --> 00:51:30,100
 

3073
00:51:27,800 --> 00:51:33,080
 for example reinforcement learning and

3074
00:51:30,090 --> 00:51:33,080
 

3075
00:51:30,100 --> 00:51:34,280
 we can we can also as well as using them

3076
00:51:33,070 --> 00:51:34,280
 

3077
00:51:33,080 --> 00:51:35,690
 as a sort of auxilary loss for

3078
00:51:34,270 --> 00:51:35,690
 

3079
00:51:34,280 --> 00:51:38,360
 reinforcement learning we can also use

3080
00:51:35,680 --> 00:51:38,360
 

3081
00:51:35,690 --> 00:51:41,510
 intrinsic motivation signals to guide

3082
00:51:38,350 --> 00:51:41,510
 

3083
00:51:38,360 --> 00:51:45,670
 the agents behavior and that's the end

3084
00:51:41,500 --> 00:51:45,670
 

3085
00:51:41,510 --> 00:51:45,670
 of my part for now thank you

3086
00:51:51,410 --> 00:51:51,410
 

3087
00:51:51,420 --> 00:51:55,990
 so we have time for two questions if you

3088
00:51:54,630 --> 00:51:55,990
 

3089
00:51:54,640 --> 00:52:00,160
 have questions please go to the

3090
00:51:55,980 --> 00:52:00,160
 

3091
00:51:55,990 --> 00:52:03,010
 microphone standing like in the back is

3092
00:52:00,150 --> 00:52:03,010
 

3093
00:52:00,160 --> 00:52:10,270
 there anyone I see next standing

3094
00:52:03,000 --> 00:52:10,270
 

3095
00:52:03,010 --> 00:52:11,290
 microphone no questions if there is no

3096
00:52:10,260 --> 00:52:11,290
 

3097
00:52:10,270 --> 00:52:12,910
 more question we're going to have the

3098
00:52:11,280 --> 00:52:12,910
 

3099
00:52:11,290 --> 00:52:14,200
 another Q&A section at the end of the

3100
00:52:12,900 --> 00:52:14,200
 

3101
00:52:12,910 --> 00:52:16,180
 tutorial so we're going to take five

3102
00:52:14,190 --> 00:52:16,180
 

3103
00:52:14,200 --> 00:52:22,990
 minutes break and then please come back

3104
00:52:16,170 --> 00:52:22,990
 

3105
00:52:16,180 --> 00:52:24,880
 by 11:00 hello welcome to the second

3106
00:52:22,980 --> 00:52:24,880
 

3107
00:52:22,990 --> 00:52:27,520
 part of the tutorial on supervised

3108
00:52:24,870 --> 00:52:27,520
 

3109
00:52:24,880 --> 00:52:29,410
 learning my name is Marco aurélio and

3110
00:52:27,510 --> 00:52:29,410
 

3111
00:52:27,520 --> 00:52:32,320
 rato I am a research scientist at

3112
00:52:29,400 --> 00:52:32,320
 

3113
00:52:29,410 --> 00:52:33,580
 Facebook a research lab in the first

3114
00:52:32,310 --> 00:52:33,580
 

3115
00:52:32,320 --> 00:52:35,410
 part of the tutorial

3116
00:52:33,570 --> 00:52:35,410
 

3117
00:52:33,580 --> 00:52:37,360
 Alex introduced and supervised learning

3118
00:52:35,400 --> 00:52:37,360
 

3119
00:52:35,410 --> 00:52:40,120
 in terms of compression and auto

3120
00:52:37,350 --> 00:52:40,120
 

3121
00:52:37,360 --> 00:52:42,670
 regressive models in the second part I

3122
00:52:40,110 --> 00:52:42,670
 

3123
00:52:40,120 --> 00:52:44,620
 will complement this view by serving

3124
00:52:42,660 --> 00:52:44,620
 

3125
00:52:42,670 --> 00:52:48,010
 various applications of unsupervised

3126
00:52:44,610 --> 00:52:48,010
 

3127
00:52:44,620 --> 00:52:51,310
 learning which are based on other

3128
00:52:48,000 --> 00:52:51,310
 

3129
00:52:48,010 --> 00:52:53,910
 frameworks and principles so I will

3130
00:52:51,300 --> 00:52:53,910
 

3131
00:52:51,310 --> 00:52:56,350
 start by reviewing advances in

3132
00:52:53,900 --> 00:52:56,350
 

3133
00:52:53,910 --> 00:52:58,260
 unsupervised feature learning then

3134
00:52:56,340 --> 00:52:58,260
 

3135
00:52:56,350 --> 00:53:02,590
 briefly mention methods to generate

3136
00:52:58,250 --> 00:53:02,590
 

3137
00:52:58,260 --> 00:53:04,090
 samples and then conclude with methods

3138
00:53:02,580 --> 00:53:04,090
 

3139
00:53:02,590 --> 00:53:06,010
 to learn to translate between two

3140
00:53:04,080 --> 00:53:06,010
 

3141
00:53:04,090 --> 00:53:09,550
 different domains in an unsupervised way

3142
00:53:06,000 --> 00:53:09,550
 

3143
00:53:06,010 --> 00:53:12,520
 and then the last few minutes will be

3144
00:53:09,540 --> 00:53:12,520
 

3145
00:53:09,550 --> 00:53:14,260
 about perspectives and a discussion

3146
00:53:12,510 --> 00:53:14,260
 

3147
00:53:12,520 --> 00:53:19,180
 about the challenges that are ahead of

3148
00:53:14,250 --> 00:53:19,180
 

3149
00:53:14,260 --> 00:53:23,350
 us so let me start with a disclaimer I

3150
00:53:19,170 --> 00:53:23,350
 

3151
00:53:19,180 --> 00:53:25,810
 will not mention all the words that are

3152
00:53:23,340 --> 00:53:25,810
 

3153
00:53:23,350 --> 00:53:29,710
 relevant but I will just highlight a few

3154
00:53:25,800 --> 00:53:29,710
 

3155
00:53:25,810 --> 00:53:33,220
 representative works and so let's start

3156
00:53:29,700 --> 00:53:33,220
 

3157
00:53:29,710 --> 00:53:35,200
 with the topic about learning supervised

3158
00:53:33,210 --> 00:53:35,200
 

3159
00:53:33,220 --> 00:53:37,540
 feature learning in the continuous case

3160
00:53:35,190 --> 00:53:37,540
 

3161
00:53:35,200 --> 00:53:39,160
 so let's say that this is your data and

3162
00:53:37,530 --> 00:53:39,160
 

3163
00:53:37,540 --> 00:53:42,070
 this is already something little bit

3164
00:53:39,150 --> 00:53:42,070
 

3165
00:53:39,160 --> 00:53:43,630
 misleading because oftentimes your data

3166
00:53:42,060 --> 00:53:43,630
 

3167
00:53:42,070 --> 00:53:46,960
 lives in a very high dimensional space

3168
00:53:43,620 --> 00:53:46,960
 

3169
00:53:43,630 --> 00:53:48,520
 but still I think it is very important

3170
00:53:46,950 --> 00:53:48,520
 

3171
00:53:46,960 --> 00:53:50,800
 that whenever you get something

3172
00:53:48,510 --> 00:53:50,800
 

3173
00:53:48,520 --> 00:53:52,570
 you look at the data right and so we a

3174
00:53:50,790 --> 00:53:52,570
 

3175
00:53:50,800 --> 00:53:54,790
 little bit of statistical analysis look

3176
00:53:52,560 --> 00:53:54,790
 

3177
00:53:52,570 --> 00:53:57,850
 at you know what is intrinsic dimension

3178
00:53:54,780 --> 00:53:57,850
 

3179
00:53:54,790 --> 00:54:00,760
 right if you if you do ETA it's

3180
00:53:57,840 --> 00:54:00,760
 

3181
00:53:57,850 --> 00:54:03,130
 something pretty important but generally

3182
00:54:00,750 --> 00:54:03,130
 

3183
00:54:00,760 --> 00:54:05,170
 speaking the goal of unsupervised

3184
00:54:03,120 --> 00:54:05,170
 

3185
00:54:03,130 --> 00:54:07,990
 feature learning is to associate a

3186
00:54:05,160 --> 00:54:07,990
 

3187
00:54:05,170 --> 00:54:10,810
 picture vector to any input at a point

3188
00:54:07,980 --> 00:54:10,810
 

3189
00:54:07,990 --> 00:54:13,060
 and this feature vector can have real

3190
00:54:10,800 --> 00:54:13,060
 

3191
00:54:10,810 --> 00:54:14,530
 values can have discrete values but the

3192
00:54:13,050 --> 00:54:14,530
 

3193
00:54:13,060 --> 00:54:17,500
 point is that from this representation

3194
00:54:14,520 --> 00:54:17,500
 

3195
00:54:14,530 --> 00:54:19,750
 you want to be able to solve a variety

3196
00:54:17,490 --> 00:54:19,750
 

3197
00:54:17,500 --> 00:54:24,550
 of different tasks that are unknown when

3198
00:54:19,740 --> 00:54:24,550
 

3199
00:54:19,750 --> 00:54:28,030
 you learn the representation in practice

3200
00:54:24,540 --> 00:54:28,030
 

3201
00:54:24,550 --> 00:54:30,030
 you need to note you always need to take

3202
00:54:28,020 --> 00:54:30,030
 

3203
00:54:28,030 --> 00:54:32,770
 into account very strong baselines and

3204
00:54:30,020 --> 00:54:32,770
 

3205
00:54:30,030 --> 00:54:33,580
 PCA k-means are still pretty strong

3206
00:54:32,760 --> 00:54:33,580
 

3207
00:54:32,770 --> 00:54:36,100
 baselines

3208
00:54:33,570 --> 00:54:36,100
 

3209
00:54:33,580 --> 00:54:39,070
 if the dimensionality of that is not too

3210
00:54:36,090 --> 00:54:39,070
 

3211
00:54:36,100 --> 00:54:40,780
 large and in practice even if you work

3212
00:54:39,060 --> 00:54:40,780
 

3213
00:54:39,070 --> 00:54:43,720
 with let's say high resolution images

3214
00:54:40,770 --> 00:54:43,720
 

3215
00:54:40,780 --> 00:54:46,300
 applying PCR k-means on image patches is

3216
00:54:43,710 --> 00:54:46,300
 

3217
00:54:43,720 --> 00:54:48,610
 oftentimes something that gives a strong

3218
00:54:46,290 --> 00:54:48,610
 

3219
00:54:46,300 --> 00:54:52,050
 base lines or learning representations

3220
00:54:48,600 --> 00:54:52,050
 

3221
00:54:48,610 --> 00:54:55,120
 so talking about learning

3222
00:54:52,040 --> 00:54:55,120
 

3223
00:54:52,050 --> 00:54:57,250
 representations to the continuous space

3224
00:54:55,110 --> 00:54:57,250
 

3225
00:54:55,120 --> 00:55:00,280
 I'm going to the part two works in the

3226
00:54:57,240 --> 00:55:00,280
 

3227
00:54:57,250 --> 00:55:03,100
 computer vision literature so I will do

3228
00:55:00,270 --> 00:55:03,100
 

3229
00:55:00,280 --> 00:55:06,340
 a brief historical overview and then

3230
00:55:03,090 --> 00:55:06,340
 

3231
00:55:03,100 --> 00:55:08,170
 talk about approaches that fall within

3232
00:55:06,330 --> 00:55:08,170
 

3233
00:55:06,340 --> 00:55:11,410
 the umbrella of such supervised learning

3234
00:55:08,160 --> 00:55:11,410
 

3235
00:55:08,170 --> 00:55:13,540
 and also talk about other approaches

3236
00:55:11,400 --> 00:55:13,540
 

3237
00:55:11,410 --> 00:55:19,960
 that are not based on supervised

3238
00:55:13,530 --> 00:55:19,960
 

3239
00:55:13,540 --> 00:55:22,660
 learning so I would say that the bedrock

3240
00:55:19,950 --> 00:55:22,660
 

3241
00:55:19,960 --> 00:55:26,130
 of unsupervised feature learning dates

3242
00:55:22,650 --> 00:55:26,130
 

3243
00:55:22,660 --> 00:55:29,350
 back to pca right early 20th century and

3244
00:55:26,120 --> 00:55:29,350
 

3245
00:55:26,130 --> 00:55:33,930
 if you apply PCA on small image patches

3246
00:55:29,340 --> 00:55:33,930
 

3247
00:55:29,350 --> 00:55:36,960
 you get essentially DCT basis which were

3248
00:55:33,920 --> 00:55:36,960
 

3249
00:55:33,930 --> 00:55:40,300
 published in the mid-70s and which led

3250
00:55:36,950 --> 00:55:40,300
 

3251
00:55:36,960 --> 00:55:42,700
 around midnight his late night is to the

3252
00:55:40,290 --> 00:55:42,700
 

3253
00:55:40,300 --> 00:55:46,559
 JPEG format in the apply math community

3254
00:55:42,690 --> 00:55:46,559
 

3255
00:55:42,700 --> 00:55:49,849
 there has also been a lot of work about

3256
00:55:46,549 --> 00:55:49,849
 

3257
00:55:46,559 --> 00:55:52,680
 coming up with representations of images

3258
00:55:49,839 --> 00:55:52,680
 

3259
00:55:49,849 --> 00:55:55,020
 against dating back to early 20th

3260
00:55:52,670 --> 00:55:55,020
 

3261
00:55:52,680 --> 00:55:57,030
 century with the work by har you know

3262
00:55:55,010 --> 00:55:57,030
 

3263
00:55:55,020 --> 00:56:00,960
 the haar wavelet and then that went all

3264
00:55:57,020 --> 00:56:00,960
 

3265
00:55:57,030 --> 00:56:03,210
 the way throughout the century early 80s

3266
00:56:00,950 --> 00:56:03,210
 

3267
00:56:00,960 --> 00:56:06,900
 we have the wavelet decomposition as we

3268
00:56:03,200 --> 00:56:06,900
 

3269
00:56:03,210 --> 00:56:12,420
 know it today in machine learning I

3270
00:56:06,890 --> 00:56:12,420
 

3271
00:56:06,900 --> 00:56:15,569
 would say the breakthrough came with in

3272
00:56:12,410 --> 00:56:15,569
 

3273
00:56:12,420 --> 00:56:17,970
 1986 with the publication of the paper

3274
00:56:15,559 --> 00:56:17,970
 

3275
00:56:15,569 --> 00:56:20,010
 by Rumi low-income and Williams about

3276
00:56:17,960 --> 00:56:20,010
 

3277
00:56:17,970 --> 00:56:22,020
 back propagation as a way to train your

3278
00:56:20,000 --> 00:56:22,020
 

3279
00:56:20,010 --> 00:56:24,740
 networks and so there was a lot of

3280
00:56:22,010 --> 00:56:24,740
 

3281
00:56:22,020 --> 00:56:29,569
 excitement about training auto-encoders

3282
00:56:24,730 --> 00:56:29,569
 

3283
00:56:24,740 --> 00:56:31,980
 and I would say that that went on until

3284
00:56:29,559 --> 00:56:31,980
 

3285
00:56:29,569 --> 00:56:36,420
 early 90s and then at that point people

3286
00:56:31,970 --> 00:56:36,420
 

3287
00:56:31,980 --> 00:56:40,579
 switched to linear systems that were

3288
00:56:36,410 --> 00:56:40,579
 

3289
00:56:36,420 --> 00:56:43,020
 more easy to understand and to train and

3290
00:56:40,569 --> 00:56:43,020
 

3291
00:56:40,579 --> 00:56:46,559
 most of the work in the computer vision

3292
00:56:43,010 --> 00:56:46,559
 

3293
00:56:43,020 --> 00:56:48,809
 literature went on designing picture

3294
00:56:46,549 --> 00:56:48,809
 

3295
00:56:46,559 --> 00:56:51,569
 descriptors like the SIP in the early

3296
00:56:48,799 --> 00:56:51,569
 

3297
00:56:48,809 --> 00:56:54,900
 nineties that had a lot of applications

3298
00:56:51,559 --> 00:56:54,900
 

3299
00:56:51,569 --> 00:56:56,789
 in the early 2000s and it's not that

3300
00:56:54,890 --> 00:56:56,789
 

3301
00:56:54,900 --> 00:56:59,279
 people having worked on picture learning

3302
00:56:56,779 --> 00:56:59,279
 

3303
00:56:56,789 --> 00:57:01,470
 at a time there is the seminal work by a

3304
00:56:59,269 --> 00:57:01,470
 

3305
00:56:59,279 --> 00:57:04,529
 bloom assassin in the mid night is about

3306
00:57:01,460 --> 00:57:04,529
 

3307
00:57:01,470 --> 00:57:05,970
 on a sparse coding for instance but most

3308
00:57:04,519 --> 00:57:05,970
 

3309
00:57:04,529 --> 00:57:08,460
 of the computer vision literature was

3310
00:57:05,960 --> 00:57:08,460
 

3311
00:57:05,970 --> 00:57:11,549
 about and hangings new york pictures

3312
00:57:08,450 --> 00:57:11,549
 

3313
00:57:08,460 --> 00:57:14,190
 right and so that changed around 2006

3314
00:57:11,539 --> 00:57:14,190
 

3315
00:57:11,549 --> 00:57:19,500
 when Safina and collaborators published

3316
00:57:14,180 --> 00:57:19,500
 

3317
00:57:14,190 --> 00:57:23,039
 a paper about training very deep very

3318
00:57:19,490 --> 00:57:23,039
 

3319
00:57:19,500 --> 00:57:25,410
 hierarchical systems using auto encoder

3320
00:57:23,029 --> 00:57:25,410
 

3321
00:57:23,039 --> 00:57:28,770
 so you would train these hierarchical

3322
00:57:25,400 --> 00:57:28,770
 

3323
00:57:25,410 --> 00:57:30,299
 models in a layer way station using

3324
00:57:28,760 --> 00:57:30,299
 

3325
00:57:28,770 --> 00:57:32,910
 water encoders a variety of different

3326
00:57:30,289 --> 00:57:32,910
 

3327
00:57:30,299 --> 00:57:34,799
 folders from probabilistic what encoder

3328
00:57:32,900 --> 00:57:34,799
 

3329
00:57:32,910 --> 00:57:37,440
 Sparsit encoders in noise and encoders

3330
00:57:34,789 --> 00:57:37,440
 

3331
00:57:34,799 --> 00:57:40,529
 and there was a lot of excitement at a

3332
00:57:37,430 --> 00:57:40,529
 

3333
00:57:37,440 --> 00:57:42,630
 time but then in 2012 electricity and

3334
00:57:40,519 --> 00:57:42,630
 

3335
00:57:40,529 --> 00:57:44,160
 collaborators showed that you can

3336
00:57:42,620 --> 00:57:44,160
 

3337
00:57:42,630 --> 00:57:47,369
 actually train very deep collusion

3338
00:57:44,150 --> 00:57:47,369
 

3339
00:57:44,160 --> 00:57:50,550
 neural network in a supervised way and

3340
00:57:47,359 --> 00:57:50,550
 

3341
00:57:47,369 --> 00:57:52,920
 so essentially you don't quite need so

3342
00:57:50,540 --> 00:57:52,920
 

3343
00:57:50,550 --> 00:57:54,510
 the conclusion at the time was that you

3344
00:57:52,910 --> 00:57:54,510
 

3345
00:57:52,920 --> 00:57:56,640
 don't quite mean unsupervised learning

3346
00:57:54,500 --> 00:57:56,640
 

3347
00:57:54,510 --> 00:57:59,610
 you can just use very large liberal

3348
00:57:56,630 --> 00:57:59,610
 

3349
00:57:56,640 --> 00:58:01,920
 datasets and got very good features but

3350
00:57:59,600 --> 00:58:01,920
 

3351
00:57:59,610 --> 00:58:03,240
 then more recently in the computer

3352
00:58:01,910 --> 00:58:03,240
 

3353
00:58:01,920 --> 00:58:05,700
 vision community people realize that

3354
00:58:03,230 --> 00:58:05,700
 

3355
00:58:03,240 --> 00:58:07,920
 okay in computer vision we have very

3356
00:58:05,690 --> 00:58:07,920
 

3357
00:58:05,700 --> 00:58:10,740
 large level data sets but labels are

3358
00:58:07,910 --> 00:58:10,740
 

3359
00:58:07,920 --> 00:58:12,120
 never enough and so now in the past few

3360
00:58:10,730 --> 00:58:12,120
 

3361
00:58:10,740 --> 00:58:14,790
 years there has been a lot of excitement

3362
00:58:12,110 --> 00:58:14,790
 

3363
00:58:12,120 --> 00:58:16,740
 about what it costs a supervised

3364
00:58:14,780 --> 00:58:16,740
 

3365
00:58:14,790 --> 00:58:19,260
 learning and so that's what I'm going to

3366
00:58:16,730 --> 00:58:19,260
 

3367
00:58:16,740 --> 00:58:21,510
 describe next and everything that I say

3368
00:58:19,250 --> 00:58:21,510
 

3369
00:58:19,260 --> 00:58:23,700
 is based on an architecture that is the

3370
00:58:21,500 --> 00:58:23,700
 

3371
00:58:21,510 --> 00:58:25,860
 convolution neural network which I'm not

3372
00:58:23,690 --> 00:58:25,860
 

3373
00:58:23,700 --> 00:58:28,620
 going to review as soon that you're

3374
00:58:25,850 --> 00:58:28,620
 

3375
00:58:25,860 --> 00:58:31,530
 familiar with it you can see there are

3376
00:58:28,610 --> 00:58:31,530
 

3377
00:58:28,620 --> 00:58:33,870
 some good references that give it some

3378
00:58:31,520 --> 00:58:33,870
 

3379
00:58:31,530 --> 00:58:35,490
 introduction and also a link to a

3380
00:58:33,860 --> 00:58:35,490
 

3381
00:58:33,870 --> 00:58:38,280
 tutorial that I gave at a summer school

3382
00:58:35,480 --> 00:58:38,280
 

3383
00:58:35,490 --> 00:58:41,430
 last year that is pretty introductory on

3384
00:58:38,270 --> 00:58:41,430
 

3385
00:58:38,280 --> 00:58:43,530
 the topic so when you do on supervised

3386
00:58:41,420 --> 00:58:43,530
 

3387
00:58:41,430 --> 00:58:46,950
 learning very much like Alex described

3388
00:58:43,520 --> 00:58:46,950
 

3389
00:58:43,530 --> 00:58:48,150
 you typically want to model the density

3390
00:58:46,940 --> 00:58:48,150
 

3391
00:58:46,950 --> 00:58:50,480
 of the input you want to kind of

3392
00:58:48,140 --> 00:58:50,480
 

3393
00:58:48,150 --> 00:58:53,520
 reconstruct every input pixel right

3394
00:58:50,470 --> 00:58:53,520
 

3395
00:58:50,480 --> 00:58:56,250
 instance supervised learning the idea is

3396
00:58:53,510 --> 00:58:56,250
 

3397
00:58:53,520 --> 00:58:58,860
 that you use domain expertise to define

3398
00:58:56,240 --> 00:58:58,860
 

3399
00:58:56,250 --> 00:59:01,650
 a conditional task so you define a

3400
00:58:58,850 --> 00:59:01,650
 

3401
00:58:58,860 --> 00:59:03,330
 subset of the input variables which used

3402
00:59:01,640 --> 00:59:03,330
 

3403
00:59:01,650 --> 00:59:06,510
 to condition you try to play the

3404
00:59:03,320 --> 00:59:06,510
 

3405
00:59:03,330 --> 00:59:09,630
 remaining ones and so just that reduces

3406
00:59:06,500 --> 00:59:09,630
 

3407
00:59:06,510 --> 00:59:12,330
 the dimensionality of the prediction

3408
00:59:09,620 --> 00:59:12,330
 

3409
00:59:09,630 --> 00:59:15,600
 tasks and the second thing is that you

3410
00:59:12,320 --> 00:59:15,600
 

3411
00:59:12,330 --> 00:59:17,760
 want to pick these subsets of variables

3412
00:59:15,590 --> 00:59:17,760
 

3413
00:59:15,600 --> 00:59:22,620
 in such a way that the model has to

3414
00:59:17,750 --> 00:59:22,620
 

3415
00:59:17,760 --> 00:59:24,900
 learn the semantics of images and

3416
00:59:22,610 --> 00:59:24,900
 

3417
00:59:22,620 --> 00:59:26,880
 oftentimes you turn the original

3418
00:59:24,890 --> 00:59:26,880
 

3419
00:59:24,900 --> 00:59:30,600
 regression problem into a classification

3420
00:59:26,870 --> 00:59:30,600
 

3421
00:59:26,880 --> 00:59:32,790
 task that is easier to to train so let

3422
00:59:30,590 --> 00:59:32,790
 

3423
00:59:30,600 --> 00:59:35,820
 me give you a few examples of this so in

3424
00:59:32,780 --> 00:59:35,820
 

3425
00:59:32,790 --> 00:59:38,010
 this world by card or at all they take

3426
00:59:35,810 --> 00:59:38,010
 

3427
00:59:35,820 --> 00:59:40,170
 two image patches from the same image

3428
00:59:38,000 --> 00:59:40,170
 

3429
00:59:38,010 --> 00:59:41,940
 all right and then they try to predict

3430
00:59:40,160 --> 00:59:41,940
 

3431
00:59:40,170 --> 00:59:44,840
 the spatial relationship between these

3432
00:59:41,930 --> 00:59:44,840
 

3433
00:59:41,940 --> 00:59:48,300
 two image spaces and so in this case

3434
00:59:44,830 --> 00:59:48,300
 

3435
00:59:44,840 --> 00:59:49,800
 they define a three by three grid given

3436
00:59:48,290 --> 00:59:49,800
 

3437
00:59:48,300 --> 00:59:52,850
 these two image patches you need to

3438
00:59:49,790 --> 00:59:52,850
 

3439
00:59:49,800 --> 00:59:57,320
 predict that the second patch comes from

3440
00:59:52,840 --> 00:59:57,320
 

3441
00:59:52,850 --> 01:00:01,750
 the third position in this gray okay and

3442
00:59:57,310 --> 01:00:01,750
 

3443
00:59:57,320 --> 01:00:04,840
 the the model is very seen so you have a

3444
01:00:01,740 --> 01:00:04,840
 

3445
01:00:01,750 --> 01:00:07,660
 a network here that is used to store

3446
01:00:04,830 --> 01:00:07,660
 

3447
01:00:04,840 --> 01:00:10,720
 features from both image spaces and then

3448
01:00:07,650 --> 01:00:10,720
 

3449
01:00:07,660 --> 01:00:13,420
 these features are fed to a classifier

3450
01:00:10,710 --> 01:00:13,420
 

3451
01:00:10,720 --> 01:00:16,180
 that has to predict in this case one of

3452
01:00:13,410 --> 01:00:16,180
 

3453
01:00:13,420 --> 01:00:18,730
 the nine levels one of the positions in

3454
01:00:16,170 --> 01:00:18,730
 

3455
01:00:16,180 --> 01:00:21,910
 the grid so it is training by

3456
01:00:18,720 --> 01:00:21,910
 

3457
01:00:18,730 --> 01:00:25,060
 cross-entropy and if you look what the

3458
01:00:21,900 --> 01:00:25,060
 

3459
01:00:21,910 --> 01:00:28,690
 network learns after this procedure you

3460
01:00:25,050 --> 01:00:28,690
 

3461
01:00:25,060 --> 01:00:30,010
 find that if you take image patched like

3462
01:00:28,680 --> 01:00:30,010
 

3463
01:00:28,690 --> 01:00:31,990
 this and you look at the nearest

3464
01:00:30,000 --> 01:00:31,990
 

3465
01:00:30,010 --> 01:00:34,120
 neighboring pits of space you find image

3466
01:00:31,980 --> 01:00:34,120
 

3467
01:00:31,990 --> 01:00:37,300
 patches that exhibit the same pattern

3468
01:00:34,110 --> 01:00:37,300
 

3469
01:00:34,120 --> 01:00:39,880
 you know our Legg like things or party

3470
01:00:37,290 --> 01:00:39,880
 

3471
01:00:37,300 --> 01:00:41,770
 animals or facings and so forth and so

3472
01:00:39,870 --> 01:00:41,770
 

3473
01:00:39,880 --> 01:00:44,020
 then they tested on the Pascal you see

3474
01:00:41,760 --> 01:00:44,020
 

3475
01:00:41,770 --> 01:00:45,700
 data set and they found this method

3476
01:00:44,010 --> 01:00:45,700
 

3477
01:00:44,020 --> 01:00:47,260
 works much better than if you were to

3478
01:00:45,690 --> 01:00:47,260
 

3479
01:00:45,700 --> 01:00:49,600
 initialize the network at random

3480
01:00:47,250 --> 01:00:49,600
 

3481
01:00:47,260 --> 01:00:52,720
 although not as good as if you were to

3482
01:00:49,590 --> 01:00:52,720
 

3483
01:00:49,600 --> 01:00:56,890
 use the large label data side image snap

3484
01:00:52,710 --> 01:00:56,890
 

3485
01:00:52,720 --> 01:01:01,540
 okay as a disclaimer more recently a few

3486
01:00:56,880 --> 01:01:01,540
 

3487
01:00:56,890 --> 01:01:03,610
 weeks ago there has been a paper showing

3488
01:01:01,530 --> 01:01:03,610
 

3489
01:01:01,540 --> 01:01:05,560
 that actually if you change little bitty

3490
01:01:03,600 --> 01:01:05,560
 

3491
01:01:03,610 --> 01:01:09,820
 architecture in this case in particular

3492
01:01:05,550 --> 01:01:09,820
 

3493
01:01:05,560 --> 01:01:12,400
 how you normalize the activations within

3494
01:01:09,810 --> 01:01:12,400
 

3495
01:01:09,820 --> 01:01:14,680
 the CNN you actually can train things

3496
01:01:12,390 --> 01:01:14,680
 

3497
01:01:12,400 --> 01:01:16,930
 from random and do as well as image

3498
01:01:14,670 --> 01:01:16,930
 

3499
01:01:14,680 --> 01:01:18,310
 sniper training so everything that I say

3500
01:01:16,920 --> 01:01:18,310
 

3501
01:01:16,930 --> 01:01:19,750
 has to be taken with a grain or so

3502
01:01:18,300 --> 01:01:19,750
 

3503
01:01:18,310 --> 01:01:25,090
 because the pill is evolving very

3504
01:01:19,740 --> 01:01:25,090
 

3505
01:01:19,750 --> 01:01:27,120
 quickly things can change there are

3506
01:01:25,080 --> 01:01:27,120
 

3507
01:01:25,090 --> 01:01:30,040
 other supervised learning methods that

3508
01:01:27,110 --> 01:01:30,040
 

3509
01:01:27,120 --> 01:01:31,840
 have been applied to static images but

3510
01:01:30,030 --> 01:01:31,840
 

3511
01:01:30,040 --> 01:01:33,940
 the and here you find a couple

3512
01:01:31,830 --> 01:01:33,940
 

3513
01:01:31,840 --> 01:01:37,540
 references but the overall idea is to

3514
01:01:33,930 --> 01:01:37,540
 

3515
01:01:33,940 --> 01:01:40,180
 use domain expertise domain knowledge to

3516
01:01:37,530 --> 01:01:40,180
 

3517
01:01:37,540 --> 01:01:42,190
 define an auxiliary task that requires

3518
01:01:40,170 --> 01:01:42,190
 

3519
01:01:40,180 --> 01:01:45,080
 some semantic understanding and

3520
01:01:42,180 --> 01:01:45,080
 

3521
01:01:42,190 --> 01:01:47,720
 oftentimes you turn the

3522
01:01:45,070 --> 01:01:47,720
 

3523
01:01:45,080 --> 01:01:49,430
 tasks of predicting pixel values in a

3524
01:01:47,710 --> 01:01:49,430
 

3525
01:01:47,720 --> 01:01:52,250
 classification task that is much easier

3526
01:01:49,420 --> 01:01:52,250
 

3527
01:01:49,430 --> 01:01:53,990
 to handle alright let's see another

3528
01:01:52,240 --> 01:01:53,990
 

3529
01:01:52,250 --> 01:01:59,349
 example such supervised learning applied

3530
01:01:53,980 --> 01:01:59,349
 

3531
01:01:53,990 --> 01:02:03,470
 to videos in this work by way all they

3532
01:01:59,339 --> 01:02:03,470
 

3533
01:01:59,349 --> 01:02:07,310
 show to the network video snippets and

3534
01:02:03,460 --> 01:02:07,310
 

3535
01:02:03,470 --> 01:02:08,410
 they need to predict whether huh

3536
01:02:07,300 --> 01:02:08,410
 

3537
01:02:07,310 --> 01:02:10,970
 interesting

3538
01:02:08,400 --> 01:02:10,970
 

3539
01:02:08,410 --> 01:02:13,190
 well this doesn't quite work the video

3540
01:02:10,960 --> 01:02:13,190
 

3541
01:02:10,970 --> 01:02:14,720
 but it's okay so they need to predict

3542
01:02:13,180 --> 01:02:14,720
 

3543
01:02:13,190 --> 01:02:18,440
 whether the video snippet display

3544
01:02:14,710 --> 01:02:18,440
 

3545
01:02:14,720 --> 01:02:20,150
 forward or backward any time okay and in

3546
01:02:18,430 --> 01:02:20,150
 

3547
01:02:18,440 --> 01:02:24,619
 order to do this the network has to

3548
01:02:20,140 --> 01:02:24,619
 

3549
01:02:20,150 --> 01:02:26,480
 understand things like gravity so here

3550
01:02:24,609 --> 01:02:26,480
 

3551
01:02:24,619 --> 01:02:29,540
 you would see a drop of water falling or

3552
01:02:26,470 --> 01:02:29,540
 

3553
01:02:26,480 --> 01:02:32,599
 upward so you need to understand gravity

3554
01:02:29,530 --> 01:02:32,599
 

3555
01:02:29,540 --> 01:02:38,150
 you need to understand causality prick

3556
01:02:32,589 --> 01:02:38,150
 

3557
01:02:32,599 --> 01:02:40,089
 shown entropy crime and so the way that

3558
01:02:38,140 --> 01:02:40,089
 

3559
01:02:38,150 --> 01:02:42,470
 we train this network is very much

3560
01:02:40,079 --> 01:02:42,470
 

3561
01:02:40,089 --> 01:02:44,420
 similar to the network that we saw

3562
01:02:42,460 --> 01:02:44,420
 

3563
01:02:42,470 --> 01:02:47,359
 before where they divide the video

3564
01:02:44,410 --> 01:02:47,359
 

3565
01:02:44,420 --> 01:02:49,430
 snippet into chunks every trunk is paid

3566
01:02:47,349 --> 01:02:49,430
 

3567
01:02:47,359 --> 01:02:51,740
 to the network in terms of RGB values an

3568
01:02:49,420 --> 01:02:51,740
 

3569
01:02:49,430 --> 01:02:53,660
 optical flow you go through a cylinder

3570
01:02:51,730 --> 01:02:53,660
 

3571
01:02:51,740 --> 01:02:55,190
 extra features and then you get these

3572
01:02:53,650 --> 01:02:55,190
 

3573
01:02:53,660 --> 01:02:57,170
 features to a classifier to a binary

3574
01:02:55,180 --> 01:02:57,170
 

3575
01:02:55,190 --> 01:03:00,170
 classifier essentially okay and you

3576
01:02:57,160 --> 01:03:00,170
 

3577
01:02:57,170 --> 01:03:03,950
 train everything by Pedro and if you use

3578
01:03:00,160 --> 01:03:03,950
 

3579
01:03:00,170 --> 01:03:06,109
 these weights initializes CNN for actual

3580
01:03:03,940 --> 01:03:06,109
 

3581
01:03:03,950 --> 01:03:08,420
 recognition on this you see f11 that

3582
01:03:06,099 --> 01:03:08,420
 

3583
01:03:06,109 --> 01:03:10,580
 aside you find that this initialization

3584
01:03:08,410 --> 01:03:10,580
 

3585
01:03:08,420 --> 01:03:14,030
 works even better than if you were to

3586
01:03:10,570 --> 01:03:14,030
 

3587
01:03:10,580 --> 01:03:18,080
 use a network trained on image not with

3588
01:03:14,020 --> 01:03:18,080
 

3589
01:03:14,030 --> 01:03:21,080
 supervision there are several other

3590
01:03:18,070 --> 01:03:21,080
 

3591
01:03:18,080 --> 01:03:23,630
 approaches to force a supervised

3592
01:03:21,070 --> 01:03:23,630
 

3593
01:03:21,080 --> 01:03:26,320
 learning on videos in general like

3594
01:03:23,620 --> 01:03:26,320
 

3595
01:03:23,630 --> 01:03:28,970
 crediting one modality from the other

3596
01:03:26,310 --> 01:03:28,970
 

3597
01:03:26,320 --> 01:03:34,010
 predicting how

3598
01:03:28,960 --> 01:03:34,010
 

3599
01:03:28,970 --> 01:03:38,240
 input frames that will shuffle but I

3600
01:03:34,000 --> 01:03:38,240
 

3601
01:03:34,010 --> 01:03:40,390
 hope that you got the overall idea there

3602
01:03:38,230 --> 01:03:40,390
 

3603
01:03:38,240 --> 01:03:42,349
 are other approaches to learn

3604
01:03:40,380 --> 01:03:42,349
 

3605
01:03:40,390 --> 01:03:44,000
 representations of images that are not

3606
01:03:42,339 --> 01:03:44,000
 

3607
01:03:42,349 --> 01:03:46,930
 based on such supervised learning and

3608
01:03:43,990 --> 01:03:46,930
 

3609
01:03:44,000 --> 01:03:52,070
 here I want to show you another example

3610
01:03:46,920 --> 01:03:52,070
 

3611
01:03:46,930 --> 01:03:55,280
 so here the idea is based on the

3612
01:03:52,060 --> 01:03:55,280
 

3613
01:03:52,070 --> 01:03:58,220
 following intuition that a CNN has a lot

3614
01:03:55,270 --> 01:03:58,220
 

3615
01:03:55,280 --> 01:04:01,609
 of good inductive biases that really

3616
01:03:58,210 --> 01:04:01,609
 

3617
01:03:58,220 --> 01:04:04,670
 match the statistics of natural images

3618
01:04:01,599 --> 01:04:04,670
 

3619
01:04:01,609 --> 01:04:06,230
 and sounds lame moreover because a

3620
01:04:04,660 --> 01:04:06,230
 

3621
01:04:04,670 --> 01:04:08,510
 communication you are not using small

3622
01:04:06,220 --> 01:04:08,510
 

3623
01:04:06,230 --> 01:04:11,330
 filters if you do if you do a pulley

3624
01:04:08,500 --> 01:04:11,330
 

3625
01:04:08,510 --> 01:04:13,970
 analysis of a small kernel you find that

3626
01:04:11,320 --> 01:04:13,970
 

3627
01:04:11,330 --> 01:04:16,400
 there is orientation and frequency

3628
01:04:13,960 --> 01:04:16,400
 

3629
01:04:13,970 --> 01:04:18,980
 selectivity and so if you put together

3630
01:04:16,390 --> 01:04:18,980
 

3631
01:04:16,400 --> 01:04:21,349
 these two findings you find that even a

3632
01:04:18,970 --> 01:04:21,349
 

3633
01:04:18,980 --> 01:04:25,490
 randomly initialized CNN extracts

3634
01:04:21,339 --> 01:04:25,490
 

3635
01:04:21,349 --> 01:04:29,960
 non-trivial pictures and so in this work

3636
01:04:25,480 --> 01:04:29,960
 

3637
01:04:25,490 --> 01:04:32,330
 by Quran at all they do something super

3638
01:04:29,950 --> 01:04:32,330
 

3639
01:04:29,960 --> 01:04:34,400
 simple and super effective so the first

3640
01:04:32,320 --> 01:04:34,400
 

3641
01:04:32,330 --> 01:04:36,109
 step they put every image they start the

3642
01:04:34,390 --> 01:04:36,109
 

3643
01:04:34,400 --> 01:04:38,599
 pictures using the current parameters of

3644
01:04:36,099 --> 01:04:38,599
 

3645
01:04:36,109 --> 01:04:41,390
 the CNN initially they are set around oh

3646
01:04:38,589 --> 01:04:41,390
 

3647
01:04:38,599 --> 01:04:43,240
 and then they run k-means in feature

3648
01:04:41,380 --> 01:04:43,240
 

3649
01:04:41,390 --> 01:04:46,640
 space so now for every image you

3650
01:04:43,230 --> 01:04:46,640
 

3651
01:04:43,240 --> 01:04:49,609
 associate a cluster ID and then in the

3652
01:04:46,630 --> 01:04:49,609
 

3653
01:04:46,640 --> 01:04:51,619
 second step they train the parameters of

3654
01:04:49,599 --> 01:04:51,619
 

3655
01:04:49,609 --> 01:04:54,589
 the model to predict from the image the

3656
01:04:51,609 --> 01:04:54,589
 

3657
01:04:51,619 --> 01:04:56,359
 cluster assignment and now when you do

3658
01:04:54,579 --> 01:04:56,359
 

3659
01:04:54,589 --> 01:04:58,040
 that you refine the pictures and then

3660
01:04:56,349 --> 01:04:58,040
 

3661
01:04:56,359 --> 01:05:00,020
 you can go back to the previous tab and

3662
01:04:58,030 --> 01:05:00,020
 

3663
01:04:58,040 --> 01:05:05,000
 we extraor the pictures and rerun

3664
01:05:00,010 --> 01:05:05,000
 

3665
01:05:00,020 --> 01:05:06,710
 k-means and so if you just be careful

3666
01:05:04,990 --> 01:05:06,710
 

3667
01:05:05,000 --> 01:05:10,790
 about a couple of things like making

3668
01:05:06,700 --> 01:05:10,790
 

3669
01:05:06,710 --> 01:05:12,560
 sure that clusters don't collapse and so

3670
01:05:10,780 --> 01:05:12,560
 

3671
01:05:10,790 --> 01:05:14,390
 in that case you need to reassign images

3672
01:05:12,550 --> 01:05:14,390
 

3673
01:05:12,560 --> 01:05:17,359
 to clusters and if you make sure to

3674
01:05:14,380 --> 01:05:17,359
 

3675
01:05:14,390 --> 01:05:20,990
 equalize the priorities the bution

3676
01:05:17,349 --> 01:05:20,990
 

3677
01:05:17,359 --> 01:05:23,359
 clusters did in training then this

3678
01:05:20,980 --> 01:05:23,359
 

3679
01:05:20,990 --> 01:05:26,930
 matter works really well and for the

3680
01:05:23,349 --> 01:05:26,930
 

3681
01:05:23,359 --> 01:05:28,610
 image net classification task you get

3682
01:05:26,920 --> 01:05:28,610
 

3683
01:05:26,930 --> 01:05:30,560
 the performance that this year on the

3684
01:05:28,600 --> 01:05:30,560
 

3685
01:05:28,610 --> 01:05:33,680
 with a red bar which is much better than

3686
01:05:30,550 --> 01:05:33,680
 

3687
01:05:30,560 --> 01:05:36,020
 if you pre train using such supervised

3688
01:05:33,670 --> 01:05:36,020
 

3689
01:05:33,680 --> 01:05:37,760
 learning approaches like the Green Line

3690
01:05:36,010 --> 01:05:37,760
 

3691
01:05:36,020 --> 01:05:39,230
 is the one that I described earlier on

3692
01:05:37,750 --> 01:05:39,230
 

3693
01:05:37,760 --> 01:05:43,040
 where you predict a special relationship

3694
01:05:39,220 --> 01:05:43,040
 

3695
01:05:39,230 --> 01:05:45,490
 between emits patches I can step it

3696
01:05:43,030 --> 01:05:45,490
 

3697
01:05:43,040 --> 01:05:49,810
 apart from supervised learning but it's

3698
01:05:45,480 --> 01:05:49,810
 

3699
01:05:45,490 --> 01:05:55,760
 remarkable progress towards that and so

3700
01:05:49,800 --> 01:05:55,760
 

3701
01:05:49,810 --> 01:05:56,950
 to conclude this part there has been a

3702
01:05:55,750 --> 01:05:56,950
 

3703
01:05:55,760 --> 01:05:58,880
 lot of progress on learning

3704
01:05:56,940 --> 01:05:58,880
 

3705
01:05:56,950 --> 01:06:03,740
 representations in a supervised manner

3706
01:05:58,870 --> 01:06:03,740
 

3707
01:05:58,880 --> 01:06:05,540
 for representing images in particular

3708
01:06:03,730 --> 01:06:05,540
 

3709
01:06:03,740 --> 01:06:06,890
 there is a lot of excitement about using

3710
01:06:05,530 --> 01:06:06,890
 

3711
01:06:05,540 --> 01:06:08,270
 such supervised learning approaches

3712
01:06:06,880 --> 01:06:08,270
 

3713
01:06:06,890 --> 01:06:10,840
 where you use some domain knowledge to

3714
01:06:08,260 --> 01:06:10,840
 

3715
01:06:08,270 --> 01:06:13,640
 define a task and auxiliary task that

3716
01:06:10,830 --> 01:06:13,640
 

3717
01:06:10,840 --> 01:06:18,560
 requires an semantic understanding and

3718
01:06:13,630 --> 01:06:18,560
 

3719
01:06:13,640 --> 01:06:21,350
 for which you try to remove the original

3720
01:06:18,550 --> 01:06:21,350
 

3721
01:06:18,560 --> 01:06:23,990
 kind of regression task when you move to

3722
01:06:21,340 --> 01:06:23,990
 

3723
01:06:21,350 --> 01:06:25,370
 a classification task and the only thing

3724
01:06:23,980 --> 01:06:25,370
 

3725
01:06:23,990 --> 01:06:26,720
 that you need to make sure is that the

3726
01:06:25,360 --> 01:06:26,720
 

3727
01:06:25,370 --> 01:06:29,470
 natural doesn't see that it doesn't

3728
01:06:26,710 --> 01:06:29,470
 

3729
01:06:26,720 --> 01:06:32,090
 collapse features or it doesn't exploit

3730
01:06:29,460 --> 01:06:32,090
 

3731
01:06:29,470 --> 01:06:33,770
 biases in the data side to perform the

3732
01:06:32,080 --> 01:06:33,770
 

3733
01:06:32,090 --> 01:06:35,900
 task that you want and so there are some

3734
01:06:33,760 --> 01:06:35,900
 

3735
01:06:33,770 --> 01:06:39,670
 subtleties here and these are very much

3736
01:06:35,890 --> 01:06:39,670
 

3737
01:06:35,900 --> 01:06:43,090
 metal dependent so let me now go on

3738
01:06:39,660 --> 01:06:43,090
 

3739
01:06:39,670 --> 01:06:46,300
 unsupervised feature learning for text

3740
01:06:43,080 --> 01:06:46,300
 

3741
01:06:43,090 --> 01:06:48,920
 as an example were in the discrete space

3742
01:06:46,290 --> 01:06:48,920
 

3743
01:06:46,300 --> 01:06:52,670
 so here there is a fundamental

3744
01:06:48,910 --> 01:06:52,670
 

3745
01:06:48,920 --> 01:06:55,880
 difference between images and texts in

3746
01:06:52,660 --> 01:06:55,880
 

3747
01:06:52,670 --> 01:06:59,090
 the sense that the atomic unit in text

3748
01:06:55,870 --> 01:06:59,090
 

3749
01:06:55,880 --> 01:07:01,790
 is a world which is a discrete total

3750
01:06:59,080 --> 01:07:01,790
 

3751
01:06:59,090 --> 01:07:05,630
 right why in images typically pixel

3752
01:07:01,780 --> 01:07:05,630
 

3753
01:07:01,790 --> 01:07:08,810
 values are treated as a real value

3754
01:07:05,620 --> 01:07:08,810
 

3755
01:07:05,630 --> 01:07:11,930
 object and while the worker is a lot of

3756
01:07:08,800 --> 01:07:11,930
 

3757
01:07:08,810 --> 01:07:16,210
 information a pixel value carries almost

3758
01:07:11,920 --> 01:07:16,210
 

3759
01:07:11,930 --> 01:07:20,180
 no information at all and so because

3760
01:07:16,200 --> 01:07:20,180
 

3761
01:07:16,210 --> 01:07:21,980
 images are continuous search is

3762
01:07:20,170 --> 01:07:21,980
 

3763
01:07:20,180 --> 01:07:24,290
 typically easy you can do grainy descent

3764
01:07:21,970 --> 01:07:24,290
 

3765
01:07:21,980 --> 01:07:25,730
 but in text we need to its a

3766
01:07:24,280 --> 01:07:25,730
 

3767
01:07:24,290 --> 01:07:27,080
 combinatorial problem so you need to

3768
01:07:25,720 --> 01:07:27,080
 

3769
01:07:25,730 --> 01:07:28,940
 have some you need to use some

3770
01:07:27,070 --> 01:07:28,940
 

3771
01:07:27,080 --> 01:07:32,290
 heuristics at the same time because

3772
01:07:28,930 --> 01:07:32,290
 

3773
01:07:28,940 --> 01:07:34,300
 images are discrete modeling is

3774
01:07:32,280 --> 01:07:34,300
 

3775
01:07:32,290 --> 01:07:36,280
 a little easier because you use a

3776
01:07:34,290 --> 01:07:36,280
 

3777
01:07:34,300 --> 01:07:38,920
 multinomial distribution so if you want

3778
01:07:36,270 --> 01:07:38,920
 

3779
01:07:36,280 --> 01:07:39,340
 to modern certainty you have it right

3780
01:07:38,910 --> 01:07:39,340
 

3781
01:07:38,920 --> 01:07:43,060
 there

3782
01:07:39,330 --> 01:07:43,060
 

3783
01:07:39,340 --> 01:07:45,460
 why for images modeling if you stay in

3784
01:07:43,050 --> 01:07:45,460
 

3785
01:07:43,060 --> 01:07:48,490
 the continued space is a little harder

3786
01:07:45,450 --> 01:07:48,490
 

3787
01:07:45,460 --> 01:07:53,430
 and so let me start again by a brief

3788
01:07:48,480 --> 01:07:53,430
 

3789
01:07:48,490 --> 01:07:56,650
 historical overview since text is is

3790
01:07:53,420 --> 01:07:56,650
 

3791
01:07:53,430 --> 01:07:58,750
 symbolic in nature I would date back

3792
01:07:56,640 --> 01:07:58,750
 

3793
01:07:56,650 --> 01:08:02,700
 everything to church boo and his

3794
01:07:58,740 --> 01:08:02,700
 

3795
01:07:58,750 --> 01:08:09,160
 contribution to logic reason right and

3796
01:08:02,690 --> 01:08:09,160
 

3797
01:08:02,700 --> 01:08:12,400
 the use of treating text as in terms of

3798
01:08:09,150 --> 01:08:12,400
 

3799
01:08:09,160 --> 01:08:13,690
 symbols has had a long influence if you

3800
01:08:12,390 --> 01:08:13,690
 

3801
01:08:12,400 --> 01:08:15,460
 think about a Turing machine what you

3802
01:08:13,680 --> 01:08:15,460
 

3803
01:08:13,690 --> 01:08:18,460
 write on the tape is a single right

3804
01:08:15,450 --> 01:08:18,460
 

3805
01:08:15,460 --> 01:08:22,140
 and so it was not until the 50s that

3806
01:08:18,450 --> 01:08:22,140
 

3807
01:08:18,460 --> 01:08:25,569
 people consider associating to symbols

3808
01:08:22,130 --> 01:08:25,569
 

3809
01:08:22,140 --> 01:08:26,980
 distributed representations right but at

3810
01:08:25,559 --> 01:08:26,980
 

3811
01:08:25,569 --> 01:08:30,130
 that time they didn't quite know how to

3812
01:08:26,970 --> 01:08:30,130
 

3813
01:08:26,980 --> 01:08:33,310
 train them and so they were considering

3814
01:08:30,120 --> 01:08:33,310
 

3815
01:08:30,130 --> 01:08:36,510
 only leaner single layer methods like

3816
01:08:33,300 --> 01:08:36,510
 

3817
01:08:33,310 --> 01:08:39,520
 the perceptron and that had clear

3818
01:08:36,500 --> 01:08:39,520
 

3819
01:08:36,510 --> 01:08:42,130
 limitations that were highlighted in the

3820
01:08:39,510 --> 01:08:42,130
 

3821
01:08:39,520 --> 01:08:45,609
 purple pipe apparel Minsky and so it was

3822
01:08:42,120 --> 01:08:45,609
 

3823
01:08:42,130 --> 01:08:48,069
 really until the introduction of a

3824
01:08:45,599 --> 01:08:48,069
 

3825
01:08:45,609 --> 01:08:52,200
 propagation in the mid 80s that people

3826
01:08:48,059 --> 01:08:52,200
 

3827
01:08:48,069 --> 01:08:52,200
 were able to actually train effectively

3828
01:08:52,460 --> 01:08:52,460
 

3829
01:08:52,470 --> 01:08:59,319
 neural nets and if you if you look at

3830
01:08:55,500 --> 01:08:59,319
 

3831
01:08:55,510 --> 01:09:02,950
 sapiens papers from 1986 you already

3832
01:08:59,309 --> 01:09:02,950
 

3833
01:08:59,319 --> 01:09:06,310
 find the notion of associating towards

3834
01:09:02,940 --> 01:09:06,310
 

3835
01:09:02,950 --> 01:09:09,160
 word embeddings so a distributed

3836
01:09:06,300 --> 01:09:09,160
 

3837
01:09:06,310 --> 01:09:14,859
 representation and so I would say that

3838
01:09:09,150 --> 01:09:14,859
 

3839
01:09:09,160 --> 01:09:18,810
 if you look at LSA so which also came in

3840
01:09:14,849 --> 01:09:18,810
 

3841
01:09:14,859 --> 01:09:21,460
 the late 80s there you you be the word

3842
01:09:18,800 --> 01:09:21,460
 

3843
01:09:18,810 --> 01:09:24,010
 document count matrix where documents

3844
01:09:21,450 --> 01:09:24,010
 

3845
01:09:21,460 --> 01:09:27,069
 could be sentences and you apply a PCA

3846
01:09:24,000 --> 01:09:27,069
 

3847
01:09:24,010 --> 01:09:28,900
 on that matrix and that or that PC

3848
01:09:27,059 --> 01:09:28,900
 

3849
01:09:27,069 --> 01:09:31,270
 decomposition already gives you word

3850
01:09:28,890 --> 01:09:31,270
 

3851
01:09:28,900 --> 01:09:34,660
 embeddings right and so again the

3852
01:09:31,260 --> 01:09:34,660
 

3853
01:09:31,270 --> 01:09:36,790
 bedrock of what we do is really PCA

3854
01:09:34,650 --> 01:09:36,790
 

3855
01:09:34,660 --> 01:09:39,819
 again very much like we saw in vision

3856
01:09:36,780 --> 01:09:39,819
 

3857
01:09:36,790 --> 01:09:42,220
 and if you think about if you pass

3858
01:09:39,809 --> 01:09:42,220
 

3859
01:09:39,819 --> 01:09:44,260
 forward and you go to 2013 what

3860
01:09:42,210 --> 01:09:44,260
 

3861
01:09:42,220 --> 01:09:47,590
 - back where - back is essentially an

3862
01:09:44,250 --> 01:09:47,590
 

3863
01:09:44,260 --> 01:09:50,020
 online version of LSA where Europe

3864
01:09:47,580 --> 01:09:50,020
 

3865
01:09:47,590 --> 01:09:52,390
 replaced the miss forever loss with

3866
01:09:50,010 --> 01:09:52,390
 

3867
01:09:50,020 --> 01:09:56,040
 croissant appearance so these things are

3868
01:09:52,380 --> 01:09:56,040
 

3869
01:09:52,390 --> 01:09:58,540
 intimately related going back to the 80s

3870
01:09:56,030 --> 01:09:58,540
 

3871
01:09:56,040 --> 01:10:01,870
 there was a lot of excitement again time

3872
01:09:58,530 --> 01:10:01,870
 

3873
01:09:58,540 --> 01:10:04,210
 we went through a winter session in

3874
01:10:01,860 --> 01:10:04,210
 

3875
01:10:01,870 --> 01:10:07,330
 which people were mostly considering

3876
01:10:04,200 --> 01:10:07,330
 

3877
01:10:04,210 --> 01:10:09,780
 count based representations in the early

3878
01:10:07,320 --> 01:10:09,780
 

3879
01:10:07,330 --> 01:10:13,840
 2000 there was a lot of excitement on

3880
01:10:09,770 --> 01:10:13,840
 

3881
01:10:09,780 --> 01:10:16,810
 topic modeling so a way to associate

3882
01:10:13,830 --> 01:10:16,810
 

3883
01:10:13,840 --> 01:10:18,910
 kind of discrete representations at the

3884
01:10:16,800 --> 01:10:18,910
 

3885
01:10:16,810 --> 01:10:20,650
 document level and the major

3886
01:10:18,900 --> 01:10:20,650
 

3887
01:10:18,910 --> 01:10:23,230
 contribution by too much McCallum in

3888
01:10:20,640 --> 01:10:23,230
 

3889
01:10:20,650 --> 01:10:25,450
 2013 was to make available a tool box to

3890
01:10:23,220 --> 01:10:25,450
 

3891
01:10:23,230 --> 01:10:27,790
 learn word embeddings and showing that

3892
01:10:25,440 --> 01:10:27,790
 

3893
01:10:25,450 --> 01:10:31,630
 this could be learned very efficiently

3894
01:10:27,780 --> 01:10:31,630
 

3895
01:10:27,790 --> 01:10:33,760
 and it's a pretty general purpose thing

3896
01:10:31,620 --> 01:10:33,760
 

3897
01:10:31,630 --> 01:10:35,800
 as we shall see data and since then it

3898
01:10:33,750 --> 01:10:35,800
 

3899
01:10:33,760 --> 01:10:38,260
 has been a lot of interesting lyric

3900
01:10:35,790 --> 01:10:38,260
 

3901
01:10:35,800 --> 01:10:40,750
 community to represent not only words

3902
01:10:38,250 --> 01:10:40,750
 

3903
01:10:38,260 --> 01:10:42,520
 but also sentences and I'm going to

3904
01:10:40,740 --> 01:10:42,520
 

3905
01:10:40,750 --> 01:10:46,600
 describe the latest contribution towards

3906
01:10:42,510 --> 01:10:46,600
 

3907
01:10:42,520 --> 01:10:49,390
 that so let me briefly summarize War -

3908
01:10:46,590 --> 01:10:49,390
 

3909
01:10:46,600 --> 01:10:53,320
 back so here the idea is that the

3910
01:10:49,380 --> 01:10:53,320
 

3911
01:10:49,390 --> 01:10:57,550
 meaning of a war depends is determined

3912
01:10:53,310 --> 01:10:57,550
 

3913
01:10:53,320 --> 01:10:59,500
 by the context so if I hide a war here

3914
01:10:57,540 --> 01:10:59,500
 

3915
01:10:57,550 --> 01:11:01,780
 probably buried in the sentence you can

3916
01:10:59,490 --> 01:11:01,780
 

3917
01:10:59,500 --> 01:11:04,000
 figure out what the mean and the missing

3918
01:11:01,770 --> 01:11:04,000
 

3919
01:11:01,780 --> 01:11:05,920
 word is so the context really determines

3920
01:11:03,990 --> 01:11:05,920
 

3921
01:11:04,000 --> 01:11:08,800
 the meaning of that word not only

3922
01:11:05,910 --> 01:11:08,800
 

3923
01:11:05,920 --> 01:11:11,940
 semantically similar words often share

3924
01:11:08,790 --> 01:11:11,940
 

3925
01:11:08,800 --> 01:11:16,360
 the same context and so forth - back

3926
01:11:11,930 --> 01:11:16,360
 

3927
01:11:11,940 --> 01:11:19,360
 leverages this insight and associates to

3928
01:11:16,350 --> 01:11:19,360
 

3929
01:11:16,360 --> 01:11:22,720
 a brick war a continuous representation

3930
01:11:19,350 --> 01:11:22,720
 

3931
01:11:19,360 --> 01:11:25,000
 a vector in R T so for every word in the

3932
01:11:22,710 --> 01:11:25,000
 

3933
01:11:22,720 --> 01:11:26,800
 dictionary you have a Victini RT and

3934
01:11:24,990 --> 01:11:26,800
 

3935
01:11:25,000 --> 01:11:28,690
 initially these things are initialized

3936
01:11:26,790 --> 01:11:28,690
 

3937
01:11:26,800 --> 01:11:31,960
 at random and then what you do you take

3938
01:11:28,680 --> 01:11:31,960
 

3939
01:11:28,690 --> 01:11:34,180
 a small window of text ok let's say some

3940
01:11:31,950 --> 01:11:34,180
 

3941
01:11:31,960 --> 01:11:35,980
 words you hide them you take the word in

3942
01:11:34,170 --> 01:11:35,980
 

3943
01:11:34,180 --> 01:11:37,780
 the middle you fetch the corresponding

3944
01:11:35,970 --> 01:11:37,780
 

3945
01:11:35,980 --> 01:11:40,660
 word embodying represented by this

3946
01:11:37,770 --> 01:11:40,660
 

3947
01:11:37,780 --> 01:11:44,680
 rectangle and then you project this word

3948
01:11:40,650 --> 01:11:44,680
 

3949
01:11:40,660 --> 01:11:46,570
 embedding into V L in a matrix through a

3950
01:11:44,670 --> 01:11:46,570
 

3951
01:11:44,680 --> 01:11:47,590
 distribution over words in the

3952
01:11:46,560 --> 01:11:47,590
 

3953
01:11:46,570 --> 01:11:50,590
 dictionary you tried

3954
01:11:47,580 --> 01:11:50,590
 

3955
01:11:47,590 --> 01:11:53,290
 predict the words in the context and you

3956
01:11:50,580 --> 01:11:53,290
 

3957
01:11:50,590 --> 01:11:56,110
 train this by press entropy loss and so

3958
01:11:53,280 --> 01:11:56,110
 

3959
01:11:53,290 --> 01:11:59,290
 it's a simple linearly linear model that

3960
01:11:56,100 --> 01:11:59,290
 

3961
01:11:56,110 --> 01:12:01,990
 is factorized that is train via back row

3962
01:11:59,280 --> 01:12:01,990
 

3963
01:11:59,290 --> 01:12:04,210
 and what you find is that word

3964
01:12:01,980 --> 01:12:04,210
 

3965
01:12:01,990 --> 01:12:08,020
 embeddings or similar words end up being

3966
01:12:04,200 --> 01:12:08,020
 

3967
01:12:04,210 --> 01:12:10,270
 nearest neighbor and the embedded space

3968
01:12:08,010 --> 01:12:10,270
 

3969
01:12:08,020 --> 01:12:12,430
 has nice geometric properties for

3970
01:12:10,260 --> 01:12:12,430
 

3971
01:12:10,270 --> 01:12:14,710
 instance if you take the word embedding

3972
01:12:12,420 --> 01:12:14,710
 

3973
01:12:12,430 --> 01:12:19,930
 of Kim's and you subtract the word the

3974
01:12:14,700 --> 01:12:19,930
 

3975
01:12:14,710 --> 01:12:22,300
 module King you get back door the

3976
01:12:19,920 --> 01:12:22,300
 

3977
01:12:19,930 --> 01:12:24,820
 represents number and so if you add that

3978
01:12:22,290 --> 01:12:24,820
 

3979
01:12:22,300 --> 01:12:26,800
 vector to the word embody of Queen you

3980
01:12:24,810 --> 01:12:26,800
 

3981
01:12:24,820 --> 01:12:29,770
 find a batter that is nearest neighbor

3982
01:12:26,790 --> 01:12:29,770
 

3983
01:12:26,800 --> 01:12:32,310
 to the world Queens and the same is true

3984
01:12:29,760 --> 01:12:32,310
 

3985
01:12:29,770 --> 01:12:36,070
 for China and for other properties and

3986
01:12:32,300 --> 01:12:36,070
 

3987
01:12:32,310 --> 01:12:39,100
 so the beauty of water back is that its

3988
01:12:36,060 --> 01:12:39,100
 

3989
01:12:36,070 --> 01:12:41,530
 simplicity and its generality so you can

3990
01:12:39,090 --> 01:12:41,530
 

3991
01:12:39,100 --> 01:12:43,690
 apply the same ideas also to graphs

3992
01:12:41,520 --> 01:12:43,690
 

3993
01:12:41,530 --> 01:12:46,750
 right so you can say a node in a graph

3994
01:12:43,680 --> 01:12:46,750
 

3995
01:12:43,690 --> 01:12:49,540
 is represented by its neighbors and so I

3996
01:12:46,740 --> 01:12:49,540
 

3997
01:12:46,750 --> 01:12:51,250
 can learn an embedding of each node in

3998
01:12:49,530 --> 01:12:51,250
 

3999
01:12:49,540 --> 01:12:54,580
 the graph by predicting who the

4000
01:12:51,240 --> 01:12:54,580
 

4001
01:12:51,250 --> 01:12:56,800
 neighbors are right and again the idea

4002
01:12:54,570 --> 01:12:56,800
 

4003
01:12:54,580 --> 01:12:59,320
 is to predict a word from the context

4004
01:12:56,790 --> 01:12:59,320
 

4005
01:12:56,800 --> 01:13:01,930
 man so now the question is how can we

4006
01:12:59,310 --> 01:13:01,930
 

4007
01:12:59,320 --> 01:13:04,210
 extend this to represent sentences

4008
01:13:01,920 --> 01:13:04,210
 

4009
01:13:01,930 --> 01:13:06,720
 obviously we can not enumerate all

4010
01:13:04,200 --> 01:13:06,720
 

4011
01:13:04,210 --> 01:13:09,160
 possible sentences the space is

4012
01:13:06,710 --> 01:13:09,160
 

4013
01:13:06,720 --> 01:13:11,020
 comunitaria they are way too much we

4014
01:13:09,150 --> 01:13:11,020
 

4015
01:13:09,160 --> 01:13:14,560
 need to leverage some compositionality

4016
01:13:11,010 --> 01:13:14,560
 

4017
01:13:11,020 --> 01:13:17,320
 that is in language and so I'm going to

4018
01:13:14,550 --> 01:13:17,320
 

4019
01:13:14,560 --> 01:13:20,290
 present the latest advancement in the

4020
01:13:17,310 --> 01:13:20,290
 

4021
01:13:17,320 --> 01:13:23,080
 field this is called birth by doubling

4022
01:13:20,280 --> 01:13:23,080
 

4023
01:13:20,290 --> 01:13:26,350
 at all and so here they take two

4024
01:13:23,070 --> 01:13:26,350
 

4025
01:13:23,080 --> 01:13:29,020
 sentences okay here you have a special

4026
01:13:26,340 --> 01:13:29,020
 

4027
01:13:26,350 --> 01:13:30,490
 topic that says I'm going to start my

4028
01:13:29,010 --> 01:13:30,490
 

4029
01:13:29,020 --> 01:13:32,530
 sentence here and you have another

4030
01:13:30,480 --> 01:13:32,530
 

4031
01:13:30,490 --> 01:13:34,360
 special token that says this is the

4032
01:13:32,520 --> 01:13:34,360
 

4033
01:13:32,530 --> 01:13:37,270
 separation between the two sentences

4034
01:13:34,350 --> 01:13:37,270
 

4035
01:13:34,360 --> 01:13:40,150
 okay and then for every token you have

4036
01:13:37,260 --> 01:13:40,150
 

4037
01:13:37,270 --> 01:13:42,250
 like a deep net so you have a lot of

4038
01:13:40,140 --> 01:13:42,250
 

4039
01:13:40,150 --> 01:13:44,310
 computational blocks that form a

4040
01:13:42,240 --> 01:13:44,310
 

4041
01:13:42,250 --> 01:13:47,260
 deepness and each computational block

4042
01:13:44,300 --> 01:13:47,260
 

4043
01:13:44,310 --> 01:13:49,510
 takes as input from all the blocks of

4044
01:13:47,250 --> 01:13:49,510
 

4045
01:13:47,260 --> 01:13:51,730
 the layer below now because we want to

4046
01:13:49,500 --> 01:13:51,730
 

4047
01:13:49,510 --> 01:13:54,850
 work with sentences we don't want to

4048
01:13:51,720 --> 01:13:54,850
 

4049
01:13:51,730 --> 01:13:55,230
 restrict the land to be a certain length

4050
01:13:54,840 --> 01:13:55,230
 

4051
01:13:54,850 --> 01:13:56,660
 right

4052
01:13:55,220 --> 01:13:56,660
 

4053
01:13:55,230 --> 01:14:00,840
 want to work with but it will land

4054
01:13:56,650 --> 01:14:00,840
 

4055
01:13:56,660 --> 01:14:03,750
 objects and so one way to do this is to

4056
01:14:00,830 --> 01:14:03,750
 

4057
01:14:00,840 --> 01:14:06,930
 use attention here I'm going to give a

4058
01:14:03,740 --> 01:14:06,930
 

4059
01:14:03,750 --> 01:14:08,970
 brief high-level idea of how attention

4060
01:14:06,920 --> 01:14:08,970
 

4061
01:14:06,930 --> 01:14:10,710
 works so for every block let's say we

4062
01:14:08,960 --> 01:14:10,710
 

4063
01:14:08,970 --> 01:14:14,880
 are looking at the world set at a given

4064
01:14:10,700 --> 01:14:14,880
 

4065
01:14:10,710 --> 01:14:17,130
 layer of this architecture for this

4066
01:14:14,870 --> 01:14:17,130
 

4067
01:14:14,880 --> 01:14:20,880
 blood you have a representation let's

4068
01:14:17,120 --> 01:14:20,880
 

4069
01:14:17,130 --> 01:14:22,950
 call it HJ and each block has a certain

4070
01:14:20,870 --> 01:14:22,950
 

4071
01:14:20,880 --> 01:14:25,770
 representation what you do you do a dot

4072
01:14:22,940 --> 01:14:25,770
 

4073
01:14:22,950 --> 01:14:27,630
 product between the representation this

4074
01:14:25,760 --> 01:14:27,630
 

4075
01:14:25,770 --> 01:14:29,940
 block and the representation all the

4076
01:14:27,620 --> 01:14:29,940
 

4077
01:14:27,630 --> 01:14:31,920
 blocks are the words in the context so

4078
01:14:29,930 --> 01:14:31,920
 

4079
01:14:29,940 --> 01:14:33,270
 now you're going to get when you do the

4080
01:14:31,910 --> 01:14:33,270
 

4081
01:14:31,920 --> 01:14:35,670
 dot product between two vectors you get

4082
01:14:33,260 --> 01:14:35,670
 

4083
01:14:33,270 --> 01:14:38,310
 a scalar and then you turn a step three

4084
01:14:35,660 --> 01:14:38,310
 

4085
01:14:35,670 --> 01:14:40,350
 here you turn these scalars into a

4086
01:14:38,300 --> 01:14:40,350
 

4087
01:14:38,310 --> 01:14:42,510
 probability distribution so numbers that

4088
01:14:40,340 --> 01:14:42,510
 

4089
01:14:40,350 --> 01:14:43,140
 are positive and sum to 1 by going to a

4090
01:14:42,500 --> 01:14:43,140
 

4091
01:14:42,510 --> 01:14:45,750
 softmax

4092
01:14:43,130 --> 01:14:45,750
 

4093
01:14:43,140 --> 01:14:48,210
 and then a step 4 you compute the

4094
01:14:45,740 --> 01:14:48,210
 

4095
01:14:45,750 --> 01:14:50,130
 representation for the layer above as a

4096
01:14:48,200 --> 01:14:50,130
 

4097
01:14:48,210 --> 01:14:52,280
 weighted sum of the representations of

4098
01:14:50,120 --> 01:14:52,280
 

4099
01:14:50,130 --> 01:14:56,520
 each block with weights that come from

4100
01:14:52,270 --> 01:14:56,520
 

4101
01:14:52,280 --> 01:14:59,970
 the output of the softmax a step 3 in

4102
01:14:56,510 --> 01:14:59,970
 

4103
01:14:56,520 --> 01:15:03,030
 practice this attention based mechanism

4104
01:14:59,960 --> 01:15:03,030
 

4105
01:14:59,970 --> 01:15:05,640
 is called transport Mumbai was vanya so

4106
01:15:03,020 --> 01:15:05,640
 

4107
01:15:03,030 --> 01:15:07,290
 it's a little bit more involved because

4108
01:15:05,630 --> 01:15:07,290
 

4109
01:15:05,640 --> 01:15:08,850
 you have different representations for

4110
01:15:07,280 --> 01:15:08,850
 

4111
01:15:07,290 --> 01:15:10,890
 computing this dot product and for

4112
01:15:08,840 --> 01:15:10,890
 

4113
01:15:08,850 --> 01:15:12,989
 computing this way the sum and you have

4114
01:15:10,880 --> 01:15:12,989
 

4115
01:15:10,890 --> 01:15:17,400
 multiple of these things at each layer

4116
01:15:12,979 --> 01:15:17,400
 

4117
01:15:12,989 --> 01:15:19,800
 but the overall idea is what I just

4118
01:15:17,390 --> 01:15:19,800
 

4119
01:15:17,400 --> 01:15:22,980
 described and so the idea is that

4120
01:15:19,790 --> 01:15:22,980
 

4121
01:15:19,800 --> 01:15:25,080
 eventually you get a representation that

4122
01:15:22,970 --> 01:15:25,080
 

4123
01:15:22,980 --> 01:15:30,360
 depends on all the words on the context

4124
01:15:25,070 --> 01:15:30,360
 

4125
01:15:25,080 --> 01:15:32,100
 and it is independent of the length and

4126
01:15:30,350 --> 01:15:32,100
 

4127
01:15:30,360 --> 01:15:33,989
 you do this at every layer and now let's

4128
01:15:32,090 --> 01:15:33,989
 

4129
01:15:32,100 --> 01:15:36,330
 see how this is strange so the way it is

4130
01:15:33,979 --> 01:15:36,330
 

4131
01:15:33,989 --> 01:15:38,610
 true is that there is a with a certain

4132
01:15:36,320 --> 01:15:38,610
 

4133
01:15:36,330 --> 01:15:41,460
 probability with a small probability you

4134
01:15:38,600 --> 01:15:41,460
 

4135
01:15:38,610 --> 01:15:44,640
 hide the wall you try to predict that

4136
01:15:41,450 --> 01:15:44,640
 

4137
01:15:41,460 --> 01:15:47,400
 word at the output ok then they have

4138
01:15:44,630 --> 01:15:47,400
 

4139
01:15:44,640 --> 01:15:48,750
 another so again this is an example of a

4140
01:15:47,390 --> 01:15:48,750
 

4141
01:15:47,400 --> 01:15:51,960
 denoising auto-encoder

4142
01:15:48,740 --> 01:15:51,960
 

4143
01:15:48,750 --> 01:15:55,170
 task and then with some other

4144
01:15:51,950 --> 01:15:55,170
 

4145
01:15:51,960 --> 01:15:57,570
 probability you take a word you replace

4146
01:15:55,160 --> 01:15:57,570
 

4147
01:15:55,170 --> 01:15:59,820
 that work with a random word and you try

4148
01:15:57,560 --> 01:15:59,820
 

4149
01:15:57,570 --> 01:16:00,870
 to predict at the output the original

4150
01:15:59,810 --> 01:16:00,870
 

4151
01:15:59,820 --> 01:16:04,200
 war that were

4152
01:16:00,860 --> 01:16:04,200
 

4153
01:16:00,870 --> 01:16:08,160
 in that slot another task is to predict

4154
01:16:04,190 --> 01:16:08,160
 

4155
01:16:04,200 --> 01:16:12,180
 the actual original work and yet another

4156
01:16:08,150 --> 01:16:12,180
 

4157
01:16:08,160 --> 01:16:13,800
 task is to say at the output of the very

4158
01:16:12,170 --> 01:16:13,800
 

4159
01:16:12,180 --> 01:16:15,750
 first auction that begin your sentence

4160
01:16:13,790 --> 01:16:15,750
 

4161
01:16:13,800 --> 01:16:18,900
 you try to predict whether the second

4162
01:16:15,740 --> 01:16:18,900
 

4163
01:16:15,750 --> 01:16:21,150
 sentence is a sentence taking a random

4164
01:16:18,890 --> 01:16:21,150
 

4165
01:16:18,900 --> 01:16:23,670
 from the previous side or whether it is

4166
01:16:21,140 --> 01:16:23,670
 

4167
01:16:21,150 --> 01:16:25,860
 the genuine next sentence that follows

4168
01:16:23,660 --> 01:16:25,860
 

4169
01:16:23,670 --> 01:16:28,770
 the first sentence so now you need to

4170
01:16:25,850 --> 01:16:28,770
 

4171
01:16:25,860 --> 01:16:30,390
 have some way to understand sentences

4172
01:16:28,760 --> 01:16:30,390
 

4173
01:16:28,770 --> 01:16:37,140
 that we represent sentences to perform

4174
01:16:30,380 --> 01:16:37,140
 

4175
01:16:30,390 --> 01:16:38,910
 this task the author's evaluate this

4176
01:16:37,130 --> 01:16:38,910
 

4177
01:16:37,140 --> 01:16:42,470
 model on the glue benchmark that

4178
01:16:38,900 --> 01:16:42,470
 

4179
01:16:38,910 --> 01:16:45,030
 consists of eleven different tasks from

4180
01:16:42,460 --> 01:16:45,030
 

4181
01:16:42,470 --> 01:16:46,920
 classification from in tainment and some

4182
01:16:45,020 --> 01:16:46,920
 

4183
01:16:45,030 --> 01:16:49,050
 support and they found that this method

4184
01:16:46,910 --> 01:16:49,050
 

4185
01:16:46,920 --> 01:16:51,720
 works much better than other

4186
01:16:49,040 --> 01:16:51,720
 

4187
01:16:49,050 --> 01:16:55,380
 alternatives to learn sentence

4188
01:16:51,710 --> 01:16:55,380
 

4189
01:16:51,720 --> 01:16:57,240
 representations and much better than war

4190
01:16:55,370 --> 01:16:57,240
 

4191
01:16:55,380 --> 01:17:00,390
 to back so you can see this method has a

4192
01:16:57,230 --> 01:17:00,390
 

4193
01:16:57,240 --> 01:17:03,870
 deep worth of work too back in the end

4194
01:17:00,380 --> 01:17:03,870
 

4195
01:17:00,390 --> 01:17:05,520
 right and this works even better than

4196
01:17:03,860 --> 01:17:05,520
 

4197
01:17:03,870 --> 01:17:07,770
 supervised approaches because on these

4198
01:17:05,510 --> 01:17:07,770
 

4199
01:17:05,520 --> 01:17:10,740
 data sets typically you don't have a lot

4200
01:17:07,760 --> 01:17:10,740
 

4201
01:17:07,770 --> 01:17:13,320
 of label data and so this is an example

4202
01:17:10,730 --> 01:17:13,320
 

4203
01:17:10,740 --> 01:17:17,310
 where unsupervised learning really

4204
01:17:13,310 --> 01:17:17,310
 

4205
01:17:13,320 --> 01:17:19,680
 shines and so the conclusion of this is

4206
01:17:17,300 --> 01:17:19,680
 

4207
01:17:17,310 --> 01:17:21,990
 that we have very good tools to

4208
01:17:19,670 --> 01:17:21,990
 

4209
01:17:19,680 --> 01:17:24,540
 represent sentences and now also we have

4210
01:17:21,980 --> 01:17:24,540
 

4211
01:17:21,990 --> 01:17:26,130
 pretty good tools to resolve we have

4212
01:17:24,530 --> 01:17:26,130
 

4213
01:17:24,540 --> 01:17:29,850
 very good tools to represent words and

4214
01:17:26,120 --> 01:17:29,850
 

4215
01:17:26,130 --> 01:17:31,230
 now also sentences and and this works in

4216
01:17:29,840 --> 01:17:31,230
 

4217
01:17:29,850 --> 01:17:33,410
 practice in lots of different

4218
01:17:31,220 --> 01:17:33,410
 

4219
01:17:31,230 --> 01:17:33,410
 applications

4220
01:17:34,060 --> 01:17:34,060
 

4221
01:17:34,070 --> 01:17:37,890
[Music]

4222
01:17:35,230 --> 01:17:37,890
 

4223
01:17:35,240 --> 01:17:39,980
 let's switch here and let me talk

4224
01:17:37,880 --> 01:17:39,980
 

4225
01:17:37,890 --> 01:17:44,430
 briefly about another application of

4226
01:17:39,970 --> 01:17:44,430
 

4227
01:17:39,980 --> 01:17:47,190
 unsupervised learning now the task is to

4228
01:17:44,420 --> 01:17:47,190
 

4229
01:17:44,430 --> 01:17:49,230
 generate samples that look like training

4230
01:17:47,180 --> 01:17:49,230
 

4231
01:17:47,190 --> 01:17:52,020
 data so here today is that you have a

4232
01:17:49,220 --> 01:17:52,020
 

4233
01:17:49,230 --> 01:17:55,920
 random number generator and you want to

4234
01:17:52,010 --> 01:17:55,920
 

4235
01:17:52,020 --> 01:18:00,390
 turn these random numbers in data points

4236
01:17:55,910 --> 01:18:00,390
 

4237
01:17:55,920 --> 01:18:02,400
 that look like your training set and so

4238
01:18:00,380 --> 01:18:02,400
 

4239
01:18:00,390 --> 01:18:04,320
 this will be useful for learning

4240
01:18:02,390 --> 01:18:04,320
 

4241
01:18:02,400 --> 01:18:07,230
 representations pretty much like Alex

4242
01:18:04,310 --> 01:18:07,230
 

4243
01:18:04,320 --> 01:18:09,210
 said if you can recreate the data most

4244
01:18:07,220 --> 01:18:09,210
 

4245
01:18:07,230 --> 01:18:11,860
 likely you have understood the

4246
01:18:09,200 --> 01:18:11,860
 

4247
01:18:09,210 --> 01:18:14,710
 underlying properties of that data

4248
01:18:11,850 --> 01:18:14,710
 

4249
01:18:11,860 --> 01:18:16,510
 it could be useful for planning so if I

4250
01:18:14,700 --> 01:18:16,510
 

4251
01:18:14,710 --> 01:18:20,230
 want to decide what to do next I can

4252
01:18:16,500 --> 01:18:20,230
 

4253
01:18:16,510 --> 01:18:23,050
 simulate him in my head what future

4254
01:18:20,220 --> 01:18:23,050
 

4255
01:18:20,230 --> 01:18:24,880
 scenarios may look like or I could just

4256
01:18:23,040 --> 01:18:24,880
 

4257
01:18:23,050 --> 01:18:28,330
 be used for fun because generating

4258
01:18:24,870 --> 01:18:28,330
 

4259
01:18:24,880 --> 01:18:30,179
 images is fun and so right now there is

4260
01:18:28,320 --> 01:18:30,179
 

4261
01:18:28,330 --> 01:18:33,100
 a lot of excitement about generative

4262
01:18:30,169 --> 01:18:33,100
 

4263
01:18:30,179 --> 01:18:35,320
 adversarial networks to generate images

4264
01:18:33,090 --> 01:18:35,320
 

4265
01:18:33,100 --> 01:18:37,239
 here are some examples here's some

4266
01:18:35,310 --> 01:18:37,239
 

4267
01:18:35,320 --> 01:18:40,239
 examples were the generation is

4268
01:18:37,229 --> 01:18:40,239
 

4269
01:18:37,239 --> 01:18:42,850
 conditional on a class label and it's

4270
01:18:40,229 --> 01:18:42,850
 

4271
01:18:40,239 --> 01:18:46,440
 really remarkable it is it blows me away

4272
01:18:42,840 --> 01:18:46,440
 

4273
01:18:42,850 --> 01:18:50,230
 no bro if you consider well we could do

4274
01:18:46,430 --> 01:18:50,230
 

4275
01:18:46,440 --> 01:18:51,730
 five ten years ago there are also other

4276
01:18:50,220 --> 01:18:51,730
 

4277
01:18:50,230 --> 01:18:53,790
 approaches like Auto regressive

4278
01:18:51,720 --> 01:18:53,790
 

4279
01:18:51,730 --> 01:18:55,960
 approaches like Alex showed before

4280
01:18:53,780 --> 01:18:55,960
 

4281
01:18:53,790 --> 01:18:56,739
 approaches based on letting variables

4282
01:18:55,950 --> 01:18:56,739
 

4283
01:18:55,960 --> 01:19:00,340
 and so forth

4284
01:18:56,729 --> 01:19:00,340
 

4285
01:18:56,739 --> 01:19:03,369
 and it seems to me that the crucial part

4286
01:19:00,330 --> 01:19:03,369
 

4287
01:19:00,340 --> 01:19:05,920
 is actually the architecture that used

4288
01:19:03,359 --> 01:19:05,920
 

4289
01:19:03,369 --> 01:19:08,199
 the specific conclusion or a method that

4290
01:19:05,910 --> 01:19:08,199
 

4291
01:19:05,920 --> 01:19:11,400
 you use more than the loss function

4292
01:19:08,189 --> 01:19:11,400
 

4293
01:19:08,199 --> 01:19:14,440
 really and there are a lot of challenges

4294
01:19:11,390 --> 01:19:14,440
 

4295
01:19:11,400 --> 01:19:16,750
 so the I would say the major challenge

4296
01:19:14,430 --> 01:19:16,750
 

4297
01:19:14,440 --> 01:19:20,139
 is how to evaluate these generative

4298
01:19:16,740 --> 01:19:20,139
 

4299
01:19:16,750 --> 01:19:22,690
 models another challenge is how do you

4300
01:19:20,129 --> 01:19:22,690
 

4301
01:19:20,139 --> 01:19:24,190
 model high dimensional distributions and

4302
01:19:22,680 --> 01:19:24,190
 

4303
01:19:22,690 --> 01:19:26,080
 how do you modern certainty because if

4304
01:19:24,180 --> 01:19:26,080
 

4305
01:19:24,190 --> 01:19:29,230
 you have to predict a lot of variables

4306
01:19:26,070 --> 01:19:29,230
 

4307
01:19:26,080 --> 01:19:30,820
 then there are many plausible things

4308
01:19:29,220 --> 01:19:30,820
 

4309
01:19:29,230 --> 01:19:32,440
 that you can possibly generate and that

4310
01:19:30,810 --> 01:19:32,440
 

4311
01:19:30,820 --> 01:19:37,150
 goes back to how you do you measure

4312
01:19:32,430 --> 01:19:37,150
 

4313
01:19:32,440 --> 01:19:38,500
 quality of the generation in NLP there

4314
01:19:37,140 --> 01:19:38,500
 

4315
01:19:37,150 --> 01:19:42,280
 are also a lot of methods to generate

4316
01:19:38,490 --> 01:19:42,280
 

4317
01:19:38,500 --> 01:19:43,630
 text as Alex mentioned there are there

4318
01:19:42,270 --> 01:19:43,630
 

4319
01:19:42,280 --> 01:19:47,969
 is a lot of work on auto regressive

4320
01:19:43,620 --> 01:19:47,969
 

4321
01:19:43,630 --> 01:19:50,710
 models like our enhanced generate text

4322
01:19:47,959 --> 01:19:50,710
 

4323
01:19:47,969 --> 01:19:52,300
 there are also other approaches that are

4324
01:19:50,700 --> 01:19:52,300
 

4325
01:19:50,710 --> 01:19:53,860
 a little older that are based on

4326
01:19:52,290 --> 01:19:53,860
 

4327
01:19:52,300 --> 01:19:57,310
 retrieval so here the idea is that you

4328
01:19:53,850 --> 01:19:57,310
 

4329
01:19:53,860 --> 01:19:58,810
 have a big set of candidates and when

4330
01:19:57,300 --> 01:19:58,810
 

4331
01:19:57,310 --> 01:20:02,530
 you want to generate you rank these

4332
01:19:58,800 --> 01:20:02,530
 

4333
01:19:58,810 --> 01:20:04,860
 candidates and then you copy/paste the

4334
01:20:02,520 --> 01:20:04,860
 

4335
01:20:02,530 --> 01:20:07,960
 highest-ranking candidate

4336
01:20:04,850 --> 01:20:07,960
 

4337
01:20:04,860 --> 01:20:09,610
 more recently the rest being also a work

4338
01:20:07,950 --> 01:20:09,610
 

4339
01:20:07,960 --> 01:20:11,679
 that tries to combine these two

4340
01:20:09,600 --> 01:20:11,679
 

4341
01:20:09,610 --> 01:20:14,080
 approaches so the idea is that you first

4342
01:20:11,669 --> 01:20:14,080
 

4343
01:20:11,679 --> 01:20:17,710
 retrieve okay and then once you retrieve

4344
01:20:14,070 --> 01:20:17,710
 

4345
01:20:14,080 --> 01:20:20,590
 you add it using a parametric model and

4346
01:20:17,700 --> 01:20:20,590
 

4347
01:20:17,710 --> 01:20:21,480
 here you have a few pointers to forwards

4348
01:20:20,580 --> 01:20:21,480
 

4349
01:20:20,590 --> 01:20:25,200
 along this

4350
01:20:21,470 --> 01:20:25,200
 

4351
01:20:21,480 --> 01:20:29,340
 there are actually many more and again

4352
01:20:25,190 --> 01:20:29,340
 

4353
01:20:25,200 --> 01:20:31,140
 the challenge is to generate long pieces

4354
01:20:29,330 --> 01:20:31,140
 

4355
01:20:29,340 --> 01:20:33,870
 of tax that are coherent we are pretty

4356
01:20:31,130 --> 01:20:33,870
 

4357
01:20:31,140 --> 01:20:37,200
 good at generating short sentences much

4358
01:20:33,860 --> 01:20:37,200
 

4359
01:20:33,870 --> 01:20:39,330
 less we are but there is a lot of room

4360
01:20:37,190 --> 01:20:39,330
 

4361
01:20:37,200 --> 01:20:41,250
 for improvement for generating a

4362
01:20:39,320 --> 01:20:41,250
 

4363
01:20:39,330 --> 01:20:44,280
 document right it's very difficult to

4364
01:20:41,240 --> 01:20:44,280
 

4365
01:20:41,250 --> 01:20:48,840
 generating that are coherent the stay on

4366
01:20:44,270 --> 01:20:48,840
 

4367
01:20:44,280 --> 01:20:52,020
 topic and again a major issue is how do

4368
01:20:48,830 --> 01:20:52,020
 

4369
01:20:48,840 --> 01:20:54,810
 you evaluate these generating models how

4370
01:20:52,010 --> 01:20:54,810
 

4371
01:20:52,020 --> 01:20:59,850
 do you automatically assess goodness of

4372
01:20:54,800 --> 01:20:59,850
 

4373
01:20:54,810 --> 01:21:01,760
 the generation let me conclude a last

4374
01:20:59,840 --> 01:21:01,760
 

4375
01:20:59,850 --> 01:21:04,500
 application of unsupervised learning

4376
01:21:01,750 --> 01:21:04,500
 

4377
01:21:01,760 --> 01:21:06,360
 talking about methods to translate

4378
01:21:04,490 --> 01:21:06,360
 

4379
01:21:04,500 --> 01:21:08,100
 between two different domains so here

4380
01:21:06,350 --> 01:21:08,100
 

4381
01:21:06,360 --> 01:21:11,190
 the idea is that you have a bunch of

4382
01:21:08,090 --> 01:21:11,190
 

4383
01:21:08,100 --> 01:21:14,340
 points in domain one a bunch of data in

4384
01:21:11,180 --> 01:21:14,340
 

4385
01:21:11,190 --> 01:21:18,420
 domain 2 and he want to learn a mapping

4386
01:21:14,330 --> 01:21:18,420
 

4387
01:21:14,340 --> 01:21:22,250
 that brings a data point from the may

4388
01:21:18,410 --> 01:21:22,250
 

4389
01:21:18,420 --> 01:21:25,430
 want to edit a point in in domain 2

4390
01:21:22,240 --> 01:21:25,430
 

4391
01:21:22,250 --> 01:21:29,220
 this could be useful for a variety of

4392
01:21:25,420 --> 01:21:29,220
 

4393
01:21:25,430 --> 01:21:32,370
 applications in general it is a useful

4394
01:21:29,210 --> 01:21:32,370
 

4395
01:21:29,220 --> 01:21:34,080
 skill that an AI agent should have

4396
01:21:32,360 --> 01:21:34,080
 

4397
01:21:32,370 --> 01:21:35,610
 because if you want to quickly adapt to

4398
01:21:34,070 --> 01:21:35,610
 

4399
01:21:34,080 --> 01:21:37,290
 in your environment you need to be able

4400
01:21:35,600 --> 01:21:37,290
 

4401
01:21:35,610 --> 01:21:38,700
 to make analogous to say oh the

4402
01:21:37,280 --> 01:21:38,700
 

4403
01:21:37,290 --> 01:21:40,350
 situation similar to something that I

4404
01:21:38,690 --> 01:21:40,350
 

4405
01:21:38,700 --> 01:21:43,650
 learned before and I can use my skills

4406
01:21:40,340 --> 01:21:43,650
 

4407
01:21:40,350 --> 01:21:46,260
 for that in computer vision there is a

4408
01:21:43,640 --> 01:21:46,260
 

4409
01:21:43,650 --> 01:21:49,020
 seminal work by su at all that is called

4410
01:21:46,250 --> 01:21:49,020
 

4411
01:21:46,260 --> 01:21:51,390
 cycle gun so here is an example where

4412
01:21:49,010 --> 01:21:51,390
 

4413
01:21:49,020 --> 01:21:54,210
 you have a bunch of photographic images

4414
01:21:51,380 --> 01:21:54,210
 

4415
01:21:51,390 --> 01:21:56,790
 for domain 1 and let's say a bunch of

4416
01:21:54,200 --> 01:21:56,790
 

4417
01:21:54,210 --> 01:21:59,670
 Monet paintings for domain 2 and the

4418
01:21:56,780 --> 01:21:59,670
 

4419
01:21:56,790 --> 01:22:03,630
 task is to say given a Monet painting

4420
01:21:59,660 --> 01:22:03,630
 

4421
01:21:59,670 --> 01:22:05,490
 how can I fantasize what the

4422
01:22:03,620 --> 01:22:05,490
 

4423
01:22:03,630 --> 01:22:07,950
 corresponding photographic image what

4424
01:22:05,480 --> 01:22:07,950
 

4425
01:22:05,490 --> 01:22:10,020
 Monet saw when he was painting right and

4426
01:22:07,940 --> 01:22:10,020
 

4427
01:22:07,950 --> 01:22:12,390
 vice versa and in the paper they give

4428
01:22:10,010 --> 01:22:12,390
 

4429
01:22:10,020 --> 01:22:15,030
 also other examples like turning zebras

4430
01:22:12,380 --> 01:22:15,030
 

4431
01:22:12,390 --> 01:22:16,740
 into horses learning summer images into

4432
01:22:15,020 --> 01:22:16,740
 

4433
01:22:15,030 --> 01:22:21,030
 winter images and so on so forth so

4434
01:22:16,730 --> 01:22:21,030
 

4435
01:22:16,740 --> 01:22:23,040
 let's see how this works so you take an

4436
01:22:21,020 --> 01:22:23,040
 

4437
01:22:21,030 --> 01:22:25,560
 image from the main one let's call it X

4438
01:22:23,030 --> 01:22:25,560
 

4439
01:22:23,040 --> 01:22:28,710
 and you have two generators you have a

4440
01:22:25,550 --> 01:22:28,710
 

4441
01:22:25,560 --> 01:22:31,080
 generator that Maps an image from domain

4442
01:22:28,700 --> 01:22:31,080
 

4443
01:22:28,710 --> 01:22:32,919
 1 to the moon - that's the blue box and

4444
01:22:31,070 --> 01:22:32,919
 

4445
01:22:31,080 --> 01:22:36,039
 you have its inator that map's

4446
01:22:32,909 --> 01:22:36,039
 

4447
01:22:32,919 --> 01:22:39,070
 an image from the main - to the main one

4448
01:22:36,029 --> 01:22:39,070
 

4449
01:22:36,039 --> 01:22:40,899
 and that's the red box no so now you

4450
01:22:39,060 --> 01:22:40,899
 

4451
01:22:39,070 --> 01:22:43,209
 take your data point from the first

4452
01:22:40,889 --> 01:22:43,209
 

4453
01:22:40,899 --> 01:22:45,579
 domain you apply the generator that

4454
01:22:43,199 --> 01:22:45,579
 

4455
01:22:43,209 --> 01:22:48,849
 takes you to domain 2 and that gives you

4456
01:22:45,569 --> 01:22:48,849
 

4457
01:22:45,579 --> 01:22:50,949
 this Y hat now the data set doesn't come

4458
01:22:48,839 --> 01:22:50,949
 

4459
01:22:48,849 --> 01:22:53,289
 with what is the ground truth Y hat

4460
01:22:50,939 --> 01:22:53,289
 

4461
01:22:50,949 --> 01:22:56,349
 right we are in their supervisory so

4462
01:22:53,279 --> 01:22:56,349
 

4463
01:22:53,289 --> 01:22:58,510
 what you do you feed that as input to

4464
01:22:56,339 --> 01:22:58,510
 

4465
01:22:56,349 --> 01:23:02,380
 the other generator that takes you back

4466
01:22:58,500 --> 01:23:02,380
 

4467
01:22:58,510 --> 01:23:04,300
 to domain 1 and now you have the exact

4468
01:23:02,370 --> 01:23:04,300
 

4469
01:23:02,380 --> 01:23:06,639
 reconstruction which you can compare to

4470
01:23:04,290 --> 01:23:06,639
 

4471
01:23:04,300 --> 01:23:08,499
 the original X that's what they call

4472
01:23:06,629 --> 01:23:08,499
 

4473
01:23:06,639 --> 01:23:10,780
 psycho core system so this is like an

4474
01:23:08,489 --> 01:23:10,780
 

4475
01:23:08,499 --> 01:23:12,550
 autumn coding task where the later

4476
01:23:10,770 --> 01:23:12,550
 

4477
01:23:10,780 --> 01:23:15,479
 presentation is actually point in the

4478
01:23:12,540 --> 01:23:15,479
 

4479
01:23:12,550 --> 01:23:18,010
 other domain and you do the same for

4480
01:23:15,469 --> 01:23:18,010
 

4481
01:23:15,479 --> 01:23:20,399
 images belonging to domain 2

4482
01:23:18,000 --> 01:23:20,399
 

4483
01:23:18,010 --> 01:23:25,300
 that's what you see at the bottom here

4484
01:23:20,389 --> 01:23:25,300
 

4485
01:23:20,399 --> 01:23:28,179
 now so far so good but this doesn't do

4486
01:23:25,290 --> 01:23:28,179
 

4487
01:23:25,300 --> 01:23:30,639
 what you want early because when you

4488
01:23:28,169 --> 01:23:30,639
 

4489
01:23:28,179 --> 01:23:32,409
 produce this data point there is no

4490
01:23:30,629 --> 01:23:32,409
 

4491
01:23:30,639 --> 01:23:34,119
 constraint that this data point actually

4492
01:23:32,399 --> 01:23:34,119
 

4493
01:23:32,409 --> 01:23:36,849
 belongs to the domain that you want

4494
01:23:34,109 --> 01:23:36,849
 

4495
01:23:34,119 --> 01:23:38,619
 right and so you need to have another

4496
01:23:36,839 --> 01:23:38,619
 

4497
01:23:36,849 --> 01:23:41,679
 constraint and the constraint that they

4498
01:23:38,609 --> 01:23:41,679
 

4499
01:23:38,619 --> 01:23:44,019
 have is an adversarial term in the loss

4500
01:23:41,669 --> 01:23:44,019
 

4501
01:23:41,679 --> 01:23:46,510
 function so they train a classifier to

4502
01:23:44,009 --> 01:23:46,510
 

4503
01:23:44,019 --> 01:23:49,179
 predict where the input comes from the

4504
01:23:46,500 --> 01:23:49,179
 

4505
01:23:46,510 --> 01:23:53,320
 output of the generator or is actually a

4506
01:23:49,169 --> 01:23:53,320
 

4507
01:23:49,179 --> 01:23:54,699
 genuine sample from domain - and when

4508
01:23:53,310 --> 01:23:54,699
 

4509
01:23:53,320 --> 01:23:57,010
 they train the generator they try to

4510
01:23:54,689 --> 01:23:57,010
 

4511
01:23:54,699 --> 01:24:00,130
 fool this classifier as much as possible

4512
01:23:57,000 --> 01:24:00,130
 

4513
01:23:57,010 --> 01:24:05,800
 by essentially trying to increase its

4514
01:24:00,120 --> 01:24:05,800
 

4515
01:24:00,130 --> 01:24:07,989
 loss that's it so turns out that this is

4516
01:24:05,790 --> 01:24:07,989
 

4517
01:24:05,800 --> 01:24:11,050
 a pretty general principle that can be

4518
01:24:07,979 --> 01:24:11,050
 

4519
01:24:07,989 --> 01:24:12,820
 applied also to text and there are a lot

4520
01:24:11,040 --> 01:24:12,820
 

4521
01:24:11,050 --> 01:24:15,369
 of applications in text that could

4522
01:24:12,810 --> 01:24:15,369
 

4523
01:24:12,820 --> 01:24:17,340
 benefit from from this idea and here I'm

4524
01:24:15,359 --> 01:24:17,340
 

4525
01:24:15,369 --> 01:24:20,260
 going to talk about some recent word

4526
01:24:17,330 --> 01:24:20,260
 

4527
01:24:17,340 --> 01:24:23,229
 macular graters and I did for machine

4528
01:24:20,250 --> 01:24:23,229
 

4529
01:24:20,260 --> 01:24:26,349
 translation so here the idea is that you

4530
01:24:23,219 --> 01:24:26,349
 

4531
01:24:23,229 --> 01:24:28,510
 have a lot of raw text in two different

4532
01:24:26,339 --> 01:24:28,510
 

4533
01:24:26,349 --> 01:24:30,610
 languages but you don't have any

4534
01:24:28,500 --> 01:24:30,610
 

4535
01:24:28,510 --> 01:24:32,709
 translation you don't have a collection

4536
01:24:30,600 --> 01:24:32,709
 

4537
01:24:30,610 --> 01:24:34,149
 of sentences in one language with a

4538
01:24:32,699 --> 01:24:34,149
 

4539
01:24:32,709 --> 01:24:36,610
 correspondent slash you don't have that

4540
01:24:34,139 --> 01:24:36,610
 

4541
01:24:34,149 --> 01:24:37,840
 you just have text in each language and

4542
01:24:36,600 --> 01:24:37,840
 

4543
01:24:36,610 --> 01:24:40,329
 this tax doesn't correspond to each

4544
01:24:37,830 --> 01:24:40,329
 

4545
01:24:37,840 --> 01:24:43,659
 other and what you want at the end is to

4546
01:24:40,319 --> 01:24:43,659
 

4547
01:24:40,329 --> 01:24:45,669
 learn to translate so this could be

4548
01:24:43,649 --> 01:24:45,669
 

4549
01:24:43,659 --> 01:24:46,300
 useful because for the vast majority of

4550
01:24:45,659 --> 01:24:46,300
 

4551
01:24:45,669 --> 01:24:48,060
 language

4552
01:24:46,290 --> 01:24:48,060
 

4553
01:24:46,300 --> 01:24:50,140
 in the world we actually don't have

4554
01:24:48,050 --> 01:24:50,140
 

4555
01:24:48,060 --> 01:24:51,820
 parallel data so we don't have a

4556
01:24:50,130 --> 01:24:51,820
 

4557
01:24:50,140 --> 01:24:54,280
 collection of sentences with a crossbow

4558
01:24:51,810 --> 01:24:54,280
 

4559
01:24:51,820 --> 01:24:59,260
 new translation but we do have road text

4560
01:24:54,270 --> 01:24:59,260
 

4561
01:24:54,280 --> 01:25:02,890
 in each language right so before I

4562
01:24:59,250 --> 01:25:02,890
 

4563
01:24:59,260 --> 01:25:05,800
 explain how one way to do and supervise

4564
01:25:02,880 --> 01:25:05,800
 

4565
01:25:02,890 --> 01:25:07,990
 machine translation let me introduce the

4566
01:25:05,790 --> 01:25:07,990
 

4567
01:25:05,800 --> 01:25:11,010
 idea of learning to translate words

4568
01:25:07,980 --> 01:25:11,010
 

4569
01:25:07,990 --> 01:25:14,410
 which is a preliminary steps towards

4570
01:25:11,000 --> 01:25:14,410
 

4571
01:25:11,010 --> 01:25:18,310
 translating sentences and so here the

4572
01:25:14,400 --> 01:25:18,310
 

4573
01:25:14,410 --> 01:25:22,030
 idea is that the context of the war is

4574
01:25:18,300 --> 01:25:22,030
 

4575
01:25:18,310 --> 01:25:23,950
 often the same across languages because

4576
01:25:22,020 --> 01:25:23,950
 

4577
01:25:22,030 --> 01:25:25,870
 even if we live in different parts of

4578
01:25:23,940 --> 01:25:25,870
 

4579
01:25:23,950 --> 01:25:28,060
 the world we shared the physical even if

4580
01:25:25,860 --> 01:25:28,060
 

4581
01:25:25,870 --> 01:25:29,710
 we live in different places we shared

4582
01:25:28,050 --> 01:25:29,710
 

4583
01:25:28,060 --> 01:25:31,950
 the same physical world so the

4584
01:25:29,700 --> 01:25:31,950
 

4585
01:25:29,710 --> 01:25:35,340
 relationship of let's say cat and Kitty

4586
01:25:31,940 --> 01:25:35,340
 

4587
01:25:31,950 --> 01:25:37,480
 is the same in Italian Japanese and

4588
01:25:35,330 --> 01:25:37,480
 

4589
01:25:35,340 --> 01:25:40,150
 these are two words that are pretty

4590
01:25:37,470 --> 01:25:40,150
 

4591
01:25:37,480 --> 01:25:42,430
 close but if I look a cat and car those

4592
01:25:40,140 --> 01:25:42,430
 

4593
01:25:40,150 --> 01:25:45,580
 are pretty part and this is true in all

4594
01:25:42,420 --> 01:25:45,580
 

4595
01:25:42,430 --> 01:25:48,370
 the languages and so here the idea let's

4596
01:25:45,570 --> 01:25:48,370
 

4597
01:25:45,580 --> 01:25:50,920
 see if this animation works right so

4598
01:25:48,360 --> 01:25:50,920
 

4599
01:25:48,370 --> 01:25:53,740
 here the idea is that let's say that you

4600
01:25:50,910 --> 01:25:53,740
 

4601
01:25:50,920 --> 01:25:54,490
 have you want to align three languages

4602
01:25:53,730 --> 01:25:54,490
 

4603
01:25:53,740 --> 01:25:58,030
 ok

4604
01:25:54,480 --> 01:25:58,030
 

4605
01:25:54,490 --> 01:26:00,130
 you first learn word embeddings in each

4606
01:25:58,020 --> 01:26:00,130
 

4607
01:25:58,030 --> 01:26:02,290
 language separately okay so let's say

4608
01:26:00,120 --> 01:26:02,290
 

4609
01:26:00,130 --> 01:26:05,200
 you you do work to back in its language

4610
01:26:02,280 --> 01:26:05,200
 

4611
01:26:02,290 --> 01:26:08,590
 so now it's war in its language has

4612
01:26:05,190 --> 01:26:08,590
 

4613
01:26:05,200 --> 01:26:10,240
 associated an M very okay and now the

4614
01:26:08,580 --> 01:26:10,240
 

4615
01:26:08,590 --> 01:26:12,070
 idea is that because the relationship

4616
01:26:10,230 --> 01:26:12,070
 

4617
01:26:10,240 --> 01:26:15,700
 between words are the same across

4618
01:26:12,060 --> 01:26:15,700
 

4619
01:26:12,070 --> 01:26:20,400
 languages a simple rotation suffices to

4620
01:26:15,690 --> 01:26:20,400
 

4621
01:26:15,700 --> 01:26:23,380
 align this word embeddings okay and so

4622
01:26:20,390 --> 01:26:23,380
 

4623
01:26:20,400 --> 01:26:25,990
 what we do we do adversarial training so

4624
01:26:23,370 --> 01:26:25,990
 

4625
01:26:23,380 --> 01:26:28,900
 we have a classifier that is trained to

4626
01:26:25,980 --> 01:26:28,900
 

4627
01:26:25,990 --> 01:26:32,980
 distinguish whether the input represent

4628
01:26:28,890 --> 01:26:32,980
 

4629
01:26:28,900 --> 01:26:35,350
 a the input comes from let's say a word

4630
01:26:32,970 --> 01:26:35,350
 

4631
01:26:32,980 --> 01:26:38,380
 embodying from language one or I wrote a

4632
01:26:35,340 --> 01:26:38,380
 

4633
01:26:35,350 --> 01:26:41,050
 word abiding from language - okay and

4634
01:26:38,370 --> 01:26:41,050
 

4635
01:26:38,380 --> 01:26:43,660
 when we train the rotation matrix we try

4636
01:26:41,040 --> 01:26:43,660
 

4637
01:26:41,050 --> 01:26:46,390
 to fool this classifier and so this

4638
01:26:43,650 --> 01:26:46,390
 

4639
01:26:43,660 --> 01:26:49,360
 gives you a rough alignment and so what

4640
01:26:46,380 --> 01:26:49,360
 

4641
01:26:46,390 --> 01:26:50,680
 you do at the end given a word you apply

4642
01:26:49,350 --> 01:26:50,680
 

4643
01:26:49,360 --> 01:26:52,390
 a rotation and you look at the nearest

4644
01:26:50,670 --> 01:26:52,390
 

4645
01:26:50,680 --> 01:26:54,520
 neighbor word in the other language and

4646
01:26:52,380 --> 01:26:54,520
 

4647
01:26:52,390 --> 01:26:56,769
 that's your word translation and then

4648
01:26:54,510 --> 01:26:56,769
 

4649
01:26:54,520 --> 01:26:58,539
 there is there are

4650
01:26:56,759 --> 01:26:58,539
 

4651
01:26:56,769 --> 01:27:01,269
 if you if you read a paper there are

4652
01:26:58,529 --> 01:27:01,269
 

4653
01:26:58,539 --> 01:27:05,649
 some refinement steps that I'm gonna

4654
01:27:01,259 --> 01:27:05,649
 

4655
01:27:01,269 --> 01:27:07,959
 skip for the sake of simplicity but what

4656
01:27:05,639 --> 01:27:07,959
 

4657
01:27:05,649 --> 01:27:13,439
 you find is that if you learn this

4658
01:27:07,949 --> 01:27:13,439
 

4659
01:27:07,959 --> 01:27:15,249
 rotation using a ground truth word

4660
01:27:13,429 --> 01:27:15,249
 

4661
01:27:13,439 --> 01:27:17,349
 translations by thousand were

4662
01:27:15,239 --> 01:27:17,349
 

4663
01:27:15,249 --> 01:27:19,419
 translations you do actually it slightly

4664
01:27:17,339 --> 01:27:19,419
 

4665
01:27:17,349 --> 01:27:22,539
 worse than if you use this unsupervised

4666
01:27:19,409 --> 01:27:22,539
 

4667
01:27:19,419 --> 01:27:26,829
 approach on some language parts because

4668
01:27:22,529 --> 01:27:26,829
 

4669
01:27:22,539 --> 01:27:29,530
 here you can learn you can ground this

4670
01:27:26,819 --> 01:27:29,530
 

4671
01:27:26,829 --> 01:27:31,599
 rotation using many more anchor points

4672
01:27:29,520 --> 01:27:31,599
 

4673
01:27:29,530 --> 01:27:33,699
 they are noisy but you can use many more

4674
01:27:31,589 --> 01:27:33,699
 

4675
01:27:31,599 --> 01:27:35,319
 and so actually when you do when you try

4676
01:27:33,689 --> 01:27:35,319
 

4677
01:27:33,699 --> 01:27:37,869
 to import this bilingual accessing you

4678
01:27:35,309 --> 01:27:37,869
 

4679
01:27:35,319 --> 01:27:40,149
 can do even better if used in supervised

4680
01:27:37,859 --> 01:27:40,149
 

4681
01:27:37,869 --> 01:27:42,489
 that loads so this approach the

4682
01:27:40,139 --> 01:27:42,489
 

4683
01:27:40,149 --> 01:27:45,579
 discourse news can be used as a stepping

4684
01:27:42,479 --> 01:27:45,579
 

4685
01:27:42,489 --> 01:27:48,010
 stone for doing translation in the

4686
01:27:45,569 --> 01:27:48,010
 

4687
01:27:45,579 --> 01:27:51,159
 sentence level but you cannot just

4688
01:27:48,000 --> 01:27:51,159
 

4689
01:27:48,010 --> 01:27:54,069
 directly use this because again we don't

4690
01:27:51,149 --> 01:27:54,069
 

4691
01:27:51,159 --> 01:27:57,429
 quite know how to represent sentences we

4692
01:27:54,059 --> 01:27:57,429
 

4693
01:27:54,069 --> 01:27:59,379
 cannot associate a single representation

4694
01:27:57,419 --> 01:27:59,379
 

4695
01:27:57,429 --> 01:28:01,599
 to each sentence like a sentence

4696
01:27:59,369 --> 01:28:01,599
 

4697
01:27:59,379 --> 01:28:03,280
 embodying a unique service applying for

4698
01:28:01,589 --> 01:28:03,280
 

4699
01:28:01,599 --> 01:28:08,739
 sentences we need to use something that

4700
01:28:03,270 --> 01:28:08,739
 

4701
01:28:03,280 --> 01:28:11,649
 is compositional and so what we use is a

4702
01:28:08,729 --> 01:28:11,649
 

4703
01:28:08,739 --> 01:28:14,499
 sequence to sequence model where sorry

4704
01:28:11,639 --> 01:28:14,499
 

4705
01:28:11,649 --> 01:28:16,839
 where let's say you take a sentence from

4706
01:28:14,489 --> 01:28:16,839
 

4707
01:28:14,499 --> 01:28:19,179
 the English data side you encode it you

4708
01:28:16,829 --> 01:28:19,179
 

4709
01:28:16,839 --> 01:28:21,030
 got a letter representation from this

4710
01:28:19,169 --> 01:28:21,030
 

4711
01:28:19,179 --> 01:28:25,899
 letter and a presentation you decode it

4712
01:28:21,020 --> 01:28:25,899
 

4713
01:28:21,030 --> 01:28:28,149
 and you get some translation in Italian

4714
01:28:25,889 --> 01:28:28,149
 

4715
01:28:25,899 --> 01:28:30,219
 right so you can imagine this this

4716
01:28:28,139 --> 01:28:30,219
 

4717
01:28:28,149 --> 01:28:31,899
 encoder and decoder is a recurrent even

4718
01:28:30,209 --> 01:28:31,899
 

4719
01:28:30,219 --> 01:28:36,639
 and it could be a transform it could be

4720
01:28:31,889 --> 01:28:36,639
 

4721
01:28:31,899 --> 01:28:40,299
 anything but now we don't have the

4722
01:28:36,629 --> 01:28:40,299
 

4723
01:28:36,639 --> 01:28:42,189
 ground truth translation for any of the

4724
01:28:40,289 --> 01:28:42,189
 

4725
01:28:40,299 --> 01:28:44,260
 sentences in English and so what we do

4726
01:28:42,179 --> 01:28:44,260
 

4727
01:28:42,189 --> 01:28:47,409
 pretty much like in the second guy in

4728
01:28:44,250 --> 01:28:47,409
 

4729
01:28:44,260 --> 01:28:50,049
 vision we feed this to another secrets

4730
01:28:47,399 --> 01:28:50,049
 

4731
01:28:47,409 --> 01:28:53,409
 to sequence model that takes this

4732
01:28:50,039 --> 01:28:53,409
 

4733
01:28:50,049 --> 01:28:54,639
 sentence and it starts a Latin

4734
01:28:53,399 --> 01:28:54,639
 

4735
01:28:53,409 --> 01:28:57,309
 representation from there

4736
01:28:54,629 --> 01:28:57,309
 

4737
01:28:54,639 --> 01:29:00,760
 reconstruct the original sentence in

4738
01:28:57,299 --> 01:29:00,760
 

4739
01:28:57,309 --> 01:29:02,979
 English and now we have a way to compute

4740
01:29:00,750 --> 01:29:02,979
 

4741
01:29:00,760 --> 01:29:06,099
 reconstruction error between this Y and

4742
01:29:02,969 --> 01:29:06,099
 

4743
01:29:02,979 --> 01:29:07,359
 this Y double art but now like in the

4744
01:29:06,089 --> 01:29:07,359
 

4745
01:29:06,099 --> 01:29:08,770
 cycle time we need to add another

4746
01:29:07,349 --> 01:29:08,770
 

4747
01:29:07,359 --> 01:29:12,550
 constraint to make sure

4748
01:29:08,760 --> 01:29:12,550
 

4749
01:29:08,770 --> 01:29:15,430
 this intermediate sentence is a valid in

4750
01:29:12,540 --> 01:29:15,430
 

4751
01:29:12,550 --> 01:29:18,310
 this case Italian sentence so in the

4752
01:29:15,420 --> 01:29:18,310
 

4753
01:29:15,430 --> 01:29:21,040
 cycle count had adversarial training in

4754
01:29:18,300 --> 01:29:21,040
 

4755
01:29:18,310 --> 01:29:24,880
 this space but now it is a little

4756
01:29:21,030 --> 01:29:24,880
 

4757
01:29:21,040 --> 01:29:27,070
 difficult to do this in in machine

4758
01:29:24,870 --> 01:29:27,070
 

4759
01:29:24,880 --> 01:29:29,110
 translation because we have a discrete

4760
01:29:27,060 --> 01:29:29,110
 

4761
01:29:27,070 --> 01:29:31,090
 sequence of tokens right and so we don't

4762
01:29:29,100 --> 01:29:31,090
 

4763
01:29:29,110 --> 01:29:33,520
 quite know how to back propagate in a

4764
01:29:31,080 --> 01:29:33,520
 

4765
01:29:31,090 --> 01:29:37,270
 very efficient way and so instead of

4766
01:29:33,510 --> 01:29:37,270
 

4767
01:29:33,520 --> 01:29:40,170
 doing that we will we do the noisy or

4768
01:29:37,260 --> 01:29:40,170
 

4769
01:29:37,270 --> 01:29:44,920
 encoding to make sure that the decoder

4770
01:29:40,160 --> 01:29:44,920
 

4771
01:29:40,170 --> 01:29:47,590
 produces fluent sentences okay so you

4772
01:29:44,910 --> 01:29:47,590
 

4773
01:29:44,920 --> 01:29:49,990
 take your input sentence you add noise

4774
01:29:47,580 --> 01:29:49,990
 

4775
01:29:47,590 --> 01:29:53,730
 and hereby noise I mean you drop words

4776
01:29:49,980 --> 01:29:53,730
 

4777
01:29:49,990 --> 01:29:56,320
 you soft swap words and you force your

4778
01:29:53,720 --> 01:29:56,320
 

4779
01:29:53,730 --> 01:29:58,990
 decoder to the construed the original

4780
01:29:56,310 --> 01:29:58,990
 

4781
01:29:56,320 --> 01:30:00,760
 plain sentence so now when the decoder

4782
01:29:58,980 --> 01:30:00,760
 

4783
01:29:58,990 --> 01:30:02,620
 is used is going to produce always

4784
01:30:00,750 --> 01:30:02,620
 

4785
01:30:00,760 --> 01:30:05,250
 current sentences because it has learned

4786
01:30:02,610 --> 01:30:05,250
 

4787
01:30:02,620 --> 01:30:08,770
 regularities in the data

4788
01:30:05,240 --> 01:30:08,770
 

4789
01:30:05,250 --> 01:30:11,740
 the last cover is that if you just do

4790
01:30:08,760 --> 01:30:11,740
 

4791
01:30:08,770 --> 01:30:13,890
 this it won't work very well because

4792
01:30:11,730 --> 01:30:13,890
 

4793
01:30:11,740 --> 01:30:18,640
 there is a chance that the decoder

4794
01:30:13,880 --> 01:30:18,640
 

4795
01:30:13,890 --> 01:30:23,190
 cheats in the sense that it may

4796
01:30:18,630 --> 01:30:23,190
 

4797
01:30:18,640 --> 01:30:25,930
 reconstruct well when you feed sentences

4798
01:30:23,180 --> 01:30:25,930
 

4799
01:30:23,190 --> 01:30:28,510
 in the same language so when you do

4800
01:30:25,920 --> 01:30:28,510
 

4801
01:30:25,930 --> 01:30:30,430
 bottom podium but it may not reconstruct

4802
01:30:28,500 --> 01:30:30,430
 

4803
01:30:28,510 --> 01:30:32,590
 why when you do translation so you need

4804
01:30:30,420 --> 01:30:32,590
 

4805
01:30:30,430 --> 01:30:35,350
 to make sure that the representation

4806
01:30:32,580 --> 01:30:35,350
 

4807
01:30:32,590 --> 01:30:38,260
 produced by the encoder is the same both

4808
01:30:35,340 --> 01:30:38,260
 

4809
01:30:35,350 --> 01:30:42,190
 when you do translation and when you do

4810
01:30:38,250 --> 01:30:42,190
 

4811
01:30:38,260 --> 01:30:44,640
 language modeling because if you use two

4812
01:30:42,180 --> 01:30:44,640
 

4813
01:30:42,190 --> 01:30:47,800
 different parts of the letter space then

4814
01:30:44,630 --> 01:30:47,800
 

4815
01:30:44,640 --> 01:30:50,950
 this is not gonna work and so there are

4816
01:30:47,790 --> 01:30:50,950
 

4817
01:30:47,800 --> 01:30:53,470
 several ways to do that if the two

4818
01:30:50,940 --> 01:30:53,470
 

4819
01:30:50,950 --> 01:30:55,780
 languages are related what we do we

4820
01:30:53,460 --> 01:30:55,780
 

4821
01:30:53,470 --> 01:31:00,100
 share all parameters in the encoder and

4822
01:30:55,770 --> 01:31:00,100
 

4823
01:30:55,780 --> 01:31:02,140
 also we learn word embeddings jointly

4824
01:31:00,090 --> 01:31:02,140
 

4825
01:31:00,100 --> 01:31:04,030
 and we break words into sub words so

4826
01:31:02,130 --> 01:31:04,030
 

4827
01:31:02,140 --> 01:31:05,350
 there is a lot of overlap because we are

4828
01:31:04,020 --> 01:31:05,350
 

4829
01:31:04,030 --> 01:31:07,180
 going to have a lot of sub words that

4830
01:31:05,340 --> 01:31:07,180
 

4831
01:31:05,350 --> 01:31:09,460
 appear in both languages like English

4832
01:31:07,170 --> 01:31:09,460
 

4833
01:31:07,180 --> 01:31:11,650
 and French right if the two languages

4834
01:31:09,450 --> 01:31:11,650
 

4835
01:31:09,460 --> 01:31:13,450
 are not related then we apply the news

4836
01:31:11,640 --> 01:31:13,450
 

4837
01:31:11,650 --> 01:31:15,430
 algorithm that I described before to

4838
01:31:13,440 --> 01:31:15,430
 

4839
01:31:13,450 --> 01:31:18,179
 align the word embeddings and we shall

4840
01:31:15,420 --> 01:31:18,179
 

4841
01:31:15,430 --> 01:31:20,719
 all parameters indium Kohler and

4842
01:31:18,169 --> 01:31:20,719
 

4843
01:31:18,179 --> 01:31:26,250
 he's the the Layton representation issue

4844
01:31:20,709 --> 01:31:26,250
 

4845
01:31:20,719 --> 01:31:27,780
 and so if you apply this on a large beta

4846
01:31:26,240 --> 01:31:27,780
 

4847
01:31:26,250 --> 01:31:29,880
 cell for machine translation you'll find

4848
01:31:27,770 --> 01:31:29,880
 

4849
01:31:27,780 --> 01:31:32,580
 that this approach works better than

4850
01:31:29,870 --> 01:31:32,580
 

4851
01:31:29,880 --> 01:31:35,090
 previous approaches on the same task and

4852
01:31:32,570 --> 01:31:35,090
 

4853
01:31:32,580 --> 01:31:37,800
 keep in mind that before 2018

4854
01:31:35,080 --> 01:31:37,800
 

4855
01:31:35,090 --> 01:31:40,170
 performance on this large ban benchmarks

4856
01:31:37,790 --> 01:31:40,170
 

4857
01:31:37,800 --> 01:31:43,409
 was essentially zero we are pretty far

4858
01:31:40,160 --> 01:31:43,409
 

4859
01:31:40,170 --> 01:31:45,690
 from the supervised state of the art but

4860
01:31:43,399 --> 01:31:45,690
 

4861
01:31:43,409 --> 01:31:48,210
 still if you're in these translations

4862
01:31:45,680 --> 01:31:48,210
 

4863
01:31:45,690 --> 01:31:51,120
 you can actually they are useful you can

4864
01:31:48,200 --> 01:31:51,120
 

4865
01:31:48,210 --> 01:31:53,790
 read them and they're pretty good if you

4866
01:31:51,110 --> 01:31:53,790
 

4867
01:31:51,120 --> 01:31:56,400
 just to give you an idea the performance

4868
01:31:53,780 --> 01:31:56,400
 

4869
01:31:53,790 --> 01:31:59,730
 that we get using 10 million monolingual

4870
01:31:56,390 --> 01:31:59,730
 

4871
01:31:56,400 --> 01:32:01,800
 sentences in each language is about the

4872
01:31:59,720 --> 01:32:01,800
 

4873
01:31:59,730 --> 01:32:04,469
 same as using hundred thousand parallel

4874
01:32:01,790 --> 01:32:04,469
 

4875
01:32:01,800 --> 01:32:05,969
 sentences if as if you were to train

4876
01:32:04,459 --> 01:32:05,969
 

4877
01:32:04,469 --> 01:32:09,030
 supervised with hundred thousand cholera

4878
01:32:05,959 --> 01:32:09,030
 

4879
01:32:05,969 --> 01:32:12,090
 sentences on languages like English

4880
01:32:09,020 --> 01:32:12,090
 

4881
01:32:09,030 --> 01:32:14,940
 French English gentleman he works also

4882
01:32:12,080 --> 01:32:14,940
 

4883
01:32:12,090 --> 01:32:16,890
 on more distant language paths like you

4884
01:32:14,930 --> 01:32:16,890
 

4885
01:32:14,940 --> 01:32:19,380
 do so here is an example what if you had

4886
01:32:16,880 --> 01:32:19,380
 

4887
01:32:16,890 --> 01:32:21,300
 never seen it looks like Arabic but the

4888
01:32:19,370 --> 01:32:21,300
 

4889
01:32:19,380 --> 01:32:23,909
 language is related to Hindi are suing

4890
01:32:21,290 --> 01:32:23,909
 

4891
01:32:21,300 --> 01:32:25,590
 and so in this case this is pretty

4892
01:32:23,899 --> 01:32:25,590
 

4893
01:32:23,909 --> 01:32:27,900
 interesting because there is actually

4894
01:32:25,580 --> 01:32:27,900
 

4895
01:32:25,590 --> 01:32:29,760
 parallel data in order between English

4896
01:32:27,890 --> 01:32:29,760
 

4897
01:32:27,900 --> 01:32:31,920
 it would do but it's like genome

4898
01:32:29,750 --> 01:32:31,920
 

4899
01:32:29,760 --> 01:32:35,130
 translation is like movie captions

4900
01:32:31,910 --> 01:32:35,130
 

4901
01:32:31,920 --> 01:32:36,989
 translation so if you don't want to say

4902
01:32:35,120 --> 01:32:36,989
 

4903
01:32:35,130 --> 01:32:39,360
 translate news it doesn't work very well

4904
01:32:36,979 --> 01:32:39,360
 

4905
01:32:36,989 --> 01:32:41,790
 but now if you take monolingual data but

4906
01:32:39,350 --> 01:32:41,790
 

4907
01:32:39,360 --> 01:32:43,739
 you know if you scrape news websites but

4908
01:32:41,780 --> 01:32:43,739
 

4909
01:32:41,790 --> 01:32:45,060
 in English you would do and you use this

4910
01:32:43,729 --> 01:32:45,060
 

4911
01:32:43,739 --> 01:32:47,610
 and supervised the approach you actually

4912
01:32:45,050 --> 01:32:47,610
 

4913
01:32:45,060 --> 01:32:50,820
 do better than if you were to use the

4914
01:32:47,600 --> 01:32:50,820
 

4915
01:32:47,610 --> 01:32:53,880
 parallel data set okay so the overall

4916
01:32:50,810 --> 01:32:53,880
 

4917
01:32:50,820 --> 01:32:56,969
 idea here is that in order to align two

4918
01:32:53,870 --> 01:32:56,969
 

4919
01:32:53,880 --> 01:33:00,060
 domains there are a few principles that

4920
01:32:56,959 --> 01:33:00,060
 

4921
01:32:56,969 --> 01:33:02,100
 apply pretty widely so one is psycho

4922
01:33:00,050 --> 01:33:02,100
 

4923
01:33:00,060 --> 01:33:03,719
 consistency so if you map to one domain

4924
01:33:02,090 --> 01:33:03,719
 

4925
01:33:02,100 --> 01:33:05,790
 you need to be able to go back to the

4926
01:33:03,709 --> 01:33:05,790
 

4927
01:33:03,719 --> 01:33:08,219
 original domain and the second one is to

4928
01:33:05,780 --> 01:33:08,219
 

4929
01:33:05,790 --> 01:33:10,489
 a constrain such that I mean when you

4930
01:33:08,209 --> 01:33:10,489
 

4931
01:33:08,219 --> 01:33:12,960
 map to one domain what you generate

4932
01:33:10,479 --> 01:33:12,960
 

4933
01:33:10,489 --> 01:33:17,190
 actually has the same characteristics of

4934
01:33:12,950 --> 01:33:17,190
 

4935
01:33:12,960 --> 01:33:20,190
 the domain that you want to have let me

4936
01:33:17,180 --> 01:33:20,190
 

4937
01:33:17,190 --> 01:33:23,550
 conclude with some discussion about what

4938
01:33:20,180 --> 01:33:23,550
 

4939
01:33:20,190 --> 01:33:25,730
 the challenges are ahead of us so in my

4940
01:33:23,540 --> 01:33:25,730
 

4941
01:33:23,550 --> 01:33:27,560
 opinion the first and

4942
01:33:25,720 --> 01:33:27,560
 

4943
01:33:25,730 --> 01:33:31,780
 the biggest challenge is really the

4944
01:33:27,550 --> 01:33:31,780
 

4945
01:33:27,560 --> 01:33:34,160
 definition of good matrix so let's say

4946
01:33:31,770 --> 01:33:34,160
 

4947
01:33:31,780 --> 01:33:37,780
 let's consider a supervised Spitzer

4948
01:33:34,150 --> 01:33:37,780
 

4949
01:33:34,160 --> 01:33:40,790
 learning it would be very good to have

4950
01:33:37,770 --> 01:33:40,790
 

4951
01:33:37,780 --> 01:33:44,090
 consensus on a good set of downstream

4952
01:33:40,780 --> 01:33:44,090
 

4953
01:33:40,790 --> 01:33:47,600
 tasks and metrics to evaluate feature

4954
01:33:44,080 --> 01:33:47,600
 

4955
01:33:44,090 --> 01:33:50,630
 learning methods in NLP dressed being

4956
01:33:47,590 --> 01:33:50,630
 

4957
01:33:47,600 --> 01:33:53,179
 quite a bit of work towards this goal

4958
01:33:50,620 --> 01:33:53,179
 

4959
01:33:50,630 --> 01:33:55,699
 and we have benchmarks like santaville

4960
01:33:53,169 --> 01:33:55,699
 

4961
01:33:53,179 --> 01:34:01,460
 like the blue benchmark that i described

4962
01:33:55,689 --> 01:34:01,460
 

4963
01:33:55,699 --> 01:34:03,500
 earlier for generation it's even

4964
01:34:01,450 --> 01:34:03,500
 

4965
01:34:01,460 --> 01:34:08,000
 trickier because there are many

4966
01:34:03,490 --> 01:34:08,000
 

4967
01:34:03,500 --> 01:34:09,590
 plausible ways to you know to predict

4968
01:34:07,990 --> 01:34:09,590
 

4969
01:34:08,000 --> 01:34:12,080
 what the next sentence is going to be

4970
01:34:09,580 --> 01:34:12,080
 

4971
01:34:09,590 --> 01:34:14,210
 right and so again in an NP there as

4972
01:34:12,070 --> 01:34:14,210
 

4973
01:34:12,080 --> 01:34:16,550
 being quite a bit of work towards this

4974
01:34:14,200 --> 01:34:16,550
 

4975
01:34:14,210 --> 01:34:18,440
 goal so for specific applications like

4976
01:34:16,540 --> 01:34:18,440
 

4977
01:34:16,550 --> 01:34:20,810
 machine translation which you can see as

4978
01:34:18,430 --> 01:34:20,810
 

4979
01:34:18,440 --> 01:34:23,060
 a conditional generation task every year

4980
01:34:20,800 --> 01:34:23,060
 

4981
01:34:20,810 --> 01:34:25,550
 there is a competition that there are a

4982
01:34:23,050 --> 01:34:25,550
 

4983
01:34:23,060 --> 01:34:27,050
 bunch of different matrix that slice the

4984
01:34:25,540 --> 01:34:27,050
 

4985
01:34:25,550 --> 01:34:29,840
 evaluation in different ways and there

4986
01:34:27,040 --> 01:34:29,840
 

4987
01:34:27,050 --> 01:34:33,020
 are very large benchmarks for comparing

4988
01:34:29,830 --> 01:34:33,020
 

4989
01:34:29,840 --> 01:34:35,440
 different models in dialogue there is a

4990
01:34:33,010 --> 01:34:35,440
 

4991
01:34:33,020 --> 01:34:39,560
 platform for evaluating dialogue systems

4992
01:34:35,430 --> 01:34:39,560
 

4993
01:34:35,440 --> 01:34:42,440
 but for instance envision I am in the

4994
01:34:39,550 --> 01:34:42,440
 

4995
01:34:39,560 --> 01:34:44,630
 community getting together and getting

4996
01:34:42,430 --> 01:34:44,630
 

4997
01:34:42,440 --> 01:34:47,510
 to a level where there is consensus on

4998
01:34:44,620 --> 01:34:47,510
 

4999
01:34:44,630 --> 01:34:50,840
 how to evaluate model and this is I

5000
01:34:47,500 --> 01:34:50,840
 

5001
01:34:47,510 --> 01:34:53,270
 think a big challenge even more I think

5002
01:34:50,830 --> 01:34:53,270
 

5003
01:34:50,840 --> 01:34:55,520
 it's a big roadblock even more important

5004
01:34:53,260 --> 01:34:55,520
 

5005
01:34:53,270 --> 01:34:57,530
 than designing better models because if

5006
01:34:55,510 --> 01:34:57,530
 

5007
01:34:55,520 --> 01:34:59,360
 we don't have a good set of tasks and

5008
01:34:57,520 --> 01:34:59,360
 

5009
01:34:57,530 --> 01:35:01,010
 matrix is really difficult to compare

5010
01:34:59,350 --> 01:35:01,010
 

5011
01:34:59,360 --> 01:35:05,390
 models and to assess whether we are

5012
01:35:01,000 --> 01:35:05,390
 

5013
01:35:01,010 --> 01:35:07,940
 making any progress mine so the second

5014
01:35:05,380 --> 01:35:07,940
 

5015
01:35:05,390 --> 01:35:12,679
 challenge is generalizing our

5016
01:35:07,930 --> 01:35:12,679
 

5017
01:35:07,940 --> 01:35:14,570
 unsupervised learning algorithms so for

5018
01:35:12,669 --> 01:35:14,570
 

5019
01:35:12,679 --> 01:35:17,179
 instance in an LP the matters that I

5020
01:35:14,560 --> 01:35:17,179
 

5021
01:35:14,570 --> 01:35:19,910
 described are based on prediction of a

5022
01:35:17,169 --> 01:35:19,910
 

5023
01:35:17,179 --> 01:35:21,739
 missing word from a sentence right but

5024
01:35:19,900 --> 01:35:21,739
 

5025
01:35:19,910 --> 01:35:24,230
 how about if I have a missing plates

5026
01:35:21,729 --> 01:35:24,230
 

5027
01:35:21,739 --> 01:35:26,840
 right how about if I want two extra

5028
01:35:24,220 --> 01:35:26,840
 

5029
01:35:24,230 --> 01:35:31,070
 features from a sentence with any

5030
01:35:26,830 --> 01:35:31,070
 

5031
01:35:26,840 --> 01:35:33,830
 missing part I would like to be able to

5032
01:35:31,060 --> 01:35:33,830
 

5033
01:35:31,070 --> 01:35:35,840
 do that in vision such supervised

5034
01:35:33,820 --> 01:35:35,840
 

5035
01:35:33,830 --> 01:35:38,860
 learning shows a lot of promise but

5036
01:35:35,830 --> 01:35:38,860
 

5037
01:35:35,840 --> 01:35:41,920
 again every auxiliary task is different

5038
01:35:38,850 --> 01:35:41,920
 

5039
01:35:38,860 --> 01:35:45,070
 lycée of the data and so many more tasks

5040
01:35:41,910 --> 01:35:45,070
 

5041
01:35:41,920 --> 01:35:49,150
 do we need to design to get a more

5042
01:35:45,060 --> 01:35:49,150
 

5043
01:35:45,070 --> 01:35:51,639
 holistic view of the data is as Alex

5044
01:35:49,140 --> 01:35:51,639
 

5045
01:35:49,150 --> 01:35:53,860
 mentioned before auto regressive models

5046
01:35:51,629 --> 01:35:53,860
 

5047
01:35:51,639 --> 01:35:56,320
 also have limitations right because you

5048
01:35:53,850 --> 01:35:56,320
 

5049
01:35:53,860 --> 01:35:58,270
 need to define in order e and depending

5050
01:35:56,310 --> 01:35:58,270
 

5051
01:35:56,320 --> 01:36:00,760
 on the application your denis may be

5052
01:35:58,260 --> 01:36:00,760
 

5053
01:35:58,270 --> 01:36:03,070
 pretty critical and then you need to

5054
01:36:00,750 --> 01:36:03,070
 

5055
01:36:00,760 --> 01:36:08,440
 also account for efficiency so when you

5056
01:36:03,060 --> 01:36:08,440
 

5057
01:36:03,070 --> 01:36:10,179
 generate this models may be slow and so

5058
01:36:08,430 --> 01:36:10,179
 

5059
01:36:08,440 --> 01:36:15,010
 I don't know what is the right answer

5060
01:36:10,169 --> 01:36:15,010
 

5061
01:36:10,179 --> 01:36:18,280
 but let me describe a more general

5062
01:36:15,000 --> 01:36:18,280
 

5063
01:36:15,010 --> 01:36:21,159
 framework and see how what are the pros

5064
01:36:18,270 --> 01:36:21,159
 

5065
01:36:18,280 --> 01:36:23,170
 and cons of this so in Multan framework

5066
01:36:21,149 --> 01:36:23,170
 

5067
01:36:21,159 --> 01:36:26,440
 is the energy based model framework by

5068
01:36:23,160 --> 01:36:26,440
 

5069
01:36:23,170 --> 01:36:27,880
 omnicom and so this is a marginal

5070
01:36:26,430 --> 01:36:27,880
 

5071
01:36:26,440 --> 01:36:29,560
 framework because essentially the only

5072
01:36:27,870 --> 01:36:29,560
 

5073
01:36:27,880 --> 01:36:33,670
 need the only thing that you need to

5074
01:36:29,550 --> 01:36:33,670
 

5075
01:36:29,560 --> 01:36:35,320
 define is an energy function so for

5076
01:36:33,660 --> 01:36:35,320
 

5077
01:36:33,670 --> 01:36:38,920
 every input at a point you want to be

5078
01:36:35,310 --> 01:36:38,920
 

5079
01:36:35,320 --> 01:36:40,960
 able to assign a scalar value and this

5080
01:36:38,910 --> 01:36:40,960
 

5081
01:36:38,920 --> 01:36:43,030
 energy function is parameterized and the

5082
01:36:40,950 --> 01:36:43,030
 

5083
01:36:40,960 --> 01:36:45,760
 goal of learning is to make sure that

5084
01:36:43,020 --> 01:36:45,760
 

5085
01:36:43,030 --> 01:36:48,159
 the energy is lower whether it is high

5086
01:36:45,750 --> 01:36:48,159
 

5087
01:36:45,760 --> 01:36:50,440
 density or training data okay as you can

5088
01:36:48,149 --> 01:36:50,440
 

5089
01:36:48,159 --> 01:36:53,770
 see in this picture so training is about

5090
01:36:50,430 --> 01:36:53,770
 

5091
01:36:50,440 --> 01:36:57,340
 shaping this energy function and so why

5092
01:36:53,760 --> 01:36:57,340
 

5093
01:36:53,770 --> 01:36:59,380
 is this useful well let's say that you

5094
01:36:57,330 --> 01:36:59,380
 

5095
01:36:57,340 --> 01:37:01,000
 have some missing variables okay well

5096
01:36:59,370 --> 01:37:01,000
 

5097
01:36:59,380 --> 01:37:05,380
 let's say that you add noise to your

5098
01:37:00,990 --> 01:37:05,380
 

5099
01:37:01,000 --> 01:37:09,310
 input data point in order to Denalis

5100
01:37:05,370 --> 01:37:09,310
 

5101
01:37:05,380 --> 01:37:12,850
 what you can do is you can run a search

5102
01:37:09,300 --> 01:37:12,850
 

5103
01:37:09,310 --> 01:37:17,110
 in your energy surface to find the

5104
01:37:12,840 --> 01:37:17,110
 

5105
01:37:12,850 --> 01:37:19,420
 closest minimum to your starting point

5106
01:37:17,100 --> 01:37:19,420
 

5107
01:37:17,110 --> 01:37:21,310
 right and because the energy country is

5108
01:37:19,410 --> 01:37:21,310
 

5109
01:37:19,420 --> 01:37:22,780
 parameterized by some deep neural net

5110
01:37:21,300 --> 01:37:22,780
 

5111
01:37:21,310 --> 01:37:24,730
 with deterministic or stochastic

5112
01:37:22,770 --> 01:37:24,730
 

5113
01:37:22,780 --> 01:37:26,110
 variables you are able to extract

5114
01:37:24,720 --> 01:37:26,110
 

5115
01:37:24,730 --> 01:37:28,780
 features from any subset of the

5116
01:37:26,100 --> 01:37:28,780
 

5117
01:37:26,110 --> 01:37:31,179
 variables but this comes with a catch

5118
01:37:28,770 --> 01:37:31,179
 

5119
01:37:28,780 --> 01:37:34,210
 and the catch is that is very hard to

5120
01:37:31,169 --> 01:37:34,210
 

5121
01:37:31,179 --> 01:37:37,119
 learn this energy functions so because

5122
01:37:34,200 --> 01:37:37,119
 

5123
01:37:34,210 --> 01:37:38,710
 you only observe positive samples if you

5124
01:37:37,109 --> 01:37:38,710
 

5125
01:37:37,119 --> 01:37:40,619
 want to learn these contrasted functions

5126
01:37:38,700 --> 01:37:40,619
 

5127
01:37:38,710 --> 01:37:44,469
 you need to be able to fantasize

5128
01:37:40,609 --> 01:37:44,469
 

5129
01:37:40,619 --> 01:37:45,810
 negative samples and by if you work in a

5130
01:37:44,459 --> 01:37:45,810
 

5131
01:37:44,469 --> 01:37:51,720
 very high dimensional space

5132
01:37:45,800 --> 01:37:51,720
 

5133
01:37:45,810 --> 01:37:53,610
 you need to push up the energy lots and

5134
01:37:51,710 --> 01:37:53,610
 

5135
01:37:51,720 --> 01:37:55,950
 lots of points right and this may be

5136
01:37:53,600 --> 01:37:55,950
 

5137
01:37:53,610 --> 01:37:58,860
 infeasible and there are different ways

5138
01:37:55,940 --> 01:37:58,860
 

5139
01:37:55,950 --> 01:38:01,230
 to define negative samples by search by

5140
01:37:58,850 --> 01:38:01,230
 

5141
01:37:58,860 --> 01:38:04,800
 simply using things like contrast

5142
01:38:01,220 --> 01:38:04,800
 

5143
01:38:01,230 --> 01:38:06,450
 divergence because it's really hard to

5144
01:38:04,790 --> 01:38:06,450
 

5145
01:38:04,800 --> 01:38:09,180
 because the space is too large to

5146
01:38:06,440 --> 01:38:09,180
 

5147
01:38:06,450 --> 01:38:11,970
 explicitly pull up the energy you may

5148
01:38:09,170 --> 01:38:11,970
 

5149
01:38:09,180 --> 01:38:14,730
 want to consider other approaches like

5150
01:38:11,960 --> 01:38:14,730
 

5151
01:38:11,970 --> 01:38:16,890
 if you use an encoder and your energy

5152
01:38:14,720 --> 01:38:16,890
 

5153
01:38:14,730 --> 01:38:18,960
 function is the reconstruction error one

5154
01:38:16,880 --> 01:38:18,960
 

5155
01:38:16,890 --> 01:38:22,080
 way to constrain the model is to have a

5156
01:38:18,950 --> 01:38:22,080
 

5157
01:38:18,960 --> 01:38:24,360
 bottleneck and so to say well if my code

5158
01:38:22,070 --> 01:38:24,360
 

5159
01:38:22,080 --> 01:38:27,480
 is low dimensional or if it is sparse if

5160
01:38:24,350 --> 01:38:27,480
 

5161
01:38:24,360 --> 01:38:29,970
 you inject noise then I simply don't

5162
01:38:27,470 --> 01:38:29,970
 

5163
01:38:27,480 --> 01:38:32,400
 have enough codes to reconstruct well

5164
01:38:29,960 --> 01:38:32,400
 

5165
01:38:29,970 --> 01:38:33,780
 every input data point and so if I don't

5166
01:38:32,390 --> 01:38:33,780
 

5167
01:38:32,400 --> 01:38:35,400
 have enough codes and I need to

5168
01:38:33,770 --> 01:38:35,400
 

5169
01:38:33,780 --> 01:38:38,550
 reconstruct well my training samples

5170
01:38:35,390 --> 01:38:38,550
 

5171
01:38:35,400 --> 01:38:42,540
 then necessarily I'm going to do a worse

5172
01:38:38,540 --> 01:38:42,540
 

5173
01:38:38,550 --> 01:38:44,370
 job noisy data right and so where my

5174
01:38:42,530 --> 01:38:44,370
 

5175
01:38:42,540 --> 01:38:46,860
 animations not work but you get the idea

5176
01:38:44,360 --> 01:38:46,860
 

5177
01:38:44,370 --> 01:38:50,310
 that if you like move this parabola

5178
01:38:46,850 --> 01:38:50,310
 

5179
01:38:46,860 --> 01:38:51,390
 where well you put a minimum where there

5180
01:38:50,300 --> 01:38:51,390
 

5181
01:38:50,310 --> 01:38:53,490
 is high density of training data

5182
01:38:51,380 --> 01:38:53,490
 

5183
01:38:51,390 --> 01:38:59,220
 necessarily as well you have high

5184
01:38:53,480 --> 01:38:59,220
 

5185
01:38:53,490 --> 01:39:01,650
 density so you have low low probability

5186
01:38:59,210 --> 01:39:01,650
 

5187
01:38:59,220 --> 01:39:03,540
 let's say and so you see that there is a

5188
01:39:01,640 --> 01:39:03,540
 

5189
01:39:01,650 --> 01:39:05,640
 trade-off so you want to model very well

5190
01:39:03,530 --> 01:39:05,640
 

5191
01:39:03,540 --> 01:39:07,350
 your data and your data might be in a

5192
01:39:05,630 --> 01:39:07,350
 

5193
01:39:05,640 --> 01:39:08,970
 very high dimensional space may be very

5194
01:39:07,340 --> 01:39:08,970
 

5195
01:39:07,350 --> 01:39:10,650
 nonlinear right and so you want to have

5196
01:39:08,960 --> 01:39:10,650
 

5197
01:39:08,970 --> 01:39:14,280
 a very flexible model by the same time

5198
01:39:10,640 --> 01:39:14,280
 

5199
01:39:10,650 --> 01:39:16,770
 you need to make this intensifier

5200
01:39:14,270 --> 01:39:16,770
 

5201
01:39:14,280 --> 01:39:18,930
 contrast even so the pull-up becomes

5202
01:39:16,760 --> 01:39:18,930
 

5203
01:39:16,770 --> 01:39:21,750
 very very hard and so the question is

5204
01:39:18,920 --> 01:39:21,750
 

5205
01:39:18,930 --> 01:39:22,740
 are there better ways to pull up this

5206
01:39:21,740 --> 01:39:22,740
 

5207
01:39:21,750 --> 01:39:24,420
 energy function

5208
01:39:22,730 --> 01:39:24,420
 

5209
01:39:22,740 --> 01:39:28,620
 are there more general frameworks that

5210
01:39:24,410 --> 01:39:28,620
 

5211
01:39:24,420 --> 01:39:32,880
 are more workable than this one and to

5212
01:39:28,610 --> 01:39:32,880
 

5213
01:39:28,620 --> 01:39:35,460
 which extent can we design losses and

5214
01:39:32,870 --> 01:39:35,460
 

5215
01:39:32,880 --> 01:39:37,170
 frameworks in a way that is agnostic to

5216
01:39:35,450 --> 01:39:37,170
 

5217
01:39:35,460 --> 01:39:39,300
 the architecture and the application

5218
01:39:37,160 --> 01:39:39,300
 

5219
01:39:37,170 --> 01:39:42,230
 right because the fact that I use a

5220
01:39:39,290 --> 01:39:42,230
 

5221
01:39:39,300 --> 01:39:42,230
 convolutional neural net

5222
01:39:42,860 --> 01:39:42,860
 

5223
01:39:42,870 --> 01:39:52,420
 gives me a lot of good biases and

5224
01:39:47,580 --> 01:39:52,420
 

5225
01:39:47,590 --> 01:39:54,340
 constrain my search space the third

5226
01:39:52,410 --> 01:39:54,340
 

5227
01:39:52,420 --> 01:39:57,700
 challenge is about uncertainty so

5228
01:39:54,330 --> 01:39:57,700
 

5229
01:39:54,340 --> 01:39:59,530
 whenever you make a prediction in a high

5230
01:39:57,690 --> 01:39:59,530
 

5231
01:39:57,700 --> 01:40:06,190
 dimensional space oftentimes there are

5232
01:39:59,520 --> 01:40:06,190
 

5233
01:39:59,530 --> 01:40:10,300
 many plausible ways too many possible

5234
01:40:06,180 --> 01:40:10,300
 

5235
01:40:06,190 --> 01:40:12,100
 future outcomes and so for instance

5236
01:40:10,290 --> 01:40:12,100
 

5237
01:40:10,300 --> 01:40:14,560
 there are many ways to complete the

5238
01:40:12,090 --> 01:40:14,560
 

5239
01:40:12,100 --> 01:40:18,040
 sentence that are both plausible right

5240
01:40:14,550 --> 01:40:18,040
 

5241
01:40:14,560 --> 01:40:19,510
 and so there are many matters to deal

5242
01:40:18,030 --> 01:40:19,510
 

5243
01:40:18,040 --> 01:40:21,820
 with uncertainty you can have let them

5244
01:40:19,500 --> 01:40:21,820
 

5245
01:40:19,510 --> 01:40:24,100
 variables you can you can use guns and

5246
01:40:21,810 --> 01:40:24,100
 

5247
01:40:21,820 --> 01:40:26,920
 so on so forth and there is a question

5248
01:40:24,090 --> 01:40:26,920
 

5249
01:40:24,100 --> 01:40:32,680
 of which method gives a better fit and

5250
01:40:26,910 --> 01:40:32,680
 

5251
01:40:26,920 --> 01:40:40,080
 which method is more efficient and and

5252
01:40:32,670 --> 01:40:40,080
 

5253
01:40:32,680 --> 01:40:43,060
 so let me conclude with a final note so

5254
01:40:40,070 --> 01:40:43,060
 

5255
01:40:40,080 --> 01:40:45,310
 our interest in unsupervised learning is

5256
01:40:43,050 --> 01:40:45,310
 

5257
01:40:43,060 --> 01:40:47,170
 because we want to learn with less

5258
01:40:45,300 --> 01:40:47,170
 

5259
01:40:45,310 --> 01:40:49,360
 supervision right as Alex said we are

5260
01:40:47,160 --> 01:40:49,360
 

5261
01:40:47,170 --> 01:40:51,610
 not interested in doing well at a task

5262
01:40:49,350 --> 01:40:51,610
 

5263
01:40:49,360 --> 01:40:53,170
 but we want to learn a skip right and so

5264
01:40:51,600 --> 01:40:53,170
 

5265
01:40:51,610 --> 01:40:54,610
 we want to learn more efficiently from

5266
01:40:53,160 --> 01:40:54,610
 

5267
01:40:53,170 --> 01:40:56,620
 less level data and there are a lot of

5268
01:40:54,600 --> 01:40:56,620
 

5269
01:40:54,610 --> 01:40:58,750
 fields in machine learning that share

5270
01:40:56,610 --> 01:40:58,750
 

5271
01:40:56,620 --> 01:41:00,340
 the same goal right of learning with us

5272
01:40:58,740 --> 01:41:00,340
 

5273
01:40:58,750 --> 01:41:02,440
 data to be able to transfer knowledge

5274
01:41:00,330 --> 01:41:02,440
 

5275
01:41:00,340 --> 01:41:04,270
 here and mention just a few

5276
01:41:02,430 --> 01:41:04,270
 

5277
01:41:02,440 --> 01:41:06,460
 so perhaps one way to think about

5278
01:41:04,260 --> 01:41:06,460
 

5279
01:41:04,270 --> 01:41:09,430
 unsupervised learning is in a broader

5280
01:41:06,450 --> 01:41:09,430
 

5281
01:41:06,460 --> 01:41:11,680
 context right and so maybe one way to

5282
01:41:09,420 --> 01:41:11,680
 

5283
01:41:09,430 --> 01:41:14,740
 approach the supervised learning is also

5284
01:41:11,670 --> 01:41:14,740
 

5285
01:41:11,680 --> 01:41:16,930
 by taking these other learning settings

5286
01:41:14,730 --> 01:41:16,930
 

5287
01:41:14,740 --> 01:41:20,410
 and gradually reduce supervision and

5288
01:41:16,920 --> 01:41:20,410
 

5289
01:41:16,930 --> 01:41:22,540
 more generally I think we should not

5290
01:41:20,400 --> 01:41:22,540
 

5291
01:41:20,410 --> 01:41:26,380
 look at unsupervised learning in

5292
01:41:22,530 --> 01:41:26,380
 

5293
01:41:22,540 --> 01:41:30,250
 isolation and so if we go back to the

5294
01:41:26,370 --> 01:41:30,250
 

5295
01:41:26,380 --> 01:41:33,850
 nice metaphor by Ali Khan we shouldn't

5296
01:41:30,240 --> 01:41:33,850
 

5297
01:41:30,250 --> 01:41:35,800
 try to just you shouldn't want to just

5298
01:41:33,840 --> 01:41:35,800
 

5299
01:41:33,850 --> 01:41:37,450
 did that sorry or just in the feeling of

5300
01:41:35,790 --> 01:41:37,450
 

5301
01:41:35,800 --> 01:41:40,060
 the case you you really want to take a

5302
01:41:37,440 --> 01:41:40,060
 

5303
01:41:37,450 --> 01:41:44,670
 whole slice right and so what I mean is

5304
01:41:40,050 --> 01:41:44,670
 

5305
01:41:40,060 --> 01:41:46,600
 that I can hardly imagine how we can get

5306
01:41:44,660 --> 01:41:46,600
 

5307
01:41:44,670 --> 01:41:49,630
 reinforcement learning to work well

5308
01:41:46,590 --> 01:41:49,630
 

5309
01:41:46,600 --> 01:41:52,100
 without leveraging the information that

5310
01:41:49,620 --> 01:41:52,100
 

5311
01:41:49,630 --> 01:41:54,410
 is present in the observations

5312
01:41:52,090 --> 01:41:54,410
 

5313
01:41:52,100 --> 01:41:57,110
 at the same time I can hardly imagine

5314
01:41:54,400 --> 01:41:57,110
 

5315
01:41:54,410 --> 01:42:01,160
 how we can do successful unsupervised

5316
01:41:57,100 --> 01:42:01,160
 

5317
01:41:57,110 --> 01:42:03,110
 learning by looking at that in isolation

5318
01:42:01,150 --> 01:42:03,110
 

5319
01:42:01,160 --> 01:42:05,450
 so it has to be part of a bigger system

5320
01:42:03,100 --> 01:42:05,450
 

5321
01:42:03,110 --> 01:42:08,270
 and if another component in your

5322
01:42:05,440 --> 01:42:08,270
 

5323
01:42:05,450 --> 01:42:11,090
 learning system receives a sparse reward

5324
01:42:08,260 --> 01:42:11,090
 

5325
01:42:08,270 --> 01:42:14,150
 that may not be sufficient for training

5326
01:42:11,080 --> 01:42:14,150
 

5327
01:42:11,090 --> 01:42:16,490
 your parameters by California about what

5328
01:42:14,140 --> 01:42:16,490
 

5329
01:42:14,150 --> 01:42:19,520
 are good and supervised tasks that you

5330
01:42:16,480 --> 01:42:19,520
 

5331
01:42:16,490 --> 01:42:22,370
 may want to consider and so the

5332
01:42:19,510 --> 01:42:22,370
 

5333
01:42:19,520 --> 01:42:27,320
 conclusion is that unsupervised learning

5334
01:42:22,360 --> 01:42:27,320
 

5335
01:42:22,370 --> 01:42:30,190
 is really about learning with hyping the

5336
01:42:27,310 --> 01:42:30,190
 

5337
01:42:27,320 --> 01:42:33,620
 agent or the model learning with less

5338
01:42:30,180 --> 01:42:33,620
 

5339
01:42:30,190 --> 01:42:35,060
 interactions with a teacher and there

5340
01:42:33,610 --> 01:42:35,060
 

5341
01:42:33,620 --> 01:42:37,370
 are several applications some of them

5342
01:42:35,050 --> 01:42:37,370
 

5343
01:42:35,060 --> 01:42:39,920
 are already pretty successful but in

5344
01:42:37,360 --> 01:42:39,920
 

5345
01:42:37,370 --> 01:42:41,840
 general there are open passion so one

5346
01:42:39,910 --> 01:42:41,840
 

5347
01:42:39,920 --> 01:42:44,570
 open question is about defining good

5348
01:42:41,830 --> 01:42:44,570
 

5349
01:42:41,840 --> 01:42:47,810
 matrix another big open question is

5350
01:42:44,560 --> 01:42:47,810
 

5351
01:42:44,570 --> 01:42:50,630
 about making our algorithms more general

5352
01:42:47,800 --> 01:42:50,630
 

5353
01:42:47,810 --> 01:42:52,580
 where it any efficiencies and the third

5354
01:42:50,620 --> 01:42:52,580
 

5355
01:42:50,630 --> 01:42:54,590
 one is about integrating supervised

5356
01:42:52,570 --> 01:42:54,590
 

5357
01:42:52,580 --> 01:42:57,020
 learning in a broader context of your

5358
01:42:54,580 --> 01:42:57,020
 

5359
01:42:54,590 --> 01:42:58,940
 learning system and with this one I want

5360
01:42:57,010 --> 01:42:58,940
 

5361
01:42:57,020 --> 01:42:59,840
 to thank you and we'll be happy to take

5362
01:42:58,930 --> 01:42:59,840
 

5363
01:42:58,940 --> 01:43:08,569
 your questions now

5364
01:42:59,830 --> 01:43:08,569
 

5365
01:42:59,840 --> 01:43:08,569
[Applause]

5366
01:43:10,220 --> 01:43:10,220
 

5367
01:43:10,230 --> 01:43:15,640
 so there is time for about two questions

5368
01:43:13,530 --> 01:43:15,640
 

5369
01:43:13,540 --> 01:43:17,470
 and then I as I asked earlier it'll be

5370
01:43:15,630 --> 01:43:17,470
 

5371
01:43:15,640 --> 01:43:19,330
 nice if you could wait a bit until we

5372
01:43:17,460 --> 01:43:19,330
 

5373
01:43:17,470 --> 01:43:21,970
 have the question and answer without

5374
01:43:19,320 --> 01:43:21,970
 

5375
01:43:19,330 --> 01:43:25,090
 leaving the room obviously people are we

5376
01:43:21,960 --> 01:43:25,090
 

5377
01:43:21,970 --> 01:43:28,090
 do it so do we have any questions from

5378
01:43:25,080 --> 01:43:28,090
 

5379
01:43:25,090 --> 01:43:29,670
 the audience so there are forests and oh

5380
01:43:28,080 --> 01:43:29,670
 

5381
01:43:28,090 --> 01:43:32,290
 yeah there you go yes please

5382
01:43:29,660 --> 01:43:32,290
 

5383
01:43:29,670 --> 01:43:34,270
 can you find the microphone that is

5384
01:43:32,280 --> 01:43:34,270
 

5385
01:43:32,290 --> 01:43:40,270
 standing somewhere and then ask a

5386
01:43:34,260 --> 01:43:40,270
 

5387
01:43:34,270 --> 01:43:41,980
 question yes at the very back while we

5388
01:43:40,260 --> 01:43:41,980
 

5389
01:43:40,270 --> 01:43:46,090
 are waiting for him to go to the

5390
01:43:41,970 --> 01:43:46,090
 

5391
01:43:41,980 --> 01:43:49,120
 microphone yes are the microphones

5392
01:43:46,080 --> 01:43:49,120
 

5393
01:43:46,090 --> 01:43:51,310
 working so thank you for the tutorial

5394
01:43:49,110 --> 01:43:51,310
 

5395
01:43:49,120 --> 01:43:53,230
 could you please comment how all this is

5396
01:43:51,300 --> 01:43:53,230
 

5397
01:43:51,310 --> 01:43:55,420
 a beautiful tools of unsupervised

5398
01:43:53,220 --> 01:43:55,420
 

5399
01:43:53,230 --> 01:43:59,220
 learning may be political for anomaly

5400
01:43:55,410 --> 01:43:59,220
 

5401
01:43:55,420 --> 01:44:12,580
 detection tasks both in say images and

5402
01:43:59,210 --> 01:44:12,580
 

5403
01:43:59,220 --> 01:44:14,670
 videos so that that's now I would say

5404
01:44:12,570 --> 01:44:14,670
 

5405
01:44:12,580 --> 01:44:16,540
 the mainstream tasks but you can see

5406
01:44:14,660 --> 01:44:16,540
 

5407
01:44:14,670 --> 01:44:21,390
 unsupervised learning being really

5408
01:44:16,530 --> 01:44:21,390
 

5409
01:44:16,540 --> 01:44:21,390
 learning only from positive data and so

5410
01:44:22,150 --> 01:44:22,150
 

5411
01:44:22,160 --> 01:44:29,700
 yeah essentially doing the S&T

5412
01:44:26,209 --> 01:44:29,700
 

5413
01:44:26,219 --> 01:44:32,100
 estimation i'm figuring out data points

5414
01:44:29,690 --> 01:44:32,100
 

5415
01:44:29,700 --> 01:44:37,469
 that are very far from what you have

5416
01:44:32,090 --> 01:44:37,469
 

5417
01:44:32,100 --> 01:44:39,810
 observed and so i think i would say that

5418
01:44:37,459 --> 01:44:39,810
 

5419
01:44:37,469 --> 01:44:41,940
 in other communities I would say that

5420
01:44:39,800 --> 01:44:41,940
 

5421
01:44:39,810 --> 01:44:43,590
 anomaly detection is the application of

5422
01:44:41,930 --> 01:44:43,590
 

5423
01:44:41,940 --> 01:44:46,310
 unsupervised learning although I would

5424
01:44:43,580 --> 01:44:46,310
 

5425
01:44:43,590 --> 01:44:48,780
 say in machine learning that's not I

5426
01:44:46,300 --> 01:44:48,780
 

5427
01:44:46,310 --> 01:44:50,760
 think the main drive is mostly about

5428
01:44:48,770 --> 01:44:50,760
 

5429
01:44:48,780 --> 01:44:55,320
 modeling data and learning

5430
01:44:50,750 --> 01:44:55,320
 

5431
01:44:50,760 --> 01:44:56,760
 representations for other tasks I agree

5432
01:44:55,310 --> 01:44:56,760
 

5433
01:44:55,320 --> 01:44:58,560
 with that a minute we say obviously with

5434
01:44:56,750 --> 01:44:58,560
 

5435
01:44:56,760 --> 01:44:59,760
 anomaly detection as a supervised

5436
01:44:58,550 --> 01:44:59,760
 

5437
01:44:58,560 --> 01:45:03,360
 problem the problem is you know

5438
01:44:59,750 --> 01:45:03,360
 

5439
01:44:59,760 --> 01:45:04,650
 massively unbalanced datasets whereas I

5440
01:45:03,350 --> 01:45:04,650
 

5441
01:45:03,360 --> 01:45:06,330
 think in sometimes you get it for free

5442
01:45:04,640 --> 01:45:06,330
 

5443
01:45:04,650 --> 01:45:08,430
 with unsupervised learning if you have a

5444
01:45:06,320 --> 01:45:08,430
 

5445
01:45:06,330 --> 01:45:10,080
 good model of the data then you can just

5446
01:45:08,420 --> 01:45:10,080
 

5447
01:45:08,430 --> 01:45:12,360
 see you know how likely something is

5448
01:45:10,070 --> 01:45:12,360
 

5449
01:45:10,080 --> 01:45:14,070
 under that model and you know it should

5450
01:45:12,350 --> 01:45:14,070
 

5451
01:45:12,360 --> 01:45:16,140
 anomaly detection should follow quite

5452
01:45:14,060 --> 01:45:16,140
 

5453
01:45:14,070 --> 01:45:30,630
 naturally I guess from from a good

5454
01:45:16,130 --> 01:45:30,630
 

5455
01:45:16,140 --> 01:45:32,130
 unsupervised the question if I

5456
01:45:30,620 --> 01:45:32,130
 

5457
01:45:30,630 --> 01:45:35,130
 understood correctly you mentioned in

5458
01:45:32,120 --> 01:45:35,130
 

5459
01:45:32,130 --> 01:45:37,200
 passing that architectures are sometimes

5460
01:45:35,120 --> 01:45:37,200
 

5461
01:45:35,130 --> 01:45:39,360
 more important in order to get good

5462
01:45:37,190 --> 01:45:39,360
 

5463
01:45:37,200 --> 01:45:40,700
 performance with gans then how you train

5464
01:45:39,350 --> 01:45:40,700
 

5465
01:45:39,360 --> 01:45:42,480
 them and which loss function could you

5466
01:45:40,690 --> 01:45:42,480
 

5467
01:45:40,700 --> 01:45:46,380
 elaborate on that a little bit

5468
01:45:42,470 --> 01:45:46,380
 

5469
01:45:42,480 --> 01:45:51,060
 Oh what times what I meant

5470
01:45:46,370 --> 01:45:51,060
 

5471
01:45:46,380 --> 01:45:56,580
 is that if you look so conclusion your

5472
01:45:51,050 --> 01:45:56,580
 

5473
01:45:51,060 --> 01:45:58,410
 network gives you a strong a very good

5474
01:45:56,570 --> 01:45:58,410
 

5475
01:45:56,580 --> 01:46:00,450
 starting point right so they are

5476
01:45:58,400 --> 01:46:00,450
 

5477
01:45:58,410 --> 01:46:04,080
 designed with very good inductive biases

5478
01:46:00,440 --> 01:46:04,080
 

5479
01:46:00,450 --> 01:46:06,120
 and often times and nowadays you see

5480
01:46:04,070 --> 01:46:06,120
 

5481
01:46:04,080 --> 01:46:09,000
 that many approaches from auto

5482
01:46:06,110 --> 01:46:09,000
 

5483
01:46:06,120 --> 01:46:10,860
 regressive models guns latent variable

5484
01:46:08,990 --> 01:46:10,860
 

5485
01:46:09,000 --> 01:46:16,200
 models are able to generate pretty

5486
01:46:10,850 --> 01:46:16,200
 

5487
01:46:10,860 --> 01:46:18,510
 realistic images right and so there

5488
01:46:16,190 --> 01:46:18,510
 

5489
01:46:16,200 --> 01:46:20,310
 seems to be enough evidence that more

5490
01:46:18,500 --> 01:46:20,310
 

5491
01:46:18,510 --> 01:46:25,260
 than the loss function

5492
01:46:20,300 --> 01:46:25,260
 

5493
01:46:20,310 --> 01:46:27,840
 the choice of architecture is oftentimes

5494
01:46:25,250 --> 01:46:27,840
 

5495
01:46:25,260 --> 01:46:30,660
 more important for the quality of the

5496
01:46:27,830 --> 01:46:30,660
 

5497
01:46:27,840 --> 01:46:32,869
 generation and so if you use a gun with

5498
01:46:30,650 --> 01:46:32,869
 

5499
01:46:30,660 --> 01:46:34,969
 a weak

5500
01:46:32,859 --> 01:46:34,969
 

5501
01:46:32,869 --> 01:46:37,010
 Lucian Network I don't know it doesn't

5502
01:46:34,959 --> 01:46:37,010
 

5503
01:46:34,969 --> 01:46:38,689
 have the latest normalization layer then

5504
01:46:37,000 --> 01:46:38,689
 

5505
01:46:37,010 --> 01:46:41,709
 the termination are not quite as good

5506
01:46:38,679 --> 01:46:41,709
 

5507
01:46:38,689 --> 01:46:45,079
 and you can get as good generations guns

5508
01:46:41,699 --> 01:46:45,079
 

5509
01:46:41,709 --> 01:46:47,149
 if you use also other approaches like

5510
01:46:45,069 --> 01:46:47,149
 

5511
01:46:45,079 --> 01:46:51,769
 you know the auto regressive methods

5512
01:46:47,139 --> 01:46:51,769
 

5513
01:46:47,149 --> 01:46:53,149
 that Alex make sure of course there is

5514
01:46:51,759 --> 01:46:53,149
 

5515
01:46:51,769 --> 01:46:56,659
 the question of how do you evaluate

5516
01:46:53,139 --> 01:46:56,659
 

5517
01:46:53,149 --> 01:46:58,639
 different models and right now for image

5518
01:46:56,649 --> 01:46:58,639
 

5519
01:46:56,659 --> 01:47:01,219
 generation there are quite a few matrix

5520
01:46:58,629 --> 01:47:01,219
 

5521
01:46:58,639 --> 01:47:05,479
 I don't think they are particularly

5522
01:47:01,209 --> 01:47:05,479
 

5523
01:47:01,219 --> 01:47:07,399
 satisfying in my view and so maybe it's

5524
01:47:05,469 --> 01:47:07,399
 

5525
01:47:05,479 --> 01:47:10,489
 not a question of defining good matrix

5526
01:47:07,389 --> 01:47:10,489
 

5527
01:47:07,399 --> 01:47:13,699
 to assess quality of these generating

5528
01:47:10,479 --> 01:47:13,699
 

5529
01:47:10,489 --> 01:47:16,429
 models and when you say architectures in

5530
01:47:13,689 --> 01:47:16,429
 

5531
01:47:13,699 --> 01:47:19,399
 this context do you mean which type of

5532
01:47:16,419 --> 01:47:19,399
 

5533
01:47:16,429 --> 01:47:21,260
 architecture or more it actually is by

5534
01:47:19,389 --> 01:47:21,260
 

5535
01:47:19,399 --> 01:47:23,030
 architecture I mean the kind of

5536
01:47:21,250 --> 01:47:23,030
 

5537
01:47:21,260 --> 01:47:26,360
 generator that you use the function that

5538
01:47:23,020 --> 01:47:26,360
 

5539
01:47:23,030 --> 01:47:28,939
 turns a sample from a Gaussian

5540
01:47:26,350 --> 01:47:28,939
 

5541
01:47:26,360 --> 01:47:31,189
 distribution to a data point so what

5542
01:47:28,929 --> 01:47:31,189
 

5543
01:47:28,939 --> 01:47:37,459
 kind of parametric function to use coop

5544
01:47:31,179 --> 01:47:37,459
 

5545
01:47:31,189 --> 01:47:39,289
 think yeah thanks for that great talk I

5546
01:47:37,449 --> 01:47:39,289
 

5547
01:47:37,459 --> 01:47:41,419
 really liked your coverage of the

5548
01:47:39,279 --> 01:47:41,419
 

5549
01:47:39,289 --> 01:47:43,039
 representation learning within specific

5550
01:47:41,409 --> 01:47:43,039
 

5551
01:47:41,419 --> 01:47:45,409
 domains and the state of the science

5552
01:47:43,029 --> 01:47:45,409
 

5553
01:47:43,039 --> 01:47:47,570
 could you comment on the state of the

5554
01:47:45,399 --> 01:47:47,570
 

5555
01:47:45,409 --> 01:47:50,419
 science as we look across very different

5556
01:47:47,560 --> 01:47:50,419
 

5557
01:47:47,570 --> 01:47:57,790
 domains for instance images and texts is

5558
01:47:50,409 --> 01:47:57,790
 

5559
01:47:50,419 --> 01:47:59,649
 there work happening in that field so

5560
01:47:57,780 --> 01:47:59,649
 

5561
01:47:57,790 --> 01:48:02,959
[Music]

5562
01:47:59,639 --> 01:48:02,959
 

5563
01:47:59,649 --> 01:48:05,030
 yes there is a lot of work on multimodal

5564
01:48:02,949 --> 01:48:05,030
 

5565
01:48:02,959 --> 01:48:10,010
 learning so learning from our doing

5566
01:48:05,020 --> 01:48:10,010
 

5567
01:48:05,030 --> 01:48:11,780
 videos text the images in fact there is

5568
01:48:10,000 --> 01:48:11,780
 

5569
01:48:10,010 --> 01:48:14,629
 a lot of excitement within the such

5570
01:48:11,770 --> 01:48:14,629
 

5571
01:48:11,780 --> 01:48:17,709
 supervised learning approachable pratima

5572
01:48:14,619 --> 01:48:17,709
 

5573
01:48:14,629 --> 01:48:17,709
 modality from the other

5574
01:48:17,919 --> 01:48:17,919
 

5575
01:48:17,929 --> 01:48:23,880
 so yeah no that's definitely sorry I

5576
01:48:22,010 --> 01:48:23,880
 

5577
01:48:22,020 --> 01:48:25,739
 apologize I think I had only one or two

5578
01:48:23,870 --> 01:48:25,739
 

5579
01:48:23,880 --> 01:48:33,690
 references about that but there is a

5580
01:48:25,729 --> 01:48:33,690
 

5581
01:48:25,739 --> 01:48:35,159
 whole subfield works alone on that just

5582
01:48:33,680 --> 01:48:35,159
 

5583
01:48:33,690 --> 01:48:37,260
 a quick pop you think we could use like

5584
01:48:35,149 --> 01:48:37,260
 

5585
01:48:35,159 --> 01:48:43,020
 cycle guns and things like these for

5586
01:48:37,250 --> 01:48:43,020
 

5587
01:48:37,260 --> 01:48:44,820
 similar cast of course then it really

5588
01:48:43,010 --> 01:48:44,820
 

5589
01:48:43,020 --> 01:48:46,590
 depends on the application because for

5590
01:48:44,810 --> 01:48:46,590
 

5591
01:48:44,820 --> 01:48:49,380
 some application going let's say you do

5592
01:48:46,580 --> 01:48:49,380
 

5593
01:48:46,590 --> 01:48:52,020
 you want to do unsupervised speech

5594
01:48:49,370 --> 01:48:52,020
 

5595
01:48:49,380 --> 01:48:53,880
 recognition right there are some

5596
01:48:52,010 --> 01:48:53,880
 

5597
01:48:52,020 --> 01:48:59,400
 challenges when you go from text to

5598
01:48:53,870 --> 01:48:59,400
 

5599
01:48:53,880 --> 01:49:01,920
 speech perhaps there is more ambiguity

5600
01:48:59,390 --> 01:49:01,920
 

5601
01:48:59,400 --> 01:49:04,679
 when you go in that direction then when

5602
01:49:01,910 --> 01:49:04,679
 

5603
01:49:01,920 --> 01:49:07,320
 you go from speech to text and so I

5604
01:49:04,669 --> 01:49:07,320
 

5605
01:49:04,679 --> 01:49:10,380
 don't want to overly generalize but I

5606
01:49:07,310 --> 01:49:10,380
 

5607
01:49:07,320 --> 01:49:12,750
 would I would say that in general there

5608
01:49:10,370 --> 01:49:12,750
 

5609
01:49:10,380 --> 01:49:16,620
 is a lot of work on that and for some

5610
01:49:12,740 --> 01:49:16,620
 

5611
01:49:12,750 --> 01:49:22,980
 applications it is less obvious for

5612
01:49:16,610 --> 01:49:22,980
 

5613
01:49:16,620 --> 01:49:24,540
 others I wanted to act if there's any

5614
01:49:22,970 --> 01:49:24,540
 

5615
01:49:22,980 --> 01:49:28,469
 way we could get access to the

5616
01:49:24,530 --> 01:49:28,469
 

5617
01:49:24,540 --> 01:49:31,139
 presentation material absolutely so it's

5618
01:49:28,459 --> 01:49:31,139
 

5619
01:49:28,469 --> 01:49:34,170
 gonna be on our websites by tomorrow so

5620
01:49:31,129 --> 01:49:34,170
 

5621
01:49:31,139 --> 01:49:36,929
 we're going to upload a PDF sometime in

5622
01:49:34,160 --> 01:49:36,929
 

5623
01:49:34,170 --> 01:49:41,730
 the afternoon so check on our websites

5624
01:49:36,919 --> 01:49:41,730
 

5625
01:49:36,929 --> 01:49:44,820
 my website for sure this is a good

5626
01:49:41,720 --> 01:49:44,820
 

5627
01:49:41,730 --> 01:49:46,770
 question is there in the worst case I'll

5628
01:49:44,810 --> 01:49:46,770
 

5629
01:49:44,820 --> 01:49:48,840
 try to tweet it as much as I can so that

5630
01:49:46,760 --> 01:49:48,840
 

5631
01:49:46,770 --> 01:49:52,580
 you'll have access to slice by tomorrow

5632
01:49:48,830 --> 01:49:52,580
 

5633
01:49:48,840 --> 01:49:54,659
 or so yes we'll try to put a link on the

5634
01:49:52,570 --> 01:49:54,659
 

5635
01:49:52,580 --> 01:50:02,489
 conference on page as well it will

5636
01:49:54,649 --> 01:50:02,489
 

5637
01:49:54,659 --> 01:50:04,230
 definitely make reveille thank you well

5638
01:50:02,479 --> 01:50:04,230
 

5639
01:50:02,489 --> 01:50:06,920
 thanks for coming to the tutorial that

5640
01:50:04,220 --> 01:50:06,920
 

5641
01:50:04,230 --> 01:50:06,920
 concludes the tutorial