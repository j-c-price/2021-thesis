1
00:00:10,560 --> 00:01:27,729
[Music]

2
00:01:34,570 --> 00:01:34,570
 

3
00:01:34,580 --> 00:01:37,980
[Music]

4
00:01:44,520 --> 00:01:44,520
 

5
00:01:44,530 --> 00:02:43,029
[Music]

6
00:02:53,900 --> 00:02:53,900
 

7
00:02:53,910 --> 00:02:57,160
[Music]

8
00:03:04,360 --> 00:03:04,360
 

9
00:03:04,370 --> 00:03:54,129
[Music]

10
00:03:56,970 --> 00:03:56,970
 

11
00:03:56,980 --> 00:04:50,300
[Music]

12
00:04:47,950 --> 00:04:50,300
 

13
00:04:47,960 --> 00:04:53,300
 if you can do so quietly that will be

14
00:04:50,290 --> 00:04:53,300
 

15
00:04:50,300 --> 00:04:55,790
 better because we want to stay on time

16
00:04:53,290 --> 00:04:55,790
 

17
00:04:53,300 --> 00:04:58,040
 so today it's a great pleasure to have

18
00:04:55,780 --> 00:04:58,040
 

19
00:04:55,790 --> 00:05:00,710
 two great speakers for the tutorial on

20
00:04:58,030 --> 00:05:00,710
 

21
00:04:58,040 --> 00:05:02,330
 automatic machine learning the first

22
00:05:00,700 --> 00:05:02,330
 

23
00:05:00,710 --> 00:05:04,460
 speaker is Frank cutter who was a

24
00:05:02,320 --> 00:05:04,460
 

25
00:05:02,330 --> 00:05:07,370
 professor in University of Fribourg and

26
00:05:04,450 --> 00:05:07,370
 

27
00:05:04,460 --> 00:05:10,640
 the second speaker is a Hindman Shirin

28
00:05:07,360 --> 00:05:10,640
 

29
00:05:07,370 --> 00:05:13,280
 Seaton professor at enduring University

30
00:05:10,630 --> 00:05:13,280
 

31
00:05:10,640 --> 00:05:14,440
 of Technology and co-founder of open

32
00:05:13,270 --> 00:05:14,440
 

33
00:05:13,280 --> 00:05:17,810
 imelda Torg

34
00:05:14,430 --> 00:05:17,810
 

35
00:05:14,440 --> 00:05:20,330
 together they organized the autumn al

36
00:05:17,800 --> 00:05:20,330
 

37
00:05:17,810 --> 00:05:22,430
 ICML worship for the past five years and

38
00:05:20,320 --> 00:05:22,430
 

39
00:05:20,330 --> 00:05:24,620
 are actually organizing the metal

40
00:05:22,420 --> 00:05:24,620
 

41
00:05:22,430 --> 00:05:28,910
 learning workshop later this week on

42
00:05:24,610 --> 00:05:28,910
 

43
00:05:24,620 --> 00:05:30,350
 Saturday Frank will start and there is

44
00:05:28,900 --> 00:05:30,350
 

45
00:05:28,910 --> 00:05:32,210
 going to be depending on time maybe a

46
00:05:30,340 --> 00:05:32,210
 

47
00:05:30,350 --> 00:05:34,160
 five-minute break during which you can

48
00:05:32,200 --> 00:05:34,160
 

49
00:05:32,210 --> 00:05:36,620
 also ask questions there are two

50
00:05:34,150 --> 00:05:36,620
 

51
00:05:34,160 --> 00:05:39,200
 microphones one on each side of the

52
00:05:36,610 --> 00:05:39,200
 

53
00:05:36,620 --> 00:05:45,230
 central aisle please line up behind them

54
00:05:39,190 --> 00:05:45,230
 

55
00:05:39,200 --> 00:05:46,870
 and we will call you all right yeah

56
00:05:45,220 --> 00:05:46,870
 

57
00:05:45,230 --> 00:05:49,510
 thank you very much for the introduction

58
00:05:46,860 --> 00:05:49,510
 

59
00:05:46,870 --> 00:05:52,760
 while people are trickling in let me

60
00:05:49,500 --> 00:05:52,760
 

61
00:05:49,510 --> 00:05:56,060
 make one point so we have slides

62
00:05:52,750 --> 00:05:56,060
 

63
00:05:52,760 --> 00:05:59,120
 available for this tutorial if you go to

64
00:05:56,050 --> 00:05:59,120
 

65
00:05:56,060 --> 00:06:06,170
 your browser and you type Auto ml dot

66
00:05:59,110 --> 00:06:06,170
 

67
00:05:59,120 --> 00:06:08,240
 org and your browser works then you can

68
00:06:06,160 --> 00:06:08,240
 

69
00:06:06,170 --> 00:06:09,710
 get the slides there now that there's a

70
00:06:08,230 --> 00:06:09,710
 

71
00:06:08,240 --> 00:06:13,910
 lot of people the Internet is maybe not

72
00:06:09,700 --> 00:06:13,910
 

73
00:06:09,710 --> 00:06:16,610
 so strong anymore so this work ten

74
00:06:13,900 --> 00:06:16,610
 

75
00:06:13,910 --> 00:06:19,070
 minutes ago likely it's not going to

76
00:06:16,600 --> 00:06:19,070
 

77
00:06:16,610 --> 00:06:27,680
 work for you other than but the slides

78
00:06:19,060 --> 00:06:27,680
 

79
00:06:19,070 --> 00:06:31,070
 are available online interesting we

80
00:06:27,670 --> 00:06:31,070
 

81
00:06:27,680 --> 00:06:32,930
 don't have that on the screen but all

82
00:06:31,060 --> 00:06:32,930
 

83
00:06:31,070 --> 00:06:39,490
 all you would see is a browser that's

84
00:06:32,920 --> 00:06:39,490
 

85
00:06:32,930 --> 00:06:39,490
 trying to load oh and it did load now No

86
00:06:39,750 --> 00:06:39,750
 

87
00:06:39,760 --> 00:06:49,010
 well nevermind Auto ml dot org that's

88
00:06:43,690 --> 00:06:49,010
 

89
00:06:43,700 --> 00:06:53,570
 where the slides are and if you go to

90
00:06:49,000 --> 00:06:53,570
 

91
00:06:49,010 --> 00:06:55,820
 Auto ml tutorial so do Auto ml tutorial

92
00:06:53,560 --> 00:06:55,820
 

93
00:06:53,570 --> 00:06:58,550
 here on auto ml dot org that's where the

94
00:06:55,810 --> 00:06:58,550
 

95
00:06:55,820 --> 00:07:00,800
 slides are and all the references in

96
00:06:58,540 --> 00:07:00,800
 

97
00:06:58,550 --> 00:07:02,900
 both my slide set and the one of your

98
00:07:00,790 --> 00:07:02,900
 

99
00:07:00,800 --> 00:07:05,240
 keen are actually clickable links so you

100
00:07:02,890 --> 00:07:05,240
 

101
00:07:02,900 --> 00:07:09,200
 can just directly use slides in order to

102
00:07:05,230 --> 00:07:09,200
 

103
00:07:05,240 --> 00:07:11,840
 explore the literature all right with

104
00:07:09,190 --> 00:07:11,840
 

105
00:07:09,200 --> 00:07:15,470
 that said let's start the tutorial so

106
00:07:11,830 --> 00:07:15,470
 

107
00:07:11,840 --> 00:07:18,490
 this tutorial is on auto ml which is the

108
00:07:15,460 --> 00:07:18,490
 

109
00:07:15,470 --> 00:07:20,990
 quest for using machine learning and

110
00:07:18,480 --> 00:07:20,990
 

111
00:07:18,490 --> 00:07:23,390
 optimization in order to make machine

112
00:07:20,980 --> 00:07:23,390
 

113
00:07:20,990 --> 00:07:25,730
 learning better in order to make machine

114
00:07:23,380 --> 00:07:25,730
 

115
00:07:23,390 --> 00:07:29,360
 learning more robust and easier to use

116
00:07:25,720 --> 00:07:29,360
 

117
00:07:25,730 --> 00:07:33,860
 for people without expert knowledge in

118
00:07:29,350 --> 00:07:33,860
 

119
00:07:29,360 --> 00:07:36,200
 machine learning so to briefly give some

120
00:07:33,850 --> 00:07:36,200
 

121
00:07:33,860 --> 00:07:37,910
 motivation we live in very exciting

122
00:07:36,190 --> 00:07:37,910
 

123
00:07:36,200 --> 00:07:40,190
 times of course for deep learning we've

124
00:07:37,900 --> 00:07:40,190
 

125
00:07:37,910 --> 00:07:41,840
 seen tremendous progress in speech

126
00:07:40,180 --> 00:07:41,840
 

127
00:07:40,190 --> 00:07:44,210
 recognition and computer vision for

128
00:07:41,830 --> 00:07:44,210
 

129
00:07:41,840 --> 00:07:46,790
 self-driving cars in reasoning and games

130
00:07:44,200 --> 00:07:46,790
 

131
00:07:44,210 --> 00:07:50,720
 and deeper enforcement learning etc but

132
00:07:46,780 --> 00:07:50,720
 

133
00:07:46,790 --> 00:07:54,140
 all these advances are based on expert

134
00:07:50,710 --> 00:07:54,140
 

135
00:07:50,720 --> 00:07:57,190
 knowledge of some very brilliant people

136
00:07:54,130 --> 00:07:57,190
 

137
00:07:54,140 --> 00:07:59,840
 who have gathered a lot of expertise in

138
00:07:57,180 --> 00:07:59,840
 

139
00:07:57,190 --> 00:08:02,870
 deep learning and know how to set all

140
00:07:59,830 --> 00:08:02,870
 

141
00:07:59,840 --> 00:08:04,460
 the hyper parameters so if you want to

142
00:08:02,860 --> 00:08:04,460
 

143
00:08:02,870 --> 00:08:07,220
 apply deep learning to a new problem

144
00:08:04,450 --> 00:08:07,220
 

145
00:08:04,460 --> 00:08:08,750
 domain that is of course it's a problem

146
00:08:07,210 --> 00:08:08,750
 

147
00:08:07,220 --> 00:08:10,730
 that deep learning is very sensitive to

148
00:08:08,740 --> 00:08:10,730
 

149
00:08:08,750 --> 00:08:13,790
 a lot of different hyper parameters such

150
00:08:10,720 --> 00:08:13,790
 

151
00:08:10,730 --> 00:08:16,370
 as architectural choices and choices in

152
00:08:13,780 --> 00:08:16,370
 

153
00:08:13,790 --> 00:08:18,530
 the optimization pipeline and the way

154
00:08:16,360 --> 00:08:18,530
 

155
00:08:16,370 --> 00:08:20,870
 you regularize your network in order to

156
00:08:18,520 --> 00:08:20,870
 

157
00:08:18,530 --> 00:08:24,230
 generalize to new applications to new

158
00:08:20,860 --> 00:08:24,230
 

159
00:08:20,870 --> 00:08:27,140
 unseen data and so it's you easily get

160
00:08:24,220 --> 00:08:27,140
 

161
00:08:24,230 --> 00:08:30,200
 between 20 to 50 design decisions that

162
00:08:27,130 --> 00:08:30,200
 

163
00:08:27,140 --> 00:08:33,560
 you need to make in order to start to

164
00:08:30,190 --> 00:08:33,560
 

165
00:08:30,200 --> 00:08:36,200
 train your network and that leads to the

166
00:08:33,550 --> 00:08:36,200
 

167
00:08:33,560 --> 00:08:40,250
 current state of deep learning practice

168
00:08:36,190 --> 00:08:40,250
 

169
00:08:36,200 --> 00:08:42,200
 given a new data set you need an expert

170
00:08:40,240 --> 00:08:42,200
 

171
00:08:40,250 --> 00:08:43,970
 that chooses the architecture and the

172
00:08:42,190 --> 00:08:43,970
 

173
00:08:42,200 --> 00:08:44,510
 hyper parameters that would work well

174
00:08:43,960 --> 00:08:44,510
 

175
00:08:43,970 --> 00:08:47,000
 for the state

176
00:08:44,500 --> 00:08:47,000
 

177
00:08:44,510 --> 00:08:48,680
 to set and then given that architecture

178
00:08:46,990 --> 00:08:48,680
 

179
00:08:47,000 --> 00:08:50,780
 and hyper parameters you can apply deep

180
00:08:48,670 --> 00:08:50,780
 

181
00:08:48,680 --> 00:08:55,690
 learning in an end-to-end fashion to

182
00:08:50,770 --> 00:08:55,690
 

183
00:08:50,780 --> 00:08:58,220
 learn features from the raw data but

184
00:08:55,680 --> 00:08:58,220
 

185
00:08:55,690 --> 00:08:59,810
 unfortunately even experts don't always

186
00:08:58,210 --> 00:08:59,810
 

187
00:08:58,220 --> 00:09:01,160
 know exactly how to choose the

188
00:08:59,800 --> 00:09:01,160
 

189
00:08:59,810 --> 00:09:03,590
 architecture and the hyper parameters

190
00:09:01,150 --> 00:09:03,590
 

191
00:09:01,160 --> 00:09:05,960
 for a new data set so you end up with

192
00:09:03,580 --> 00:09:05,960
 

193
00:09:03,590 --> 00:09:08,510
 this iterative loop with a human in the

194
00:09:05,950 --> 00:09:08,510
 

195
00:09:05,960 --> 00:09:10,010
 loop the human expert having to choose

196
00:09:08,500 --> 00:09:10,010
 

197
00:09:08,510 --> 00:09:11,750
 architectures and different hyper

198
00:09:10,000 --> 00:09:11,750
 

199
00:09:10,010 --> 00:09:14,240
 parameters exploring that space in order

200
00:09:11,740 --> 00:09:14,240
 

201
00:09:11,750 --> 00:09:15,590
 to at the end actually get a nice deep

202
00:09:14,230 --> 00:09:15,590
 

203
00:09:14,240 --> 00:09:18,140
 learning system that get

204
00:09:15,580 --> 00:09:18,140
 

205
00:09:15,590 --> 00:09:20,960
 state-of-the-art performance what Auto

206
00:09:18,130 --> 00:09:20,960
 

207
00:09:18,140 --> 00:09:23,570
 ml tries to do is to provide this kind

208
00:09:20,950 --> 00:09:23,570
 

209
00:09:20,960 --> 00:09:26,600
 of one-stop shop you put in the data and

210
00:09:23,560 --> 00:09:26,600
 

211
00:09:23,570 --> 00:09:30,010
 then there is this big box here that

212
00:09:26,590 --> 00:09:30,010
 

213
00:09:26,600 --> 00:09:33,320
 does end-to-end learning that does learn

214
00:09:30,000 --> 00:09:33,320
 

215
00:09:30,010 --> 00:09:35,630
 features from raw data but also already

216
00:09:33,310 --> 00:09:35,630
 

217
00:09:33,320 --> 00:09:38,690
 decides about which architectures to use

218
00:09:35,620 --> 00:09:38,690
 

219
00:09:35,630 --> 00:09:40,610
 which hyper parameters to use etc one

220
00:09:38,680 --> 00:09:40,610
 

221
00:09:38,690 --> 00:09:44,000
 way to achieve this is of course to

222
00:09:40,600 --> 00:09:44,000
 

223
00:09:40,610 --> 00:09:46,100
 mimic this manual approach up here where

224
00:09:43,990 --> 00:09:46,100
 

225
00:09:44,000 --> 00:09:47,750
 you have well some deep neural network

226
00:09:46,090 --> 00:09:47,750
 

227
00:09:46,100 --> 00:09:49,970
 so you can have some learning box that

228
00:09:47,740 --> 00:09:49,970
 

229
00:09:47,750 --> 00:09:51,770
 could be just a deep learning framework

230
00:09:49,960 --> 00:09:51,770
 

231
00:09:49,970 --> 00:09:53,180
 and you have the expert choosing

232
00:09:51,760 --> 00:09:53,180
 

233
00:09:51,770 --> 00:09:56,270
 architecture and hyper parameters here

234
00:09:53,170 --> 00:09:56,270
 

235
00:09:53,180 --> 00:09:58,640
 you replace that by a meta level

236
00:09:56,260 --> 00:09:58,640
 

237
00:09:56,270 --> 00:10:00,860
 learning and optimization process that

238
00:09:58,630 --> 00:10:00,860
 

239
00:09:58,640 --> 00:10:02,720
 reasons about which architectures work

240
00:10:00,850 --> 00:10:02,720
 

241
00:10:00,860 --> 00:10:05,000
 well which hyper parameters work well on

242
00:10:02,710 --> 00:10:05,000
 

243
00:10:02,720 --> 00:10:09,680
 this particular data set and that can

244
00:10:04,990 --> 00:10:09,680
 

245
00:10:05,000 --> 00:10:12,560
 also reason across data sets one note

246
00:10:09,670 --> 00:10:12,560
 

247
00:10:09,680 --> 00:10:14,780
 this learning box here this is not

248
00:10:12,550 --> 00:10:14,780
 

249
00:10:12,560 --> 00:10:16,790
 constrained to be a deep neural network

250
00:10:14,770 --> 00:10:16,790
 

251
00:10:14,780 --> 00:10:19,220
 it can also be some traditional machine

252
00:10:16,780 --> 00:10:19,220
 

253
00:10:16,790 --> 00:10:21,500
 learning pipeline we need to clean and

254
00:10:19,210 --> 00:10:21,500
 

255
00:10:19,220 --> 00:10:23,300
 pre process your data select an engineer

256
00:10:21,490 --> 00:10:23,300
 

257
00:10:21,500 --> 00:10:25,250
 better features select model family and

258
00:10:23,290 --> 00:10:25,250
 

259
00:10:23,300 --> 00:10:27,410
 the type of parameters and construct

260
00:10:25,240 --> 00:10:27,410
 

261
00:10:25,250 --> 00:10:30,440
 ensemble as all most of these models and

262
00:10:27,400 --> 00:10:30,440
 

263
00:10:27,410 --> 00:10:33,020
 you can also do metal learning metal

264
00:10:30,430 --> 00:10:33,020
 

265
00:10:30,440 --> 00:10:36,340
 reasoning about which types of hyper

266
00:10:33,010 --> 00:10:36,340
 

267
00:10:33,020 --> 00:10:39,260
 parameters work well in these frameworks

268
00:10:36,330 --> 00:10:39,260
 

269
00:10:36,340 --> 00:10:41,840
 so the outline of the tutorial is as

270
00:10:39,250 --> 00:10:41,840
 

271
00:10:39,260 --> 00:10:43,100
 follows will first talk about modern

272
00:10:41,830 --> 00:10:43,100
 

273
00:10:41,840 --> 00:10:46,130
 approaches to hyper parameter

274
00:10:43,090 --> 00:10:46,130
 

275
00:10:43,100 --> 00:10:49,160
 optimization which might sound as a

276
00:10:46,120 --> 00:10:49,160
 

277
00:10:46,130 --> 00:10:50,690
 somewhat boring topic but if you don't

278
00:10:49,150 --> 00:10:50,690
 

279
00:10:49,160 --> 00:10:53,420
 get automated hyper parameter

280
00:10:50,680 --> 00:10:53,420
 

281
00:10:50,690 --> 00:10:56,120
 optimization in inside of your inner

282
00:10:53,410 --> 00:10:56,120
 

283
00:10:53,420 --> 00:10:58,130
 loop then well you can't really do Auto

284
00:10:56,110 --> 00:10:58,130
 

285
00:10:56,120 --> 00:11:00,860
 ml so it's it's really important to

286
00:10:58,120 --> 00:11:00,860
 

287
00:10:58,130 --> 00:11:02,990
 have a robust and efficient core of your

288
00:11:00,850 --> 00:11:02,990
 

289
00:11:00,860 --> 00:11:04,280
 Brahim learning pipeline then you'll

290
00:11:02,980 --> 00:11:04,280
 

291
00:11:02,990 --> 00:11:06,590
 talk about neural architecture search

292
00:11:04,270 --> 00:11:06,590
 

293
00:11:04,280 --> 00:11:08,660
 and then about metal learning

294
00:11:06,580 --> 00:11:08,660
 

295
00:11:06,590 --> 00:11:10,700
 I'll cover the first two of these and

296
00:11:08,650 --> 00:11:10,700
 

297
00:11:08,660 --> 00:11:14,600
 jerkiness gonna talk about metal

298
00:11:10,690 --> 00:11:14,600
 

299
00:11:10,700 --> 00:11:17,060
 learning all of these are based on a

300
00:11:14,590 --> 00:11:17,060
 

301
00:11:14,600 --> 00:11:18,920
 book that we edited in particular these

302
00:11:17,050 --> 00:11:18,920
 

303
00:11:17,060 --> 00:11:21,860
 are the three first chapters of that

304
00:11:18,910 --> 00:11:21,860
 

305
00:11:18,920 --> 00:11:26,120
 book basically review articles about

306
00:11:21,850 --> 00:11:26,120
 

307
00:11:21,860 --> 00:11:28,430
 these three different fields so the

308
00:11:26,110 --> 00:11:28,430
 

309
00:11:26,120 --> 00:11:30,160
 outline for my part is well first I'll

310
00:11:28,420 --> 00:11:30,160
 

311
00:11:28,430 --> 00:11:33,080
 talk about hyper primate optimization

312
00:11:30,150 --> 00:11:33,080
 

313
00:11:30,160 --> 00:11:35,420
 phrasing auto ml as a hyper primate

314
00:11:33,070 --> 00:11:35,420
 

315
00:11:33,080 --> 00:11:37,760
 optimization problem then blackbox

316
00:11:35,410 --> 00:11:37,760
 

317
00:11:35,420 --> 00:11:39,470
 optimization methods for hyper parameter

318
00:11:37,750 --> 00:11:39,470
 

319
00:11:37,760 --> 00:11:41,570
 optimization and then going beyond

320
00:11:39,460 --> 00:11:41,570
 

321
00:11:39,470 --> 00:11:43,700
 blackbox optimization in order to gain

322
00:11:41,560 --> 00:11:43,700
 

323
00:11:41,570 --> 00:11:47,090
 efficiency for a neural architecture

324
00:11:43,690 --> 00:11:47,090
 

325
00:11:43,700 --> 00:11:49,040
 search the overview will basically mimic

326
00:11:47,080 --> 00:11:49,040
 

327
00:11:47,090 --> 00:11:51,920
 that first there is search based design

328
00:11:49,030 --> 00:11:51,920
 

329
00:11:49,040 --> 00:11:53,540
 how do you write this as a optimization

330
00:11:51,910 --> 00:11:53,540
 

331
00:11:51,920 --> 00:11:57,650
 problem and then again blackbox

332
00:11:53,530 --> 00:11:57,650
 

333
00:11:53,540 --> 00:12:00,530
 optimization and speed-up techniques the

334
00:11:57,640 --> 00:12:00,530
 

335
00:11:57,650 --> 00:12:02,090
 first part is based on this review

336
00:12:00,520 --> 00:12:02,090
 

337
00:12:00,530 --> 00:12:06,020
 article that I wrote together with my

338
00:12:02,080 --> 00:12:06,020
 

339
00:12:02,090 --> 00:12:08,660
 brilliant PhD student Matias Feuer who

340
00:12:06,010 --> 00:12:08,660
 

341
00:12:06,020 --> 00:12:10,490
 deserves a lot of credit for this so

342
00:12:08,650 --> 00:12:10,490
 

343
00:12:08,660 --> 00:12:13,910
 let's get straight in auto ml as hyper

344
00:12:10,480 --> 00:12:13,910
 

345
00:12:10,490 --> 00:12:15,350
 permit optimization let's define the

346
00:12:13,900 --> 00:12:15,350
 

347
00:12:13,910 --> 00:12:17,900
 hyper primate optimization problem we're

348
00:12:15,340 --> 00:12:17,900
 

349
00:12:15,350 --> 00:12:19,040
 talking about first so we have some

350
00:12:17,890 --> 00:12:19,040
 

351
00:12:17,900 --> 00:12:22,460
 algorithm some machine learning

352
00:12:19,030 --> 00:12:22,460
 

353
00:12:19,040 --> 00:12:25,340
 algorithm a and it has hyper parameters

354
00:12:22,450 --> 00:12:25,340
 

355
00:12:22,460 --> 00:12:26,900
 lambda with domain capital lambda and so

356
00:12:25,330 --> 00:12:26,900
 

357
00:12:25,340 --> 00:12:29,120
 we'll use lambda throughout the tutorial

358
00:12:26,890 --> 00:12:29,120
 

359
00:12:26,900 --> 00:12:30,830
 for hyper parameters and you have a loss

360
00:12:29,110 --> 00:12:30,830
 

361
00:12:29,120 --> 00:12:32,720
 that depends on your algorithm with

362
00:12:30,820 --> 00:12:32,720
 

363
00:12:30,830 --> 00:12:34,850
 hyper parameters trained on the training

364
00:12:32,710 --> 00:12:34,850
 

365
00:12:32,720 --> 00:12:38,360
 set and validated on a validation set

366
00:12:34,840 --> 00:12:38,360
 

367
00:12:34,850 --> 00:12:41,390
 and the hpo problem is just to find the

368
00:12:38,350 --> 00:12:41,390
 

369
00:12:38,360 --> 00:12:46,220
 hyper parameter setting that minimizes

370
00:12:41,380 --> 00:12:46,220
 

371
00:12:41,390 --> 00:12:48,200
 this loss the validation loss all right

372
00:12:46,210 --> 00:12:48,200
 

373
00:12:46,220 --> 00:12:49,700
 what are these hyper parameters well

374
00:12:48,190 --> 00:12:49,700
 

375
00:12:48,200 --> 00:12:51,770
 there is continuous hyper parameters of

376
00:12:49,690 --> 00:12:51,770
 

377
00:12:49,700 --> 00:12:53,660
 course such as learning rates there is

378
00:12:51,760 --> 00:12:53,660
 

379
00:12:51,770 --> 00:12:56,000
 integer hyper parameters such as number

380
00:12:53,650 --> 00:12:56,000
 

381
00:12:53,660 --> 00:12:57,650
 of units there's also categorical hyper

382
00:12:55,990 --> 00:12:57,650
 

383
00:12:56,000 --> 00:12:59,630
 parameters that are maybe a bit more

384
00:12:57,640 --> 00:12:59,630
 

385
00:12:57,650 --> 00:13:02,000
 interesting than continuous and integer

386
00:12:59,620 --> 00:13:02,000
 

387
00:12:59,630 --> 00:13:03,740
 ones these categorical parameters are

388
00:13:01,990 --> 00:13:03,740
 

389
00:13:02,000 --> 00:13:06,170
 discrete so we can't offer any eight

390
00:13:03,730 --> 00:13:06,170
 

391
00:13:03,740 --> 00:13:08,930
 through them the finite domain and

392
00:13:06,160 --> 00:13:08,930
 

393
00:13:06,170 --> 00:13:10,490
 unordered so some examples here might be

394
00:13:08,920 --> 00:13:10,490
 

395
00:13:08,930 --> 00:13:11,389
 well you might just choose between

396
00:13:10,480 --> 00:13:11,389
 

397
00:13:10,490 --> 00:13:13,220
 different

398
00:13:11,379 --> 00:13:13,220
 

399
00:13:11,389 --> 00:13:14,839
 algorithms machine learning algorithms

400
00:13:13,210 --> 00:13:14,839
 

401
00:13:13,220 --> 00:13:17,209
 support vector machines or random

402
00:13:14,829 --> 00:13:17,209
 

403
00:13:14,839 --> 00:13:18,799
 forests on your networks or you might

404
00:13:17,199 --> 00:13:18,799
 

405
00:13:17,209 --> 00:13:21,019
 choose activation functions and your

406
00:13:18,789 --> 00:13:21,019
 

407
00:13:18,799 --> 00:13:23,959
 neural net this is a rail you leaky rail

408
00:13:21,009 --> 00:13:23,959
 

409
00:13:21,019 --> 00:13:27,649
 you sort NHS or so and you might also

410
00:13:23,949 --> 00:13:27,649
 

411
00:13:23,959 --> 00:13:29,389
 choose operators that operate on your

412
00:13:27,639 --> 00:13:29,389
 

413
00:13:27,649 --> 00:13:32,329
 latent feature representations such as

414
00:13:29,379 --> 00:13:32,329
 

415
00:13:29,389 --> 00:13:36,139
 convolutions several convolutions max

416
00:13:32,319 --> 00:13:36,139
 

417
00:13:32,329 --> 00:13:37,879
 pooling etc etc and these categorical

418
00:13:36,129 --> 00:13:37,879
 

419
00:13:36,139 --> 00:13:39,619
 parameters will come in very handy later

420
00:13:37,869 --> 00:13:39,619
 

421
00:13:37,879 --> 00:13:40,850
 on when we do neural architecture search

422
00:13:39,609 --> 00:13:40,850
 

423
00:13:39,619 --> 00:13:42,889
 and want to write neural architecture

424
00:13:40,840 --> 00:13:42,889
 

425
00:13:40,850 --> 00:13:46,850
 search as I have a primate optimization

426
00:13:42,879 --> 00:13:46,850
 

427
00:13:42,889 --> 00:13:50,299
 problem with a fairly generic version of

428
00:13:46,840 --> 00:13:50,299
 

429
00:13:46,850 --> 00:13:51,619
 what constitutes a hyper parameter in a

430
00:13:50,289 --> 00:13:51,619
 

431
00:13:50,299 --> 00:13:56,569
 special case of course of categorical

432
00:13:51,609 --> 00:13:56,569
 

433
00:13:51,619 --> 00:13:58,850
 list of binaries another type of hyper

434
00:13:56,559 --> 00:13:58,850
 

435
00:13:56,569 --> 00:14:00,709
 parameter is a conditional hyper

436
00:13:58,840 --> 00:14:00,709
 

437
00:13:58,850 --> 00:14:04,429
 parameter and so conditional hyper

438
00:14:00,699 --> 00:14:04,429
 

439
00:14:00,709 --> 00:14:06,739
 parameter is be our only active if some

440
00:14:04,419 --> 00:14:06,739
 

441
00:14:04,429 --> 00:14:09,470
 other hyper parameters a are set in a

442
00:14:06,729 --> 00:14:09,470
 

443
00:14:06,739 --> 00:14:11,420
 certain way and these conditional hyper

444
00:14:09,460 --> 00:14:11,420
 

445
00:14:09,470 --> 00:14:13,639
 parameters allow us to write pretty

446
00:14:11,410 --> 00:14:13,639
 

447
00:14:11,420 --> 00:14:15,860
 generic search spaces as a hyper

448
00:14:13,629 --> 00:14:15,860
 

449
00:14:13,639 --> 00:14:18,230
 chromate optimization problem so one

450
00:14:15,850 --> 00:14:18,230
 

451
00:14:15,860 --> 00:14:21,199
 example is Adams second momentum

452
00:14:18,220 --> 00:14:21,199
 

453
00:14:18,230 --> 00:14:23,360
 parameter if you don't use atom then

454
00:14:21,189 --> 00:14:23,360
 

455
00:14:21,199 --> 00:14:24,829
 this hyper parameter is not going to be

456
00:14:23,350 --> 00:14:24,829
 

457
00:14:23,360 --> 00:14:27,290
 active it's not even going to be

458
00:14:24,819 --> 00:14:27,290
 

459
00:14:24,829 --> 00:14:29,449
 inspected and is provably unimportant if

460
00:14:27,280 --> 00:14:29,449
 

461
00:14:27,290 --> 00:14:32,470
 you don't pick atom then you don't need

462
00:14:29,439 --> 00:14:32,470
 

463
00:14:29,449 --> 00:14:35,660
 to pick the second momentum parameter

464
00:14:32,460 --> 00:14:35,660
 

465
00:14:32,470 --> 00:14:37,699
 likewise if you have a parameter that's

466
00:14:35,650 --> 00:14:37,699
 

467
00:14:35,660 --> 00:14:40,339
 a convolution or kernel size of a

468
00:14:37,689 --> 00:14:40,339
 

469
00:14:37,699 --> 00:14:42,679
 certain layer if the type of that layer

470
00:14:40,329 --> 00:14:42,679
 

471
00:14:40,339 --> 00:14:45,040
 is not convolutional then this hyper

472
00:14:42,669 --> 00:14:45,040
 

473
00:14:42,679 --> 00:14:47,749
 parameter is not going to be active and

474
00:14:45,030 --> 00:14:47,749
 

475
00:14:45,040 --> 00:14:50,209
 as a final example well if you have a

476
00:14:47,739 --> 00:14:50,209
 

477
00:14:47,749 --> 00:14:51,949
 support vector machines kernel you only

478
00:14:50,199 --> 00:14:51,949
 

479
00:14:50,209 --> 00:14:53,569
 need to choose that if you actually use

480
00:14:51,939 --> 00:14:53,569
 

481
00:14:51,949 --> 00:14:56,619
 a support vector machine if you use a

482
00:14:53,559 --> 00:14:56,619
 

483
00:14:53,569 --> 00:14:58,819
 random forest instead then provably that

484
00:14:56,609 --> 00:14:58,819
 

485
00:14:56,619 --> 00:15:01,790
 SVM's kernel parameter is not going to

486
00:14:58,809 --> 00:15:01,790
 

487
00:14:58,819 --> 00:15:05,360
 be inspected so where's that notion we

488
00:15:01,780 --> 00:15:05,360
 

489
00:15:01,790 --> 00:15:07,790
 can actually write Auto ml as a hyper

490
00:15:05,350 --> 00:15:07,790
 

491
00:15:05,360 --> 00:15:08,179
 parameter optimization problem as

492
00:15:07,780 --> 00:15:08,179
 

493
00:15:07,790 --> 00:15:10,160
 follows

494
00:15:08,169 --> 00:15:10,160
 

495
00:15:08,179 --> 00:15:11,389
 so there's this combined algorithm

496
00:15:10,150 --> 00:15:11,389
 

497
00:15:10,160 --> 00:15:13,040
 selection and hyper parameter

498
00:15:11,379 --> 00:15:13,040
 

499
00:15:11,389 --> 00:15:17,329
 optimization problem where you have

500
00:15:13,030 --> 00:15:17,329
 

501
00:15:13,040 --> 00:15:20,410
 several algorithms a 1 to a n and each

502
00:15:17,319 --> 00:15:20,410
 

503
00:15:17,329 --> 00:15:23,179
 of these have hyper parameter spaces and

504
00:15:20,400 --> 00:15:23,179
 

505
00:15:20,410 --> 00:15:25,130
 you can have a loss function and now the

506
00:15:23,169 --> 00:15:25,130
 

507
00:15:23,179 --> 00:15:27,830
 problem is to find the

508
00:15:25,120 --> 00:15:27,830
 

509
00:15:25,130 --> 00:15:30,920
 nation of the best algorithm and it's

510
00:15:27,820 --> 00:15:30,920
 

511
00:15:27,830 --> 00:15:34,970
 hyper parameters to jointly minimizes

512
00:15:30,910 --> 00:15:34,970
 

513
00:15:30,920 --> 00:15:37,160
 loss and with that you can actually

514
00:15:34,960 --> 00:15:37,160
 

515
00:15:34,970 --> 00:15:38,630
 write this combined algorithm selection

516
00:15:37,150 --> 00:15:38,630
 

517
00:15:37,160 --> 00:15:41,420
 hyper primate optimization problem as

518
00:15:38,620 --> 00:15:41,420
 

519
00:15:38,630 --> 00:15:43,460
 just a hyper primate optimization

520
00:15:41,410 --> 00:15:43,460
 

521
00:15:41,420 --> 00:15:45,710
 problem where you have one top-level

522
00:15:43,450 --> 00:15:45,710
 

523
00:15:43,460 --> 00:15:47,720
 hyper parameter that's a choice of

524
00:15:45,700 --> 00:15:47,720
 

525
00:15:45,710 --> 00:15:49,370
 machine learning algorithm and all the

526
00:15:47,710 --> 00:15:49,370
 

527
00:15:47,720 --> 00:15:53,060
 other hyper parameters are conditional

528
00:15:49,360 --> 00:15:53,060
 

529
00:15:49,370 --> 00:15:55,310
 on that choice you don't have to stop at

530
00:15:53,050 --> 00:15:55,310
 

531
00:15:53,060 --> 00:15:58,400
 one level of conditionality actually

532
00:15:55,300 --> 00:15:58,400
 

533
00:15:55,310 --> 00:16:01,280
 kind of the first bottom ml system that

534
00:15:58,390 --> 00:16:01,280
 

535
00:15:58,400 --> 00:16:03,740
 we worked on Auto Rekha had four levels

536
00:16:01,270 --> 00:16:03,740
 

537
00:16:01,280 --> 00:16:06,230
 of different of conditionality and the

538
00:16:03,730 --> 00:16:06,230
 

539
00:16:03,740 --> 00:16:08,900
 total of 768 hyper parameters so pretty

540
00:16:06,220 --> 00:16:08,900
 

541
00:16:06,230 --> 00:16:10,450
 pretty big space that wouldn't typically

542
00:16:08,890 --> 00:16:10,450
 

543
00:16:08,900 --> 00:16:13,730
 be called hyper parameter optimization

544
00:16:10,440 --> 00:16:13,730
 

545
00:16:10,450 --> 00:16:17,720
 but the same type of algorithms actually

546
00:16:13,720 --> 00:16:17,720
 

547
00:16:13,730 --> 00:16:20,480
 do apply it to that as applied to a low

548
00:16:17,710 --> 00:16:20,480
 

549
00:16:17,720 --> 00:16:22,100
 dimensional continuous space so I am

550
00:16:20,470 --> 00:16:22,100
 

551
00:16:20,480 --> 00:16:24,470
 referring to all of this as hyper

552
00:16:22,090 --> 00:16:24,470
 

553
00:16:22,100 --> 00:16:28,760
 prominent optimization just for

554
00:16:24,460 --> 00:16:28,760
 

555
00:16:24,470 --> 00:16:30,590
 simplicity all right so I hope to have

556
00:16:28,750 --> 00:16:30,590
 

557
00:16:28,760 --> 00:16:32,270
 convinced you that hyper parameter

558
00:16:30,580 --> 00:16:32,270
 

559
00:16:30,590 --> 00:16:34,340
 optimization where's the general notion

560
00:16:32,260 --> 00:16:34,340
 

561
00:16:32,270 --> 00:16:37,220
 of what constitutes a hyper parameter is

562
00:16:34,330 --> 00:16:37,220
 

563
00:16:34,340 --> 00:16:39,500
 a pretty powerful beast so let's talk

564
00:16:37,210 --> 00:16:39,500
 

565
00:16:37,220 --> 00:16:44,210
 about some blackbox optimization methods

566
00:16:39,490 --> 00:16:44,210
 

567
00:16:39,500 --> 00:16:45,710
 for doing have a permit optimization so

568
00:16:44,200 --> 00:16:45,710
 

569
00:16:44,210 --> 00:16:47,630
 if you have for example a deep neural

570
00:16:45,700 --> 00:16:47,630
 

571
00:16:45,710 --> 00:16:50,660
 network and you have different hyper

572
00:16:47,620 --> 00:16:50,660
 

573
00:16:47,630 --> 00:16:52,220
 parameter settings here you have a black

574
00:16:50,650 --> 00:16:52,220
 

575
00:16:50,660 --> 00:16:54,320
 box that trains the steep neural

576
00:16:52,210 --> 00:16:54,320
 

577
00:16:52,220 --> 00:16:56,360
 networks and validates it you get our

578
00:16:54,310 --> 00:16:56,360
 

579
00:16:54,320 --> 00:16:58,670
 validation performance well what black

580
00:16:56,350 --> 00:16:58,670
 

581
00:16:56,360 --> 00:17:03,920
 box optimization does is treat this as a

582
00:16:58,660 --> 00:17:03,920
 

583
00:16:58,670 --> 00:17:07,900
 black box and seek for the input lambda

584
00:17:03,910 --> 00:17:07,900
 

585
00:17:03,920 --> 00:17:10,850
 that maximizes the validation accuracy

586
00:17:07,890 --> 00:17:10,850
 

587
00:17:07,900 --> 00:17:13,640
 so you just search for the lambda that

588
00:17:10,840 --> 00:17:13,640
 

589
00:17:10,850 --> 00:17:15,319
 maximizes f of lambda and with that you

590
00:17:13,630 --> 00:17:15,319
 

591
00:17:13,640 --> 00:17:17,079
 throw away all information about what

592
00:17:15,309 --> 00:17:17,079
 

593
00:17:15,319 --> 00:17:19,670
 happens inside of that black box and

594
00:17:17,069 --> 00:17:19,670
 

595
00:17:17,079 --> 00:17:22,579
 while you give up a lot by doing that

596
00:17:19,660 --> 00:17:22,579
 

597
00:17:19,670 --> 00:17:24,380
 but it gives you a generic interface for

598
00:17:22,569 --> 00:17:24,380
 

599
00:17:22,579 --> 00:17:27,709
 applying this to all kinds of different

600
00:17:24,370 --> 00:17:27,709
 

601
00:17:24,380 --> 00:17:29,900
 problems this black box function is of

602
00:17:27,699 --> 00:17:29,900
 

603
00:17:27,709 --> 00:17:31,430
 course expensive to evaluate so a sample

604
00:17:29,890 --> 00:17:31,430
 

605
00:17:29,900 --> 00:17:34,360
 efficiency is important we can't

606
00:17:31,420 --> 00:17:34,360
 

607
00:17:31,430 --> 00:17:34,360
 evaluate this too often

608
00:17:35,140 --> 00:17:35,140
 

609
00:17:35,150 --> 00:17:39,860
 the arguably simplest approach for doing

610
00:17:37,990 --> 00:17:39,860
 

611
00:17:38,000 --> 00:17:42,800
 hyper parameter optimization is grid

612
00:17:39,850 --> 00:17:42,800
 

613
00:17:39,860 --> 00:17:45,590
 search and this is of course exponential

614
00:17:42,790 --> 00:17:45,590
 

615
00:17:42,800 --> 00:17:47,990
 in the number of hyper parameters it's

616
00:17:45,580 --> 00:17:47,990
 

617
00:17:45,590 --> 00:17:49,370
 useful for if you have one or two hyper

618
00:17:47,980 --> 00:17:49,370
 

619
00:17:47,990 --> 00:17:51,050
 parameters but if you have a lot of

620
00:17:49,360 --> 00:17:51,050
 

621
00:17:49,370 --> 00:17:54,530
 hyper parameters it's probably not the

622
00:17:51,040 --> 00:17:54,530
 

623
00:17:51,050 --> 00:17:57,040
 best method random search in contrast is

624
00:17:54,520 --> 00:17:57,040
 

625
00:17:54,530 --> 00:17:59,600
 is only exponential in the number of

626
00:17:57,030 --> 00:17:59,600
 

627
00:17:57,040 --> 00:18:01,490
 important type of parameters so if you

628
00:17:59,590 --> 00:18:01,490
 

629
00:17:59,600 --> 00:18:07,670
 have for example one unimportant hyper

630
00:18:01,480 --> 00:18:07,670
 

631
00:18:01,490 --> 00:18:10,640
 parameter here and and you have one

632
00:18:07,660 --> 00:18:10,640
 

633
00:18:07,670 --> 00:18:13,190
 important parameter then what random

634
00:18:10,630 --> 00:18:13,190
 

635
00:18:10,640 --> 00:18:15,410
 search does with this budget of nine

636
00:18:13,180 --> 00:18:15,410
 

637
00:18:13,190 --> 00:18:17,540
 function evaluations here you get nine

638
00:18:15,400 --> 00:18:17,540
 

639
00:18:15,410 --> 00:18:19,670
 different values for this important

640
00:18:17,530 --> 00:18:19,670
 

641
00:18:17,540 --> 00:18:21,920
 parameter and therefore it can use your

642
00:18:19,660 --> 00:18:21,920
 

643
00:18:19,670 --> 00:18:24,050
 budget much more effectively than grid

644
00:18:21,910 --> 00:18:24,050
 

645
00:18:21,920 --> 00:18:26,210
 search which would only get three values

646
00:18:24,040 --> 00:18:26,210
 

647
00:18:24,050 --> 00:18:27,980
 for this important parameter and get

648
00:18:26,200 --> 00:18:27,980
 

649
00:18:26,210 --> 00:18:29,570
 each of these evaluations three times

650
00:18:27,970 --> 00:18:29,570
 

651
00:18:27,980 --> 00:18:32,210
 because the unimportant part method

652
00:18:29,560 --> 00:18:32,210
 

653
00:18:29,570 --> 00:18:34,250
 doesn't matter so random search

654
00:18:32,200 --> 00:18:34,250
 

655
00:18:32,210 --> 00:18:36,020
 definitely does perform better than grid

656
00:18:34,240 --> 00:18:36,020
 

657
00:18:34,250 --> 00:18:39,380
 search if you have some unimportant

658
00:18:36,010 --> 00:18:39,380
 

659
00:18:36,020 --> 00:18:41,750
 parameters and often often times you

660
00:18:39,370 --> 00:18:41,750
 

661
00:18:39,380 --> 00:18:43,610
 have parameters that are not completely

662
00:18:41,740 --> 00:18:43,610
 

663
00:18:41,750 --> 00:18:44,870
 unimportant but you definitely have some

664
00:18:43,600 --> 00:18:44,870
 

665
00:18:43,610 --> 00:18:46,790
 hyper parameters that are far more

666
00:18:44,860 --> 00:18:46,790
 

667
00:18:44,870 --> 00:18:50,750
 important than others such as learning

668
00:18:46,780 --> 00:18:50,750
 

669
00:18:46,790 --> 00:18:53,120
 rates and so random search is a useful

670
00:18:50,740 --> 00:18:53,120
 

671
00:18:50,750 --> 00:18:56,150
 baseline because of that but it does

672
00:18:53,110 --> 00:18:56,150
 

673
00:18:53,120 --> 00:18:57,320
 have the problem that it doesn't follow

674
00:18:56,140 --> 00:18:57,320
 

675
00:18:56,150 --> 00:19:00,620
 the information it doesn't actually

676
00:18:57,310 --> 00:19:00,620
 

677
00:18:57,320 --> 00:19:02,120
 learn anything about the space if in one

678
00:19:00,610 --> 00:19:02,120
 

679
00:19:00,620 --> 00:19:03,560
 part of the space performance is

680
00:19:02,110 --> 00:19:03,560
 

681
00:19:02,120 --> 00:19:05,390
 dramatically better than in other parts

682
00:19:03,550 --> 00:19:05,390
 

683
00:19:03,560 --> 00:19:08,750
 of the space random search will just not

684
00:19:05,380 --> 00:19:08,750
 

685
00:19:05,390 --> 00:19:10,790
 care about that at all and that is a pop

686
00:19:08,740 --> 00:19:10,790
 

687
00:19:08,750 --> 00:19:12,920
 tomoow and one approach that does care

688
00:19:10,780 --> 00:19:12,920
 

689
00:19:10,790 --> 00:19:15,170
 about this is based on optimization

690
00:19:12,910 --> 00:19:15,170
 

691
00:19:12,920 --> 00:19:17,780
 which is a very popular approach for

692
00:19:15,160 --> 00:19:17,780
 

693
00:19:15,170 --> 00:19:18,860
 black box function optimization made

694
00:19:17,770 --> 00:19:18,860
 

695
00:19:17,780 --> 00:19:21,380
 works as follows so you fit a

696
00:19:18,850 --> 00:19:21,380
 

697
00:19:18,860 --> 00:19:23,750
 probabilistic model to the function

698
00:19:21,370 --> 00:19:23,750
 

699
00:19:21,380 --> 00:19:25,940
 evaluations that you have made so let's

700
00:19:23,740 --> 00:19:25,940
 

701
00:19:23,750 --> 00:19:28,490
 say we have evaluated the function here

702
00:19:25,930 --> 00:19:28,490
 

703
00:19:25,940 --> 00:19:30,860
 and here so so we have one hyper

704
00:19:28,480 --> 00:19:30,860
 

705
00:19:28,490 --> 00:19:33,770
 parameter here it's just a single hyper

706
00:19:30,850 --> 00:19:33,770
 

707
00:19:30,860 --> 00:19:36,800
 parameter because I can't make a nice

708
00:19:33,760 --> 00:19:36,800
 

709
00:19:33,770 --> 00:19:38,960
 nice figure in n dimensions but it

710
00:19:36,790 --> 00:19:38,960
 

711
00:19:36,800 --> 00:19:39,740
 directly translates to that case and

712
00:19:38,950 --> 00:19:39,740
 

713
00:19:38,960 --> 00:19:43,010
 here

714
00:19:39,730 --> 00:19:43,010
 

715
00:19:39,740 --> 00:19:43,840
 you have the true function here in

716
00:19:43,000 --> 00:19:43,840
 

717
00:19:43,010 --> 00:19:47,740
 dashed

718
00:19:43,830 --> 00:19:47,740
 

719
00:19:43,840 --> 00:19:51,190
 and we have evaluated it here and here

720
00:19:47,730 --> 00:19:51,190
 

721
00:19:47,740 --> 00:19:53,620
 and what Basin optimization does is to

722
00:19:51,180 --> 00:19:53,620
 

723
00:19:51,190 --> 00:19:56,200
 fit a probabilistic model to these

724
00:19:53,610 --> 00:19:56,200
 

725
00:19:53,620 --> 00:19:58,510
 function evaluations in order to predict

726
00:19:56,190 --> 00:19:58,510
 

727
00:19:56,200 --> 00:20:00,550
 for unseen hyper parameter settings what

728
00:19:58,500 --> 00:20:00,550
 

729
00:19:58,510 --> 00:20:02,770
 the performance would be like typically

730
00:20:00,540 --> 00:20:02,770
 

731
00:20:00,550 --> 00:20:06,090
 you use Gaussian processes for this

732
00:20:02,760 --> 00:20:06,090
 

733
00:20:02,770 --> 00:20:09,420
 which give you a mean function here in a

734
00:20:06,080 --> 00:20:09,420
 

735
00:20:06,090 --> 00:20:13,000
 solid line and an uncertainty estimate

736
00:20:09,410 --> 00:20:13,000
 

737
00:20:09,420 --> 00:20:15,130
 here in this blue funnel and you use

738
00:20:12,990 --> 00:20:15,130
 

739
00:20:13,000 --> 00:20:16,810
 this uncertainty estimate in order to

740
00:20:15,120 --> 00:20:16,810
 

741
00:20:15,130 --> 00:20:19,180
 trade off exploration versus

742
00:20:16,800 --> 00:20:19,180
 

743
00:20:16,810 --> 00:20:21,700
 exploitation so exploitation in areas

744
00:20:19,170 --> 00:20:21,700
 

745
00:20:19,180 --> 00:20:23,580
 where the function is predicted to be

746
00:20:21,690 --> 00:20:23,580
 

747
00:20:21,700 --> 00:20:25,900
 high in this case we're maximizing and

748
00:20:23,570 --> 00:20:25,900
 

749
00:20:23,580 --> 00:20:28,810
 exploration in parts of the space where

750
00:20:25,890 --> 00:20:28,810
 

751
00:20:25,900 --> 00:20:30,880
 you haven't evaluated yet the straight

752
00:20:28,800 --> 00:20:30,880
 

753
00:20:28,810 --> 00:20:33,220
 of is formalized by means of a so-called

754
00:20:30,870 --> 00:20:33,220
 

755
00:20:30,880 --> 00:20:34,630
 acquisition function and there's various

756
00:20:33,210 --> 00:20:34,630
 

757
00:20:33,220 --> 00:20:37,810
 different acquisition functions one

758
00:20:34,620 --> 00:20:37,810
 

759
00:20:34,630 --> 00:20:40,720
 popular one is the expected improvement

760
00:20:37,800 --> 00:20:40,720
 

761
00:20:37,810 --> 00:20:43,060
 over the best points in so far and this

762
00:20:40,710 --> 00:20:43,060
 

763
00:20:40,720 --> 00:20:45,010
 is plotted here what you then do is to

764
00:20:43,050 --> 00:20:45,010
 

765
00:20:43,060 --> 00:20:47,530
 optimize this acquisition function over

766
00:20:45,000 --> 00:20:47,530
 

767
00:20:45,010 --> 00:20:50,320
 your hyper parameter space to find the

768
00:20:47,520 --> 00:20:50,320
 

769
00:20:47,530 --> 00:20:51,580
 Maximizer then you evaluate your

770
00:20:50,310 --> 00:20:51,580
 

771
00:20:50,320 --> 00:20:55,630
 function at that point

772
00:20:51,570 --> 00:20:55,630
 

773
00:20:51,580 --> 00:20:57,340
 refit your model to in particular get

774
00:20:55,620 --> 00:20:57,340
 

775
00:20:55,630 --> 00:20:59,740
 low uncertainty estimates around here

776
00:20:57,330 --> 00:20:59,740
 

777
00:20:57,340 --> 00:21:02,230
 and also a somewhat different fit

778
00:20:59,730 --> 00:21:02,230
 

779
00:20:59,740 --> 00:21:03,940
 globally then you recompute the

780
00:21:02,220 --> 00:21:03,940
 

781
00:21:02,230 --> 00:21:07,150
 acquisition function optimize your

782
00:21:03,930 --> 00:21:07,150
 

783
00:21:03,940 --> 00:21:09,190
 acquisition function again and evaluate

784
00:21:07,140 --> 00:21:09,190
 

785
00:21:07,150 --> 00:21:12,210
 the next point and you iterate until

786
00:21:09,180 --> 00:21:12,210
 

787
00:21:09,190 --> 00:21:14,740
 you're out of time now

788
00:21:12,200 --> 00:21:14,740
 

789
00:21:12,210 --> 00:21:17,410
 this is a very popular approach it's

790
00:21:14,730 --> 00:21:17,410
 

791
00:21:14,740 --> 00:21:20,230
 been around for over 40 years and it's

792
00:21:17,400 --> 00:21:20,230
 

793
00:21:17,410 --> 00:21:23,050
 very sample efficient it also works when

794
00:21:20,220 --> 00:21:23,050
 

795
00:21:20,230 --> 00:21:26,320
 the objective is non convex noisy as

796
00:21:23,040 --> 00:21:26,320
 

797
00:21:23,050 --> 00:21:27,940
 unknown derivatives so all the the types

798
00:21:26,310 --> 00:21:27,940
 

799
00:21:26,320 --> 00:21:30,340
 of characteristics that we do actually

800
00:21:27,930 --> 00:21:30,340
 

801
00:21:27,940 --> 00:21:32,320
 face in hyper parameter optimization and

802
00:21:30,330 --> 00:21:32,320
 

803
00:21:30,340 --> 00:21:34,650
 there's also recent convergence results

804
00:21:32,310 --> 00:21:34,650
 

805
00:21:32,320 --> 00:21:39,060
 depending on some assumptions on

806
00:21:34,640 --> 00:21:39,060
 

807
00:21:34,650 --> 00:21:41,890
 smoothness of your response surface etc

808
00:21:39,050 --> 00:21:41,890
 

809
00:21:39,060 --> 00:21:44,200
 one example for Bazin optimization I got

810
00:21:41,880 --> 00:21:44,200
 

811
00:21:41,890 --> 00:21:46,140
 an email today from Mendota fighters who

812
00:21:44,190 --> 00:21:46,140
 

813
00:21:44,200 --> 00:21:49,270
 knew that I'm giving this tutorial and

814
00:21:46,130 --> 00:21:49,270
 

815
00:21:46,140 --> 00:21:51,340
 he told me finally he can tell the world

816
00:21:49,260 --> 00:21:51,340
 

817
00:21:49,270 --> 00:21:54,640
 that based optimization was actually

818
00:21:51,330 --> 00:21:54,640
 

819
00:21:51,340 --> 00:21:56,029
 very crucial inside of alphago so here's

820
00:21:54,630 --> 00:21:56,029
 

821
00:21:54,640 --> 00:21:58,789
 some quotes of his

822
00:21:56,019 --> 00:21:58,789
 

823
00:21:56,029 --> 00:22:03,409
 which are quotes from a forthcoming

824
00:21:58,779 --> 00:22:03,409
 

825
00:21:58,789 --> 00:22:05,090
 paper of his with you Chan Chan so I'm

826
00:22:03,399 --> 00:22:05,090
 

827
00:22:03,409 --> 00:22:06,919
 just going to read this to you so during

828
00:22:05,080 --> 00:22:06,919
 

829
00:22:05,090 --> 00:22:08,749
 the development of alphago its many

830
00:22:06,909 --> 00:22:08,749
 

831
00:22:06,919 --> 00:22:11,029
 hyper parameters were tuned with phasing

832
00:22:08,739 --> 00:22:11,029
 

833
00:22:08,749 --> 00:22:13,340
 optimization multiple times this

834
00:22:11,019 --> 00:22:13,340
 

835
00:22:11,029 --> 00:22:14,929
 automatic tuning process resulted in

836
00:22:13,330 --> 00:22:14,929
 

837
00:22:13,340 --> 00:22:17,179
 substantial improvements in playing

838
00:22:14,919 --> 00:22:17,179
 

839
00:22:14,929 --> 00:22:19,190
 strings for example prior to the match

840
00:22:17,169 --> 00:22:19,190
 

841
00:22:17,179 --> 00:22:21,259
 release at all we chew on the latest

842
00:22:19,180 --> 00:22:21,259
 

843
00:22:19,190 --> 00:22:25,159
 alphago agent and thus improved its win

844
00:22:21,249 --> 00:22:25,159
 

845
00:22:21,259 --> 00:22:27,799
 rate from 50% to 66.5% in self play

846
00:22:25,149 --> 00:22:27,799
 

847
00:22:25,159 --> 00:22:30,559
 games this tuned version was deployed in

848
00:22:27,789 --> 00:22:30,559
 

849
00:22:27,799 --> 00:22:31,789
 the final match of course since we - and

850
00:22:30,549 --> 00:22:31,789
 

851
00:22:30,559 --> 00:22:33,559
 alphago many times during its

852
00:22:31,779 --> 00:22:33,559
 

853
00:22:31,789 --> 00:22:34,999
 development cycle the compounded

854
00:22:33,549 --> 00:22:34,999
 

855
00:22:33,559 --> 00:22:37,159
 contribution was even higher than this

856
00:22:34,989 --> 00:22:37,159
 

857
00:22:34,999 --> 00:22:39,679
 percentage so I think this goes to show

858
00:22:37,149 --> 00:22:39,679
 

859
00:22:37,159 --> 00:22:42,200
 nicely that in practice based

860
00:22:39,669 --> 00:22:42,200
 

861
00:22:39,679 --> 00:22:45,409
 optimization definitely can have impact

862
00:22:42,190 --> 00:22:45,409
 

863
00:22:42,200 --> 00:22:47,029
 and I would have been very surprised if

864
00:22:45,399 --> 00:22:47,029
 

865
00:22:45,409 --> 00:22:48,529
 Bayesian optimization was not used in

866
00:22:47,019 --> 00:22:48,529
 

867
00:22:47,029 --> 00:22:51,019
 alphago knowing that nando is at

868
00:22:48,519 --> 00:22:51,019
 

869
00:22:48,529 --> 00:22:54,979
 deep-lined and was involved in the

870
00:22:51,009 --> 00:22:54,979
 

871
00:22:51,019 --> 00:22:57,080
 project good

872
00:22:54,969 --> 00:22:57,080
 

873
00:22:54,979 --> 00:23:00,200
 so based optimization is a very powerful

874
00:22:57,070 --> 00:23:00,200
 

875
00:22:57,080 --> 00:23:01,700
 method however there is some problems

876
00:23:00,190 --> 00:23:01,700
 

877
00:23:00,200 --> 00:23:04,729
 for the standard Gaussian process

878
00:23:01,690 --> 00:23:04,729
 

879
00:23:01,700 --> 00:23:07,519
 approach for this general and general

880
00:23:04,719 --> 00:23:07,519
 

881
00:23:04,729 --> 00:23:09,349
 auto ml framework where we have a lot of

882
00:23:07,509 --> 00:23:09,349
 

883
00:23:07,519 --> 00:23:13,219
 hyper parameters and highly conditional

884
00:23:09,339 --> 00:23:13,219
 

885
00:23:09,349 --> 00:23:15,859
 space this mixed discrete continuous

886
00:23:13,209 --> 00:23:15,859
 

887
00:23:13,219 --> 00:23:17,599
 hyper parameters you have high

888
00:23:15,849 --> 00:23:17,599
 

889
00:23:15,859 --> 00:23:19,960
 dimensionality with low effective

890
00:23:17,589 --> 00:23:19,960
 

891
00:23:17,599 --> 00:23:22,909
 dimensionality and for each of these

892
00:23:19,950 --> 00:23:22,909
 

893
00:23:19,960 --> 00:23:25,219
 problems there are fixes but they're

894
00:23:22,899 --> 00:23:25,219
 

895
00:23:22,909 --> 00:23:28,940
 kind of just non-standard fixes the

896
00:23:25,209 --> 00:23:28,940
 

897
00:23:25,219 --> 00:23:30,830
 noise is also sometimes heteroskedastic

898
00:23:28,930 --> 00:23:30,830
 

899
00:23:28,940 --> 00:23:34,369
 so different hyper parameter settings

900
00:23:30,820 --> 00:23:34,369
 

901
00:23:30,830 --> 00:23:37,729
 yield different noise for example it

902
00:23:34,359 --> 00:23:37,729
 

903
00:23:34,369 --> 00:23:39,889
 might also not be Gaussian it might be

904
00:23:37,719 --> 00:23:39,889
 

905
00:23:37,729 --> 00:23:41,779
 multimodal for example if some runs

906
00:23:39,879 --> 00:23:41,779
 

907
00:23:39,889 --> 00:23:45,460
 diverge and some runs converge with

908
00:23:41,769 --> 00:23:45,460
 

909
00:23:41,779 --> 00:23:48,830
 different learning rate settings and

910
00:23:45,450 --> 00:23:48,830
 

911
00:23:45,460 --> 00:23:51,200
 Gaussian processes are not necessarily

912
00:23:48,820 --> 00:23:51,200
 

913
00:23:48,830 --> 00:23:53,719
 the most robust method in terms of their

914
00:23:51,190 --> 00:23:53,719
 

915
00:23:51,200 --> 00:23:56,059
 internal header area you need to set

916
00:23:53,709 --> 00:23:56,059
 

917
00:23:53,719 --> 00:23:57,739
 high per priors in order to get the best

918
00:23:56,049 --> 00:23:57,739
 

919
00:23:56,059 --> 00:23:59,779
 performance and you need to pick the

920
00:23:57,729 --> 00:23:59,779
 

921
00:23:57,739 --> 00:24:02,089
 right kernel for your application that

922
00:23:59,769 --> 00:24:02,089
 

923
00:23:59,779 --> 00:24:04,070
 had their there is actually some work on

924
00:24:02,079 --> 00:24:04,070
 

925
00:24:02,089 --> 00:24:06,589
 picking the right kernel automatically

926
00:24:04,060 --> 00:24:06,589
 

927
00:24:04,070 --> 00:24:08,509
 and this this goes towards pushing

928
00:24:06,579 --> 00:24:08,509
 

929
00:24:06,589 --> 00:24:09,700
 Gaussian processes towards more

930
00:24:08,499 --> 00:24:09,700
 

931
00:24:08,509 --> 00:24:12,010
 robustness

932
00:24:09,690 --> 00:24:12,010
 

933
00:24:09,700 --> 00:24:14,050
 so I'm looking forward to that being

934
00:24:12,000 --> 00:24:14,050
 

935
00:24:12,010 --> 00:24:17,050
 applied in in a lot of different

936
00:24:14,040 --> 00:24:17,050
 

937
00:24:14,050 --> 00:24:22,320
 packages also the model overhead is not

938
00:24:17,040 --> 00:24:22,320
 

939
00:24:17,050 --> 00:24:24,880
 necessarily it can be a problem if

940
00:24:22,310 --> 00:24:24,880
 

941
00:24:22,320 --> 00:24:26,620
 because Gaussian processes scale cubic

942
00:24:24,870 --> 00:24:26,620
 

943
00:24:24,880 --> 00:24:27,970
 Li in the number of data points so if

944
00:24:26,610 --> 00:24:27,970
 

945
00:24:26,620 --> 00:24:29,680
 the black box function is truly

946
00:24:27,960 --> 00:24:29,680
 

947
00:24:27,970 --> 00:24:31,990
 expensive then typically this doesn't

948
00:24:29,670 --> 00:24:31,990
 

949
00:24:29,680 --> 00:24:35,050
 matter too much but in multi fidelity

950
00:24:31,980 --> 00:24:35,050
 

951
00:24:31,990 --> 00:24:37,090
 approaches then you can actually get

952
00:24:35,040 --> 00:24:37,090
 

953
00:24:35,050 --> 00:24:38,920
 quite a lot of function evaluations and

954
00:24:37,080 --> 00:24:38,920
 

955
00:24:37,090 --> 00:24:41,260
 then this model overhead can actually

956
00:24:38,910 --> 00:24:41,260
 

957
00:24:38,920 --> 00:24:43,800
 matter a lot and you need to resort to

958
00:24:41,250 --> 00:24:43,800
 

959
00:24:41,260 --> 00:24:46,050
 approximate Gaussian processes a

960
00:24:43,790 --> 00:24:46,050
 

961
00:24:43,800 --> 00:24:48,700
 different model that has also been used

962
00:24:46,040 --> 00:24:48,700
 

963
00:24:46,050 --> 00:24:53,080
 sometimes in particular in my own work

964
00:24:48,690 --> 00:24:53,080
 

965
00:24:48,700 --> 00:24:55,420
 is a random forest model which is kind

966
00:24:53,070 --> 00:24:55,420
 

967
00:24:53,080 --> 00:24:57,610
 of a simple approach that works out of

968
00:24:55,410 --> 00:24:57,610
 

969
00:24:55,420 --> 00:25:00,270
 the box it's not very sensitive to its

970
00:24:57,600 --> 00:25:00,270
 

971
00:24:57,610 --> 00:25:03,130
 hyper parameters it can directly choose

972
00:25:00,260 --> 00:25:03,130
 

973
00:25:00,270 --> 00:25:04,840
 the important features that can directly

974
00:25:03,120 --> 00:25:04,840
 

975
00:25:03,130 --> 00:25:06,790
 work in high dimensions with mixed

976
00:25:04,830 --> 00:25:06,790
 

977
00:25:04,840 --> 00:25:08,830
 continuous discrete hyper parameters etc

978
00:25:06,780 --> 00:25:08,830
 

979
00:25:06,790 --> 00:25:10,150
 the only thing that's not so nice about

980
00:25:08,820 --> 00:25:10,150
 

981
00:25:08,830 --> 00:25:11,830
 it is that it's not a great

982
00:25:10,140 --> 00:25:11,830
 

983
00:25:10,150 --> 00:25:14,200
 probabilistic model it doesn't have this

984
00:25:11,820 --> 00:25:14,200
 

985
00:25:11,830 --> 00:25:15,940
 nice probabilistic interpretation what

986
00:25:14,190 --> 00:25:15,940
 

987
00:25:14,200 --> 00:25:18,760
 we and we do need uncertainty estimates

988
00:25:15,930 --> 00:25:18,760
 

989
00:25:15,940 --> 00:25:21,370
 for based optimization and what we use

990
00:25:18,750 --> 00:25:21,370
 

991
00:25:18,760 --> 00:25:23,020
 here is typically frequentist

992
00:25:21,360 --> 00:25:23,020
 

993
00:25:21,370 --> 00:25:24,940
 uncertainty estimates so we just model

994
00:25:23,010 --> 00:25:24,940
 

995
00:25:23,020 --> 00:25:26,830
 the variance across the individual trees

996
00:25:24,930 --> 00:25:26,830
 

997
00:25:24,940 --> 00:25:28,840
 prediction so if all the trees predict

998
00:25:26,820 --> 00:25:28,840
 

999
00:25:26,830 --> 00:25:30,070
 the same thing then we are certain if

1000
00:25:28,830 --> 00:25:30,070
 

1001
00:25:28,840 --> 00:25:33,460
 the pre trees predict something

1002
00:25:30,060 --> 00:25:33,460
 

1003
00:25:30,070 --> 00:25:35,050
 different we uncertain so it's a simple

1004
00:25:33,450 --> 00:25:35,050
 

1005
00:25:33,460 --> 00:25:39,250
 model that that we actually use in the

1006
00:25:35,040 --> 00:25:39,250
 

1007
00:25:35,050 --> 00:25:40,570
 700 dimensional space there's also work

1008
00:25:39,240 --> 00:25:40,570
 

1009
00:25:39,250 --> 00:25:42,610
 on based optimization with neural

1010
00:25:40,560 --> 00:25:42,610
 

1011
00:25:40,570 --> 00:25:45,340
 networks in particular there's two

1012
00:25:42,600 --> 00:25:45,340
 

1013
00:25:42,610 --> 00:25:47,320
 different types of works one just fits a

1014
00:25:45,330 --> 00:25:47,320
 

1015
00:25:45,340 --> 00:25:49,180
 standard neural network and then takes

1016
00:25:47,310 --> 00:25:49,180
 

1017
00:25:47,320 --> 00:25:50,680
 the last layer and there's a base and

1018
00:25:49,170 --> 00:25:50,680
 

1019
00:25:49,180 --> 00:25:53,560
 linear regression of the learned

1020
00:25:50,670 --> 00:25:53,560
 

1021
00:25:50,680 --> 00:25:56,230
 features in that last layer it's called

1022
00:25:53,550 --> 00:25:56,230
 

1023
00:25:53,560 --> 00:25:58,540
 dingo didnt deep networks for global

1024
00:25:56,220 --> 00:25:58,540
 

1025
00:25:56,230 --> 00:26:00,430
 optimization and the follow-up work that

1026
00:25:58,530 --> 00:26:00,430
 

1027
00:25:58,540 --> 00:26:01,660
 we did is using fully based in neural

1028
00:26:00,420 --> 00:26:01,660
 

1029
00:26:00,430 --> 00:26:04,540
 networks that are trained with

1030
00:26:01,650 --> 00:26:04,540
 

1031
00:26:01,660 --> 00:26:06,040
 stochastic gradient HM C and I'll just

1032
00:26:04,530 --> 00:26:06,040
 

1033
00:26:04,540 --> 00:26:08,410
 just give a full base in the

1034
00:26:06,030 --> 00:26:08,410
 

1035
00:26:06,040 --> 00:26:12,100
 interpretation and these networks can

1036
00:26:08,400 --> 00:26:12,100
 

1037
00:26:08,410 --> 00:26:14,320
 give you very nice predictions so for

1038
00:26:12,090 --> 00:26:14,320
 

1039
00:26:12,100 --> 00:26:17,880
 the regime of the data where there's

1040
00:26:14,310 --> 00:26:17,880
 

1041
00:26:14,320 --> 00:26:20,470
 lots of data it gives you very good

1042
00:26:17,870 --> 00:26:20,470
 

1043
00:26:17,880 --> 00:26:22,270
 predictions with very low uncertainties

1044
00:26:20,460 --> 00:26:22,270
 

1045
00:26:20,470 --> 00:26:22,900
 and in the areas of the space where

1046
00:26:22,260 --> 00:26:22,900
 

1047
00:26:22,270 --> 00:26:25,990
 there is not a lot

1048
00:26:22,890 --> 00:26:25,990
 

1049
00:26:22,900 --> 00:26:27,640
 data it gives you large uncertainty so

1050
00:26:25,980 --> 00:26:27,640
 

1051
00:26:25,990 --> 00:26:29,500
 that's precisely what you need in order

1052
00:26:27,630 --> 00:26:29,500
 

1053
00:26:27,640 --> 00:26:32,620
 to do have a problem in order to do

1054
00:26:29,490 --> 00:26:32,620
 

1055
00:26:29,500 --> 00:26:33,820
 based optimization however so far to the

1056
00:26:32,610 --> 00:26:33,820
 

1057
00:26:32,620 --> 00:26:35,500
 best of my knowledge this actually

1058
00:26:33,810 --> 00:26:35,500
 

1059
00:26:33,820 --> 00:26:37,690
 hasn't been studied for high dimensional

1060
00:26:35,490 --> 00:26:37,690
 

1061
00:26:35,500 --> 00:26:40,060
 spaces for conditional hyper parameter

1062
00:26:37,680 --> 00:26:40,060
 

1063
00:26:37,690 --> 00:26:41,950
 space is discrete spaces etc so that is

1064
00:26:40,050 --> 00:26:41,950
 

1065
00:26:40,060 --> 00:26:47,470
 actually a definitely an important area

1066
00:26:41,940 --> 00:26:47,470
 

1067
00:26:41,950 --> 00:26:48,760
 to follow up on a final model for

1068
00:26:47,460 --> 00:26:48,760
 

1069
00:26:47,470 --> 00:26:51,430
 Bayesian optimization I want to talk

1070
00:26:48,750 --> 00:26:51,430
 

1071
00:26:48,760 --> 00:26:53,590
 about is the stree of person estimators

1072
00:26:51,420 --> 00:26:53,590
 

1073
00:26:51,430 --> 00:26:55,810
 it's actually quite different it does

1074
00:26:53,580 --> 00:26:55,810
 

1075
00:26:53,590 --> 00:26:57,970
 not model this probability of the

1076
00:26:55,800 --> 00:26:57,970
 

1077
00:26:55,810 --> 00:27:00,640
 function given the hyper parameter but

1078
00:26:57,960 --> 00:27:00,640
 

1079
00:26:57,970 --> 00:27:02,650
 instead it models a density estimate of

1080
00:27:00,630 --> 00:27:02,650
 

1081
00:27:00,640 --> 00:27:04,600
 the probability that a hyper parameter

1082
00:27:02,640 --> 00:27:04,600
 

1083
00:27:02,650 --> 00:27:07,180
 setting is good and the probability that

1084
00:27:04,590 --> 00:27:07,180
 

1085
00:27:04,600 --> 00:27:09,310
 a hyper parameter setting is bad so I'm

1086
00:27:07,170 --> 00:27:09,310
 

1087
00:27:07,180 --> 00:27:11,740
 making an example here so let's say we

1088
00:27:09,300 --> 00:27:11,740
 

1089
00:27:09,310 --> 00:27:13,420
 have evaluated this function for x we

1090
00:27:11,730 --> 00:27:13,420
 

1091
00:27:11,740 --> 00:27:16,330
 want to minimize now there's two bad

1092
00:27:13,410 --> 00:27:16,330
 

1093
00:27:13,420 --> 00:27:19,000
 points up here and two good points here

1094
00:27:16,320 --> 00:27:19,000
 

1095
00:27:16,330 --> 00:27:22,120
 and so you fit these density estimators

1096
00:27:18,990 --> 00:27:22,120
 

1097
00:27:19,000 --> 00:27:24,280
 and then you look at the probability of

1098
00:27:22,110 --> 00:27:24,280
 

1099
00:27:22,120 --> 00:27:26,590
 being good divided by the probability of

1100
00:27:24,270 --> 00:27:26,590
 

1101
00:27:24,280 --> 00:27:29,410
 being bad and use that as an equation

1102
00:27:26,580 --> 00:27:29,410
 

1103
00:27:26,590 --> 00:27:31,720
 function this acquisition function has

1104
00:27:29,400 --> 00:27:31,720
 

1105
00:27:29,410 --> 00:27:34,620
 been proven to actually be equivalent to

1106
00:27:31,710 --> 00:27:34,620
 

1107
00:27:31,720 --> 00:27:37,960
 expected improvement by Burke's trial

1108
00:27:34,610 --> 00:27:37,960
 

1109
00:27:34,620 --> 00:27:40,570
 which is why people use this as a based

1110
00:27:37,950 --> 00:27:40,570
 

1111
00:27:37,960 --> 00:27:41,950
 optimization algorithm so then you

1112
00:27:40,560 --> 00:27:41,950
 

1113
00:27:40,570 --> 00:27:44,020
 optimize this acquisition for you can

1114
00:27:41,940 --> 00:27:44,020
 

1115
00:27:41,950 --> 00:27:47,130
 get this new point refit this kernel

1116
00:27:44,010 --> 00:27:47,130
 

1117
00:27:44,020 --> 00:27:49,600
 density estimator for bad point

1118
00:27:47,120 --> 00:27:49,600
 

1119
00:27:47,130 --> 00:27:53,140
 recompute your acquisition function

1120
00:27:49,590 --> 00:27:53,140
 

1121
00:27:49,600 --> 00:27:54,640
 maximize that then evaluate here and by

1122
00:27:53,130 --> 00:27:54,640
 

1123
00:27:53,140 --> 00:27:57,190
 evaluating here now you have another

1124
00:27:54,630 --> 00:27:57,190
 

1125
00:27:54,640 --> 00:27:58,900
 good point and this point here was

1126
00:27:57,180 --> 00:27:58,900
 

1127
00:27:57,190 --> 00:28:02,980
 previously good and now you have other

1128
00:27:58,890 --> 00:28:02,980
 

1129
00:27:58,900 --> 00:28:04,750
 good points so this becomes bad so there

1130
00:28:02,970 --> 00:28:04,750
 

1131
00:28:02,980 --> 00:28:06,820
 there's a hyper parameter saying which

1132
00:28:04,740 --> 00:28:06,820
 

1133
00:28:04,750 --> 00:28:09,130
 quantile of points you call good and

1134
00:28:06,810 --> 00:28:09,130
 

1135
00:28:06,820 --> 00:28:13,300
 which quantile of points I'm after which

1136
00:28:09,120 --> 00:28:13,300
 

1137
00:28:09,130 --> 00:28:15,550
 you call it bad and so the the pros of

1138
00:28:13,290 --> 00:28:15,550
 

1139
00:28:13,300 --> 00:28:17,230
 this method are that it is very

1140
00:28:15,540 --> 00:28:17,230
 

1141
00:28:15,550 --> 00:28:19,630
 efficient these kernel density

1142
00:28:17,220 --> 00:28:19,630
 

1143
00:28:17,230 --> 00:28:22,120
 estimators can be fit very quickly it's

1144
00:28:19,620 --> 00:28:22,120
 

1145
00:28:19,630 --> 00:28:24,760
 very parallelizable and very robust that

1146
00:28:22,110 --> 00:28:24,760
 

1147
00:28:22,120 --> 00:28:26,350
 kind of just works for all kinds of

1148
00:28:24,750 --> 00:28:26,350
 

1149
00:28:24,760 --> 00:28:30,010
 different dimensionalities

1150
00:28:26,340 --> 00:28:30,010
 

1151
00:28:26,350 --> 00:28:31,930
 etc one con is that typically it is

1152
00:28:30,000 --> 00:28:31,930
 

1153
00:28:30,010 --> 00:28:34,150
 actually less sample efficient than

1154
00:28:31,920 --> 00:28:34,150
 

1155
00:28:31,930 --> 00:28:37,560
 Gaussian processes when you have the

1156
00:28:34,140 --> 00:28:37,560
 

1157
00:28:34,150 --> 00:28:37,560
 right kernel for the Gaussian process

1158
00:28:38,110 --> 00:28:38,110
 

1159
00:28:38,120 --> 00:28:41,660
 a final model that is not based on

1160
00:28:40,000 --> 00:28:41,660
 

1161
00:28:40,010 --> 00:28:43,400
 optimization that I wanted to mention as

1162
00:28:41,650 --> 00:28:43,400
 

1163
00:28:41,660 --> 00:28:45,020
 population-based methods because they

1164
00:28:43,390 --> 00:28:45,020
 

1165
00:28:43,400 --> 00:28:47,210
 also use the neural architecture search

1166
00:28:45,010 --> 00:28:47,210
 

1167
00:28:45,020 --> 00:28:48,950
 and there you have a population of

1168
00:28:47,200 --> 00:28:48,950
 

1169
00:28:47,210 --> 00:28:50,630
 different configurations you want to

1170
00:28:48,940 --> 00:28:50,630
 

1171
00:28:48,950 --> 00:28:51,860
 maintain diversity and you don't want to

1172
00:28:50,620 --> 00:28:51,860
 

1173
00:28:50,630 --> 00:28:54,620
 stagnate you don't want all the

1174
00:28:51,850 --> 00:28:54,620
 

1175
00:28:51,860 --> 00:28:56,600
 configurations to be the same but you

1176
00:28:54,610 --> 00:28:56,600
 

1177
00:28:54,620 --> 00:28:59,360
 also want your population to improve

1178
00:28:56,590 --> 00:28:59,360
 

1179
00:28:56,600 --> 00:29:00,770
 over time one example of population

1180
00:28:59,350 --> 00:29:00,770
 

1181
00:28:59,360 --> 00:29:03,680
 based methods are evolutionary

1182
00:29:00,760 --> 00:29:03,680
 

1183
00:29:00,770 --> 00:29:06,260
 strategies the end day work as follows

1184
00:29:03,670 --> 00:29:06,260
 

1185
00:29:03,680 --> 00:29:10,070
 so you have one incumbent point you

1186
00:29:06,250 --> 00:29:10,070
 

1187
00:29:06,260 --> 00:29:12,860
 sample for example from a normal

1188
00:29:10,060 --> 00:29:12,860
 

1189
00:29:10,070 --> 00:29:16,490
 distribution around that point evaluate

1190
00:29:12,850 --> 00:29:16,490
 

1191
00:29:12,860 --> 00:29:19,280
 all these different points then pick the

1192
00:29:16,480 --> 00:29:19,280
 

1193
00:29:16,490 --> 00:29:21,560
 best ones and move your incumbent over

1194
00:29:19,270 --> 00:29:21,560
 

1195
00:29:19,280 --> 00:29:24,980
 to be a weighted average of your good

1196
00:29:21,550 --> 00:29:24,980
 

1197
00:29:21,560 --> 00:29:27,290
 points a popular variant of evolutionary

1198
00:29:24,970 --> 00:29:27,290
 

1199
00:29:24,980 --> 00:29:28,940
 strategies as gmaes which is a

1200
00:29:27,280 --> 00:29:28,940
 

1201
00:29:27,290 --> 00:29:32,150
 covariance matrix adaptation

1202
00:29:28,930 --> 00:29:32,150
 

1203
00:29:28,940 --> 00:29:34,610
 evolutionary strategy which basically

1204
00:29:32,140 --> 00:29:34,610
 

1205
00:29:32,150 --> 00:29:39,080
 wins the black box optimization

1206
00:29:34,600 --> 00:29:39,080
 

1207
00:29:34,610 --> 00:29:40,850
 challenge every year that is about cheap

1208
00:29:39,070 --> 00:29:40,850
 

1209
00:29:39,080 --> 00:29:42,530
 function evaluations so if you have a

1210
00:29:40,840 --> 00:29:42,530
 

1211
00:29:40,850 --> 00:29:45,650
 budget of something like a million

1212
00:29:42,520 --> 00:29:45,650
 

1213
00:29:42,530 --> 00:29:47,390
 function evaluations then these

1214
00:29:45,640 --> 00:29:47,390
 

1215
00:29:45,650 --> 00:29:48,620
 evolutionary strategies are a lot better

1216
00:29:47,380 --> 00:29:48,620
 

1217
00:29:47,390 --> 00:29:50,420
 than based optimization basically

1218
00:29:48,610 --> 00:29:50,420
 

1219
00:29:48,620 --> 00:29:52,070
 optimization is really just not targeted

1220
00:29:50,410 --> 00:29:52,070
 

1221
00:29:50,420 --> 00:29:58,040
 towards that and would also be far too

1222
00:29:52,060 --> 00:29:58,040
 

1223
00:29:52,070 --> 00:29:59,930
 slow in that setting and well recently

1224
00:29:58,030 --> 00:29:59,930
 

1225
00:29:58,040 --> 00:30:01,220
 we also looked at how would CMA has

1226
00:29:59,920 --> 00:30:01,220
 

1227
00:29:59,930 --> 00:30:02,810
 actually do for Hyper permit

1228
00:30:01,210 --> 00:30:02,810
 

1229
00:30:01,220 --> 00:30:05,150
 optimization and show that it is

1230
00:30:02,800 --> 00:30:05,150
 

1231
00:30:02,810 --> 00:30:07,700
 actually quite competitive if you have

1232
00:30:05,140 --> 00:30:07,700
 

1233
00:30:05,150 --> 00:30:09,530
 parallel resources then it can actually

1234
00:30:07,690 --> 00:30:09,530
 

1235
00:30:07,700 --> 00:30:12,980
 be better than all the based

1236
00:30:09,520 --> 00:30:12,980
 

1237
00:30:09,530 --> 00:30:15,410
 optimization methods that we tried it

1238
00:30:12,970 --> 00:30:15,410
 

1239
00:30:12,980 --> 00:30:20,720
 however only works on purely continuous

1240
00:30:15,400 --> 00:30:20,720
 

1241
00:30:15,410 --> 00:30:22,640
 basis alright so that was an overview of

1242
00:30:20,710 --> 00:30:22,640
 

1243
00:30:20,720 --> 00:30:24,800
 different black box optimization methods

1244
00:30:22,630 --> 00:30:24,800
 

1245
00:30:22,640 --> 00:30:27,170
 now I'll talk about how to speed things

1246
00:30:24,790 --> 00:30:27,170
 

1247
00:30:24,800 --> 00:30:29,540
 up why do we need to speed things up

1248
00:30:27,160 --> 00:30:29,540
 

1249
00:30:27,170 --> 00:30:31,910
 because this black box view is just

1250
00:30:29,530 --> 00:30:31,910
 

1251
00:30:29,540 --> 00:30:34,040
 really too slow for deep learning and

1252
00:30:31,900 --> 00:30:34,040
 

1253
00:30:31,910 --> 00:30:36,620
 for big datasets if doing a single

1254
00:30:34,030 --> 00:30:36,620
 

1255
00:30:34,040 --> 00:30:39,260
 function evaluation takes a week or so

1256
00:30:36,610 --> 00:30:39,260
 

1257
00:30:36,620 --> 00:30:41,510
 then while getting 50 samples would take

1258
00:30:39,250 --> 00:30:41,510
 

1259
00:30:39,260 --> 00:30:43,100
 a year and obviously this we don't have

1260
00:30:41,500 --> 00:30:43,100
 

1261
00:30:41,510 --> 00:30:46,790
 a year in order to do I primate

1262
00:30:43,090 --> 00:30:46,790
 

1263
00:30:43,100 --> 00:30:49,700
 optimization so there is four different

1264
00:30:46,780 --> 00:30:49,700
 

1265
00:30:46,790 --> 00:30:50,360
 types of approaches for going beyond

1266
00:30:49,690 --> 00:30:50,360
 

1267
00:30:49,700 --> 00:30:53,690
 black box up

1268
00:30:50,350 --> 00:30:53,690
 

1269
00:30:50,360 --> 00:30:55,790
 zatia meta-learning I will not talk

1270
00:30:53,680 --> 00:30:55,790
 

1271
00:30:53,690 --> 00:30:57,260
 about because that is part three of this

1272
00:30:55,780 --> 00:30:57,260
 

1273
00:30:55,790 --> 00:30:59,210
 tutorial and Joakim will talk about that

1274
00:30:57,250 --> 00:30:59,210
 

1275
00:30:57,260 --> 00:31:02,180
 later and there's a lot of different

1276
00:30:59,200 --> 00:31:02,180
 

1277
00:30:59,210 --> 00:31:04,520
 approaches here and I will briefly talk

1278
00:31:02,170 --> 00:31:04,520
 

1279
00:31:02,180 --> 00:31:06,530
 about these first two and then focus on

1280
00:31:04,510 --> 00:31:06,530
 

1281
00:31:04,520 --> 00:31:08,930
 multi fidelity optimization because that

1282
00:31:06,520 --> 00:31:08,930
 

1283
00:31:06,530 --> 00:31:11,480
 is kind of the most mature method right

1284
00:31:08,920 --> 00:31:11,480
 

1285
00:31:08,930 --> 00:31:15,170
 now and is at a stage where it's you can

1286
00:31:11,470 --> 00:31:15,170
 

1287
00:31:11,480 --> 00:31:17,660
 use it as a tool so hyper primate a

1288
00:31:15,160 --> 00:31:17,660
 

1289
00:31:15,170 --> 00:31:21,470
 gradient descent is the first one I want

1290
00:31:17,650 --> 00:31:21,470
 

1291
00:31:17,660 --> 00:31:22,880
 to talk about this formulates the hyper

1292
00:31:21,460 --> 00:31:22,880
 

1293
00:31:21,470 --> 00:31:26,630
 primate optimization as a bi-level

1294
00:31:22,870 --> 00:31:26,630
 

1295
00:31:22,880 --> 00:31:28,910
 program where you have this outer

1296
00:31:26,620 --> 00:31:28,910
 

1297
00:31:26,630 --> 00:31:32,840
 objective where you want to minimize the

1298
00:31:28,900 --> 00:31:32,840
 

1299
00:31:28,910 --> 00:31:35,060
 validation loss off but as a function of

1300
00:31:32,830 --> 00:31:35,060
 

1301
00:31:32,840 --> 00:31:38,840
 your hyper parameters lambda and your

1302
00:31:35,050 --> 00:31:38,840
 

1303
00:31:35,060 --> 00:31:43,310
 internal model parameters W the WS are

1304
00:31:38,830 --> 00:31:43,310
 

1305
00:31:38,840 --> 00:31:46,760
 set by optimizing an internal and inner

1306
00:31:43,300 --> 00:31:46,760
 

1307
00:31:43,310 --> 00:31:50,180
 loss namely the training loss given a

1308
00:31:46,750 --> 00:31:50,180
 

1309
00:31:46,760 --> 00:31:53,270
 hyper parameter setting lambda so for

1310
00:31:50,170 --> 00:31:53,270
 

1311
00:31:50,180 --> 00:31:57,140
 example you optimize with SGD to get

1312
00:31:53,260 --> 00:31:57,140
 

1313
00:31:53,270 --> 00:32:01,190
 double w star and then you want to get

1314
00:31:57,130 --> 00:32:01,190
 

1315
00:31:57,140 --> 00:32:03,620
 this validation loss and your you can

1316
00:32:01,180 --> 00:32:03,620
 

1317
00:32:01,190 --> 00:32:06,590
 also phrases as getting gradients with

1318
00:32:03,610 --> 00:32:06,590
 

1319
00:32:03,620 --> 00:32:08,510
 respect to the validation loss by for

1320
00:32:06,580 --> 00:32:08,510
 

1321
00:32:06,590 --> 00:32:11,780
 example deriving through the entire

1322
00:32:08,500 --> 00:32:11,780
 

1323
00:32:08,510 --> 00:32:16,250
 optimization process that leads to this

1324
00:32:11,770 --> 00:32:16,250
 

1325
00:32:11,780 --> 00:32:18,500
 w star this sounds expensive to start

1326
00:32:16,240 --> 00:32:18,500
 

1327
00:32:16,250 --> 00:32:23,420
 with and it is indeed expensive in terms

1328
00:32:18,490 --> 00:32:23,420
 

1329
00:32:18,500 --> 00:32:25,280
 of memory or time so you can choose but

1330
00:32:23,410 --> 00:32:25,280
 

1331
00:32:23,420 --> 00:32:27,770
 if you have a lot of hyper parameters

1332
00:32:25,270 --> 00:32:27,770
 

1333
00:32:25,280 --> 00:32:29,270
 then this can actually be very useful

1334
00:32:27,760 --> 00:32:29,270
 

1335
00:32:27,770 --> 00:32:31,010
 because you don't just get a single

1336
00:32:29,260 --> 00:32:31,010
 

1337
00:32:29,270 --> 00:32:36,230
 function evaluation out but you get a

1338
00:32:31,000 --> 00:32:36,230
 

1339
00:32:31,010 --> 00:32:37,910
 whole gradient out however this this

1340
00:32:36,220 --> 00:32:37,910
 

1341
00:32:36,230 --> 00:32:40,790
 approach is somewhat expensive and

1342
00:32:37,900 --> 00:32:40,790
 

1343
00:32:37,910 --> 00:32:43,430
 another approach is actually to

1344
00:32:40,780 --> 00:32:43,430
 

1345
00:32:40,790 --> 00:32:45,950
 interleave optimization steps with

1346
00:32:43,420 --> 00:32:45,950
 

1347
00:32:43,430 --> 00:32:48,470
 respect to the validation performance

1348
00:32:45,940 --> 00:32:48,470
 

1349
00:32:45,950 --> 00:32:50,210
 you do a gradient step of the hyper

1350
00:32:48,460 --> 00:32:50,210
 

1351
00:32:48,470 --> 00:32:52,580
 parameters and then you do a gradient

1352
00:32:50,200 --> 00:32:52,580
 

1353
00:32:50,210 --> 00:32:56,390
 step with respect to the parameters of

1354
00:32:52,570 --> 00:32:56,390
 

1355
00:32:52,580 --> 00:32:58,820
 the training loss this interestingly

1356
00:32:56,380 --> 00:32:58,820
 

1357
00:32:56,390 --> 00:33:01,490
 enough works actually quite well in

1358
00:32:58,810 --> 00:33:01,490
 

1359
00:32:58,820 --> 00:33:02,950
 practice but there is no theory about

1360
00:33:01,480 --> 00:33:02,950
 

1361
00:33:01,490 --> 00:33:05,470
 why this works and

1362
00:33:02,940 --> 00:33:05,470
 

1363
00:33:02,950 --> 00:33:08,380
 where this might fail and and so on so

1364
00:33:05,460 --> 00:33:08,380
 

1365
00:33:05,470 --> 00:33:09,730
 this is actually a very interesting area

1366
00:33:08,370 --> 00:33:09,730
 

1367
00:33:08,380 --> 00:33:13,720
 of research that I would encourage

1368
00:33:09,720 --> 00:33:13,720
 

1369
00:33:09,730 --> 00:33:15,940
 people to work on and just in general if

1370
00:33:13,710 --> 00:33:15,940
 

1371
00:33:13,720 --> 00:33:18,460
 you can do gradient based optimization

1372
00:33:15,930 --> 00:33:18,460
 

1373
00:33:15,940 --> 00:33:20,470
 then of course there's a lot of cool

1374
00:33:18,450 --> 00:33:20,470
 

1375
00:33:18,460 --> 00:33:21,790
 things that that we can do and neural

1376
00:33:20,460 --> 00:33:21,790
 

1377
00:33:20,470 --> 00:33:24,430
 architecture search is actually also

1378
00:33:21,780 --> 00:33:24,430
 

1379
00:33:21,790 --> 00:33:25,930
 going this direction with using a lot of

1380
00:33:24,420 --> 00:33:25,930
 

1381
00:33:24,430 --> 00:33:29,580
 gradient based neural architecture

1382
00:33:25,920 --> 00:33:29,580
 

1383
00:33:25,930 --> 00:33:32,050
 search I'll talk about that in a bit

1384
00:33:29,570 --> 00:33:32,050
 

1385
00:33:29,580 --> 00:33:34,350
 the second way of going beyond blackbox

1386
00:33:32,040 --> 00:33:34,350
 

1387
00:33:32,050 --> 00:33:37,360
 optimization is to do probabilistic

1388
00:33:34,340 --> 00:33:37,360
 

1389
00:33:34,350 --> 00:33:39,940
 extrapolations of your learning curve in

1390
00:33:37,350 --> 00:33:39,940
 

1391
00:33:37,360 --> 00:33:41,320
 order to do early stopping so you have

1392
00:33:39,930 --> 00:33:41,320
 

1393
00:33:39,940 --> 00:33:43,270
 an initial learning curve and you want

1394
00:33:41,310 --> 00:33:43,270
 

1395
00:33:41,320 --> 00:33:44,920
 to know well where does this go I will

1396
00:33:43,260 --> 00:33:44,920
 

1397
00:33:43,270 --> 00:33:48,940
 just do well or will this not do well

1398
00:33:44,910 --> 00:33:48,940
 

1399
00:33:44,920 --> 00:33:50,440
 and so you can learn to extrapolate for

1400
00:33:48,930 --> 00:33:50,440
 

1401
00:33:48,940 --> 00:33:53,650
 example you can use parametric learning

1402
00:33:50,430 --> 00:33:53,650
 

1403
00:33:50,440 --> 00:33:55,540
 curve models that are fit with MCMC to

1404
00:33:53,640 --> 00:33:55,540
 

1405
00:33:53,650 --> 00:33:57,250
 do these predictions or you can also use

1406
00:33:55,530 --> 00:33:57,250
 

1407
00:33:55,540 --> 00:34:01,510
 Bayesian neural networks in order to do

1408
00:33:57,240 --> 00:34:01,510
 

1409
00:33:57,250 --> 00:34:04,210
 these extrapolations the final way of

1410
00:34:01,500 --> 00:34:04,210
 

1411
00:34:01,510 --> 00:34:06,760
 going beyond the blackbox function is to

1412
00:34:04,200 --> 00:34:06,760
 

1413
00:34:04,210 --> 00:34:10,030
 do multi fidelity optimization so what

1414
00:34:06,750 --> 00:34:10,030
 

1415
00:34:06,760 --> 00:34:12,130
 is multi fidelity optimization it's

1416
00:34:10,020 --> 00:34:12,130
 

1417
00:34:10,030 --> 00:34:14,620
 about using cheap approximations of the

1418
00:34:12,120 --> 00:34:14,620
 

1419
00:34:12,130 --> 00:34:16,810
 blackbox function and cheaper

1420
00:34:14,610 --> 00:34:16,810
 

1421
00:34:14,620 --> 00:34:18,760
 approximations the performance on which

1422
00:34:16,800 --> 00:34:18,760
 

1423
00:34:16,810 --> 00:34:21,310
 correlates with the actual blackbox

1424
00:34:18,750 --> 00:34:21,310
 

1425
00:34:18,760 --> 00:34:23,470
 function so you want a cheaper

1426
00:34:21,300 --> 00:34:23,470
 

1427
00:34:21,310 --> 00:34:25,179
 proximation where if a hyper parameter

1428
00:34:23,460 --> 00:34:25,179
 

1429
00:34:23,470 --> 00:34:27,399
 setting does well there typically it

1430
00:34:25,169 --> 00:34:27,399
 

1431
00:34:25,179 --> 00:34:29,080
 also does well on the expensive blackbox

1432
00:34:27,389 --> 00:34:29,080
 

1433
00:34:27,399 --> 00:34:30,909
 how can you get these cheap

1434
00:34:29,070 --> 00:34:30,909
 

1435
00:34:29,080 --> 00:34:32,380
 approximations well in a variety of

1436
00:34:30,899 --> 00:34:32,380
 

1437
00:34:30,909 --> 00:34:34,750
 different ways for example you could

1438
00:34:32,370 --> 00:34:34,750
 

1439
00:34:32,380 --> 00:34:38,710
 look at subsets of your data you could

1440
00:34:34,740 --> 00:34:38,710
 

1441
00:34:34,750 --> 00:34:40,659
 look at fewer epochs of SGD in bazin

1442
00:34:38,700 --> 00:34:40,659
 

1443
00:34:38,710 --> 00:34:43,240
 deep learning you could do shorter MCMC

1444
00:34:40,649 --> 00:34:43,240
 

1445
00:34:40,659 --> 00:34:44,770
 chains you could do fewer trials and

1446
00:34:43,230 --> 00:34:44,770
 

1447
00:34:43,240 --> 00:34:47,470
 deep reinforcement learning and we've

1448
00:34:44,760 --> 00:34:47,470
 

1449
00:34:44,770 --> 00:34:49,360
 actually done all these for you could

1450
00:34:47,460 --> 00:34:49,360
 

1451
00:34:47,470 --> 00:34:50,649
 also do down sampled images and object

1452
00:34:49,350 --> 00:34:50,649
 

1453
00:34:49,360 --> 00:34:52,840
 your cognition we haven't done that but

1454
00:34:50,639 --> 00:34:52,840
 

1455
00:34:50,649 --> 00:34:55,300
 I'm pretty sure that that also would

1456
00:34:52,830 --> 00:34:55,300
 

1457
00:34:52,840 --> 00:34:56,890
 work and this approach also is actually

1458
00:34:55,290 --> 00:34:56,890
 

1459
00:34:55,300 --> 00:34:59,410
 applicable in a lot of different domains

1460
00:34:56,880 --> 00:34:59,410
 

1461
00:34:56,890 --> 00:35:01,180
 such as fluid simulations for example

1462
00:34:59,400 --> 00:35:01,180
 

1463
00:34:59,410 --> 00:35:03,190
 you could use less particles in a fluid

1464
00:35:01,170 --> 00:35:03,190
 

1465
00:35:01,180 --> 00:35:04,890
 simulation or I do shorter simulations

1466
00:35:03,180 --> 00:35:04,890
 

1467
00:35:03,190 --> 00:35:08,260
 so it's it's really a generic

1468
00:35:04,880 --> 00:35:08,260
 

1469
00:35:04,890 --> 00:35:11,230
 optimization trick that applies beyond

1470
00:35:08,250 --> 00:35:11,230
 

1471
00:35:08,260 --> 00:35:13,140
 hyper parameter optimization so how do

1472
00:35:11,220 --> 00:35:13,140
 

1473
00:35:11,230 --> 00:35:15,220
 you use these multiple multi Fidelity's

1474
00:35:13,130 --> 00:35:15,220
 

1475
00:35:13,140 --> 00:35:19,180
 here's an example

1476
00:35:15,210 --> 00:35:19,180
 

1477
00:35:15,220 --> 00:35:21,609
 this is an SVM fit on em list this is

1478
00:35:19,170 --> 00:35:21,609
 

1479
00:35:19,180 --> 00:35:24,070
 the entire M this dataset 50,000 data

1480
00:35:21,599 --> 00:35:24,070
 

1481
00:35:21,609 --> 00:35:26,500
 points and here there's roughly 400 data

1482
00:35:24,060 --> 00:35:26,500
 

1483
00:35:24,070 --> 00:35:28,780
 points and the two most important hyper

1484
00:35:26,490 --> 00:35:28,780
 

1485
00:35:26,500 --> 00:35:30,460
 parameters C and gamma and you see that

1486
00:35:28,770 --> 00:35:30,460
 

1487
00:35:28,780 --> 00:35:33,550
 the response surfaces actually look

1488
00:35:30,450 --> 00:35:33,550
 

1489
00:35:30,460 --> 00:35:36,369
 quite similar in particular you don't

1490
00:35:33,540 --> 00:35:36,369
 

1491
00:35:33,550 --> 00:35:38,109
 want to do a lot of function evaluations

1492
00:35:36,359 --> 00:35:38,109
 

1493
00:35:36,369 --> 00:35:40,630
 over here in order to figure out that

1494
00:35:38,099 --> 00:35:40,630
 

1495
00:35:38,109 --> 00:35:42,400
 this area of the space is bad if you can

1496
00:35:40,620 --> 00:35:42,400
 

1497
00:35:40,630 --> 00:35:45,790
 figure this out already on this much

1498
00:35:42,390 --> 00:35:45,790
 

1499
00:35:42,400 --> 00:35:48,130
 cheaper approximations and importantly

1500
00:35:45,780 --> 00:35:48,130
 

1501
00:35:45,790 --> 00:35:49,780
 doing function evaluations over here is

1502
00:35:48,120 --> 00:35:49,780
 

1503
00:35:48,130 --> 00:35:53,170
 about ten thousand times cheaper than

1504
00:35:49,770 --> 00:35:53,170
 

1505
00:35:49,780 --> 00:35:55,080
 over here because SVM scale super

1506
00:35:53,160 --> 00:35:55,080
 

1507
00:35:53,170 --> 00:35:58,599
 linearly and the number of data points

1508
00:35:55,070 --> 00:35:58,599
 

1509
00:35:55,080 --> 00:36:01,020
 so you want to do many cheap evaluations

1510
00:35:58,589 --> 00:36:01,020
 

1511
00:35:58,599 --> 00:36:03,940
 on the small subsets and few expensive

1512
00:36:01,010 --> 00:36:03,940
 

1513
00:36:01,020 --> 00:36:05,349
 evaluations on the large data set and if

1514
00:36:03,930 --> 00:36:05,349
 

1515
00:36:03,940 --> 00:36:08,410
 you do that you can actually get up to a

1516
00:36:05,339 --> 00:36:08,410
 

1517
00:36:05,349 --> 00:36:11,290
 tenth up to 1,000 fold speed ups over

1518
00:36:08,400 --> 00:36:11,290
 

1519
00:36:08,410 --> 00:36:15,580
 doing Basin optimization on the

1520
00:36:11,280 --> 00:36:15,580
 

1521
00:36:11,290 --> 00:36:17,410
 expensive black box fraction how do you

1522
00:36:15,570 --> 00:36:17,410
 

1523
00:36:15,580 --> 00:36:21,640
 do that well you fit a Gaussian process

1524
00:36:17,400 --> 00:36:21,640
 

1525
00:36:17,410 --> 00:36:26,859
 model that takes us input the hyper

1526
00:36:21,630 --> 00:36:26,859
 

1527
00:36:21,640 --> 00:36:28,869
 parameters and the budget and then ya

1528
00:36:26,849 --> 00:36:28,869
 

1529
00:36:26,859 --> 00:36:33,040
 predicts how well that will work and

1530
00:36:28,859 --> 00:36:33,040
 

1531
00:36:28,869 --> 00:36:35,349
 then chooses balls lambda and the budget

1532
00:36:33,030 --> 00:36:35,349
 

1533
00:36:33,040 --> 00:36:37,330
 in order to maximize the bank for the

1534
00:36:35,339 --> 00:36:37,330
 

1535
00:36:35,349 --> 00:36:39,460
 back so for example you maximize

1536
00:36:37,320 --> 00:36:39,460
 

1537
00:36:37,330 --> 00:36:41,410
 information gained her time spent

1538
00:36:39,450 --> 00:36:41,410
 

1539
00:36:39,460 --> 00:36:44,080
 information gain about where the global

1540
00:36:41,400 --> 00:36:44,080
 

1541
00:36:41,410 --> 00:36:46,109
 optimizer lies and there's a variety of

1542
00:36:44,070 --> 00:36:46,109
 

1543
00:36:44,080 --> 00:36:51,099
 different methods that that all fall

1544
00:36:46,099 --> 00:36:51,099
 

1545
00:36:46,109 --> 00:36:53,730
 fall into this general general sense of

1546
00:36:51,089 --> 00:36:53,730
 

1547
00:36:51,099 --> 00:36:56,080
 multi fidelity based optimization

1548
00:36:53,720 --> 00:36:56,080
 

1549
00:36:53,730 --> 00:36:57,460
 however this is not trivial there

1550
00:36:56,070 --> 00:36:57,460
 

1551
00:36:56,080 --> 00:37:00,130
 there's quite a few approximations

1552
00:36:57,450 --> 00:37:00,130
 

1553
00:36:57,460 --> 00:37:04,089
 involved it's kind of hard to code up

1554
00:37:00,120 --> 00:37:04,089
 

1555
00:37:00,130 --> 00:37:05,980
 and also well you need to get the right

1556
00:37:04,079 --> 00:37:05,980
 

1557
00:37:04,089 --> 00:37:07,720
 colonel if you have high dimensional

1558
00:37:05,970 --> 00:37:07,720
 

1559
00:37:05,980 --> 00:37:10,480
 income and conditional spaces this might

1560
00:37:07,710 --> 00:37:10,480
 

1561
00:37:07,720 --> 00:37:13,900
 not actually be the best approach and a

1562
00:37:10,470 --> 00:37:13,900
 

1563
00:37:10,480 --> 00:37:16,270
 much simpler approach would be to for

1564
00:37:13,890 --> 00:37:16,270
 

1565
00:37:13,900 --> 00:37:18,130
 example just sample a bunch of random

1566
00:37:16,260 --> 00:37:18,130
 

1567
00:37:16,270 --> 00:37:21,270
 configurations on the cheapest setting

1568
00:37:18,120 --> 00:37:21,270
 

1569
00:37:18,130 --> 00:37:24,250
 on the cheapest fidelity take the best

1570
00:37:21,260 --> 00:37:24,250
 

1571
00:37:21,270 --> 00:37:27,369
 fraction there off move them to the next

1572
00:37:24,240 --> 00:37:27,369
 

1573
00:37:24,250 --> 00:37:28,020
 budget take the best fraction there off

1574
00:37:27,359 --> 00:37:28,020
 

1575
00:37:27,369 --> 00:37:30,270
 move them to

1576
00:37:28,010 --> 00:37:30,270
 

1577
00:37:28,020 --> 00:37:32,300
 next budget take the next best fraction

1578
00:37:30,260 --> 00:37:32,300
 

1579
00:37:30,270 --> 00:37:35,280
 and move it to the next budget this is

1580
00:37:32,290 --> 00:37:35,280
 

1581
00:37:32,300 --> 00:37:37,770
 the approach called success of having

1582
00:37:35,270 --> 00:37:37,770
 

1583
00:37:35,280 --> 00:37:39,480
 and here's another visualization of

1584
00:37:37,760 --> 00:37:39,480
 

1585
00:37:37,770 --> 00:37:42,270
 success of having for a different a

1586
00:37:39,470 --> 00:37:42,270
 

1587
00:37:39,480 --> 00:37:44,910
 fidelity namely wall clock time so you

1588
00:37:42,260 --> 00:37:44,910
 

1589
00:37:42,270 --> 00:37:50,160
 you do you evaluate a bunch of hyper

1590
00:37:44,900 --> 00:37:50,160
 

1591
00:37:44,910 --> 00:37:52,170
 parameter settings all for this all for

1592
00:37:50,150 --> 00:37:52,170
 

1593
00:37:50,160 --> 00:37:56,040
 this initial wall clock time you take

1594
00:37:52,160 --> 00:37:56,040
 

1595
00:37:52,170 --> 00:37:57,840
 the top performing ones continue them to

1596
00:37:56,030 --> 00:37:57,840
 

1597
00:37:56,040 --> 00:38:00,090
 the next fidelity take the top

1598
00:37:57,830 --> 00:38:00,090
 

1599
00:37:57,840 --> 00:38:02,270
 performing ones continue them take the

1600
00:38:00,080 --> 00:38:02,270
 

1601
00:38:00,090 --> 00:38:04,590
 top performing ones continue them and

1602
00:38:02,260 --> 00:38:04,590
 

1603
00:38:02,270 --> 00:38:06,480
 voila you get something that that you

1604
00:38:04,580 --> 00:38:06,480
 

1605
00:38:04,590 --> 00:38:08,430
 would do as a human you wouldn't

1606
00:38:06,470 --> 00:38:08,430
 

1607
00:38:06,480 --> 00:38:11,010
 continue this poor learning curve here

1608
00:38:08,420 --> 00:38:11,010
 

1609
00:38:08,430 --> 00:38:12,840
 all the way for a week in order to see

1610
00:38:11,000 --> 00:38:12,840
 

1611
00:38:11,010 --> 00:38:15,290
 that it didn't work but you would cut it

1612
00:38:12,830 --> 00:38:15,290
 

1613
00:38:12,840 --> 00:38:15,290
 off early

1614
00:38:15,520 --> 00:38:15,520
 

1615
00:38:15,530 --> 00:38:20,000
 there could be one problem however if

1616
00:38:18,020 --> 00:38:20,000
 

1617
00:38:18,030 --> 00:38:22,200
 this learning curve he actually does

1618
00:38:19,990 --> 00:38:22,200
 

1619
00:38:20,000 --> 00:38:24,600
 continue going up and give you the best

1620
00:38:22,190 --> 00:38:24,600
 

1621
00:38:22,200 --> 00:38:26,280
 performance then success of having would

1622
00:38:24,590 --> 00:38:26,280
 

1623
00:38:24,600 --> 00:38:28,860
 never find it even with an infinite

1624
00:38:26,270 --> 00:38:28,860
 

1625
00:38:26,280 --> 00:38:32,730
 amount of compute power and this is a

1626
00:38:28,850 --> 00:38:32,730
 

1627
00:38:28,860 --> 00:38:35,760
 problem that the extension of so called

1628
00:38:32,720 --> 00:38:35,760
 

1629
00:38:32,730 --> 00:38:39,000
 hyper band fixes so hyper band actually

1630
00:38:35,750 --> 00:38:39,000
 

1631
00:38:35,760 --> 00:38:41,820
 called success of having iteratively one

1632
00:38:38,990 --> 00:38:41,820
 

1633
00:38:39,000 --> 00:38:45,830
 time in the most aggressive way with a

1634
00:38:41,810 --> 00:38:45,830
 

1635
00:38:41,820 --> 00:38:50,660
 starting from the lowest fidelity then

1636
00:38:45,820 --> 00:38:50,660
 

1637
00:38:45,830 --> 00:38:50,660
 my computer is freezing this is not good

1638
00:38:50,710 --> 00:38:50,710
 

1639
00:38:50,720 --> 00:39:03,390
 interesting oh great hmm okay the

1640
00:39:01,609 --> 00:39:03,390
 

1641
00:39:01,619 --> 00:39:04,050
 presentation just died for no apparent

1642
00:39:03,380 --> 00:39:04,050
 

1643
00:39:03,390 --> 00:39:07,460
 reason

1644
00:39:04,040 --> 00:39:07,460
 

1645
00:39:04,050 --> 00:39:07,460
 I will restarted

1646
00:39:12,180 --> 00:39:12,180
 

1647
00:39:12,190 --> 00:39:15,060
 sorry about that

1648
00:39:22,180 --> 00:39:22,180
 

1649
00:39:22,190 --> 00:39:28,049
 so we're we mean yeah we're talking

1650
00:39:25,370 --> 00:39:28,049
 

1651
00:39:25,380 --> 00:39:30,630
 about hyper band so hyper band first

1652
00:39:28,039 --> 00:39:30,630
 

1653
00:39:28,049 --> 00:39:34,380
 called success of having on this most

1654
00:39:30,620 --> 00:39:34,380
 

1655
00:39:30,630 --> 00:39:38,190
 aggressive record then calls it again

1656
00:39:34,370 --> 00:39:38,190
 

1657
00:39:34,380 --> 00:39:40,619
 starting from this less aggressive I'm

1658
00:39:38,180 --> 00:39:40,619
 

1659
00:39:38,190 --> 00:39:42,569
 setting then calls it again starting

1660
00:39:40,609 --> 00:39:42,569
 

1661
00:39:40,619 --> 00:39:46,470
 only from here and then calls it again

1662
00:39:42,559 --> 00:39:46,470
 

1663
00:39:42,569 --> 00:39:49,140
 only using the full black box so given

1664
00:39:46,460 --> 00:39:49,140
 

1665
00:39:46,470 --> 00:39:54,059
 an infinite compute budget you would

1666
00:39:49,130 --> 00:39:54,059
 

1667
00:39:49,140 --> 00:39:56,249
 actually find issues where so so you

1668
00:39:54,049 --> 00:39:56,249
 

1669
00:39:54,059 --> 00:39:58,529
 would find the best configuration even

1670
00:39:56,239 --> 00:39:58,529
 

1671
00:39:56,249 --> 00:40:02,700
 if this is the best configuration and

1672
00:39:58,519 --> 00:40:02,700
 

1673
00:39:58,529 --> 00:40:05,150
 continuous up here I think I'll stop

1674
00:40:02,690 --> 00:40:05,150
 

1675
00:40:02,700 --> 00:40:10,549
 using the laser pointer because then my

1676
00:40:05,140 --> 00:40:10,549
 

1677
00:40:05,150 --> 00:40:13,099
 presentation does not want to continue

1678
00:40:10,539 --> 00:40:13,099
 

1679
00:40:10,549 --> 00:40:16,319
 so hype event has a lot of advantages

1680
00:40:13,089 --> 00:40:16,319
 

1681
00:40:13,099 --> 00:40:17,489
 you can you have strong anytime

1682
00:40:16,309 --> 00:40:17,489
 

1683
00:40:16,319 --> 00:40:20,460
 performance due to these multi

1684
00:40:17,479 --> 00:40:20,460
 

1685
00:40:17,489 --> 00:40:22,789
 Fidelity's its general purpose for low

1686
00:40:20,450 --> 00:40:22,789
 

1687
00:40:20,460 --> 00:40:24,809
 dimensional and high dimensional spaces

1688
00:40:22,779 --> 00:40:24,809
 

1689
00:40:22,789 --> 00:40:26,940
 conditionalities and so on it's all not

1690
00:40:24,799 --> 00:40:26,940
 

1691
00:40:24,809 --> 00:40:29,099
 a problem it's very easy to implement

1692
00:40:26,930 --> 00:40:29,099
 

1693
00:40:26,940 --> 00:40:32,190
 it's also scalable and easily

1694
00:40:29,089 --> 00:40:32,190
 

1695
00:40:29,099 --> 00:40:34,589
 parallelizable however it is based on

1696
00:40:32,180 --> 00:40:34,589
 

1697
00:40:32,190 --> 00:40:36,749
 random search and so it doesn't exploit

1698
00:40:34,579 --> 00:40:36,749
 

1699
00:40:34,589 --> 00:40:38,549
 knowledge about which hyper parameter

1700
00:40:36,739 --> 00:40:38,549
 

1701
00:40:36,749 --> 00:40:40,529
 settings work well and that's where

1702
00:40:38,539 --> 00:40:40,529
 

1703
00:40:38,549 --> 00:40:42,720
 based optimization is strong so

1704
00:40:40,519 --> 00:40:42,720
 

1705
00:40:40,529 --> 00:40:44,400
 therefore we combine them to get the

1706
00:40:42,710 --> 00:40:44,400
 

1707
00:40:42,720 --> 00:40:45,900
 best of both worlds we use based

1708
00:40:44,390 --> 00:40:45,900
 

1709
00:40:44,400 --> 00:40:47,910
 optimization in order to pick these

1710
00:40:45,890 --> 00:40:47,910
 

1711
00:40:45,900 --> 00:40:52,200
 configurations and then use type AB and

1712
00:40:47,900 --> 00:40:52,200
 

1713
00:40:47,910 --> 00:40:53,940
 in order to allocate their budgets just

1714
00:40:52,190 --> 00:40:53,940
 

1715
00:40:52,200 --> 00:40:55,349
 to visualize how this this works so

1716
00:40:53,930 --> 00:40:55,349
 

1717
00:40:53,940 --> 00:40:58,069
 hyper band versus random search

1718
00:40:55,339 --> 00:40:58,069
 

1719
00:40:55,349 --> 00:41:00,630
 initially hyper band is much faster

1720
00:40:58,059 --> 00:41:00,630
 

1721
00:40:58,069 --> 00:41:02,970
 given enough time it's actually not much

1722
00:41:00,620 --> 00:41:02,970
 

1723
00:41:00,630 --> 00:41:04,890
 faster than random search based

1724
00:41:02,960 --> 00:41:04,890
 

1725
00:41:02,970 --> 00:41:06,599
 optimization is a reverse initially

1726
00:41:04,880 --> 00:41:06,599
 

1727
00:41:04,890 --> 00:41:07,980
 there's no speed-up over based of a

1728
00:41:06,589 --> 00:41:07,980
 

1729
00:41:06,599 --> 00:41:09,749
 random search because you need to

1730
00:41:07,970 --> 00:41:09,749
 

1731
00:41:07,980 --> 00:41:12,480
 explore the space first in order to

1732
00:41:09,739 --> 00:41:12,480
 

1733
00:41:09,749 --> 00:41:14,400
 figure out where the good points lie but

1734
00:41:12,470 --> 00:41:14,400
 

1735
00:41:12,480 --> 00:41:16,710
 given enough time you actually converge

1736
00:41:14,390 --> 00:41:16,710
 

1737
00:41:14,400 --> 00:41:18,269
 faster to do to the optimal you can

1738
00:41:16,700 --> 00:41:18,269
 

1739
00:41:16,710 --> 00:41:20,630
 speed this up by doing matter learning

1740
00:41:18,259 --> 00:41:20,630
 

1741
00:41:18,269 --> 00:41:23,130
 and you're keen we'll talk about that

1742
00:41:20,620 --> 00:41:23,130
 

1743
00:41:20,630 --> 00:41:25,589
 but if you don't have matter learning if

1744
00:41:23,120 --> 00:41:25,589
 

1745
00:41:23,130 --> 00:41:26,759
 you learn from scratch then well it's

1746
00:41:25,579 --> 00:41:26,759
 

1747
00:41:25,589 --> 00:41:29,489
 not going to be better than random

1748
00:41:26,749 --> 00:41:29,489
 

1749
00:41:26,759 --> 00:41:31,440
 search to start with but using multi

1750
00:41:29,479 --> 00:41:31,440
 

1751
00:41:29,489 --> 00:41:32,700
 fidelity's you can actually make it

1752
00:41:31,430 --> 00:41:32,700
 

1753
00:41:31,440 --> 00:41:36,410
 better than random search in the

1754
00:41:32,690 --> 00:41:36,410
 

1755
00:41:32,700 --> 00:41:36,410
 beginning and also in

1756
00:41:36,640 --> 00:41:36,640
 

1757
00:41:36,650 --> 00:41:43,349
 yeah so this barb approach has almost

1758
00:41:41,029 --> 00:41:43,349
 

1759
00:41:41,039 --> 00:41:46,920
 linear speed ups by parallelization so

1760
00:41:43,339 --> 00:41:46,920
 

1761
00:41:43,349 --> 00:41:50,430
 that is also not a problem and that is

1762
00:41:46,910 --> 00:41:50,430
 

1763
00:41:46,920 --> 00:41:52,349
 why for practitioners right now if you

1764
00:41:50,420 --> 00:41:52,349
 

1765
00:41:50,430 --> 00:41:53,880
 ask which tool should I use I would

1766
00:41:52,339 --> 00:41:53,880
 

1767
00:41:52,349 --> 00:41:56,369
 recommend if you have multiple

1768
00:41:53,870 --> 00:41:56,369
 

1769
00:41:53,880 --> 00:41:59,009
 fidelities I would recommend Bob because

1770
00:41:56,359 --> 00:41:59,009
 

1771
00:41:56,369 --> 00:42:00,809
 it's it does combine the advantages of

1772
00:41:58,999 --> 00:42:00,809
 

1773
00:41:59,009 --> 00:42:03,029
 TPE and hyper band it's kind of this

1774
00:42:00,799 --> 00:42:03,029
 

1775
00:42:00,809 --> 00:42:05,990
 this tool that if you don't want to

1776
00:42:03,019 --> 00:42:05,990
 

1777
00:42:03,029 --> 00:42:08,309
 think about it it would work quite well

1778
00:42:05,980 --> 00:42:08,309
 

1779
00:42:05,990 --> 00:42:11,549
 other multi fidelity based optimization

1780
00:42:08,299 --> 00:42:11,549
 

1781
00:42:08,309 --> 00:42:14,249
 methods might also work well if you fit

1782
00:42:11,539 --> 00:42:14,249
 

1783
00:42:11,549 --> 00:42:16,589
 the kernel correctly if you don't have

1784
00:42:14,239 --> 00:42:16,589
 

1785
00:42:14,249 --> 00:42:19,230
 access to multiple Fidelity's I would

1786
00:42:16,579 --> 00:42:19,230
 

1787
00:42:16,589 --> 00:42:21,119
 still recommend if you have low

1788
00:42:19,220 --> 00:42:21,119
 

1789
00:42:19,230 --> 00:42:22,890
 dimensional continuous basis then there

1790
00:42:21,109 --> 00:42:22,890
 

1791
00:42:21,119 --> 00:42:25,380
 based optimization with gossip processes

1792
00:42:22,880 --> 00:42:25,380
 

1793
00:42:22,890 --> 00:42:27,180
 for example spearmint if you have high

1794
00:42:25,370 --> 00:42:27,180
 

1795
00:42:25,380 --> 00:42:29,069
 dimensional categorical conditional

1796
00:42:27,170 --> 00:42:29,069
 

1797
00:42:27,180 --> 00:42:32,430
 spaces I'd record my based optimization

1798
00:42:29,059 --> 00:42:32,430
 

1799
00:42:29,069 --> 00:42:34,380
 with random forests as in smack or this

1800
00:42:32,420 --> 00:42:34,380
 

1801
00:42:32,430 --> 00:42:36,329
 TPE method based on kernel density

1802
00:42:34,370 --> 00:42:36,329
 

1803
00:42:34,380 --> 00:42:37,740
 estimates and if you're purely

1804
00:42:36,319 --> 00:42:37,740
 

1805
00:42:36,329 --> 00:42:40,380
 continuous and you have a relatively

1806
00:42:37,730 --> 00:42:40,380
 

1807
00:42:37,740 --> 00:42:43,650
 large budget and maybe want to paralyze

1808
00:42:40,370 --> 00:42:43,650
 

1809
00:42:40,380 --> 00:42:48,089
 really nicely then CMAs can actually be

1810
00:42:43,640 --> 00:42:48,089
 

1811
00:42:43,650 --> 00:42:49,890
 very competitive there is several open

1812
00:42:48,079 --> 00:42:49,890
 

1813
00:42:48,089 --> 00:42:52,289
 source tools that are based on hyper

1814
00:42:49,880 --> 00:42:52,289
 

1815
00:42:49,890 --> 00:42:54,989
 permit optimization so what Rebecca

1816
00:42:52,279 --> 00:42:54,989
 

1817
00:42:52,289 --> 00:42:58,259
 already mentioned this is based on the

1818
00:42:54,979 --> 00:42:58,259
 

1819
00:42:54,989 --> 00:43:00,059
 bechamel on e framework in Java and

1820
00:42:58,249 --> 00:43:00,059
 

1821
00:42:58,259 --> 00:43:01,529
 smack as an optimizer there is hyper

1822
00:43:00,049 --> 00:43:01,529
 

1823
00:43:00,059 --> 00:43:02,670
 objects can learn which is based on Sai

1824
00:43:01,519 --> 00:43:02,670
 

1825
00:43:01,529 --> 00:43:05,279
 could learn and TPE

1826
00:43:02,660 --> 00:43:05,279
 

1827
00:43:02,670 --> 00:43:07,380
 this author SQL learn also based on

1828
00:43:05,269 --> 00:43:07,380
 

1829
00:43:05,279 --> 00:43:10,019
 scikit-learn and smack or in a newer

1830
00:43:07,370 --> 00:43:10,019
 

1831
00:43:07,380 --> 00:43:11,910
 version on Bob this also uses meta

1832
00:43:10,009 --> 00:43:11,910
 

1833
00:43:10,019 --> 00:43:13,799
 learning in order to jumpstart the

1834
00:43:11,900 --> 00:43:13,799
 

1835
00:43:11,910 --> 00:43:15,989
 optimization and post or assembling and

1836
00:43:13,789 --> 00:43:15,989
 

1837
00:43:13,799 --> 00:43:18,299
 it won the automail competitions well

1838
00:43:15,979 --> 00:43:18,299
 

1839
00:43:15,989 --> 00:43:20,789
 the first one and the second one there

1840
00:43:18,289 --> 00:43:20,789
 

1841
00:43:18,299 --> 00:43:22,549
 is a third one going on right now and

1842
00:43:20,779 --> 00:43:22,549
 

1843
00:43:20,789 --> 00:43:24,690
 there's going to be a workshop about

1844
00:43:22,539 --> 00:43:24,690
 

1845
00:43:22,549 --> 00:43:29,759
 challenges of machine learning where

1846
00:43:24,680 --> 00:43:29,759
 

1847
00:43:24,690 --> 00:43:31,710
 that's going to be discussed there is a

1848
00:43:29,749 --> 00:43:31,710
 

1849
00:43:29,759 --> 00:43:33,420
 teapot which is also based since I could

1850
00:43:31,700 --> 00:43:33,420
 

1851
00:43:31,710 --> 00:43:34,859
 learn and evolutionary algorithms and it

1852
00:43:33,410 --> 00:43:34,859
 

1853
00:43:33,420 --> 00:43:38,369
 focuses more on pipeline construction

1854
00:43:34,849 --> 00:43:38,369
 

1855
00:43:34,859 --> 00:43:40,019
 and finally there's h2o auto ml which is

1856
00:43:38,359 --> 00:43:40,019
 

1857
00:43:38,369 --> 00:43:41,630
 based mostly on random search but also

1858
00:43:40,009 --> 00:43:41,630
 

1859
00:43:40,019 --> 00:43:45,859
 stacking and there

1860
00:43:41,620 --> 00:43:45,859
 

1861
00:43:41,630 --> 00:43:48,039
 efficient of limitations I would like to

1862
00:43:45,849 --> 00:43:48,039
 

1863
00:43:45,859 --> 00:43:51,890
 mention that auto ml can be viewed as a

1864
00:43:48,029 --> 00:43:51,890
 

1865
00:43:48,039 --> 00:43:52,400
 democratization of machine learning Auto

1866
00:43:51,880 --> 00:43:52,400
 

1867
00:43:51,890 --> 00:43:54,109
 s :

1868
00:43:52,390 --> 00:43:54,109
 

1869
00:43:52,400 --> 00:43:56,720
 actually didn't only win against other

1870
00:43:54,099 --> 00:43:56,720
 

1871
00:43:54,109 --> 00:44:00,099
 auto ml systems it also won against

1872
00:43:56,710 --> 00:44:00,099
 

1873
00:43:56,720 --> 00:44:02,299
 human expert in a Kegel like competition

1874
00:44:00,089 --> 00:44:02,299
 

1875
00:44:00,099 --> 00:44:05,299
 particularly perform better than up to

1876
00:44:02,289 --> 00:44:05,299
 

1877
00:44:02,299 --> 00:44:09,049
 130 different teams and it's really easy

1878
00:44:05,289 --> 00:44:09,049
 

1879
00:44:05,299 --> 00:44:12,589
 to use it's BSD license on its a plugin

1880
00:44:09,039 --> 00:44:12,589
 

1881
00:44:09,049 --> 00:44:15,890
 estimator for a second and it's already

1882
00:44:12,579 --> 00:44:15,890
 

1883
00:44:12,589 --> 00:44:17,720
 has a lot of community adoption so this

1884
00:44:15,880 --> 00:44:17,720
 

1885
00:44:15,890 --> 00:44:19,730
 really opens the door kind of for

1886
00:44:17,710 --> 00:44:19,730
 

1887
00:44:17,720 --> 00:44:22,009
 everyone to use effective machine

1888
00:44:19,720 --> 00:44:22,009
 

1889
00:44:19,730 --> 00:44:25,220
 learning that can in some cases actually

1890
00:44:21,999 --> 00:44:25,220
 

1891
00:44:22,009 --> 00:44:26,660
 work better than human expert if the

1892
00:44:25,210 --> 00:44:26,660
 

1893
00:44:25,220 --> 00:44:28,880
 human experts don't have domain

1894
00:44:26,650 --> 00:44:28,880
 

1895
00:44:26,660 --> 00:44:30,289
 knowledge domain knowledge kind of beats

1896
00:44:28,870 --> 00:44:30,289
 

1897
00:44:28,880 --> 00:44:33,019
 everything if you can generate better

1898
00:44:30,279 --> 00:44:33,019
 

1899
00:44:30,289 --> 00:44:37,160
 features then that is of course very

1900
00:44:33,009 --> 00:44:37,160
 

1901
00:44:33,019 --> 00:44:39,799
 helpful one quick example application of

1902
00:44:37,150 --> 00:44:39,799
 

1903
00:44:37,160 --> 00:44:42,170
 auto SQL learn so in a collaboration

1904
00:44:39,789 --> 00:44:42,170
 

1905
00:44:39,799 --> 00:44:43,160
 with Freud works robotics group they

1906
00:44:42,160 --> 00:44:43,160
 

1907
00:44:42,170 --> 00:44:44,900
 were interested in this binary

1908
00:44:43,150 --> 00:44:44,900
 

1909
00:44:43,160 --> 00:44:46,430
 classification task the robot had an

1910
00:44:44,890 --> 00:44:46,430
 

1911
00:44:44,900 --> 00:44:47,990
 object in its hand and wanted to place

1912
00:44:46,420 --> 00:44:47,990
 

1913
00:44:46,430 --> 00:44:51,769
 it down and was wondering will the

1914
00:44:47,980 --> 00:44:51,769
 

1915
00:44:47,990 --> 00:44:53,690
 object fall over or not we use the data

1916
00:44:51,759 --> 00:44:53,690
 

1917
00:44:51,769 --> 00:44:55,460
 set of 30,000 data points for which a

1918
00:44:53,680 --> 00:44:55,460
 

1919
00:44:53,690 --> 00:44:58,609
 bachelor student actually manually

1920
00:44:55,450 --> 00:44:58,609
 

1921
00:44:55,460 --> 00:45:02,089
 created 50 features and then back in the

1922
00:44:58,599 --> 00:45:02,089
 

1923
00:44:58,609 --> 00:45:04,369
 day this was 2015 he was done cafe and

1924
00:45:02,079 --> 00:45:04,369
 

1925
00:45:02,089 --> 00:45:07,190
 spent about three months to get to an

1926
00:45:04,359 --> 00:45:07,190
 

1927
00:45:04,369 --> 00:45:08,930
 error rate of two percent which was a

1928
00:45:07,180 --> 00:45:08,930
 

1929
00:45:07,190 --> 00:45:11,569
 very nice bachelor thesis everybody was

1930
00:45:08,920 --> 00:45:11,569
 

1931
00:45:08,930 --> 00:45:13,700
 very happy and after that water s

1932
00:45:11,559 --> 00:45:13,700
 

1933
00:45:11,569 --> 00:45:15,109
 coloring are completed and well we

1934
00:45:13,690 --> 00:45:15,109
 

1935
00:45:13,700 --> 00:45:18,230
 actually got to zero point six percent

1936
00:45:15,099 --> 00:45:18,230
 

1937
00:45:15,109 --> 00:45:20,509
 error rate in 30 minutes this is not to

1938
00:45:18,220 --> 00:45:20,509
 

1939
00:45:18,230 --> 00:45:22,069
 dis the bachelor student he actually

1940
00:45:20,499 --> 00:45:22,069
 

1941
00:45:20,509 --> 00:45:23,720
 generated these features without which

1942
00:45:22,059 --> 00:45:23,720
 

1943
00:45:22,069 --> 00:45:26,839
 auto rescue learned wouldn't have done

1944
00:45:23,710 --> 00:45:26,839
 

1945
00:45:23,720 --> 00:45:29,930
 anything but it goes to show that deep

1946
00:45:26,829 --> 00:45:29,930
 

1947
00:45:26,839 --> 00:45:32,210
 learning if you don't have expertise and

1948
00:45:29,920 --> 00:45:32,210
 

1949
00:45:29,930 --> 00:45:34,640
 if you haven't played with this a lot

1950
00:45:32,200 --> 00:45:34,640
 

1951
00:45:32,210 --> 00:45:37,430
 before is not necessarily the best tool

1952
00:45:34,630 --> 00:45:37,430
 

1953
00:45:34,640 --> 00:45:39,859
 for a given application so you should

1954
00:45:37,420 --> 00:45:39,859
 

1955
00:45:37,430 --> 00:45:42,789
 also look at simple baselines in

1956
00:45:39,849 --> 00:45:42,789
 

1957
00:45:39,859 --> 00:45:45,319
 particular if you have feature rise data

1958
00:45:42,779 --> 00:45:45,319
 

1959
00:45:42,789 --> 00:45:48,230
 again not to dis deep learning all the

1960
00:45:45,309 --> 00:45:48,230
 

1961
00:45:45,319 --> 00:45:49,430
 rest will be deep learning because deep

1962
00:45:48,220 --> 00:45:49,430
 

1963
00:45:48,230 --> 00:45:52,279
 learning is of course awesome and can

1964
00:45:49,420 --> 00:45:52,279
 

1965
00:45:49,430 --> 00:45:54,559
 generate abstract representations of the

1966
00:45:52,269 --> 00:45:54,559
 

1967
00:45:52,279 --> 00:45:55,250
 data from raw features so the second

1968
00:45:54,549 --> 00:45:55,250
 

1969
00:45:54,559 --> 00:45:59,600
 part will be on your

1970
00:45:55,240 --> 00:45:59,600
 

1971
00:45:55,250 --> 00:46:03,590
 architecture search this is based on

1972
00:45:59,590 --> 00:46:03,590
 

1973
00:45:59,600 --> 00:46:05,450
 this review article I wrote with my PhD

1974
00:46:03,580 --> 00:46:05,450
 

1975
00:46:03,590 --> 00:46:07,700
 student Thomas L scheme and a research

1976
00:46:05,440 --> 00:46:07,700
 

1977
00:46:05,450 --> 00:46:10,940
 scientist at Bosch yan Heinrich Medicine

1978
00:46:07,690 --> 00:46:10,940
 

1979
00:46:07,700 --> 00:46:14,750
 and this is also a chapter three of the

1980
00:46:10,930 --> 00:46:14,750
 

1981
00:46:10,940 --> 00:46:17,630
 auto ml book so let's first talk about

1982
00:46:14,740 --> 00:46:17,630
 

1983
00:46:14,750 --> 00:46:20,540
 the search based design the simplest

1984
00:46:17,620 --> 00:46:20,540
 

1985
00:46:17,630 --> 00:46:23,570
 search based design is a chain

1986
00:46:20,530 --> 00:46:23,570
 

1987
00:46:20,540 --> 00:46:26,240
 structured space so you just have to

1988
00:46:23,560 --> 00:46:26,240
 

1989
00:46:23,570 --> 00:46:28,010
 decide how many layers do I want and for

1990
00:46:26,230 --> 00:46:28,010
 

1991
00:46:26,240 --> 00:46:29,060
 each layer what is the type of this

1992
00:46:28,000 --> 00:46:29,060
 

1993
00:46:28,010 --> 00:46:31,160
 layer is this going to be a

1994
00:46:29,050 --> 00:46:31,160
 

1995
00:46:29,060 --> 00:46:34,790
 convolutional layer max pooling fully

1996
00:46:31,150 --> 00:46:34,790
 

1997
00:46:31,160 --> 00:46:37,190
 connected etc there was historically the

1998
00:46:34,780 --> 00:46:37,190
 

1999
00:46:34,790 --> 00:46:39,430
 first type of space people looked at but

2000
00:46:37,180 --> 00:46:39,430
 

2001
00:46:37,190 --> 00:46:42,140
 then over time people look more at

2002
00:46:39,420 --> 00:46:42,140
 

2003
00:46:39,430 --> 00:46:44,750
 complex spaces with multiple branches

2004
00:46:42,130 --> 00:46:44,750
 

2005
00:46:42,140 --> 00:46:49,370
 and skip connections I'm inspired by

2006
00:46:44,740 --> 00:46:49,370
 

2007
00:46:44,750 --> 00:46:52,790
 resonates etc since last year the types

2008
00:46:49,360 --> 00:46:52,790
 

2009
00:46:49,370 --> 00:46:55,100
 of search spaces pretty much everyone is

2010
00:46:52,780 --> 00:46:55,100
 

2011
00:46:52,790 --> 00:46:59,590
 using is is now actually a cell search

2012
00:46:55,090 --> 00:46:59,590
 

2013
00:46:55,100 --> 00:47:03,050
 space in which you you parameterize a

2014
00:46:59,580 --> 00:47:03,050
 

2015
00:46:59,590 --> 00:47:05,960
 building block that takes some input and

2016
00:47:03,040 --> 00:47:05,960
 

2017
00:47:03,050 --> 00:47:08,750
 deal some output and then you stack

2018
00:47:05,950 --> 00:47:08,750
 

2019
00:47:05,960 --> 00:47:11,320
 these building blocks together very much

2020
00:47:08,740 --> 00:47:11,320
 

2021
00:47:08,750 --> 00:47:16,100
 like a residual networks or Inception

2022
00:47:11,310 --> 00:47:16,100
 

2023
00:47:11,320 --> 00:47:19,780
 networks typically people have two

2024
00:47:16,090 --> 00:47:19,780
 

2025
00:47:16,100 --> 00:47:22,700
 different cells one regular cell and one

2026
00:47:19,770 --> 00:47:22,700
 

2027
00:47:19,780 --> 00:47:27,500
 reduction cell that reduces a spatial

2028
00:47:22,690 --> 00:47:27,500
 

2029
00:47:22,700 --> 00:47:29,750
 resolution of the image and typically

2030
00:47:27,490 --> 00:47:29,750
 

2031
00:47:27,500 --> 00:47:31,850
 this macro architecture here is just a

2032
00:47:29,740 --> 00:47:31,850
 

2033
00:47:29,750 --> 00:47:35,360
 chain structure you could also think of

2034
00:47:31,840 --> 00:47:35,360
 

2035
00:47:31,850 --> 00:47:37,550
 doing a multi branch or skip connections

2036
00:47:35,350 --> 00:47:37,550
 

2037
00:47:35,360 --> 00:47:42,080
 in this macro architecture nobody really

2038
00:47:37,540 --> 00:47:42,080
 

2039
00:47:37,550 --> 00:47:44,000
 does that but the the the multi branch

2040
00:47:42,070 --> 00:47:44,000
 

2041
00:47:42,080 --> 00:47:47,360
 and the skip connections are in this

2042
00:47:43,990 --> 00:47:47,360
 

2043
00:47:44,000 --> 00:47:49,370
 individual cells that you stack these

2044
00:47:47,350 --> 00:47:49,370
 

2045
00:47:47,360 --> 00:47:53,990
 cell search spaces have two advantages

2046
00:47:49,360 --> 00:47:53,990
 

2047
00:47:49,370 --> 00:47:56,620
 the first one is that you can well they

2048
00:47:53,980 --> 00:47:56,620
 

2049
00:47:53,990 --> 00:47:59,060
 actually very small I'm compared to a

2050
00:47:56,610 --> 00:47:59,060
 

2051
00:47:56,620 --> 00:48:03,350
 general space where you parameterize

2052
00:47:59,050 --> 00:48:03,350
 

2053
00:47:59,060 --> 00:48:04,880
 this entire network here and therefore

2054
00:48:03,340 --> 00:48:04,880
 

2055
00:48:03,350 --> 00:48:07,250
 you can search them more efficiently and

2056
00:48:04,870 --> 00:48:07,250
 

2057
00:48:04,880 --> 00:48:08,600
 the second advantage is that if you have

2058
00:48:07,240 --> 00:48:08,600
 

2059
00:48:07,250 --> 00:48:11,720
 found a cell

2060
00:48:08,590 --> 00:48:11,720
 

2061
00:48:08,600 --> 00:48:13,910
 a small data set such as safar then you

2062
00:48:11,710 --> 00:48:13,910
 

2063
00:48:11,720 --> 00:48:16,190
 can actually use that cell on a larger

2064
00:48:13,900 --> 00:48:16,190
 

2065
00:48:13,910 --> 00:48:17,960
 data set such as image net and this

2066
00:48:16,180 --> 00:48:17,960
 

2067
00:48:16,190 --> 00:48:20,870
 actually does give you better

2068
00:48:17,950 --> 00:48:20,870
 

2069
00:48:17,960 --> 00:48:22,910
 performance and has has improved the

2070
00:48:20,860 --> 00:48:22,910
 

2071
00:48:20,870 --> 00:48:27,200
 state of the art for for image net and

2072
00:48:22,900 --> 00:48:27,200
 

2073
00:48:22,910 --> 00:48:28,730
 other large data sets and one

2074
00:48:27,190 --> 00:48:28,730
 

2075
00:48:27,200 --> 00:48:29,990
 disadvantage of using the cell search

2076
00:48:28,720 --> 00:48:29,990
 

2077
00:48:28,730 --> 00:48:32,450
 space is that well

2078
00:48:29,980 --> 00:48:32,450
 

2079
00:48:29,990 --> 00:48:34,790
 you're constrained to well you you will

2080
00:48:32,440 --> 00:48:34,790
 

2081
00:48:32,450 --> 00:48:37,220
 not find entirely new architectures you

2082
00:48:34,780 --> 00:48:37,220
 

2083
00:48:34,790 --> 00:48:38,690
 will only find cells that you will stack

2084
00:48:37,210 --> 00:48:38,690
 

2085
00:48:37,220 --> 00:48:40,310
 in this macro architecture because the

2086
00:48:38,680 --> 00:48:40,310
 

2087
00:48:38,690 --> 00:48:44,720
 macro architecture is defined manually

2088
00:48:40,300 --> 00:48:44,720
 

2089
00:48:40,310 --> 00:48:46,790
 so if you want to find yeah completely

2090
00:48:44,710 --> 00:48:46,790
 

2091
00:48:44,720 --> 00:48:50,660
 new architectures then one might want to

2092
00:48:46,780 --> 00:48:50,660
 

2093
00:48:46,790 --> 00:48:52,490
 go beyond the cell search space again so

2094
00:48:50,650 --> 00:48:52,490
 

2095
00:48:50,660 --> 00:48:54,620
 I promise that I'll talk about how you

2096
00:48:52,480 --> 00:48:54,620
 

2097
00:48:52,490 --> 00:48:56,750
 can phrase neural architecture searches

2098
00:48:54,610 --> 00:48:56,750
 

2099
00:48:54,620 --> 00:49:01,720
 have a permit optimization so here's a

2100
00:48:56,740 --> 00:49:01,720
 

2101
00:48:56,750 --> 00:49:04,130
 cell search space that I'm Sofitel used

2102
00:49:01,710 --> 00:49:04,130
 

2103
00:49:01,720 --> 00:49:06,920
 basically they have this recurrent

2104
00:49:04,120 --> 00:49:06,920
 

2105
00:49:04,130 --> 00:49:11,150
 neural network controller which would

2106
00:49:06,910 --> 00:49:11,150
 

2107
00:49:06,920 --> 00:49:14,180
 pick first well the first hidden state

2108
00:49:11,140 --> 00:49:14,180
 

2109
00:49:11,150 --> 00:49:15,770
 that goes into your cell then the second

2110
00:49:14,170 --> 00:49:15,770
 

2111
00:49:14,180 --> 00:49:18,950
 hidden state that goes into yourself

2112
00:49:15,760 --> 00:49:18,950
 

2113
00:49:15,770 --> 00:49:21,050
 then it would pick the operator to use

2114
00:49:18,940 --> 00:49:21,050
 

2115
00:49:18,950 --> 00:49:22,490
 on your first hidden state then the

2116
00:49:21,040 --> 00:49:22,490
 

2117
00:49:21,050 --> 00:49:25,100
 operator for your second hidden state

2118
00:49:22,480 --> 00:49:25,100
 

2119
00:49:22,490 --> 00:49:28,490
 and then a combination operator how do

2120
00:49:25,090 --> 00:49:28,490
 

2121
00:49:25,100 --> 00:49:30,920
 you combine the the results of your two

2122
00:49:28,480 --> 00:49:30,920
 

2123
00:49:28,490 --> 00:49:34,370
 branches and it could be just an

2124
00:49:30,910 --> 00:49:34,370
 

2125
00:49:30,920 --> 00:49:36,350
 addition or concatenation and so you end

2126
00:49:34,360 --> 00:49:36,350
 

2127
00:49:34,370 --> 00:49:39,320
 up with five categorical choices for

2128
00:49:36,340 --> 00:49:39,320
 

2129
00:49:36,350 --> 00:49:42,740
 each of these blocks the first category

2130
00:49:39,310 --> 00:49:42,740
 

2131
00:49:39,320 --> 00:49:44,780
 choices has has domain 0 to n minus 1 if

2132
00:49:42,730 --> 00:49:44,780
 

2133
00:49:42,740 --> 00:49:47,140
 you're in the end block because those

2134
00:49:44,770 --> 00:49:47,140
 

2135
00:49:44,780 --> 00:49:51,050
 are the hidden States you could pick

2136
00:49:47,130 --> 00:49:51,050
 

2137
00:49:47,140 --> 00:49:53,900
 this these categorical choices here have

2138
00:49:51,040 --> 00:49:53,900
 

2139
00:49:51,050 --> 00:49:56,600
 just different operations such as max

2140
00:49:53,890 --> 00:49:56,600
 

2141
00:49:53,900 --> 00:49:59,660
 pooling separable separable convolutions

2142
00:49:56,590 --> 00:49:59,660
 

2143
00:49:56,600 --> 00:50:02,630
 etc and this last category of choice has

2144
00:49:59,650 --> 00:50:02,630
 

2145
00:49:59,660 --> 00:50:04,190
 very few options so you have 5

2146
00:50:02,620 --> 00:50:04,190
 

2147
00:50:02,630 --> 00:50:07,850
 categorical choices for each of these

2148
00:50:04,180 --> 00:50:07,850
 

2149
00:50:04,190 --> 00:50:11,180
 blocks and you have B blocks with B

2150
00:50:07,840 --> 00:50:11,180
 

2151
00:50:07,850 --> 00:50:14,180
 typically being set to 5 and verse 5

2152
00:50:11,170 --> 00:50:14,180
 

2153
00:50:11,180 --> 00:50:16,700
 this gives you 25 hyper parameters that

2154
00:50:14,170 --> 00:50:16,700
 

2155
00:50:14,180 --> 00:50:18,410
 fully define the cell and there isn't

2156
00:50:16,690 --> 00:50:18,410
 

2157
00:50:16,700 --> 00:50:20,360
 even any conditionality going on here

2158
00:50:18,400 --> 00:50:20,360
 

2159
00:50:18,410 --> 00:50:23,360
 this is just categorical hyper part

2160
00:50:20,350 --> 00:50:23,360
 

2161
00:50:20,360 --> 00:50:27,110
 and standard hyper permit optimization

2162
00:50:23,350 --> 00:50:27,110
 

2163
00:50:23,360 --> 00:50:29,420
 methods can be used in the space if you

2164
00:50:27,100 --> 00:50:29,420
 

2165
00:50:27,110 --> 00:50:32,420
 have an unrestricted search space then

2166
00:50:29,410 --> 00:50:32,420
 

2167
00:50:29,420 --> 00:50:34,340
 you can still write that as a hyper

2168
00:50:32,410 --> 00:50:34,340
 

2169
00:50:32,420 --> 00:50:36,650
 parameter optimization problem but then

2170
00:50:34,330 --> 00:50:36,650
 

2171
00:50:34,340 --> 00:50:39,110
 you have conditionalities and you can

2172
00:50:36,640 --> 00:50:39,110
 

2173
00:50:36,650 --> 00:50:42,980
 only well you have to have a limitation

2174
00:50:39,100 --> 00:50:42,980
 

2175
00:50:39,110 --> 00:50:44,300
 of the maximum number of of layers so

2176
00:50:42,970 --> 00:50:44,300
 

2177
00:50:42,980 --> 00:50:46,160
 for example if you just have a chain

2178
00:50:44,290 --> 00:50:46,160
 

2179
00:50:44,300 --> 00:50:49,880
 structure at space but you're not gonna

2180
00:50:46,150 --> 00:50:49,880
 

2181
00:50:46,160 --> 00:50:51,320
 restrict this then well as an hyper

2182
00:50:49,870 --> 00:50:51,320
 

2183
00:50:49,880 --> 00:50:53,330
 permit optimization problem you still

2184
00:50:51,310 --> 00:50:53,330
 

2185
00:50:51,320 --> 00:50:56,450
 need to say well a maximum number of

2186
00:50:53,320 --> 00:50:56,450
 

2187
00:50:53,330 --> 00:50:59,720
 layers such as 10,000 or so and each of

2188
00:50:56,440 --> 00:50:59,720
 

2189
00:50:56,450 --> 00:51:01,280
 the layers that is not active well so

2190
00:50:59,710 --> 00:51:01,280
 

2191
00:50:59,720 --> 00:51:03,890
 you have a top-level hyper parameter

2192
00:51:01,270 --> 00:51:03,890
 

2193
00:51:01,280 --> 00:51:05,210
 that's a number of layers and everything

2194
00:51:03,880 --> 00:51:05,210
 

2195
00:51:03,890 --> 00:51:08,960
 that's larger than that all the hyper

2196
00:51:05,200 --> 00:51:08,960
 

2197
00:51:05,210 --> 00:51:10,700
 parameters are just not active all right

2198
00:51:08,950 --> 00:51:10,700
 

2199
00:51:08,960 --> 00:51:12,050
 so that's how you can write a neural

2200
00:51:10,690 --> 00:51:12,050
 

2201
00:51:10,700 --> 00:51:15,830
 architecture searches have a prominent

2202
00:51:12,040 --> 00:51:15,830
 

2203
00:51:12,050 --> 00:51:18,080
 optimization let's next talk about some

2204
00:51:15,820 --> 00:51:18,080
 

2205
00:51:15,830 --> 00:51:23,870
 blackbox optimization methods for neural

2206
00:51:18,070 --> 00:51:23,870
 

2207
00:51:18,080 --> 00:51:25,520
 architecture search the method that that

2208
00:51:23,860 --> 00:51:25,520
 

2209
00:51:23,870 --> 00:51:28,460
 really popularized neural architecture

2210
00:51:25,510 --> 00:51:28,460
 

2211
00:51:25,520 --> 00:51:29,810
 search in the community versus neural

2212
00:51:28,450 --> 00:51:29,810
 

2213
00:51:28,460 --> 00:51:33,920
 architecture search was reinforcement

2214
00:51:29,800 --> 00:51:33,920
 

2215
00:51:29,810 --> 00:51:35,810
 learning by berestov and shortly what

2216
00:51:33,910 --> 00:51:35,810
 

2217
00:51:33,920 --> 00:51:39,550
 they used is a recurrent neural network

2218
00:51:35,800 --> 00:51:39,550
 

2219
00:51:35,810 --> 00:51:43,190
 controller to sample the architecture

2220
00:51:39,540 --> 00:51:43,190
 

2221
00:51:39,550 --> 00:51:45,770
 individual blocks at a time and then

2222
00:51:43,180 --> 00:51:45,770
 

2223
00:51:43,190 --> 00:51:47,390
 they train the child network with that

2224
00:51:45,760 --> 00:51:47,390
 

2225
00:51:45,770 --> 00:51:50,380
 architecture to get some accuracy and

2226
00:51:47,380 --> 00:51:50,380
 

2227
00:51:47,390 --> 00:51:54,320
 used reinforced in order to train the

2228
00:51:50,370 --> 00:51:54,320
 

2229
00:51:50,380 --> 00:51:55,880
 parameters of the controller this became

2230
00:51:54,310 --> 00:51:55,880
 

2231
00:51:54,320 --> 00:51:57,110
 really popular because they were the

2232
00:51:55,870 --> 00:51:57,110
 

2233
00:51:55,880 --> 00:52:00,710
 first to achieve state-of-the-art

2234
00:51:57,100 --> 00:52:00,710
 

2235
00:51:57,110 --> 00:52:02,350
 results on C 410 and also on a language

2236
00:52:00,700 --> 00:52:02,350
 

2237
00:52:00,710 --> 00:52:04,220
 data set penn treebank

2238
00:52:02,340 --> 00:52:04,220
 

2239
00:52:02,350 --> 00:52:07,790
 however they use very large

2240
00:52:04,210 --> 00:52:07,790
 

2241
00:52:04,220 --> 00:52:09,310
 computational resources 800 GPUs for

2242
00:52:07,780 --> 00:52:09,310
 

2243
00:52:07,790 --> 00:52:12,860
 three to four weeks

2244
00:52:09,300 --> 00:52:12,860
 

2245
00:52:09,310 --> 00:52:15,680
 12800 architectures evaluated etc that

2246
00:52:12,850 --> 00:52:15,680
 

2247
00:52:12,860 --> 00:52:19,670
 is why recent work really looks at going

2248
00:52:15,670 --> 00:52:19,670
 

2249
00:52:15,680 --> 00:52:21,200
 beyond the black box function but but if

2250
00:52:19,660 --> 00:52:21,200
 

2251
00:52:19,670 --> 00:52:23,420
 you have these large computational

2252
00:52:21,190 --> 00:52:23,420
 

2253
00:52:21,200 --> 00:52:25,100
 resources then this was the first paper

2254
00:52:23,410 --> 00:52:25,100
 

2255
00:52:23,420 --> 00:52:26,450
 that could actually really achieve new

2256
00:52:25,090 --> 00:52:26,450
 

2257
00:52:25,100 --> 00:52:30,290
 state-of-the-art performance with an

2258
00:52:26,440 --> 00:52:30,290
 

2259
00:52:26,450 --> 00:52:32,330
 automated pipeline there is lots of

2260
00:52:30,280 --> 00:52:32,330
 

2261
00:52:30,290 --> 00:52:33,490
 other work on blackboard optimization

2262
00:52:32,320 --> 00:52:33,490
 

2263
00:52:32,330 --> 00:52:35,490
 for example new revolution

2264
00:52:33,480 --> 00:52:35,490
 

2265
00:52:33,490 --> 00:52:38,080
 algorithms actually go back to the 1990s

2266
00:52:35,480 --> 00:52:38,080
 

2267
00:52:35,490 --> 00:52:40,030
 typically what they did is to optimize

2268
00:52:38,070 --> 00:52:40,030
 

2269
00:52:38,080 --> 00:52:42,609
 both the architecture and the weights

2270
00:52:40,020 --> 00:52:42,609
 

2271
00:52:40,030 --> 00:52:44,830
 with evolutionary methods and that

2272
00:52:42,599 --> 00:52:44,830
 

2273
00:52:42,609 --> 00:52:46,810
 doesn't scale to very large networks as

2274
00:52:44,820 --> 00:52:46,810
 

2275
00:52:44,830 --> 00:52:49,930
 well as stochastic gradient descent

2276
00:52:46,800 --> 00:52:49,930
 

2277
00:52:46,810 --> 00:52:51,849
 which is why more recent methods based

2278
00:52:49,920 --> 00:52:51,849
 

2279
00:52:49,930 --> 00:52:53,470
 on evolution actually only use evolution

2280
00:52:51,839 --> 00:52:53,470
 

2281
00:52:51,849 --> 00:52:54,910
 in order to get you in architecture and

2282
00:52:53,460 --> 00:52:54,910
 

2283
00:52:53,470 --> 00:52:58,630
 then train it again with stochastic

2284
00:52:54,900 --> 00:52:58,630
 

2285
00:52:54,910 --> 00:53:00,460
 gradient descent and here you can see

2286
00:52:58,620 --> 00:53:00,460
 

2287
00:52:58,630 --> 00:53:02,170
 nicely that initially you could just

2288
00:53:00,450 --> 00:53:02,170
 

2289
00:53:00,460 --> 00:53:04,420
 start with some very simple architecture

2290
00:53:02,160 --> 00:53:04,420
 

2291
00:53:02,170 --> 00:53:06,280
 and then you evolve this over time to

2292
00:53:04,410 --> 00:53:06,280
 

2293
00:53:04,420 --> 00:53:09,580
 become more and more complex and better

2294
00:53:06,270 --> 00:53:09,580
 

2295
00:53:06,280 --> 00:53:11,710
 performing and these evolutionary

2296
00:53:09,570 --> 00:53:11,710
 

2297
00:53:09,580 --> 00:53:14,880
 strategies what evolutionary strategies

2298
00:53:11,700 --> 00:53:14,880
 

2299
00:53:11,710 --> 00:53:19,450
 evolutionary algorithms are actually the

2300
00:53:14,870 --> 00:53:19,450
 

2301
00:53:14,880 --> 00:53:23,560
 best-known approach on this one data set

2302
00:53:19,440 --> 00:53:23,560
 

2303
00:53:19,450 --> 00:53:26,890
 C 410 they were actually compared in a

2304
00:53:23,550 --> 00:53:26,890
 

2305
00:53:23,560 --> 00:53:30,270
 head-to-head comparison by koalas group

2306
00:53:26,880 --> 00:53:30,270
 

2307
00:53:26,890 --> 00:53:31,990
 and evolution actually did better than

2308
00:53:30,260 --> 00:53:31,990
 

2309
00:53:30,270 --> 00:53:34,390
 reinforcement learning and this

2310
00:53:31,980 --> 00:53:34,390
 

2311
00:53:31,990 --> 00:53:37,180
 evolution approach really just uses as

2312
00:53:34,380 --> 00:53:37,180
 

2313
00:53:34,390 --> 00:53:38,619
 fixed length search space there is no

2314
00:53:37,170 --> 00:53:38,619
 

2315
00:53:37,180 --> 00:53:40,119
 neural networks going on in the

2316
00:53:38,609 --> 00:53:40,119
 

2317
00:53:38,619 --> 00:53:43,930
 controller there is it's just a simple

2318
00:53:40,109 --> 00:53:43,930
 

2319
00:53:40,119 --> 00:53:45,730
 evolutionary algorithm picking these 50

2320
00:53:43,920 --> 00:53:45,730
 

2321
00:53:43,930 --> 00:53:48,160
 different hyper parameters so 50 because

2322
00:53:45,720 --> 00:53:48,160
 

2323
00:53:45,730 --> 00:53:52,869
 it's two cells each with 25 hyper

2324
00:53:48,150 --> 00:53:52,869
 

2325
00:53:48,160 --> 00:53:54,490
 parameters yeah another approach that

2326
00:53:52,859 --> 00:53:54,490
 

2327
00:53:52,869 --> 00:53:56,290
 has also been used for neural

2328
00:53:54,480 --> 00:53:56,290
 

2329
00:53:54,490 --> 00:54:00,640
 architecture search before it became

2330
00:53:56,280 --> 00:54:00,640
 

2331
00:53:56,290 --> 00:54:01,570
 mainstream is based optimization so back

2332
00:54:00,630 --> 00:54:01,570
 

2333
00:54:00,640 --> 00:54:04,630
 in 2013

2334
00:54:01,560 --> 00:54:04,630
 

2335
00:54:01,570 --> 00:54:06,250
 I'm James Burke's track she did this

2336
00:54:04,620 --> 00:54:06,250
 

2337
00:54:04,630 --> 00:54:09,190
 joint optimization of a computer vision

2338
00:54:06,240 --> 00:54:09,190
 

2339
00:54:06,250 --> 00:54:12,910
 pipeline was 238 hyper parameters using

2340
00:54:09,180 --> 00:54:12,910
 

2341
00:54:09,190 --> 00:54:15,280
 TPE and in 2016 we had auto net that

2342
00:54:12,900 --> 00:54:15,280
 

2343
00:54:12,910 --> 00:54:17,589
 actually was also part of the auto ml

2344
00:54:15,270 --> 00:54:17,589
 

2345
00:54:15,280 --> 00:54:19,390
 challenge and was the first Auto DL

2346
00:54:17,579 --> 00:54:19,390
 

2347
00:54:17,589 --> 00:54:24,000
 system to win a competition data set

2348
00:54:19,380 --> 00:54:24,000
 

2349
00:54:19,390 --> 00:54:27,280
 against human expert based optimization

2350
00:54:23,990 --> 00:54:27,280
 

2351
00:54:24,000 --> 00:54:28,570
 Wisconsin processes is a bit harder to

2352
00:54:27,270 --> 00:54:28,570
 

2353
00:54:27,280 --> 00:54:30,690
 use for neural architecture search

2354
00:54:28,560 --> 00:54:30,690
 

2355
00:54:28,570 --> 00:54:33,130
 because you need the right kernel and

2356
00:54:30,680 --> 00:54:33,130
 

2357
00:54:30,690 --> 00:54:35,530
 there are some kernels for neural

2358
00:54:33,120 --> 00:54:35,530
 

2359
00:54:33,130 --> 00:54:41,290
 architecture search already in 2013

2360
00:54:35,520 --> 00:54:41,290
 

2361
00:54:35,530 --> 00:54:45,030
 there was this arc kernel and in 2018

2362
00:54:41,280 --> 00:54:45,030
 

2363
00:54:41,290 --> 00:54:45,030
 there's a nice board kernel

2364
00:54:46,240 --> 00:54:46,240
 

2365
00:54:46,250 --> 00:54:51,170
 what related method to Basin

2366
00:54:49,660 --> 00:54:51,170
 

2367
00:54:49,670 --> 00:54:53,060
 optimization is sequential model based

2368
00:54:51,160 --> 00:54:53,060
 

2369
00:54:51,170 --> 00:54:56,620
 optimization which also uses a surrogate

2370
00:54:53,050 --> 00:54:56,620
 

2371
00:54:53,060 --> 00:54:59,210
 model in order to pick the the

2372
00:54:56,610 --> 00:54:59,210
 

2373
00:54:56,620 --> 00:55:01,400
 architecture to evaluate next and also

2374
00:54:59,200 --> 00:55:01,400
 

2375
00:54:59,210 --> 00:55:02,510
 yield it state-of-the-art results

2376
00:55:01,390 --> 00:55:02,510
 

2377
00:55:01,400 --> 00:55:06,860
 compared to reinforcement learning

2378
00:55:02,500 --> 00:55:06,860
 

2379
00:55:02,510 --> 00:55:08,960
 recently all right so that was based on

2380
00:55:06,850 --> 00:55:08,960
 

2381
00:55:06,860 --> 00:55:10,820
 black box optimization from your

2382
00:55:08,950 --> 00:55:10,820
 

2383
00:55:08,960 --> 00:55:13,100
 architecture search we already saw most

2384
00:55:10,810 --> 00:55:13,100
 

2385
00:55:10,820 --> 00:55:15,140
 of black box optimization for Hyper

2386
00:55:13,090 --> 00:55:15,140
 

2387
00:55:13,100 --> 00:55:18,260
 permit optimization anyways and now we

2388
00:55:15,130 --> 00:55:18,260
 

2389
00:55:15,140 --> 00:55:20,420
 look at going beyond the black box there

2390
00:55:18,250 --> 00:55:20,420
 

2391
00:55:18,260 --> 00:55:22,810
 is four different approaches again I

2392
00:55:20,410 --> 00:55:22,810
 

2393
00:55:20,420 --> 00:55:24,080
 will not talk about these last two

2394
00:55:22,800 --> 00:55:24,080
 

2395
00:55:22,810 --> 00:55:26,270
 meta-learning

2396
00:55:24,070 --> 00:55:26,270
 

2397
00:55:24,080 --> 00:55:27,560
 well I'll say two sentences if there's

2398
00:55:26,260 --> 00:55:27,560
 

2399
00:55:26,270 --> 00:55:31,040
 actually only one paper it's here at

2400
00:55:27,550 --> 00:55:31,040
 

2401
00:55:27,560 --> 00:55:32,840
 nibs that basically learn to controller

2402
00:55:31,030 --> 00:55:32,840
 

2403
00:55:31,040 --> 00:55:34,790
 across different data sets and then

2404
00:55:32,830 --> 00:55:34,790
 

2405
00:55:32,840 --> 00:55:36,890
 given a new data set has already a

2406
00:55:34,780 --> 00:55:36,890
 

2407
00:55:34,790 --> 00:55:41,330
 controller that's that's warm started

2408
00:55:36,880 --> 00:55:41,330
 

2409
00:55:36,890 --> 00:55:43,070
 and can find good architectures for this

2410
00:55:41,320 --> 00:55:43,070
 

2411
00:55:41,330 --> 00:55:45,530
 new data set faster than when learnt

2412
00:55:43,060 --> 00:55:45,530
 

2413
00:55:43,070 --> 00:55:48,850
 from scratch there's two papers using

2414
00:55:45,520 --> 00:55:48,850
 

2415
00:55:45,530 --> 00:55:49,930
 multi fidelity optimization and with Bob

2416
00:55:48,840 --> 00:55:49,930
 

2417
00:55:48,850 --> 00:55:52,910
[Music]

2418
00:55:49,920 --> 00:55:52,910
 

2419
00:55:49,930 --> 00:55:54,680
 yeah one just doing standard joint

2420
00:55:52,900 --> 00:55:54,680
 

2421
00:55:52,910 --> 00:55:57,830
 architecture search and hyper permit

2422
00:55:54,670 --> 00:55:57,830
 

2423
00:55:54,680 --> 00:56:00,380
 optimization and one doing the same in a

2424
00:55:57,820 --> 00:56:00,380
 

2425
00:55:57,830 --> 00:56:02,090
 reinforcement learning context where you

2426
00:56:00,370 --> 00:56:02,090
 

2427
00:56:00,380 --> 00:56:03,500
 optimize the policy network and

2428
00:56:02,080 --> 00:56:03,500
 

2429
00:56:02,090 --> 00:56:06,320
 parameters of the reinforcement learning

2430
00:56:03,490 --> 00:56:06,320
 

2431
00:56:03,500 --> 00:56:09,320
 algorithm etc all these three papers

2432
00:56:06,310 --> 00:56:09,320
 

2433
00:56:06,320 --> 00:56:11,150
 here don't do neural architecture search

2434
00:56:09,310 --> 00:56:11,150
 

2435
00:56:09,320 --> 00:56:12,920
 in this really big space but actually

2436
00:56:11,140 --> 00:56:12,920
 

2437
00:56:11,150 --> 00:56:15,170
 have only a couple of different options

2438
00:56:12,910 --> 00:56:15,170
 

2439
00:56:12,920 --> 00:56:17,150
 for the convolutional neural networks

2440
00:56:15,160 --> 00:56:17,150
 

2441
00:56:15,170 --> 00:56:21,130
 but also some hyper parameters that are

2442
00:56:17,140 --> 00:56:21,130
 

2443
00:56:17,150 --> 00:56:23,060
 optimized I will talk about these two

2444
00:56:21,120 --> 00:56:23,060
 

2445
00:56:21,130 --> 00:56:25,280
 approaches weight inheritance and

2446
00:56:23,050 --> 00:56:25,280
 

2447
00:56:23,060 --> 00:56:27,740
 network morphism and wage hearing and

2448
00:56:25,270 --> 00:56:27,740
 

2449
00:56:25,280 --> 00:56:29,690
 one-shot models because those actually

2450
00:56:27,730 --> 00:56:29,690
 

2451
00:56:27,740 --> 00:56:31,820
 we didn't see an hyper prominent

2452
00:56:29,680 --> 00:56:31,820
 

2453
00:56:29,690 --> 00:56:35,180
 optimization these are special to neural

2454
00:56:31,810 --> 00:56:35,180
 

2455
00:56:31,820 --> 00:56:37,370
 architecture search the first one is

2456
00:56:35,170 --> 00:56:37,370
 

2457
00:56:35,180 --> 00:56:39,350
 based on well network morphisms network

2458
00:56:37,360 --> 00:56:39,350
 

2459
00:56:37,370 --> 00:56:41,270
 morphisms are operators in your

2460
00:56:39,340 --> 00:56:41,270
 

2461
00:56:39,350 --> 00:56:43,130
 architecture space that change the

2462
00:56:41,260 --> 00:56:43,130
 

2463
00:56:41,270 --> 00:56:45,500
 network structure but not the model

2464
00:56:43,120 --> 00:56:45,500
 

2465
00:56:43,130 --> 00:56:48,050
 function so for example if you have a

2466
00:56:45,490 --> 00:56:48,050
 

2467
00:56:45,500 --> 00:56:49,730
 network and you add one layer that you

2468
00:56:48,040 --> 00:56:49,730
 

2469
00:56:48,050 --> 00:56:52,790
 initialized with an identity mapping

2470
00:56:49,720 --> 00:56:52,790
 

2471
00:56:49,730 --> 00:56:55,730
 then you have a new architecture but you

2472
00:56:52,780 --> 00:56:55,730
 

2473
00:56:52,790 --> 00:56:57,640
 have the same function and this allows

2474
00:56:55,720 --> 00:56:57,640
 

2475
00:56:55,730 --> 00:56:59,319
 for efficient moves

2476
00:56:57,630 --> 00:56:59,319
 

2477
00:56:57,640 --> 00:57:01,660
 architecture space so you have some

2478
00:56:59,309 --> 00:57:01,660
 

2479
00:56:59,319 --> 00:57:03,160
 architecture is a given performance you

2480
00:57:01,650 --> 00:57:03,160
 

2481
00:57:01,660 --> 00:57:05,980
 apply some network morphisms to for

2482
00:57:03,150 --> 00:57:05,980
 

2483
00:57:03,160 --> 00:57:07,779
 example add a layer or sorry extend a

2484
00:57:05,970 --> 00:57:07,779
 

2485
00:57:05,980 --> 00:57:11,769
 layer add a layer or add a script

2486
00:57:07,769 --> 00:57:11,769
 

2487
00:57:07,779 --> 00:57:14,529
 connection then you train for a little

2488
00:57:11,759 --> 00:57:14,529
 

2489
00:57:11,769 --> 00:57:17,049
 bit just a few epochs to optimize these

2490
00:57:14,519 --> 00:57:17,049
 

2491
00:57:14,529 --> 00:57:20,220
 new weights typically improve

2492
00:57:17,039 --> 00:57:20,220
 

2493
00:57:17,049 --> 00:57:22,529
 performance and then you pick the

2494
00:57:20,210 --> 00:57:22,529
 

2495
00:57:20,220 --> 00:57:25,569
 resulting model that's the best and

2496
00:57:22,519 --> 00:57:25,569
 

2497
00:57:22,529 --> 00:57:27,190
 iterate a very simple approach that

2498
00:57:25,559 --> 00:57:27,190
 

2499
00:57:25,569 --> 00:57:28,390
 actually yields a very efficient

2500
00:57:27,180 --> 00:57:28,390
 

2501
00:57:27,190 --> 00:57:31,000
 architecture search you can do

2502
00:57:28,380 --> 00:57:31,000
 

2503
00:57:28,390 --> 00:57:35,470
 architecture search in about a day on a

2504
00:57:30,990 --> 00:57:35,470
 

2505
00:57:31,000 --> 00:57:36,970
 GPU using this the second approach I

2506
00:57:35,460 --> 00:57:36,970
 

2507
00:57:35,470 --> 00:57:39,849
 want to talk about is weight sharing and

2508
00:57:36,960 --> 00:57:39,849
 

2509
00:57:36,970 --> 00:57:41,559
 one-shot models so in 2016 there was a

2510
00:57:39,839 --> 00:57:41,559
 

2511
00:57:39,849 --> 00:57:45,309
 paper on convolutional neural fabrics

2512
00:57:41,549 --> 00:57:45,309
 

2513
00:57:41,559 --> 00:57:47,319
 that basically embed an exponentially

2514
00:57:45,299 --> 00:57:47,319
 

2515
00:57:45,309 --> 00:57:50,140
 large number of architectures in this

2516
00:57:47,309 --> 00:57:50,140
 

2517
00:57:47,319 --> 00:57:54,010
 so-called fabric here this is a fabric

2518
00:57:50,130 --> 00:57:54,010
 

2519
00:57:50,140 --> 00:57:57,069
 and each path in this fabric is one

2520
00:57:54,000 --> 00:57:57,069
 

2521
00:57:54,010 --> 00:58:03,910
 neuron network so the idea is kind of

2522
00:57:57,059 --> 00:58:03,910
 

2523
00:57:57,069 --> 00:58:07,059
 similar to to dropout you train well

2524
00:58:03,900 --> 00:58:07,059
 

2525
00:58:03,910 --> 00:58:10,750
 actually not here yet so you trained us

2526
00:58:07,049 --> 00:58:10,750
 

2527
00:58:07,059 --> 00:58:14,980
 in your neural fabric you trained all of

2528
00:58:10,740 --> 00:58:14,980
 

2529
00:58:10,750 --> 00:58:18,789
 the all of the different paths at the

2530
00:58:14,970 --> 00:58:18,789
 

2531
00:58:14,980 --> 00:58:21,509
 same time and basically you get an

2532
00:58:18,779 --> 00:58:21,509
 

2533
00:58:18,789 --> 00:58:24,130
 exponential Samba of different

2534
00:58:21,499 --> 00:58:24,130
 

2535
00:58:21,509 --> 00:58:27,849
 architectures but there there wasn't any

2536
00:58:24,120 --> 00:58:27,849
 

2537
00:58:24,130 --> 00:58:30,609
 the relationship to dropout wasn't that

2538
00:58:27,839 --> 00:58:30,609
 

2539
00:58:27,849 --> 00:58:32,440
 clear here yet that became more clear in

2540
00:58:30,599 --> 00:58:32,440
 

2541
00:58:30,609 --> 00:58:34,180
 this paper simplifying one short

2542
00:58:32,430 --> 00:58:34,180
 

2543
00:58:32,440 --> 00:58:39,519
 architecture search which actually used

2544
00:58:34,170 --> 00:58:39,519
 

2545
00:58:34,180 --> 00:58:41,170
 path dropout in this well one-shot model

2546
00:58:39,509 --> 00:58:41,170
 

2547
00:58:39,519 --> 00:58:43,960
 they call this neural fabric the

2548
00:58:41,160 --> 00:58:43,960
 

2549
00:58:41,170 --> 00:58:45,819
 one-shot model and they use pass dropout

2550
00:58:43,950 --> 00:58:45,819
 

2551
00:58:43,960 --> 00:58:47,650
 to make sure that the individual models

2552
00:58:45,809 --> 00:58:47,650
 

2553
00:58:45,819 --> 00:58:49,690
 don't really rely on the other models

2554
00:58:47,640 --> 00:58:49,690
 

2555
00:58:47,650 --> 00:58:51,670
 that that you don't just do well as an

2556
00:58:49,680 --> 00:58:51,670
 

2557
00:58:49,690 --> 00:58:56,470
 ensemble but also that individual models

2558
00:58:51,660 --> 00:58:56,470
 

2559
00:58:51,670 --> 00:58:58,569
 do well there is another related

2560
00:58:56,460 --> 00:58:58,569
 

2561
00:58:56,470 --> 00:59:01,660
 approach that uses reinforcement

2562
00:58:58,559 --> 00:59:01,660
 

2563
00:58:58,569 --> 00:59:03,940
 learning also using this one-shot model

2564
00:59:01,650 --> 00:59:03,940
 

2565
00:59:01,660 --> 00:59:06,400
 and that samples patent one path at a

2566
00:59:03,930 --> 00:59:06,400
 

2567
00:59:03,940 --> 00:59:08,680
 time in that one short model trains it

2568
00:59:06,390 --> 00:59:08,680
 

2569
00:59:06,400 --> 00:59:11,360
 its weights and therefore also trains

2570
00:59:08,670 --> 00:59:11,360
 

2571
00:59:08,680 --> 00:59:16,580
 all the weights of all the other

2572
00:59:11,350 --> 00:59:16,580
 

2573
00:59:11,360 --> 00:59:18,650
 the chair some part of the path and

2574
00:59:16,570 --> 00:59:18,650
 

2575
00:59:16,580 --> 00:59:20,570
 there is one other approach that

2576
00:59:18,640 --> 00:59:20,570
 

2577
00:59:18,650 --> 00:59:23,720
 somewhat different for weight sharing

2578
00:59:20,560 --> 00:59:23,720
 

2579
00:59:20,570 --> 00:59:25,430
 this one trains a hyper network that

2580
00:59:23,710 --> 00:59:25,430
 

2581
00:59:23,720 --> 00:59:27,680
 generates the weights of different

2582
00:59:25,420 --> 00:59:27,680
 

2583
00:59:25,430 --> 00:59:29,960
 architectures so the architecture is an

2584
00:59:27,670 --> 00:59:29,960
 

2585
00:59:27,680 --> 00:59:32,390
 input to the network and the network

2586
00:59:29,950 --> 00:59:32,390
 

2587
00:59:29,960 --> 00:59:34,880
 outputs the parameters to use for this

2588
00:59:32,380 --> 00:59:34,880
 

2589
00:59:32,390 --> 00:59:36,470
 particular network and what's shared is

2590
00:59:34,870 --> 00:59:36,470
 

2591
00:59:34,880 --> 00:59:38,240
 well there's only one hyper network

2592
00:59:36,460 --> 00:59:38,240
 

2593
00:59:36,470 --> 00:59:39,230
 that's being used to generate the

2594
00:59:38,230 --> 00:59:39,230
 

2595
00:59:38,240 --> 00:59:40,960
 weights for all the different

2596
00:59:39,220 --> 00:59:40,960
 

2597
00:59:39,230 --> 00:59:44,500
 architectures so this can also be

2598
00:59:40,950 --> 00:59:44,500
 

2599
00:59:40,960 --> 00:59:47,750
 understood in this way to here in view

2600
00:59:44,490 --> 00:59:47,750
 

2601
00:59:44,500 --> 00:59:49,310
 one of the most currently most popular

2602
00:59:47,740 --> 00:59:49,310
 

2603
00:59:47,750 --> 00:59:50,930
 approaches for neural architecture

2604
00:59:49,300 --> 00:59:50,930
 

2605
00:59:49,310 --> 00:59:52,520
 search is Dart

2606
00:59:50,920 --> 00:59:52,520
 

2607
00:59:50,930 --> 00:59:54,920
 stands for differentiable neural

2608
00:59:52,510 --> 00:59:54,920
 

2609
00:59:52,520 --> 00:59:58,220
 architecture search and this relaxes a

2610
00:59:54,910 --> 00:59:58,220
 

2611
00:59:54,920 --> 01:00:03,110
 discrete math problem and to look at the

2612
00:59:58,210 --> 01:00:03,110
 

2613
00:59:58,220 --> 01:00:05,360
 one-shot model but for each of the for

2614
01:00:03,100 --> 01:00:05,360
 

2615
01:00:03,110 --> 01:00:10,940
 each of the operators you have a weight

2616
01:00:05,350 --> 01:00:10,940
 

2617
01:00:05,360 --> 01:00:15,320
 alpha that that you use to multiply the

2618
01:00:10,930 --> 01:00:15,320
 

2619
01:00:10,940 --> 01:00:18,590
 result of this operator so you have this

2620
01:00:15,310 --> 01:00:18,590
 

2621
01:00:15,320 --> 01:00:20,870
 cell architecture and you don't know

2622
01:00:18,580 --> 01:00:20,870
 

2623
01:00:18,590 --> 01:00:22,640
 from here to way to here which operator

2624
01:00:20,860 --> 01:00:22,640
 

2625
01:00:20,870 --> 01:00:25,250
 I should you use you could use three

2626
01:00:22,630 --> 01:00:25,250
 

2627
01:00:22,640 --> 01:00:27,860
 different ones and you initialize them

2628
01:00:25,240 --> 01:00:27,860
 

2629
01:00:25,250 --> 01:00:30,050
 uniformly giving them all the same

2630
01:00:27,850 --> 01:00:30,050
 

2631
01:00:27,860 --> 01:00:32,330
 weight and just add the result here and

2632
01:00:30,040 --> 01:00:32,330
 

2633
01:00:30,050 --> 01:00:34,700
 then you actually in the space you do

2634
01:00:32,320 --> 01:00:34,700
 

2635
01:00:32,330 --> 01:00:36,080
 gradient based optimization using a very

2636
01:00:34,690 --> 01:00:36,080
 

2637
01:00:34,700 --> 01:00:38,090
 similar approach as I mentioned before

2638
01:00:36,070 --> 01:00:38,090
 

2639
01:00:36,080 --> 01:00:39,770
 and have a problem at optimization by

2640
01:00:38,080 --> 01:00:39,770
 

2641
01:00:38,090 --> 01:00:41,540
 look at unit I'll to interleave

2642
01:00:39,760 --> 01:00:41,540
 

2643
01:00:39,770 --> 01:00:43,610
 optimization steps with respect to

2644
01:00:41,530 --> 01:00:43,610
 

2645
01:00:41,540 --> 01:00:45,620
 validation error of the architecture

2646
01:00:43,600 --> 01:00:45,620
 

2647
01:00:43,610 --> 01:00:48,380
 parameters and with respect to training

2648
01:00:45,610 --> 01:00:48,380
 

2649
01:00:45,620 --> 01:00:51,710
 error of the weights and then you end up

2650
01:00:48,370 --> 01:00:51,710
 

2651
01:00:48,380 --> 01:00:53,570
 with some architectural parameters that

2652
01:00:51,700 --> 01:00:53,570
 

2653
01:00:51,710 --> 01:00:55,760
 are much stronger than the others and in

2654
01:00:53,560 --> 01:00:55,760
 

2655
01:00:53,570 --> 01:01:00,070
 the end you just take an arc Max and

2656
01:00:55,750 --> 01:01:00,070
 

2657
01:00:55,760 --> 01:01:03,770
 take the the most promising architecture

2658
01:01:00,060 --> 01:01:03,770
 

2659
01:01:00,070 --> 01:01:05,450
 yeah and this is basically because you

2660
01:01:03,760 --> 01:01:05,450
 

2661
01:01:03,770 --> 01:01:08,270
 have these interleaved optimization

2662
01:01:05,440 --> 01:01:08,270
 

2663
01:01:05,450 --> 01:01:09,770
 steps of validation one step gradient

2664
01:01:08,260 --> 01:01:09,770
 

2665
01:01:08,270 --> 01:01:12,430
 step with respect to validation error

2666
01:01:09,760 --> 01:01:12,430
 

2667
01:01:09,770 --> 01:01:15,410
 one gradient step with respect to

2668
01:01:12,420 --> 01:01:15,410
 

2669
01:01:12,430 --> 01:01:18,700
 training or error you're only maybe a

2670
01:01:15,400 --> 01:01:18,700
 

2671
01:01:15,410 --> 01:01:21,020
 factor of two or three slower than a

2672
01:01:18,690 --> 01:01:21,020
 

2673
01:01:18,700 --> 01:01:21,720
 normal optimization of your training

2674
01:01:21,010 --> 01:01:21,720
 

2675
01:01:21,020 --> 01:01:25,680
 error

2676
01:01:21,710 --> 01:01:25,680
 

2677
01:01:21,720 --> 01:01:27,500
 and therefore you can now do on your

2678
01:01:25,670 --> 01:01:27,500
 

2679
01:01:25,680 --> 01:01:29,910
 electric just search very efficiently

2680
01:01:27,490 --> 01:01:29,910
 

2681
01:01:27,500 --> 01:01:31,560
 there is a lot of questions here like

2682
01:01:29,900 --> 01:01:31,560
 

2683
01:01:29,910 --> 01:01:34,020
 why does this work if you just inter

2684
01:01:31,550 --> 01:01:34,020
 

2685
01:01:31,560 --> 01:01:37,950
 leave these steps that are currently

2686
01:01:34,010 --> 01:01:37,950
 

2687
01:01:34,020 --> 01:01:39,599
 being being studied there's a lot of

2688
01:01:37,940 --> 01:01:39,599
 

2689
01:01:37,950 --> 01:01:42,090
 different promising works and our review

2690
01:01:39,589 --> 01:01:42,090
 

2691
01:01:39,599 --> 01:01:44,130
 for time reasons I'll just give you a

2692
01:01:42,080 --> 01:01:44,130
 

2693
01:01:42,090 --> 01:01:46,230
 very brief overview so there is a

2694
01:01:44,120 --> 01:01:46,230
 

2695
01:01:44,130 --> 01:01:47,580
 several iclear submissions that are

2696
01:01:46,220 --> 01:01:47,580
 

2697
01:01:46,230 --> 01:01:50,190
 actually following up on darts

2698
01:01:47,570 --> 01:01:50,190
 

2699
01:01:47,580 --> 01:01:53,940
 there's also interesting follow-up work

2700
01:01:50,180 --> 01:01:53,940
 

2701
01:01:50,190 --> 01:01:55,530
 on hyper networks and there's also an

2702
01:01:53,930 --> 01:01:55,530
 

2703
01:01:53,940 --> 01:01:56,940
 interesting line of work on multi

2704
01:01:55,520 --> 01:01:56,940
 

2705
01:01:55,530 --> 01:01:59,490
 objective optimization and neural

2706
01:01:56,930 --> 01:01:59,490
 

2707
01:01:56,940 --> 01:02:01,349
 architecture search - very explicitly

2708
01:01:59,480 --> 01:02:01,349
 

2709
01:01:59,490 --> 01:02:04,400
 give you neural networks that work well

2710
01:02:01,339 --> 01:02:04,400
 

2711
01:02:01,349 --> 01:02:07,050
 with resource constraints for example I

2712
01:02:04,390 --> 01:02:07,050
 

2713
01:02:04,400 --> 01:02:08,880
 do want to leave you with a couple of

2714
01:02:07,040 --> 01:02:08,880
 

2715
01:02:07,050 --> 01:02:10,770
 remarks on experimentation and neural

2716
01:02:08,870 --> 01:02:10,770
 

2717
01:02:08,880 --> 01:02:13,140
 architecture search so the final result

2718
01:02:10,760 --> 01:02:13,140
 

2719
01:02:10,770 --> 01:02:15,480
 of different methods is often quite

2720
01:02:13,130 --> 01:02:15,480
 

2721
01:02:13,140 --> 01:02:17,070
 incomparable due to a variety of

2722
01:02:15,470 --> 01:02:17,070
 

2723
01:02:15,480 --> 01:02:18,990
 different reasons namely different

2724
01:02:17,060 --> 01:02:18,990
 

2725
01:02:17,070 --> 01:02:20,430
 training pipelines are being used there

2726
01:02:18,980 --> 01:02:20,430
 

2727
01:02:18,990 --> 01:02:22,080
 is no source code available often

2728
01:02:20,420 --> 01:02:22,080
 

2729
01:02:20,430 --> 01:02:23,640
 there's different hyper parameter

2730
01:02:22,070 --> 01:02:23,640
 

2731
01:02:22,080 --> 01:02:26,130
 choices and there's different search

2732
01:02:23,630 --> 01:02:26,130
 

2733
01:02:23,640 --> 01:02:28,080
 faces and initial models and that that

2734
01:02:26,120 --> 01:02:28,080
 

2735
01:02:26,130 --> 01:02:31,560
 all makes it very hard to compare

2736
01:02:28,070 --> 01:02:31,560
 

2737
01:02:28,080 --> 01:02:32,820
 different different methods it's nice

2738
01:02:31,550 --> 01:02:32,820
 

2739
01:02:31,560 --> 01:02:34,650
 when people are release the final

2740
01:02:32,810 --> 01:02:34,650
 

2741
01:02:32,820 --> 01:02:36,390
 architectures but it doesn't help for

2742
01:02:34,640 --> 01:02:36,390
 

2743
01:02:34,650 --> 01:02:38,400
 comparing the different algorithms to

2744
01:02:36,380 --> 01:02:38,400
 

2745
01:02:36,390 --> 01:02:39,810
 each other for the different hyper

2746
01:02:38,390 --> 01:02:39,810
 

2747
01:02:38,400 --> 01:02:41,849
 parameter choices there's actually very

2748
01:02:39,800 --> 01:02:41,849
 

2749
01:02:39,810 --> 01:02:43,589
 different hyper parameters during the

2750
01:02:41,839 --> 01:02:43,589
 

2751
01:02:41,849 --> 01:02:47,190
 training and then during the final

2752
01:02:43,579 --> 01:02:47,190
 

2753
01:02:43,589 --> 01:02:48,780
 evaluation and it's quite important

2754
01:02:47,180 --> 01:02:48,780
 

2755
01:02:47,190 --> 01:02:50,849
 where these hyper parameters come from

2756
01:02:48,770 --> 01:02:50,849
 

2757
01:02:48,780 --> 01:02:54,450
 if you have to do hyper parameter

2758
01:02:50,839 --> 01:02:54,450
 

2759
01:02:50,849 --> 01:02:57,720
 optimization of your method in order to

2760
01:02:54,440 --> 01:02:57,720
 

2761
01:02:54,450 --> 01:02:59,609
 then do well then well it doesn't work

2762
01:02:57,710 --> 01:02:59,609
 

2763
01:02:57,720 --> 01:03:01,470
 really well as an auto ml system because

2764
01:02:59,599 --> 01:03:01,470
 

2765
01:02:59,609 --> 01:03:03,119
 that should just work off-the-shelf

2766
01:03:01,460 --> 01:03:03,119
 

2767
01:03:01,470 --> 01:03:06,690
 given a new data set and you shouldn't

2768
01:03:03,109 --> 01:03:06,690
 

2769
01:03:03,119 --> 01:03:08,070
 have to optimize it first and well it

2770
01:03:06,680 --> 01:03:08,070
 

2771
01:03:06,690 --> 01:03:09,839
 also matters whether you start from

2772
01:03:08,060 --> 01:03:09,839
 

2773
01:03:08,070 --> 01:03:11,640
 random or whether you already start from

2774
01:03:09,829 --> 01:03:11,640
 

2775
01:03:09,839 --> 01:03:15,510
 a state-of-the-art neural network and

2776
01:03:11,630 --> 01:03:15,510
 

2777
01:03:11,640 --> 01:03:17,160
 and only improve that locally so if you

2778
01:03:15,500 --> 01:03:17,160
 

2779
01:03:15,510 --> 01:03:19,680
 review neural architecture search papers

2780
01:03:17,150 --> 01:03:19,680
 

2781
01:03:17,160 --> 01:03:21,930
 I would I would ask you to maybe look

2782
01:03:19,670 --> 01:03:21,930
 

2783
01:03:19,680 --> 01:03:25,400
 beyond just the error numbers on C far

2784
01:03:21,920 --> 01:03:25,400
 

2785
01:03:21,930 --> 01:03:29,609
 but look at that the method itself and

2786
01:03:25,390 --> 01:03:29,609
 

2787
01:03:25,400 --> 01:03:32,550
 look ask for these details also we just

2788
01:03:29,599 --> 01:03:32,550
 

2789
01:03:29,609 --> 01:03:34,900
 need benchmarks that are not just C far

2790
01:03:32,540 --> 01:03:34,900
 

2791
01:03:32,550 --> 01:03:37,960
 but that are maybe C for

2792
01:03:34,890 --> 01:03:37,960
 

2793
01:03:34,900 --> 01:03:39,609
 with this search space with this

2794
01:03:37,950 --> 01:03:39,609
 

2795
01:03:37,960 --> 01:03:41,980
 training pipeline with these hyper

2796
01:03:39,599 --> 01:03:41,980
 

2797
01:03:39,609 --> 01:03:43,150
 parameters and then we can compare these

2798
01:03:41,970 --> 01:03:43,150
 

2799
01:03:41,980 --> 01:03:46,810
 different neural architecture search

2800
01:03:43,140 --> 01:03:46,810
 

2801
01:03:43,150 --> 01:03:48,369
 methods on a fair basis

2802
01:03:46,800 --> 01:03:48,369
 

2803
01:03:46,810 --> 01:03:51,430
 finally experiments are often very

2804
01:03:48,359 --> 01:03:51,430
 

2805
01:03:48,369 --> 01:03:53,800
 expensive and so we there's also a need

2806
01:03:51,420 --> 01:03:53,800
 

2807
01:03:51,430 --> 01:03:57,160
 for cheap benchmarks that actually give

2808
01:03:53,790 --> 01:03:57,160
 

2809
01:03:53,800 --> 01:03:59,050
 us some some information about which

2810
01:03:57,150 --> 01:03:59,050
 

2811
01:03:57,160 --> 01:04:00,490
 methods are worked well so we can do a

2812
01:03:59,040 --> 01:04:00,490
 

2813
01:03:59,050 --> 01:04:05,079
 lot of runs and get statistical

2814
01:04:00,480 --> 01:04:05,079
 

2815
01:04:00,490 --> 01:04:06,250
 significance all right to wrap up hyper

2816
01:04:05,069 --> 01:04:06,250
 

2817
01:04:05,079 --> 01:04:07,690
 parameter optimization annual

2818
01:04:06,240 --> 01:04:07,690
 

2819
01:04:06,250 --> 01:04:10,359
 architecture search are really exciting

2820
01:04:07,680 --> 01:04:10,359
 

2821
01:04:07,690 --> 01:04:13,359
 fields there is several ways to go

2822
01:04:10,349 --> 01:04:13,359
 

2823
01:04:10,359 --> 01:04:15,099
 beyond neck box optimization and one

2824
01:04:13,349 --> 01:04:15,099
 

2825
01:04:13,359 --> 01:04:18,490
 shameless advertisements are we are

2826
01:04:15,089 --> 01:04:18,490
 

2827
01:04:15,099 --> 01:04:20,560
 building an auto ml automatic deep

2828
01:04:18,480 --> 01:04:20,560
 

2829
01:04:18,490 --> 01:04:22,660
 learning team we want to actually build

2830
01:04:20,550 --> 01:04:22,660
 

2831
01:04:20,560 --> 01:04:23,710
 a research library of different building

2832
01:04:22,650 --> 01:04:23,710
 

2833
01:04:22,660 --> 01:04:26,290
 blocks for efficient neural architecture

2834
01:04:23,700 --> 01:04:26,290
 

2835
01:04:23,710 --> 01:04:28,960
 search going to build an open-source

2836
01:04:26,280 --> 01:04:28,960
 

2837
01:04:26,290 --> 01:04:31,140
 framework auto-pay torch and we have

2838
01:04:28,950 --> 01:04:31,140
 

2839
01:04:28,960 --> 01:04:34,810
 several openings on all kinds of levels

2840
01:04:31,130 --> 01:04:34,810
 

2841
01:04:31,140 --> 01:04:38,380
 with that I'm at the end of my part and

2842
01:04:34,800 --> 01:04:38,380
 

2843
01:04:34,810 --> 01:04:41,170
 we'll now take a five-minute break I

2844
01:04:38,370 --> 01:04:41,170
 

2845
01:04:38,380 --> 01:04:43,000
 encourage you to stretch your legs maybe

2846
01:04:41,160 --> 01:04:43,000
 

2847
01:04:41,170 --> 01:04:44,650
 go to the washroom if you need to but

2848
01:04:42,990 --> 01:04:44,650
 

2849
01:04:43,000 --> 01:04:50,660
 we'll also take some questions during

2850
01:04:44,640 --> 01:04:50,660
 

2851
01:04:44,650 --> 01:04:59,000
 this period all right is that questions

2852
01:04:50,650 --> 01:04:59,000
 

2853
01:04:50,660 --> 01:05:03,450
[Applause]

2854
01:04:58,990 --> 01:05:03,450
 

2855
01:04:59,000 --> 01:05:05,220
 there's mics I don't really see so if

2856
01:05:03,440 --> 01:05:05,220
 

2857
01:05:03,450 --> 01:05:07,400
 you're the mic just tap on it and ask

2858
01:05:05,210 --> 01:05:07,400
 

2859
01:05:05,220 --> 01:05:07,400
 away

2860
01:05:14,609 --> 01:05:14,609
 

2861
01:05:14,619 --> 01:05:21,400
 and joking maybe you could I have a

2862
01:05:18,269 --> 01:05:21,400
 

2863
01:05:18,279 --> 01:05:23,380
 question for searching on these

2864
01:05:21,390 --> 01:05:23,380
 

2865
01:05:21,400 --> 01:05:24,819
 continuous type of parameters it's

2866
01:05:23,370 --> 01:05:24,819
 

2867
01:05:23,380 --> 01:05:27,249
 always seems like there's an extra

2868
01:05:24,809 --> 01:05:27,249
 

2869
01:05:24,819 --> 01:05:29,319
 question of the range and maybe the

2870
01:05:27,239 --> 01:05:29,319
 

2871
01:05:27,249 --> 01:05:31,059
 scale of the hyper parameter and how do

2872
01:05:29,309 --> 01:05:31,059
 

2873
01:05:29,319 --> 01:05:36,579
 you handle that sort of thing

2874
01:05:31,049 --> 01:05:36,579
 

2875
01:05:31,059 --> 01:05:40,960
 I'm very good question so if you have a

2876
01:05:36,569 --> 01:05:40,960
 

2877
01:05:36,579 --> 01:05:44,680
 very large range then it's going to be

2878
01:05:40,950 --> 01:05:44,680
 

2879
01:05:40,960 --> 01:05:46,239
 very hard to find the optimum so it's

2880
01:05:44,670 --> 01:05:46,239
 

2881
01:05:44,680 --> 01:05:48,880
 gonna be less efficient if you have a

2882
01:05:46,229 --> 01:05:48,880
 

2883
01:05:46,239 --> 01:05:50,499
 small range then well you might actually

2884
01:05:48,870 --> 01:05:50,499
 

2885
01:05:48,880 --> 01:05:52,210
 not include the optimum for this

2886
01:05:50,489 --> 01:05:52,210
 

2887
01:05:50,499 --> 01:05:54,849
 particular data set so this this

2888
01:05:52,200 --> 01:05:54,849
 

2889
01:05:52,210 --> 01:05:55,509
 definitely is an issue what you can do

2890
01:05:54,839 --> 01:05:55,509
 

2891
01:05:54,849 --> 01:05:58,239
 with TPE

2892
01:05:55,499 --> 01:05:58,239
 

2893
01:05:55,509 --> 01:06:00,369
 is you can actually specify a prior over

2894
01:05:58,229 --> 01:06:00,369
 

2895
01:05:58,239 --> 01:06:03,160
 what you expect to be a good hyper

2896
01:06:00,359 --> 01:06:03,160
 

2897
01:06:00,369 --> 01:06:04,690
 parameter setting so the priors worked

2898
01:06:03,150 --> 01:06:04,690
 

2899
01:06:03,160 --> 01:06:06,190
 much nicer and TPE then they work in

2900
01:06:04,680 --> 01:06:06,190
 

2901
01:06:04,690 --> 01:06:07,869
 standard Basin optimization because the

2902
01:06:06,180 --> 01:06:07,869
 

2903
01:06:06,190 --> 01:06:10,660
 prior and basin optimization would be

2904
01:06:07,859 --> 01:06:10,660
 

2905
01:06:07,869 --> 01:06:13,180
 well I expect the function value to be

2906
01:06:10,650 --> 01:06:13,180
 

2907
01:06:10,660 --> 01:06:14,920
 this given this hyper parameter in TPE

2908
01:06:13,170 --> 01:06:14,920
 

2909
01:06:13,180 --> 01:06:16,779
 the prior is well I expect the learning

2910
01:06:14,910 --> 01:06:16,779
 

2911
01:06:14,920 --> 01:06:18,489
 rate to typically be good around 10 to

2912
01:06:16,769 --> 01:06:18,489
 

2913
01:06:16,779 --> 01:06:20,229
 the minus 3 or 10 to the minus 2 that's

2914
01:06:18,479 --> 01:06:20,229
 

2915
01:06:18,489 --> 01:06:23,430
 a prior that's much easier for humans to

2916
01:06:20,219 --> 01:06:23,430
 

2917
01:06:20,229 --> 01:06:26,049
 to give and then you can just specify a

2918
01:06:23,420 --> 01:06:26,049
 

2919
01:06:23,430 --> 01:06:30,099
 Gaussian centered where you think it

2920
01:06:26,039 --> 01:06:30,099
 

2921
01:06:26,049 --> 01:06:32,170
 should be centered but also giving the

2922
01:06:30,089 --> 01:06:32,170
 

2923
01:06:30,099 --> 01:06:34,749
 search an opportunity to overwhelm that

2924
01:06:32,160 --> 01:06:34,749
 

2925
01:06:32,170 --> 01:06:39,609
 prior if there is enough data suggesting

2926
01:06:34,739 --> 01:06:39,609
 

2927
01:06:34,749 --> 01:06:41,769
 otherwise thank you the recent advance

2928
01:06:39,599 --> 01:06:41,769
 

2929
01:06:39,609 --> 01:06:43,809
 of neural active research we've seen

2930
01:06:41,759 --> 01:06:43,809
 

2931
01:06:41,769 --> 01:06:46,569
 methods which uses reinforcement

2932
01:06:43,799 --> 01:06:46,569
 

2933
01:06:43,809 --> 01:06:48,700
 learning or genetic algorithms while for

2934
01:06:46,559 --> 01:06:48,700
 

2935
01:06:46,569 --> 01:06:51,130
 hyper parameter optimization we tend to

2936
01:06:48,690 --> 01:06:51,130
 

2937
01:06:48,700 --> 01:06:52,660
 see more patient immunizations do you

2938
01:06:51,120 --> 01:06:52,660
 

2939
01:06:51,130 --> 01:06:54,969
 see any reason for this change or is

2940
01:06:52,650 --> 01:06:54,969
 

2941
01:06:52,660 --> 01:06:59,680
 this matter of taste of the different

2942
01:06:54,959 --> 01:06:59,680
 

2943
01:06:54,969 --> 01:07:01,809
 authors and I I think the the reason

2944
01:06:59,670 --> 01:07:01,809
 

2945
01:06:59,680 --> 01:07:05,319
 clearly is that Bayesian optimization

2946
01:07:01,799 --> 01:07:05,319
 

2947
01:07:01,809 --> 01:07:06,640
 works really nicely for low dimensional

2948
01:07:05,309 --> 01:07:06,640
 

2949
01:07:05,319 --> 01:07:08,680
 continuous spaces with Gaussian

2950
01:07:06,630 --> 01:07:08,680
 

2951
01:07:06,640 --> 01:07:10,900
 processes and most of the basin

2952
01:07:08,670 --> 01:07:10,900
 

2953
01:07:08,680 --> 01:07:14,469
 optimization community really comes from

2954
01:07:10,890 --> 01:07:14,469
 

2955
01:07:10,900 --> 01:07:16,390
 Gaussian processes so that that's where

2956
01:07:14,459 --> 01:07:16,390
 

2957
01:07:14,469 --> 01:07:17,859
 that's the type of problems that they

2958
01:07:16,380 --> 01:07:17,859
 

2959
01:07:16,390 --> 01:07:19,390
 deal with all the time where got some

2960
01:07:17,849 --> 01:07:19,390
 

2961
01:07:17,859 --> 01:07:21,430
 processes do well and that that's where

2962
01:07:19,380 --> 01:07:21,430
 

2963
01:07:19,390 --> 01:07:24,999
 Basin optimization methods are being

2964
01:07:21,420 --> 01:07:24,999
 

2965
01:07:21,430 --> 01:07:26,309
 developed with new kernels they also do

2966
01:07:24,989 --> 01:07:26,309
 

2967
01:07:24,999 --> 01:07:28,049
 apply to

2968
01:07:26,299 --> 01:07:28,049
 

2969
01:07:26,309 --> 01:07:30,749
 a neural architecture search etc and we

2970
01:07:28,039 --> 01:07:30,749
 

2971
01:07:28,049 --> 01:07:33,749
 are starting to see some approaches and

2972
01:07:30,739 --> 01:07:33,749
 

2973
01:07:30,749 --> 01:07:35,609
 well we've used random forests etc in

2974
01:07:33,739 --> 01:07:35,609
 

2975
01:07:33,749 --> 01:07:37,469
 this high dimensional spaces before but

2976
01:07:35,599 --> 01:07:37,469
 

2977
01:07:35,609 --> 01:07:40,709
 we are kind of an outlier in the basin

2978
01:07:37,459 --> 01:07:40,709
 

2979
01:07:37,469 --> 01:07:42,359
 optimization research community with

2980
01:07:40,699 --> 01:07:42,359
 

2981
01:07:40,709 --> 01:07:44,999
 that most other people use Gaussian

2982
01:07:42,349 --> 01:07:44,999
 

2983
01:07:42,359 --> 01:07:46,439
 processes I think also with based on

2984
01:07:44,989 --> 01:07:46,439
 

2985
01:07:44,999 --> 01:07:49,259
 your own networks they're extremely

2986
01:07:46,429 --> 01:07:49,259
 

2987
01:07:46,439 --> 01:07:51,449
 promising and that could really appeal

2988
01:07:49,249 --> 01:07:51,449
 

2989
01:07:49,259 --> 01:07:53,069
 to another whole different community

2990
01:07:51,439 --> 01:07:53,069
 

2991
01:07:51,449 --> 01:07:55,380
 namely the base and deep learning crowd

2992
01:07:53,059 --> 01:07:55,380
 

2993
01:07:53,069 --> 01:07:56,819
 so when they start using Bayesian

2994
01:07:55,370 --> 01:07:56,819
 

2995
01:07:55,380 --> 01:07:58,829
 optimization with Bayesian your own

2996
01:07:56,809 --> 01:07:58,829
 

2997
01:07:56,819 --> 01:08:00,900
 networks I think will also see a lot

2998
01:07:58,819 --> 01:08:00,900
 

2999
01:07:58,829 --> 01:08:04,529
 more of that type of work for neural

3000
01:08:00,890 --> 01:08:04,529
 

3001
01:08:00,900 --> 01:08:07,170
 architecture search the flipside why

3002
01:08:04,519 --> 01:08:07,170
 

3003
01:08:04,529 --> 01:08:08,839
 haven't we seen a lot of reinforcement

3004
01:08:07,160 --> 01:08:08,839
 

3005
01:08:07,170 --> 01:08:11,130
 learning for high performance

3006
01:08:08,829 --> 01:08:11,130
 

3007
01:08:08,839 --> 01:08:12,749
 optimization that's a good question that

3008
01:08:11,120 --> 01:08:12,749
 

3009
01:08:11,130 --> 01:08:16,199
 might also actually work quite well and

3010
01:08:12,739 --> 01:08:16,199
 

3011
01:08:12,749 --> 01:08:18,210
 we do see evolutionary algorithms for

3012
01:08:16,189 --> 01:08:18,210
 

3013
01:08:16,199 --> 01:08:20,339
 Hyper primate optimization just kind of

3014
01:08:18,200 --> 01:08:20,339
 

3015
01:08:18,210 --> 01:08:22,829
 nodded nips because these people these

3016
01:08:20,329 --> 01:08:22,829
 

3017
01:08:20,339 --> 01:08:26,150
 papers tend to be rejected at nips they

3018
01:08:22,819 --> 01:08:26,150
 

3019
01:08:22,829 --> 01:08:29,429
 appear at Gecko and other more

3020
01:08:26,140 --> 01:08:29,429
 

3021
01:08:26,150 --> 01:08:37,949
 evolutionary algorithms community thank

3022
01:08:29,419 --> 01:08:37,949
 

3023
01:08:29,429 --> 01:08:40,349
 you I say hi so I just was curious if

3024
01:08:37,939 --> 01:08:40,349
 

3025
01:08:37,949 --> 01:08:44,219
 you had any thoughts on I guess I'd call

3026
01:08:40,339 --> 01:08:44,219
 

3027
01:08:40,349 --> 01:08:46,349
 it Otto Otto ml so learning a neural

3028
01:08:44,209 --> 01:08:46,349
 

3029
01:08:44,219 --> 01:08:49,049
 network that knows how to optimize the

3030
01:08:46,339 --> 01:08:49,049
 

3031
01:08:46,349 --> 01:08:52,349
 high parameters of other neural networks

3032
01:08:49,039 --> 01:08:52,349
 

3033
01:08:49,049 --> 01:08:55,139
 generically obviously this slower but it

3034
01:08:52,339 --> 01:08:55,139
 

3035
01:08:52,349 --> 01:08:58,409
 has the theoretical advantage that you

3036
01:08:55,129 --> 01:08:58,409
 

3037
01:08:55,139 --> 01:09:00,779
 run this exactly once for everything on

3038
01:08:58,399 --> 01:09:00,779
 

3039
01:08:58,409 --> 01:09:04,949
 the outer loop and then you only run the

3040
01:09:00,769 --> 01:09:04,949
 

3041
01:09:00,779 --> 01:09:07,409
 like normal two-stage optimization is

3042
01:09:04,939 --> 01:09:07,409
 

3043
01:09:04,949 --> 01:09:09,420
 there any work in this area I'm yet so

3044
01:09:07,399 --> 01:09:09,420
 

3045
01:09:07,409 --> 01:09:11,849
 this is a very good question um in fact

3046
01:09:09,410 --> 01:09:11,849
 

3047
01:09:09,420 --> 01:09:15,480
 there will be an auto dl competition

3048
01:09:11,839 --> 01:09:15,480
 

3049
01:09:11,849 --> 01:09:18,239
 where you have to submit a system that

3050
01:09:15,470 --> 01:09:18,239
 

3051
01:09:15,480 --> 01:09:20,639
 just does everything and the system kind

3052
01:09:18,229 --> 01:09:20,639
 

3053
01:09:18,239 --> 01:09:25,040
 of optimally would be the result of this

3054
01:09:20,629 --> 01:09:25,040
 

3055
01:09:20,639 --> 01:09:27,359
 process that generates this Auto DL

3056
01:09:25,030 --> 01:09:27,359
 

3057
01:09:25,040 --> 01:09:29,429
 framework and and what you want to do is

3058
01:09:27,349 --> 01:09:29,429
 

3059
01:09:27,359 --> 01:09:31,259
 of course well use a lot of different

3060
01:09:29,419 --> 01:09:31,259
 

3061
01:09:29,429 --> 01:09:33,540
 data sets reason across data set so

3062
01:09:31,249 --> 01:09:33,540
 

3063
01:09:31,259 --> 01:09:37,199
 we'll see a lot more about that in the

3064
01:09:33,530 --> 01:09:37,199
 

3065
01:09:33,540 --> 01:09:38,400
 meta learning part I think well that

3066
01:09:37,189 --> 01:09:38,400
 

3067
01:09:37,199 --> 01:09:39,150
 that's really where you can get this

3068
01:09:38,390 --> 01:09:39,150
 

3069
01:09:38,400 --> 01:09:41,460
 inductive pie

3070
01:09:39,140 --> 01:09:41,460
 

3071
01:09:39,150 --> 01:09:43,890
 from what are these these bases

3072
01:09:41,450 --> 01:09:43,890
 

3073
01:09:41,460 --> 01:09:48,299
 typically like and what performs well in

3074
01:09:43,880 --> 01:09:48,299
 

3075
01:09:43,890 --> 01:09:50,250
 these spaces and yeah there's not much

3076
01:09:48,289 --> 01:09:50,250
 

3077
01:09:48,299 --> 01:09:53,190
 work on that yet but it's definitely

3078
01:09:50,240 --> 01:09:53,190
 

3079
01:09:50,250 --> 01:09:55,920
 very exciting and I think we will see

3080
01:09:53,180 --> 01:09:55,920
 

3081
01:09:53,190 --> 01:09:56,429
 quite a bit of work in that realm okay

3082
01:09:55,910 --> 01:09:56,429
 

3083
01:09:55,920 --> 01:10:03,420
 cool

3084
01:09:56,419 --> 01:10:03,420
 

3085
01:09:56,429 --> 01:10:07,739
 thank you other mic you're keen if you

3086
01:10:03,410 --> 01:10:07,739
 

3087
01:10:03,420 --> 01:10:11,040
 want to set up okay perhaps it's a

3088
01:10:07,729 --> 01:10:11,040
 

3089
01:10:07,739 --> 01:10:13,409
 little bit naive question but do you do

3090
01:10:11,030 --> 01:10:13,409
 

3091
01:10:11,040 --> 01:10:16,770
 you have a high expectation that these

3092
01:10:13,399 --> 01:10:16,770
 

3093
01:10:13,409 --> 01:10:19,890
 search methods will lead to finding any

3094
01:10:16,760 --> 01:10:19,890
 

3095
01:10:16,770 --> 01:10:24,360
 very interesting and novel architectures

3096
01:10:19,880 --> 01:10:24,360
 

3097
01:10:19,890 --> 01:10:26,460
 like for instance with NASA okay well it

3098
01:10:24,350 --> 01:10:26,460
 

3099
01:10:24,360 --> 01:10:28,530
 did work a little bit better on imagenet

3100
01:10:26,450 --> 01:10:28,530
 

3101
01:10:26,460 --> 01:10:30,630
 but it wasn't a massive improvement and

3102
01:10:28,520 --> 01:10:30,630
 

3103
01:10:28,530 --> 01:10:33,150
 it doesn't really seem we learned

3104
01:10:30,620 --> 01:10:33,150
 

3105
01:10:30,630 --> 01:10:35,219
 anything about building better

3106
01:10:33,140 --> 01:10:35,219
 

3107
01:10:33,150 --> 01:10:37,380
 architectures as compared to let's say

3108
01:10:35,209 --> 01:10:37,380
 

3109
01:10:35,219 --> 01:10:41,730
 building ResNet where we did learn

3110
01:10:37,370 --> 01:10:41,730
 

3111
01:10:37,380 --> 01:10:45,870
 something this is a good question and I

3112
01:10:41,720 --> 01:10:45,870
 

3113
01:10:41,730 --> 01:10:49,949
 think will I really leave it for time to

3114
01:10:45,860 --> 01:10:49,949
 

3115
01:10:45,870 --> 01:10:51,360
 tell I think now that we have more

3116
01:10:49,939 --> 01:10:51,360
 

3117
01:10:49,949 --> 01:10:53,840
 efficient neural architecture search

3118
01:10:51,350 --> 01:10:53,840
 

3119
01:10:51,360 --> 01:10:57,330
 methods we're actually a lot of people

3120
01:10:53,830 --> 01:10:57,330
 

3121
01:10:53,840 --> 01:10:59,909
 can use them without having access to

3122
01:10:57,320 --> 01:10:59,909
 

3123
01:10:57,330 --> 01:11:01,590
 Google scale in first computational

3124
01:10:59,899 --> 01:11:01,590
 

3125
01:10:59,909 --> 01:11:03,960
 infrastructure I think we'll see

3126
01:11:01,580 --> 01:11:03,960
 

3127
01:11:01,590 --> 01:11:05,730
 thousands of people starting to use

3128
01:11:03,950 --> 01:11:05,730
 

3129
01:11:03,960 --> 01:11:09,780
 these methods and maybe come up with

3130
01:11:05,720 --> 01:11:09,780
 

3131
01:11:05,730 --> 01:11:11,640
 something cooler and really understand

3132
01:11:09,770 --> 01:11:11,640
 

3133
01:11:09,780 --> 01:11:14,370
 more about the space we're not there yet

3134
01:11:11,630 --> 01:11:14,370
 

3135
01:11:11,640 --> 01:11:17,310
 but I would expect that we will get

3136
01:11:14,360 --> 01:11:17,310
 

3137
01:11:14,370 --> 01:11:22,699
 there in the next couple of years thank

3138
01:11:17,300 --> 01:11:22,699
 

3139
01:11:17,310 --> 01:11:30,480
 you yeah do you want to set up and then

3140
01:11:22,689 --> 01:11:30,480
 

3141
01:11:22,699 --> 01:11:33,360
 maybe one final question how do you

3142
01:11:30,470 --> 01:11:33,360
 

3143
01:11:30,480 --> 01:11:35,090
 compare reinforcement learning approach

3144
01:11:33,350 --> 01:11:35,090
 

3145
01:11:33,360 --> 01:11:38,000
 and the patient method

3146
01:11:35,080 --> 01:11:38,000
 

3147
01:11:35,090 --> 01:11:40,700
 simple efficiency polarization

3148
01:11:37,990 --> 01:11:40,700
 

3149
01:11:38,000 --> 01:11:45,200
 computation efficiency Oh different kind

3150
01:11:40,690 --> 01:11:45,200
 

3151
01:11:40,700 --> 01:11:46,820
 of problems so I would generally say

3152
01:11:45,190 --> 01:11:46,820
 

3153
01:11:45,200 --> 01:11:49,790
 based on optimization tends to be more

3154
01:11:46,810 --> 01:11:49,790
 

3155
01:11:46,820 --> 01:11:53,270
 sample efficient but they haven't been

3156
01:11:49,780 --> 01:11:53,270
 

3157
01:11:49,790 --> 01:11:56,750
 any head-to-head comparisons so actually

3158
01:11:53,260 --> 01:11:56,750
 

3159
01:11:53,270 --> 01:11:59,090
 we're working on one after that I can I

3160
01:11:56,740 --> 01:11:59,090
 

3161
01:11:56,750 --> 01:12:00,590
 have some data for that but my

3162
01:11:59,080 --> 01:12:00,590
 

3163
01:11:59,090 --> 01:12:02,120
 expectation would be that that it's

3164
01:12:00,580 --> 01:12:02,120
 

3165
01:12:00,590 --> 01:12:03,590
 going to move more efficient in terms of

3166
01:12:02,110 --> 01:12:03,590
 

3167
01:12:02,120 --> 01:12:06,170
 Bayesian optimization based optimization

3168
01:12:03,580 --> 01:12:06,170
 

3169
01:12:03,590 --> 01:12:08,690
 tends to work well particularly for

3170
01:12:06,160 --> 01:12:08,690
 

3171
01:12:06,170 --> 01:12:11,120
 these low dimensional spaces where

3172
01:12:08,680 --> 01:12:11,120
 

3173
01:12:08,690 --> 01:12:14,120
 reinforcement learning is not geared

3174
01:12:11,110 --> 01:12:14,120
 

3175
01:12:11,120 --> 01:12:18,430
 towards that as much thank you thank you

3176
01:12:14,110 --> 01:12:18,430
 

3177
01:12:14,120 --> 01:12:22,100
 and I think we'll if we can move over

3178
01:12:18,420 --> 01:12:22,100
 

3179
01:12:18,430 --> 01:12:25,700
 all right thank you and there'll be

3180
01:12:22,090 --> 01:12:25,700
 

3181
01:12:22,100 --> 01:12:27,080
 another question period later yeah sorry

3182
01:12:25,690 --> 01:12:27,080
 

3183
01:12:25,700 --> 01:12:29,540
 we have to cut the questions a bit short

3184
01:12:27,070 --> 01:12:29,540
 

3185
01:12:27,080 --> 01:12:34,660
 because we're running short on time next

3186
01:12:29,530 --> 01:12:34,660
 

3187
01:12:29,540 --> 01:12:34,660
 up is going to be walking thank you

3188
01:12:51,000 --> 01:12:51,000
 

3189
01:12:51,010 --> 01:12:55,340
 so good afternoon my name is Joaquin

3190
01:12:53,470 --> 01:12:55,340
 

3191
01:12:53,480 --> 01:12:57,790
 Valley shogun and i will talk about

3192
01:12:55,330 --> 01:12:57,790
 

3193
01:12:55,340 --> 01:13:00,230
 learning how to learn

3194
01:12:57,780 --> 01:13:00,230
 

3195
01:12:57,790 --> 01:13:02,390
 Frank installers a lot about what you

3196
01:13:00,220 --> 01:13:02,390
 

3197
01:13:00,230 --> 01:13:04,160
 can do when you're giving a new task and

3198
01:13:02,380 --> 01:13:04,160
 

3199
01:13:02,390 --> 01:13:06,440
 you have to start from scratch but

3200
01:13:04,150 --> 01:13:06,440
 

3201
01:13:04,160 --> 01:13:08,210
 typically when as humans we never run

3202
01:13:06,430 --> 01:13:08,210
 

3203
01:13:06,440 --> 01:13:10,490
 into the situation we always have some

3204
01:13:08,200 --> 01:13:10,490
 

3205
01:13:08,210 --> 01:13:13,160
 kind of prior experience that we can use

3206
01:13:10,480 --> 01:13:13,160
 

3207
01:13:10,490 --> 01:13:15,640
 to solve the task more efficiently than

3208
01:13:13,150 --> 01:13:15,640
 

3209
01:13:13,160 --> 01:13:18,260
 when we would have to start on scratch

3210
01:13:15,630 --> 01:13:18,260
 

3211
01:13:15,640 --> 01:13:20,930
 so I'll talk about that so learning is a

3212
01:13:18,250 --> 01:13:20,930
 

3213
01:13:18,260 --> 01:13:23,810
 never-ending process imagine that you

3214
01:13:20,920 --> 01:13:23,810
 

3215
01:13:20,930 --> 01:13:25,430
 are a small baby the first time if you

3216
01:13:23,800 --> 01:13:25,430
 

3217
01:13:23,810 --> 01:13:27,290
 learn to walk it's very very hard you

3218
01:13:25,420 --> 01:13:27,290
 

3219
01:13:25,430 --> 01:13:29,780
 fall down a lot you have to many many

3220
01:13:27,280 --> 01:13:29,780
 

3221
01:13:27,290 --> 01:13:31,940
 tries before you can pull it off but

3222
01:13:29,770 --> 01:13:31,940
 

3223
01:13:29,780 --> 01:13:34,100
 next time when you learn to ride a bike

3224
01:13:31,930 --> 01:13:34,100
 

3225
01:13:31,940 --> 01:13:36,650
 or do something else with motor skills

3226
01:13:34,090 --> 01:13:36,650
 

3227
01:13:34,100 --> 01:13:38,330
 this becomes a lot easier because we

3228
01:13:36,640 --> 01:13:38,330
 

3229
01:13:36,650 --> 01:13:40,370
 have first learned how to walk right

3230
01:13:38,320 --> 01:13:40,370
 

3231
01:13:38,330 --> 01:13:42,470
 babies don't start riding bicycles that

3232
01:13:40,360 --> 01:13:42,470
 

3233
01:13:40,370 --> 01:13:45,290
 first start riding a while learning how

3234
01:13:42,460 --> 01:13:45,290
 

3235
01:13:42,470 --> 01:13:47,240
 to walk and every time we encounter a

3236
01:13:45,280 --> 01:13:47,240
 

3237
01:13:45,290 --> 01:13:49,850
 new task we learn how to do it more

3238
01:13:47,230 --> 01:13:49,850
 

3239
01:13:47,240 --> 01:13:52,460
 efficiently based on prior experience

3240
01:13:49,840 --> 01:13:52,460
 

3241
01:13:49,850 --> 01:13:54,440
 and a process at arc we call meta

3242
01:13:52,450 --> 01:13:54,440
 

3243
01:13:52,460 --> 01:13:57,160
 learning we learn more efficiently with

3244
01:13:54,430 --> 01:13:57,160
 

3245
01:13:54,440 --> 01:13:59,630
 less trial and error and let's stay down

3246
01:13:57,150 --> 01:13:59,630
 

3247
01:13:57,160 --> 01:14:02,630
 so it's actually happening here that we

3248
01:13:59,620 --> 01:14:02,630
 

3249
01:13:59,630 --> 01:14:05,780
 we transfer an inductive bias from prior

3250
01:14:02,620 --> 01:14:05,780
 

3251
01:14:02,630 --> 01:14:08,600
 learning iterations to the new task

3252
01:14:05,770 --> 01:14:08,600
 

3253
01:14:05,780 --> 01:14:10,580
 right so not to buy this any assumptions

3254
01:14:08,590 --> 01:14:10,580
 

3255
01:14:08,600 --> 01:14:12,620
 or priors that you put into your

3256
01:14:10,570 --> 01:14:12,620
 

3257
01:14:10,580 --> 01:14:15,530
 learning system except for the training

3258
01:14:12,610 --> 01:14:15,530
 

3259
01:14:12,620 --> 01:14:17,810
 data and if we can extract useful

3260
01:14:15,520 --> 01:14:17,810
 

3261
01:14:15,530 --> 01:14:20,480
 information like constrains or beliefs

3262
01:14:17,800 --> 01:14:20,480
 

3263
01:14:17,810 --> 01:14:23,000
 or representations from previous tasks

3264
01:14:20,470 --> 01:14:23,000
 

3265
01:14:20,480 --> 01:14:24,770
 the new task becomes much easier and

3266
01:14:22,990 --> 01:14:24,770
 

3267
01:14:23,000 --> 01:14:27,200
 we'll see a range of different

3268
01:14:24,760 --> 01:14:27,200
 

3269
01:14:24,770 --> 01:14:31,370
 techniques that do exactly that the

3270
01:14:27,190 --> 01:14:31,370
 

3271
01:14:27,200 --> 01:14:32,870
 underlying part is that the prior tasks

3272
01:14:31,360 --> 01:14:32,870
 

3273
01:14:31,370 --> 01:14:35,480
 have to be similar

3274
01:14:32,860 --> 01:14:35,480
 

3275
01:14:32,870 --> 01:14:37,580
 you can't just transfer information or

3276
01:14:35,470 --> 01:14:37,580
 

3277
01:14:35,480 --> 01:14:38,960
 assumptions from tiles that are very

3278
01:14:37,570 --> 01:14:38,960
 

3279
01:14:37,580 --> 01:14:42,530
 different they have to be somehow

3280
01:14:38,950 --> 01:14:42,530
 

3281
01:14:38,960 --> 01:14:48,380
 similar if not then you may actually

3282
01:14:42,520 --> 01:14:48,380
 

3283
01:14:42,530 --> 01:14:50,000
 harm the learning right okay so we call

3284
01:14:48,370 --> 01:14:50,000
 

3285
01:14:48,380 --> 01:14:52,160
 this field also meta learning because we

3286
01:14:49,990 --> 01:14:52,160
 

3287
01:14:50,000 --> 01:14:55,160
 collect metadata about prior learning

3288
01:14:52,150 --> 01:14:55,160
 

3289
01:14:52,160 --> 01:14:56,240
 episodes and we transfer that to the

3290
01:14:55,150 --> 01:14:56,240
 

3291
01:14:55,160 --> 01:14:58,520
 method learner so the method learner

3292
01:14:56,230 --> 01:14:58,520
 

3293
01:14:56,240 --> 01:15:00,170
 gets a bunch of metadata and has to make

3294
01:14:58,510 --> 01:15:00,170
 

3295
01:14:58,520 --> 01:15:03,440
 sense of that and use it in a useful way

3296
01:15:00,160 --> 01:15:03,440
 

3297
01:15:00,170 --> 01:15:04,350
 to then construct a base learner that

3298
01:15:03,430 --> 01:15:04,350
 

3299
01:15:03,440 --> 01:15:06,480
 then will do the actual

3300
01:15:04,340 --> 01:15:06,480
 

3301
01:15:04,350 --> 01:15:08,460
 Modelling sometimes this is summer

3302
01:15:06,470 --> 01:15:08,460
 

3303
01:15:06,480 --> 01:15:10,380
 squash together and the method learner

3304
01:15:08,450 --> 01:15:10,380
 

3305
01:15:08,460 --> 01:15:11,630
 will directly build models which will

3306
01:15:10,370 --> 01:15:11,630
 

3307
01:15:10,380 --> 01:15:16,260
 come back to that

3308
01:15:11,620 --> 01:15:16,260
 

3309
01:15:11,630 --> 01:15:19,440
 so I'm subdividing this field into three

3310
01:15:16,250 --> 01:15:19,440
 

3311
01:15:16,260 --> 01:15:21,930
 levels each one requiring more and more

3312
01:15:19,430 --> 01:15:21,930
 

3313
01:15:19,440 --> 01:15:25,020
 similar tasks so the first type of

3314
01:15:21,920 --> 01:15:25,020
 

3315
01:15:21,930 --> 01:15:27,330
 problems is where the tasks can be very

3316
01:15:25,010 --> 01:15:27,330
 

3317
01:15:25,020 --> 01:15:30,240
 different from each other and then we

3318
01:15:27,320 --> 01:15:30,240
 

3319
01:15:27,330 --> 01:15:32,970
 just generalize general knowledge about

3320
01:15:30,230 --> 01:15:32,970
 

3321
01:15:30,240 --> 01:15:35,340
 tasks right to be what humans do you

3322
01:15:32,960 --> 01:15:35,340
 

3323
01:15:32,970 --> 01:15:37,680
 when you're confronted with a task which

3324
01:15:35,330 --> 01:15:37,680
 

3325
01:15:35,340 --> 01:15:39,750
 you're not familiar with you just try

3326
01:15:37,670 --> 01:15:39,750
 

3327
01:15:37,680 --> 01:15:41,670
 whatever word dwell on the past

3328
01:15:39,740 --> 01:15:41,670
 

3329
01:15:39,750 --> 01:15:43,530
 in general not very specific just

3330
01:15:41,660 --> 01:15:43,530
 

3331
01:15:41,670 --> 01:15:46,710
 something that always works that's the

3332
01:15:43,520 --> 01:15:46,710
 

3333
01:15:43,530 --> 01:15:48,600
 thing you try first then later on when

3334
01:15:46,700 --> 01:15:48,600
 

3335
01:15:46,710 --> 01:15:51,780
 we have more information about the task

3336
01:15:48,590 --> 01:15:51,780
 

3337
01:15:48,600 --> 01:15:54,930
 and so I can I can compare my tasks to

3338
01:15:51,770 --> 01:15:54,930
 

3339
01:15:51,780 --> 01:15:57,600
 my new to prior tasks then I can reason

3340
01:15:54,920 --> 01:15:57,600
 

3341
01:15:54,930 --> 01:15:59,870
 about ok how is this task different from

3342
01:15:57,590 --> 01:15:59,870
 

3343
01:15:57,600 --> 01:16:01,980
 my new tasks and then I can actually

3344
01:15:59,860 --> 01:16:01,980
 

3345
01:15:59,870 --> 01:16:04,530
 transfer information transfer

3346
01:16:01,970 --> 01:16:04,530
 

3347
01:16:01,980 --> 01:16:08,040
 information in a much more useful way

3348
01:16:04,520 --> 01:16:08,040
 

3349
01:16:04,530 --> 01:16:10,650
 and then finally we come to tasks which

3350
01:16:08,030 --> 01:16:10,650
 

3351
01:16:08,040 --> 01:16:13,350
 are so similar that I can actually take

3352
01:16:10,640 --> 01:16:13,350
 

3353
01:16:10,650 --> 01:16:18,120
 a train model from a prior task and then

3354
01:16:13,340 --> 01:16:18,120
 

3355
01:16:13,350 --> 01:16:19,800
 repurpose it for solving my new tasks so

3356
01:16:18,110 --> 01:16:19,800
 

3357
01:16:18,120 --> 01:16:22,380
 the first type of tasks this is where we

3358
01:16:19,790 --> 01:16:22,380
 

3359
01:16:19,800 --> 01:16:24,350
 the task can be quite different and so

3360
01:16:22,370 --> 01:16:24,350
 

3361
01:16:22,380 --> 01:16:27,930
 there we look at the performances of

3362
01:16:24,340 --> 01:16:27,930
 

3363
01:16:24,350 --> 01:16:30,750
 previous approaches and we also look at

3364
01:16:27,920 --> 01:16:30,750
 

3365
01:16:27,930 --> 01:16:33,330
 exactly what we did and we encode it as

3366
01:16:30,740 --> 01:16:33,330
 

3367
01:16:30,750 --> 01:16:36,330
 a configuration that's the whole set of

3368
01:16:33,320 --> 01:16:36,330
 

3369
01:16:33,330 --> 01:16:38,970
 assumptions like the neuro architecture

3370
01:16:36,320 --> 01:16:38,970
 

3371
01:16:36,330 --> 01:16:41,160
 the algorithm the pipeline of steps tree

3372
01:16:38,960 --> 01:16:41,160
 

3373
01:16:38,970 --> 01:16:43,890
 builds high parameters that you tuned

3374
01:16:41,150 --> 01:16:43,890
 

3375
01:16:41,160 --> 01:16:44,550
 all these things that uniquely define

3376
01:16:43,880 --> 01:16:44,550
 

3377
01:16:43,890 --> 01:16:46,830
 your model

3378
01:16:44,540 --> 01:16:46,830
 

3379
01:16:44,550 --> 01:16:49,050
 those are your configuration is the

3380
01:16:46,820 --> 01:16:49,050
 

3381
01:16:46,830 --> 01:16:51,900
 lambda here okay so giving a

3382
01:16:49,040 --> 01:16:51,900
 

3383
01:16:49,050 --> 01:16:54,180
 configuration and a task I get the

3384
01:16:51,890 --> 01:16:54,180
 

3385
01:16:51,900 --> 01:16:56,190
 performance out of it and then my metal

3386
01:16:54,170 --> 01:16:56,190
 

3387
01:16:54,180 --> 01:16:59,100
 learner has to take this configuration

3388
01:16:56,180 --> 01:16:59,100
 

3389
01:16:56,190 --> 01:17:02,780
 and a performance and I figure out how

3390
01:16:59,090 --> 01:17:02,780
 

3391
01:16:59,100 --> 01:17:05,190
 to use that usefully to solve a new task

3392
01:17:02,770 --> 01:17:05,190
 

3393
01:17:02,780 --> 01:17:07,920
 so the first and simplest way to do this

3394
01:17:05,180 --> 01:17:07,920
 

3395
01:17:05,190 --> 01:17:10,170
 is simply remember what worked well in

3396
01:17:07,910 --> 01:17:10,170
 

3397
01:17:07,920 --> 01:17:12,480
 the past then you build a general

3398
01:17:10,160 --> 01:17:12,480
 

3399
01:17:10,170 --> 01:17:14,940
 ranking of which are the best approaches

3400
01:17:12,470 --> 01:17:14,940
 

3401
01:17:12,480 --> 01:17:16,920
 and then when you encounter a new task

3402
01:17:14,930 --> 01:17:16,920
 

3403
01:17:14,940 --> 01:17:18,870
 we just go top to bottom you

3404
01:17:16,910 --> 01:17:18,870
 

3405
01:17:16,920 --> 01:17:21,270
 why the method that works well in

3406
01:17:18,860 --> 01:17:21,270
 

3407
01:17:18,870 --> 01:17:23,640
 general always the best then the second

3408
01:17:21,260 --> 01:17:23,640
 

3409
01:17:21,270 --> 01:17:25,739
 third and search one very general and

3410
01:17:23,630 --> 01:17:25,739
 

3411
01:17:23,640 --> 01:17:26,850
 it's also useful as a warm storing

3412
01:17:25,729 --> 01:17:26,850
 

3413
01:17:25,739 --> 01:17:29,250
 technique so if you're considering

3414
01:17:26,840 --> 01:17:29,250
 

3415
01:17:26,850 --> 01:17:32,510
 versus Bayesian optimization you can

3416
01:17:29,240 --> 01:17:32,510
 

3417
01:17:29,250 --> 01:17:34,770
 start with the overall top 10

3418
01:17:32,500 --> 01:17:34,770
 

3419
01:17:32,510 --> 01:17:38,280
 configurations to warm start that

3420
01:17:34,760 --> 01:17:38,280
 

3421
01:17:34,770 --> 01:17:41,100
 approach nothing you can do is check you

3422
01:17:38,270 --> 01:17:41,100
 

3423
01:17:38,280 --> 01:17:42,960
 can configure your your design if your

3424
01:17:41,090 --> 01:17:42,960
 

3425
01:17:41,100 --> 01:17:45,270
 meta learner and you facing a problem

3426
01:17:42,950 --> 01:17:45,270
 

3427
01:17:42,960 --> 01:17:47,280
 with many variables the first thing you

3428
01:17:45,260 --> 01:17:47,280
 

3429
01:17:45,270 --> 01:17:50,190
 want to do is got eliminated variables

3430
01:17:47,270 --> 01:17:50,190
 

3431
01:17:47,280 --> 01:17:52,140
 that make the problem harder so to do

3432
01:17:50,180 --> 01:17:52,140
 

3433
01:17:50,190 --> 01:17:54,600
 here so here we for instance look at

3434
01:17:52,130 --> 01:17:54,600
 

3435
01:17:52,140 --> 01:17:56,130
 which high parameters are really

3436
01:17:54,590 --> 01:17:56,130
 

3437
01:17:54,600 --> 01:17:58,739
 important to chew and which one can I

3438
01:17:56,120 --> 01:17:58,739
 

3439
01:17:56,130 --> 01:18:00,510
 just leave at default one way of doing

3440
01:17:58,729 --> 01:18:00,510
 

3441
01:17:58,739 --> 01:18:03,420
 this is called functional and no pilots

3442
01:18:00,500 --> 01:18:03,420
 

3443
01:18:00,510 --> 01:18:05,670
 work by young contain and monitor so

3444
01:18:03,410 --> 01:18:05,670
 

3445
01:18:03,420 --> 01:18:08,010
 here you take every high parameter

3446
01:18:05,660 --> 01:18:08,010
 

3447
01:18:05,670 --> 01:18:10,170
 individually you vary it you see what

3448
01:18:08,000 --> 01:18:10,170
 

3449
01:18:08,010 --> 01:18:11,550
 the effect is on a performance and if

3450
01:18:10,160 --> 01:18:11,550
 

3451
01:18:10,170 --> 01:18:13,679
 you see that the high parameter has

3452
01:18:11,540 --> 01:18:13,679
 

3453
01:18:11,550 --> 01:18:15,719
 large effect on performance you say this

3454
01:18:13,669 --> 01:18:15,719
 

3455
01:18:13,679 --> 01:18:18,600
 is the one I want to keep if not you

3456
01:18:15,709 --> 01:18:18,600
 

3457
01:18:15,719 --> 01:18:20,400
 throw it out of your search space ok now

3458
01:18:18,590 --> 01:18:20,400
 

3459
01:18:18,600 --> 01:18:22,170
 you can say sometimes you have high

3460
01:18:20,390 --> 01:18:22,170
 

3461
01:18:20,400 --> 01:18:23,610
 parameters that have a lot of variance

3462
01:18:22,160 --> 01:18:23,610
 

3463
01:18:22,170 --> 01:18:26,250
 but they also have a very good default

3464
01:18:23,600 --> 01:18:26,250
 

3465
01:18:23,610 --> 01:18:29,130
 if I just keep it a default I'll be fine

3466
01:18:26,240 --> 01:18:29,130
 

3467
01:18:26,250 --> 01:18:31,199
 that's defined in tunability so here you

3468
01:18:29,120 --> 01:18:31,199
 

3469
01:18:29,130 --> 01:18:34,530
 first learn a good default and then you

3470
01:18:31,189 --> 01:18:34,530
 

3471
01:18:31,199 --> 01:18:37,350
 measure how much improvement can I still

3472
01:18:34,520 --> 01:18:37,350
 

3473
01:18:34,530 --> 01:18:39,630
 get if I chew in the parameter and then

3474
01:18:37,340 --> 01:18:39,630
 

3475
01:18:37,350 --> 01:18:42,380
 I rank my high parameters based on the

3476
01:18:39,620 --> 01:18:42,380
 

3477
01:18:39,630 --> 01:18:44,850
 chin ability and I use only the top ones

3478
01:18:42,370 --> 01:18:44,850
 

3479
01:18:42,380 --> 01:18:47,670
 and the final thing you can do is she

3480
01:18:44,840 --> 01:18:47,670
 

3481
01:18:44,850 --> 01:18:50,190
 can look at which task did I saw they

3482
01:18:47,660 --> 01:18:50,190
 

3483
01:18:47,670 --> 01:18:53,040
 were similar and what did not work

3484
01:18:50,180 --> 01:18:53,040
 

3485
01:18:50,190 --> 01:18:54,239
 because I will not try that again it's

3486
01:18:53,030 --> 01:18:54,239
 

3487
01:18:53,040 --> 01:18:56,190
 like it's a learning experience you try

3488
01:18:54,229 --> 01:18:56,190
 

3489
01:18:54,239 --> 01:19:00,170
 you try something before it failed

3490
01:18:56,180 --> 01:19:00,170
 

3491
01:18:56,190 --> 01:19:05,040
 probably you don't try that again right

3492
01:19:00,160 --> 01:19:05,040
 

3493
01:19:00,170 --> 01:19:08,250
 ok now you can look at how similar tasks

3494
01:19:05,030 --> 01:19:08,250
 

3495
01:19:05,040 --> 01:19:10,500
 are in hindsight so here you have a new

3496
01:19:08,240 --> 01:19:10,500
 

3497
01:19:08,250 --> 01:19:13,679
 problem you try a couple of approaches

3498
01:19:10,490 --> 01:19:13,679
 

3499
01:19:10,500 --> 01:19:15,810
 and then you see how how these different

3500
01:19:13,669 --> 01:19:15,810
 

3501
01:19:13,679 --> 01:19:18,210
 approaches worked and then you can start

3502
01:19:15,800 --> 01:19:18,210
 

3503
01:19:15,810 --> 01:19:21,270
 reasoning ok if this work and this

3504
01:19:18,200 --> 01:19:21,270
 

3505
01:19:18,210 --> 01:19:25,260
 dinner work which other tasks do I know

3506
01:19:21,260 --> 01:19:25,260
 

3507
01:19:21,270 --> 01:19:26,980
 from the past where this was the same so

3508
01:19:25,250 --> 01:19:26,980
 

3509
01:19:25,260 --> 01:19:29,080
 if two tasks

3510
01:19:26,970 --> 01:19:29,080
 

3511
01:19:26,980 --> 01:19:31,180
 the same performances for simple similar

3512
01:19:29,070 --> 01:19:31,180
 

3513
01:19:29,080 --> 01:19:33,700
 configurations then that means that

3514
01:19:31,170 --> 01:19:33,700
 

3515
01:19:31,180 --> 01:19:36,070
 those tasks must be somehow similar to

3516
01:19:33,690 --> 01:19:36,070
 

3517
01:19:33,700 --> 01:19:37,630
 each other right and only if the

3518
01:19:36,060 --> 01:19:37,630
 

3519
01:19:36,070 --> 01:19:38,290
 expression is is with these relative

3520
01:19:37,620 --> 01:19:38,290
 

3521
01:19:37,630 --> 01:19:40,750
 landmarks

3522
01:19:38,280 --> 01:19:40,750
 

3523
01:19:38,290 --> 01:19:42,970
 you just remember performance

3524
01:19:40,740 --> 01:19:42,970
 

3525
01:19:40,750 --> 01:19:47,650
 differences between two configurations

3526
01:19:42,960 --> 01:19:47,650
 

3527
01:19:42,970 --> 01:19:49,660
 and if two tasks have correlating world

3528
01:19:47,640 --> 01:19:49,660
 

3529
01:19:47,650 --> 01:19:51,970
 of landmarks then that means that those

3530
01:19:49,650 --> 01:19:51,970
 

3531
01:19:49,660 --> 01:19:53,770
 tasks are likely similar are you going

3532
01:19:51,960 --> 01:19:53,770
 

3533
01:19:51,970 --> 01:19:57,010
 to use that so the way you use that is

3534
01:19:53,760 --> 01:19:57,010
 

3535
01:19:53,770 --> 01:20:00,010
 you first gonna start with a good

3536
01:19:57,000 --> 01:20:00,010
 

3537
01:19:57,010 --> 01:20:02,320
 configuration you valuate it and then

3538
01:20:00,000 --> 01:20:02,320
 

3539
01:20:00,010 --> 01:20:05,140
 you say okay which tasks are now similar

3540
01:20:02,310 --> 01:20:05,140
 

3541
01:20:02,320 --> 01:20:07,540
 based on this outcome then on the

3542
01:20:05,130 --> 01:20:07,540
 

3543
01:20:05,140 --> 01:20:09,400
 similar tasks you look which other

3544
01:20:07,530 --> 01:20:09,400
 

3545
01:20:07,540 --> 01:20:12,130
 configurations do I know that were

3546
01:20:09,390 --> 01:20:12,130
 

3547
01:20:09,400 --> 01:20:14,710
 better than my current one and then I

3548
01:20:12,120 --> 01:20:14,710
 

3549
01:20:12,130 --> 01:20:16,980
 used the best one of those and I start

3550
01:20:14,700 --> 01:20:16,980
 

3551
01:20:14,710 --> 01:20:19,630
 again I get one new evaluation point I

3552
01:20:16,970 --> 01:20:19,630
 

3553
01:20:16,980 --> 01:20:22,180
 again update my similarity of my data

3554
01:20:19,620 --> 01:20:22,180
 

3555
01:20:19,630 --> 01:20:24,820
 sets I choose another competitor based

3556
01:20:22,170 --> 01:20:24,820
 

3557
01:20:22,180 --> 01:20:27,100
 on what worked well on the Summa tasks

3558
01:20:24,810 --> 01:20:27,100
 

3559
01:20:24,820 --> 01:20:30,130
 and that gives and I repeat this way

3560
01:20:27,090 --> 01:20:30,130
 

3561
01:20:27,100 --> 01:20:32,440
 until I stop doing that this is a very

3562
01:20:30,120 --> 01:20:32,440
 

3563
01:20:30,130 --> 01:20:33,910
 very effective way of solving this space

3564
01:20:32,430 --> 01:20:33,910
 

3565
01:20:32,440 --> 01:20:35,460
 if you've no other information to work

3566
01:20:33,900 --> 01:20:35,460
 

3567
01:20:33,910 --> 01:20:38,710
 with

3568
01:20:35,450 --> 01:20:38,710
 

3569
01:20:35,460 --> 01:20:41,230
 now we've seen based optimization so you

3570
01:20:38,700 --> 01:20:41,230
 

3571
01:20:38,710 --> 01:20:45,130
 also build all on that and so basically

3572
01:20:41,220 --> 01:20:45,130
 

3573
01:20:41,230 --> 01:20:47,980
 here you learn solving a single task you

3574
01:20:45,120 --> 01:20:47,980
 

3575
01:20:45,130 --> 01:20:49,390
 try a number of configurations you have

3576
01:20:47,970 --> 01:20:49,390
 

3577
01:20:47,980 --> 01:20:52,000
 performances you'll build this

3578
01:20:49,380 --> 01:20:52,000
 

3579
01:20:49,390 --> 01:20:56,340
 regression model and then you can use

3580
01:20:51,990 --> 01:20:56,340
 

3581
01:20:52,000 --> 01:20:58,540
 that to choose a new configuration but

3582
01:20:56,330 --> 01:20:58,540
 

3583
01:20:56,340 --> 01:21:00,160
 unfortunately when you do this and your

3584
01:20:58,530 --> 01:21:00,160
 

3585
01:20:58,540 --> 01:21:02,860
 task is done and you're giving a new

3586
01:21:00,150 --> 01:21:02,860
 

3587
01:21:00,160 --> 01:21:05,680
 task you kind of forget all this useful

3588
01:21:02,850 --> 01:21:05,680
 

3589
01:21:02,860 --> 01:21:07,960
 information you don't want it what if we

3590
01:21:05,670 --> 01:21:07,960
 

3591
01:21:05,680 --> 01:21:09,850
 can actually learn these circuit models

3592
01:21:07,950 --> 01:21:09,850
 

3593
01:21:07,960 --> 01:21:12,490
 learn very well what worked on a

3594
01:21:09,840 --> 01:21:12,490
 

3595
01:21:09,850 --> 01:21:14,110
 specific task and then transfer that to

3596
01:21:12,480 --> 01:21:14,110
 

3597
01:21:12,490 --> 01:21:17,230
 new tasks so I don't have to start from

3598
01:21:14,100 --> 01:21:17,230
 

3599
01:21:14,110 --> 01:21:20,560
 scratch again so one way of doing is

3600
01:21:17,220 --> 01:21:20,560
 

3601
01:21:17,230 --> 01:21:25,090
 this surrogates all transfer so here we

3602
01:21:20,550 --> 01:21:25,090
 

3603
01:21:20,560 --> 01:21:28,450
 say that the task is similar if well you

3604
01:21:25,080 --> 01:21:28,450
 

3605
01:21:25,090 --> 01:21:30,340
 assume that if the task is similar then

3606
01:21:28,440 --> 01:21:30,340
 

3607
01:21:28,450 --> 01:21:32,440
 a surrogate I learned on the similar

3608
01:21:30,330 --> 01:21:32,440
 

3609
01:21:30,340 --> 01:21:35,770
 tasks will again be useful I can start

3610
01:21:32,430 --> 01:21:35,770
 

3611
01:21:32,440 --> 01:21:37,210
 from there there's basically two ways of

3612
01:21:35,760 --> 01:21:37,210
 

3613
01:21:35,770 --> 01:21:38,590
 doing this first of all you build a

3614
01:21:37,200 --> 01:21:38,590
 

3615
01:21:37,210 --> 01:21:40,289
 surrogate model you remember your

3616
01:21:38,580 --> 01:21:40,289
 

3617
01:21:38,590 --> 01:21:42,269
 surrogate model you store it

3618
01:21:40,279 --> 01:21:42,269
 

3619
01:21:40,289 --> 01:21:44,519
 per task so for every task you have the

3620
01:21:42,259 --> 01:21:44,519
 

3621
01:21:42,269 --> 01:21:48,090
 circuit model that you that you learned

3622
01:21:44,509 --> 01:21:48,090
 

3623
01:21:44,519 --> 01:21:50,159
 and then when you have a new task you're

3624
01:21:48,080 --> 01:21:50,159
 

3625
01:21:48,090 --> 01:21:52,260
 going to combine all these circuit

3626
01:21:50,149 --> 01:21:52,260
 

3627
01:21:50,159 --> 01:21:54,690
 models from all these tasks and there's

3628
01:21:52,250 --> 01:21:54,690
 

3629
01:21:52,260 --> 01:21:57,329
 two two ways of doing this one is to

3630
01:21:54,680 --> 01:21:57,329
 

3631
01:21:54,690 --> 01:21:58,980
 first measure a distance between tasks

3632
01:21:57,319 --> 01:21:58,980
 

3633
01:21:57,329 --> 01:22:01,500
 and then you basically wait the

3634
01:21:58,970 --> 01:22:01,500
 

3635
01:21:58,980 --> 01:22:03,719
 contribution of each circuit model based

3636
01:22:01,490 --> 01:22:03,719
 

3637
01:22:01,500 --> 01:22:06,000
 on how similar tasks are if you have a

3638
01:22:03,709 --> 01:22:06,000
 

3639
01:22:03,719 --> 01:22:07,800
 very similar task then you give more way

3640
01:22:05,990 --> 01:22:07,800
 

3641
01:22:06,000 --> 01:22:10,519
 to the surrogate model if it's a very

3642
01:22:07,790 --> 01:22:10,519
 

3643
01:22:07,800 --> 01:22:13,559
 distant asks you're going to ignore it

3644
01:22:10,509 --> 01:22:13,559
 

3645
01:22:10,519 --> 01:22:16,289
 another nicer way I think is to build a

3646
01:22:13,549 --> 01:22:16,289
 

3647
01:22:13,559 --> 01:22:18,510
 new Gaussian process mixture that's wait

3648
01:22:16,279 --> 01:22:18,510
 

3649
01:22:16,289 --> 01:22:20,579
 by the current performance so first of

3650
01:22:18,500 --> 01:22:20,579
 

3651
01:22:18,510 --> 01:22:25,409
 all you built a mixture you let all of

3652
01:22:20,569 --> 01:22:25,409
 

3653
01:22:20,579 --> 01:22:28,230
 these GPS make a prediction and then you

3654
01:22:25,399 --> 01:22:28,230
 

3655
01:22:25,409 --> 01:22:30,059
 see okay on the new task however they

3656
01:22:28,220 --> 01:22:30,059
 

3657
01:22:28,230 --> 01:22:31,590
 will they're doing and if some of them

3658
01:22:30,049 --> 01:22:31,590
 

3659
01:22:30,059 --> 01:22:33,090
 were doing very well you increase their

3660
01:22:31,580 --> 01:22:33,090
 

3661
01:22:31,590 --> 01:22:35,849
 weights is some of them are doing very

3662
01:22:33,080 --> 01:22:35,849
 

3663
01:22:33,090 --> 01:22:38,159
 badly you decrease their weight it's

3664
01:22:35,839 --> 01:22:38,159
 

3665
01:22:35,849 --> 01:22:40,349
 like having a bunch of advisors if some

3666
01:22:38,149 --> 01:22:40,349
 

3667
01:22:38,159 --> 01:22:41,760
 of them gives you good advice you listen

3668
01:22:40,339 --> 01:22:41,760
 

3669
01:22:40,349 --> 01:22:47,789
 more to them if not you listen as to

3670
01:22:41,750 --> 01:22:47,789
 

3671
01:22:41,760 --> 01:22:49,710
 them now this one thing here always with

3672
01:22:47,779 --> 01:22:49,710
 

3673
01:22:47,789 --> 01:22:53,699
 the GPS this always is question of

3674
01:22:49,700 --> 01:22:53,699
 

3675
01:22:49,710 --> 01:22:55,710
 scalability and of course if you have if

3676
01:22:53,689 --> 01:22:55,710
 

3677
01:22:53,699 --> 01:22:58,230
 I give you millions of data points from

3678
01:22:55,700 --> 01:22:58,230
 

3679
01:22:55,710 --> 01:23:01,070
 previous tasks it's a pity you cannot

3680
01:22:58,220 --> 01:23:01,070
 

3681
01:22:58,230 --> 01:23:04,260
 use them in one large surrogate model

3682
01:23:01,060 --> 01:23:04,260
 

3683
01:23:01,070 --> 01:23:08,099
 and this was kind of solved by people at

3684
01:23:04,250 --> 01:23:08,099
 

3685
01:23:04,260 --> 01:23:11,750
 Amazon pepperoni and its colleagues so

3686
01:23:08,089 --> 01:23:11,750
 

3687
01:23:08,099 --> 01:23:15,030
 they say okay I can't I can't train a

3688
01:23:11,740 --> 01:23:15,030
 

3689
01:23:11,750 --> 01:23:17,219
 Gaussian process but I can I can train

3690
01:23:15,020 --> 01:23:17,219
 

3691
01:23:15,030 --> 01:23:19,679
 it I can train a linear patient

3692
01:23:17,209 --> 01:23:19,679
 

3693
01:23:17,219 --> 01:23:23,900
 optimizer which is linear and trying ok

3694
01:23:19,669 --> 01:23:23,900
 

3695
01:23:19,679 --> 01:23:26,070
 now of course this linear surrogate here

3696
01:23:23,890 --> 01:23:26,070
 

3697
01:23:23,900 --> 01:23:29,190
 it's probably not a good approximation

3698
01:23:26,060 --> 01:23:29,190
 

3699
01:23:26,070 --> 01:23:32,789
 for for my my high parameter that I'm

3700
01:23:29,180 --> 01:23:32,789
 

3701
01:23:29,190 --> 01:23:34,920
 trying to to model right so but what if

3702
01:23:32,779 --> 01:23:34,920
 

3703
01:23:32,789 --> 01:23:36,360
 I can add the the second degree

3704
01:23:34,910 --> 01:23:36,360
 

3705
01:23:34,920 --> 01:23:41,010
 polynomial the third degree polynomial

3706
01:23:36,350 --> 01:23:41,010
 

3707
01:23:36,360 --> 01:23:46,679
 then I can represent the the behavior of

3708
01:23:41,000 --> 01:23:46,679
 

3709
01:23:41,010 --> 01:23:48,389
 this type of parameter much better then

3710
01:23:46,669 --> 01:23:48,389
 

3711
01:23:46,679 --> 01:23:51,989
 the question is okay if you do that then

3712
01:23:48,379 --> 01:23:51,989
 

3713
01:23:48,389 --> 01:23:53,690
 which polynomials should you use because

3714
01:23:51,979 --> 01:23:53,690
 

3715
01:23:51,989 --> 01:23:55,699
 some high parameters

3716
01:23:53,680 --> 01:23:55,699
 

3717
01:23:53,690 --> 01:23:57,860
 a very linear some are very nonlinear so

3718
01:23:55,689 --> 01:23:57,860
 

3719
01:23:55,699 --> 01:24:00,560
 you have to learn from each high

3720
01:23:57,850 --> 01:24:00,560
 

3721
01:23:57,860 --> 01:24:04,160
 parameter - ball that in this case we

3722
01:24:00,550 --> 01:24:04,160
 

3723
01:24:00,560 --> 01:24:05,420
 learn this with a neural network so we

3724
01:24:04,150 --> 01:24:05,420
 

3725
01:24:04,160 --> 01:24:07,520
 give them their own network all the

3726
01:24:05,410 --> 01:24:07,520
 

3727
01:24:05,420 --> 01:24:09,739
 configurations all the performances and

3728
01:24:07,510 --> 01:24:09,739
 

3729
01:24:07,520 --> 01:24:11,600
 the neural network has to learn what's

3730
01:24:09,729 --> 01:24:11,600
 

3731
01:24:09,739 --> 01:24:15,620
 the optimal basis expansion what's the

3732
01:24:11,590 --> 01:24:15,620
 

3733
01:24:11,600 --> 01:24:18,640
 optimal set of have dimensions to build

3734
01:24:15,610 --> 01:24:18,640
 

3735
01:24:15,620 --> 01:24:24,440
 a small limb if you do that then you can

3736
01:24:18,630 --> 01:24:24,440
 

3737
01:24:18,640 --> 01:24:27,160
 simply fit a linear Bayesian regression

3738
01:24:24,430 --> 01:24:27,160
 

3739
01:24:24,440 --> 01:24:29,020
 model in there and you can use that

3740
01:24:27,150 --> 01:24:29,020
 

3741
01:24:27,160 --> 01:24:32,570
 perfectly well for making predictions

3742
01:24:29,010 --> 01:24:32,570
 

3743
01:24:29,020 --> 01:24:35,840
 that scales much better another thing

3744
01:24:32,560 --> 01:24:35,840
 

3745
01:24:32,570 --> 01:24:37,250
 you can do you can combine the patient

3746
01:24:35,830 --> 01:24:37,250
 

3747
01:24:35,840 --> 01:24:39,170
 while your circuit models from different

3748
01:24:37,240 --> 01:24:39,170
 

3749
01:24:37,250 --> 01:24:40,969
 tasks using multi tiles equation

3750
01:24:39,160 --> 01:24:40,969
 

3751
01:24:39,170 --> 01:24:43,040
 optimization so the way this works is

3752
01:24:40,959 --> 01:24:43,040
 

3753
01:24:40,969 --> 01:24:45,380
 like in the figure here so assume you

3754
01:24:43,030 --> 01:24:45,380
 

3755
01:24:43,040 --> 01:24:47,780
 have three tasks the green the red and

3756
01:24:45,370 --> 01:24:47,780
 

3757
01:24:45,380 --> 01:24:50,330
 the blue at some point you want to be

3758
01:24:47,770 --> 01:24:50,330
 

3759
01:24:47,780 --> 01:24:52,340
 late for the blue tasks but you have a

3760
01:24:50,320 --> 01:24:52,340
 

3761
01:24:50,330 --> 01:24:54,410
 lot of uncertainty now if you can

3762
01:24:52,330 --> 01:24:54,410
 

3763
01:24:52,340 --> 01:24:56,570
 include information from the red and the

3764
01:24:54,400 --> 01:24:56,570
 

3765
01:24:54,410 --> 01:24:59,420
 green tasks you get less you get more

3766
01:24:56,560 --> 01:24:59,420
 

3767
01:24:56,570 --> 01:25:01,699
 information about this high parameter

3768
01:24:59,410 --> 01:25:01,699
 

3769
01:24:59,420 --> 01:25:04,070
 for instance so you you get a much

3770
01:25:01,689 --> 01:25:04,070
 

3771
01:25:01,699 --> 01:25:06,350
 better estimate of performance so you

3772
01:25:04,060 --> 01:25:06,350
 

3773
01:25:04,070 --> 01:25:08,810
 actually use information from the from

3774
01:25:06,340 --> 01:25:08,810
 

3775
01:25:06,350 --> 01:25:11,239
 other tasks for this specific task the

3776
01:25:08,800 --> 01:25:11,239
 

3777
01:25:08,810 --> 01:25:13,340
 blue one this is very nice it's not so

3778
01:25:11,229 --> 01:25:13,340
 

3779
01:25:11,239 --> 01:25:15,560
 scalable and this was soft not so long

3780
01:25:13,330 --> 01:25:15,560
 

3781
01:25:13,340 --> 01:25:17,810
 ago by French group using Bayesian

3782
01:25:15,550 --> 01:25:17,810
 

3783
01:25:15,560 --> 01:25:20,739
 neural networks these are neural

3784
01:25:17,800 --> 01:25:20,739
 

3785
01:25:17,810 --> 01:25:23,870
 networks that outputs a mean and

3786
01:25:20,729 --> 01:25:23,870
 

3787
01:25:20,739 --> 01:25:25,730
 uncertainty and the nice thing is you

3788
01:25:23,860 --> 01:25:25,730
 

3789
01:25:23,870 --> 01:25:29,480
 can also use these in the multi task way

3790
01:25:25,720 --> 01:25:29,480
 

3791
01:25:25,730 --> 01:25:31,190
 and then you can exactly do this in the

3792
01:25:29,470 --> 01:25:31,190
 

3793
01:25:29,480 --> 01:25:35,120
 multi task away and be efficient as well

3794
01:25:31,180 --> 01:25:35,120
 

3795
01:25:31,190 --> 01:25:38,270
 a final thing that's been done by people

3796
01:25:35,110 --> 01:25:38,270
 

3797
01:25:35,120 --> 01:25:40,730
 at Google this year it's an algorithm

3798
01:25:38,260 --> 01:25:40,730
 

3799
01:25:38,270 --> 01:25:43,820
 where you assume that the tasks that you

3800
01:25:40,720 --> 01:25:43,820
 

3801
01:25:40,730 --> 01:25:45,440
 get in are sequential and every task you

3802
01:25:43,810 --> 01:25:45,440
 

3803
01:25:43,820 --> 01:25:49,489
 have is sort of similar to the previous

3804
01:25:45,430 --> 01:25:49,489
 

3805
01:25:45,440 --> 01:25:52,910
 one what you then do is you look at your

3806
01:25:49,479 --> 01:25:52,910
 

3807
01:25:49,489 --> 01:25:55,400
 previous tasks you see where you were

3808
01:25:52,900 --> 01:25:55,400
 

3809
01:25:52,910 --> 01:25:57,230
 wrong this graph is you can see

3810
01:25:55,390 --> 01:25:57,230
 

3811
01:25:55,400 --> 01:25:59,210
 sometimes you were under estimating

3812
01:25:57,220 --> 01:25:59,210
 

3813
01:25:57,230 --> 01:26:01,100
 sometimes you were overestimating and

3814
01:25:59,200 --> 01:26:01,100
 

3815
01:25:59,210 --> 01:26:03,560
 you're gonna transfer that to new tasks

3816
01:26:01,090 --> 01:26:03,560
 

3817
01:26:01,100 --> 01:26:04,789
 because you're going to start with the

3818
01:26:03,550 --> 01:26:04,789
 

3819
01:26:03,560 --> 01:26:06,559
 prior

3820
01:26:04,779 --> 01:26:06,559
 

3821
01:26:04,789 --> 01:26:08,839
 equal to these residuals but goes to

3822
01:26:06,549 --> 01:26:08,839
 

3823
01:26:06,559 --> 01:26:12,260
 these residuals right so if you were

3824
01:26:08,829 --> 01:26:12,260
 

3825
01:26:08,839 --> 01:26:14,119
 previously overestimating this point now

3826
01:26:12,250 --> 01:26:14,119
 

3827
01:26:12,260 --> 01:26:16,159
 you start with a prior that starts under

3828
01:26:14,109 --> 01:26:16,159
 

3829
01:26:14,119 --> 01:26:17,989
 estimating it and that way you correct

3830
01:26:16,149 --> 01:26:17,989
 

3831
01:26:16,159 --> 01:26:20,119
 for that and then you can transfer

3832
01:26:17,979 --> 01:26:20,119
 

3833
01:26:17,989 --> 01:26:24,079
 useful information from previous tasks

3834
01:26:20,109 --> 01:26:24,079
 

3835
01:26:20,119 --> 01:26:26,359
 to the new tasks that's very cool okay

3836
01:26:24,069 --> 01:26:26,359
 

3837
01:26:24,079 --> 01:26:28,879
 this is all without any information

3838
01:26:26,349 --> 01:26:28,879
 

3839
01:26:26,359 --> 01:26:31,219
 about the tasks what if I can now look

3840
01:26:28,869 --> 01:26:31,219
 

3841
01:26:28,879 --> 01:26:34,309
 at my task and tell how it's similar or

3842
01:26:31,209 --> 01:26:34,309
 

3843
01:26:31,219 --> 01:26:37,999
 dissimilar to other tasks now to do that

3844
01:26:34,299 --> 01:26:37,999
 

3845
01:26:34,309 --> 01:26:40,909
 I need some way to characterize my tasks

3846
01:26:37,989 --> 01:26:40,909
 

3847
01:26:37,999 --> 01:26:43,219
 and we call this meta features these are

3848
01:26:40,899 --> 01:26:43,219
 

3849
01:26:40,909 --> 01:26:45,289
 measurable properties of the task that

3850
01:26:43,209 --> 01:26:45,289
 

3851
01:26:43,219 --> 01:26:46,699
 we can use these are simple things like

3852
01:26:45,279 --> 01:26:46,699
 

3853
01:26:45,289 --> 01:26:49,189
 the number of instances and features

3854
01:26:46,689 --> 01:26:49,189
 

3855
01:26:46,699 --> 01:26:51,109
 which will come back to that so we give

3856
01:26:49,179 --> 01:26:51,109
 

3857
01:26:49,189 --> 01:26:53,119
 our meta learner basically our

3858
01:26:51,099 --> 01:26:53,119
 

3859
01:26:51,109 --> 01:26:55,459
 performances or configurations and the

3860
01:26:53,109 --> 01:26:55,459
 

3861
01:26:53,119 --> 01:26:58,669
 meta features of the tasks and then it

3862
01:26:55,449 --> 01:26:58,669
 

3863
01:26:55,459 --> 01:27:01,429
 can use as many features to measure how

3864
01:26:58,659 --> 01:27:01,429
 

3865
01:26:58,669 --> 01:27:03,289
 similar two tasks are and it's very

3866
01:27:01,419 --> 01:27:03,289
 

3867
01:27:01,429 --> 01:27:04,429
 useful information of course if you can

3868
01:27:03,279 --> 01:27:04,429
 

3869
01:27:03,289 --> 01:27:06,919
 if you have a good estimate of how

3870
01:27:04,419 --> 01:27:06,919
 

3871
01:27:04,429 --> 01:27:08,959
 similar a previous task is you can

3872
01:27:06,909 --> 01:27:08,959
 

3873
01:27:06,919 --> 01:27:12,949
 meaningfully transfer more information

3874
01:27:08,949 --> 01:27:12,949
 

3875
01:27:08,959 --> 01:27:14,989
 from that task to the new task so one

3876
01:27:12,939 --> 01:27:14,989
 

3877
01:27:12,949 --> 01:27:17,089
 way of doing this well yeah this meta

3878
01:27:14,979 --> 01:27:17,089
 

3879
01:27:14,989 --> 01:27:18,739
 features one way is having using this

3880
01:27:17,079 --> 01:27:18,739
 

3881
01:27:17,089 --> 01:27:20,719
 handcrafted meta features that have been

3882
01:27:18,729 --> 01:27:20,719
 

3883
01:27:18,739 --> 01:27:22,099
 in literature for a while these are

3884
01:27:20,709 --> 01:27:22,099
 

3885
01:27:20,719 --> 01:27:24,649
 simple things like the number of

3886
01:27:22,089 --> 01:27:24,649
 

3887
01:27:22,099 --> 01:27:27,859
 instances because more instance make

3888
01:27:24,639 --> 01:27:27,859
 

3889
01:27:24,649 --> 01:27:29,419
 easier learning the number of features

3890
01:27:27,849 --> 01:27:29,419
 

3891
01:27:27,859 --> 01:27:32,059
 become more features make it typically

3892
01:27:29,409 --> 01:27:32,059
 

3893
01:27:29,419 --> 01:27:34,539
 harder then with missing values and

3894
01:27:32,049 --> 01:27:34,539
 

3895
01:27:32,059 --> 01:27:37,099
 outliers also make their learning harder

3896
01:27:34,529 --> 01:27:37,099
 

3897
01:27:34,539 --> 01:27:38,929
 statistical things just as skills and

3898
01:27:37,089 --> 01:27:38,929
 

3899
01:27:37,099 --> 01:27:40,819
 good choice it's also correlation is my

3900
01:27:38,919 --> 01:27:40,819
 

3901
01:27:38,929 --> 01:27:42,409
 feature do they have many features that

3902
01:27:40,809 --> 01:27:42,409
 

3903
01:27:40,819 --> 01:27:44,510
 are correlated with my targets that's

3904
01:27:42,399 --> 01:27:44,510
 

3905
01:27:42,409 --> 01:27:46,069
 good do we have features that are

3906
01:27:44,500 --> 01:27:46,069
 

3907
01:27:44,510 --> 01:27:47,689
 correlated in between that's not so good

3908
01:27:46,059 --> 01:27:47,689
 

3909
01:27:46,069 --> 01:27:49,909
 so you want to collect the kind of

3910
01:27:47,679 --> 01:27:49,909
 

3911
01:27:47,689 --> 01:27:52,099
 information there's also things like

3912
01:27:49,899 --> 01:27:52,099
 

3913
01:27:49,909 --> 01:27:53,329
 information theoretic things like class

3914
01:27:52,089 --> 01:27:53,329
 

3915
01:27:52,099 --> 01:27:55,760
 entropy or the mutual information

3916
01:27:53,319 --> 01:27:55,760
 

3917
01:27:53,329 --> 01:27:57,169
 between a feature in a task tells you

3918
01:27:55,750 --> 01:27:57,169
 

3919
01:27:55,760 --> 01:27:59,449
 about how much information there is in

3920
01:27:57,159 --> 01:27:59,449
 

3921
01:27:57,169 --> 01:28:02,149
 the task things like model-based

3922
01:27:59,439 --> 01:28:02,149
 

3923
01:27:59,449 --> 01:28:04,249
 features where you build a simple like

3924
01:28:02,139 --> 01:28:04,249
 

3925
01:28:02,149 --> 01:28:06,349
 decision tree and you see how composite

3926
01:28:04,239 --> 01:28:06,349
 

3927
01:28:04,249 --> 01:28:09,109
 tree is that tells something about how

3928
01:28:06,339 --> 01:28:09,109
 

3929
01:28:06,349 --> 01:28:11,539
 complex a task is things like land

3930
01:28:09,099 --> 01:28:11,539
 

3931
01:28:09,109 --> 01:28:13,129
 markers where you simply run a number of

3932
01:28:11,529 --> 01:28:13,129
 

3933
01:28:11,539 --> 01:28:15,079
 simple algorithms I use the performance

3934
01:28:13,119 --> 01:28:15,079
 

3935
01:28:13,129 --> 01:28:19,179
 as kind of a landmark of how difficult

3936
01:28:15,069 --> 01:28:19,179
 

3937
01:28:15,079 --> 01:28:19,179
 this task is for that type of

3938
01:28:19,940 --> 01:28:19,940
 

3939
01:28:19,950 --> 01:28:24,000
 besides those you can also learn

3940
01:28:21,860 --> 01:28:24,000
 

3941
01:28:21,870 --> 01:28:27,600
 representation that's to be useful for

3942
01:28:23,990 --> 01:28:27,600
 

3943
01:28:24,000 --> 01:28:29,100
 its like images and sounds so here one

3944
01:28:27,590 --> 01:28:29,100
 

3945
01:28:27,600 --> 01:28:32,070
 way of doing this is using deep metric

3946
01:28:29,090 --> 01:28:32,070
 

3947
01:28:29,100 --> 01:28:34,200
 learning and common way of doing this is

3948
01:28:32,060 --> 01:28:34,200
 

3949
01:28:32,070 --> 01:28:36,870
 using Siamese networks so here you take

3950
01:28:34,190 --> 01:28:36,870
 

3951
01:28:34,200 --> 01:28:41,730
 two data sets to image data sets can be

3952
01:28:36,860 --> 01:28:41,730
 

3953
01:28:36,870 --> 01:28:43,830
 Amnesty and cipher you put them true

3954
01:28:41,720 --> 01:28:43,830
 

3955
01:28:41,730 --> 01:28:46,470
 first feature structure can be

3956
01:28:43,820 --> 01:28:46,470
 

3957
01:28:43,830 --> 01:28:48,090
 convolutional nets then you give them

3958
01:28:46,460 --> 01:28:48,090
 

3959
01:28:46,470 --> 01:28:49,470
 too many features structure then you

3960
01:28:48,080 --> 01:28:49,470
 

3961
01:28:48,090 --> 01:28:52,010
 push them through some layers and that

3962
01:28:49,460 --> 01:28:52,010
 

3963
01:28:49,470 --> 01:28:55,590
 at the end gives you a vector for each

3964
01:28:52,000 --> 01:28:55,590
 

3965
01:28:52,010 --> 01:28:58,220
 data set now if they're so similar those

3966
01:28:55,580 --> 01:28:58,220
 

3967
01:28:55,590 --> 01:29:01,260
 sectors are supposed to be similar right

3968
01:28:58,210 --> 01:29:01,260
 

3969
01:28:58,220 --> 01:29:03,210
 so then you are going to use an external

3970
01:29:01,250 --> 01:29:03,210
 

3971
01:29:01,260 --> 01:29:05,760
 ground truth measure for instance using

3972
01:29:03,200 --> 01:29:05,760
 

3973
01:29:03,210 --> 01:29:07,980
 meta features it gives you an error

3974
01:29:05,750 --> 01:29:07,980
 

3975
01:29:05,760 --> 01:29:11,100
 signal and then you can say okay if the

3976
01:29:07,970 --> 01:29:11,100
 

3977
01:29:07,980 --> 01:29:13,170
 two vectors are very dissimilar with the

3978
01:29:11,090 --> 01:29:13,170
 

3979
01:29:11,100 --> 01:29:15,450
 task of actually very similar you back

3980
01:29:13,160 --> 01:29:15,450
 

3981
01:29:13,170 --> 01:29:19,350
 propagated error to the network and that

3982
01:29:15,440 --> 01:29:19,350
 

3983
01:29:15,450 --> 01:29:21,570
 way you you learn a representation so

3984
01:29:19,340 --> 01:29:21,570
 

3985
01:29:19,350 --> 01:29:23,670
 after the training this will allow you

3986
01:29:21,560 --> 01:29:23,670
 

3987
01:29:21,570 --> 01:29:25,410
 to put in a new data set you get a

3988
01:29:23,660 --> 01:29:25,410
 

3989
01:29:23,670 --> 01:29:28,200
 vector back and you can use a vector as

3990
01:29:25,400 --> 01:29:28,200
 

3991
01:29:25,410 --> 01:29:34,680
 a way to measure distances between this

3992
01:29:28,190 --> 01:29:34,680
 

3993
01:29:28,200 --> 01:29:37,140
 task and and your task of that type now

3994
01:29:34,670 --> 01:29:37,140
 

3995
01:29:34,680 --> 01:29:40,020
 if you know how similar two tasks are

3996
01:29:37,130 --> 01:29:40,020
 

3997
01:29:37,140 --> 01:29:42,780
 one thing you want to do is to use

3998
01:29:40,010 --> 01:29:42,780
 

3999
01:29:40,020 --> 01:29:45,060
 things that worked well before right

4000
01:29:42,770 --> 01:29:45,060
 

4001
01:29:42,780 --> 01:29:47,550
 what way of doing this is to a genetic

4002
01:29:45,050 --> 01:29:47,550
 

4003
01:29:45,060 --> 01:29:49,950
 optimization so here you want to start

4004
01:29:47,540 --> 01:29:49,950
 

4005
01:29:47,550 --> 01:29:52,620
 your genetic process your evolution

4006
01:29:49,940 --> 01:29:52,620
 

4007
01:29:49,950 --> 01:29:55,830
 using pipelines or configurations that

4008
01:29:52,610 --> 01:29:55,830
 

4009
01:29:52,620 --> 01:29:57,930
 worked well before so you look at the

4010
01:29:55,820 --> 01:29:57,930
 

4011
01:29:55,830 --> 01:30:00,990
 meta features you use something like the

4012
01:29:57,920 --> 01:30:00,990
 

4013
01:29:57,930 --> 01:30:04,020
 l1 norm to measure which tasks is

4014
01:30:00,980 --> 01:30:04,020
 

4015
01:30:00,990 --> 01:30:05,760
 similar you use the pipelines or the

4016
01:30:04,010 --> 01:30:05,760
 

4017
01:30:04,020 --> 01:30:08,370
 configurations that work well on those

4018
01:30:05,750 --> 01:30:08,370
 

4019
01:30:05,760 --> 01:30:10,530
 similar tasks and you start with those

4020
01:30:08,360 --> 01:30:10,530
 

4021
01:30:08,370 --> 01:30:14,280
 you start with those pipelines that were

4022
01:30:10,520 --> 01:30:14,280
 

4023
01:30:10,530 --> 01:30:15,840
 worked well on similar programs you can

4024
01:30:14,270 --> 01:30:15,840
 

4025
01:30:14,280 --> 01:30:18,620
 also do this for based optimization so

4026
01:30:15,830 --> 01:30:18,620
 

4027
01:30:15,840 --> 01:30:20,580
 here you start you initialize your

4028
01:30:18,610 --> 01:30:20,580
 

4029
01:30:18,620 --> 01:30:22,710
 visualization using a number of

4030
01:30:20,570 --> 01:30:22,710
 

4031
01:30:20,580 --> 01:30:25,200
 configurations that's worked well on

4032
01:30:22,700 --> 01:30:25,200
 

4033
01:30:22,710 --> 01:30:26,760
 similar tasks Franco vada explains that

4034
01:30:25,190 --> 01:30:26,760
 

4035
01:30:25,200 --> 01:30:29,160
 this gives you significant boost in

4036
01:30:26,750 --> 01:30:29,160
 

4037
01:30:26,760 --> 01:30:30,120
 performance and it's something that's

4038
01:30:29,150 --> 01:30:30,120
 

4039
01:30:29,160 --> 01:30:32,400
 always generally

4040
01:30:30,110 --> 01:30:32,400
 

4041
01:30:30,120 --> 01:30:34,200
 useful right you learn which tasks are

4042
01:30:32,390 --> 01:30:34,200
 

4043
01:30:32,400 --> 01:30:38,510
 similar you transfer information from

4044
01:30:34,190 --> 01:30:38,510
 

4045
01:30:34,200 --> 01:30:38,510
 those similar tasks to new tasks

4046
01:30:38,650 --> 01:30:38,650
 

4047
01:30:38,660 --> 01:30:50,670
 another nice approach by nikolewski here

4048
01:30:42,760 --> 01:30:50,670
 

4049
01:30:42,770 --> 01:30:56,640
 at Microsoft it's using collaborative

4050
01:30:50,660 --> 01:30:56,640
 

4051
01:30:50,670 --> 01:30:58,530
 filtering so here you consider that the

4052
01:30:56,630 --> 01:30:58,530
 

4053
01:30:56,640 --> 01:31:03,570
 configurations are rated by the task

4054
01:30:58,520 --> 01:31:03,570
 

4055
01:30:58,530 --> 01:31:06,360
 just as users rate movies here the task

4056
01:31:03,560 --> 01:31:06,360
 

4057
01:31:03,570 --> 01:31:07,950
 radio configuration right if corporation

4058
01:31:06,350 --> 01:31:07,950
 

4059
01:31:06,360 --> 01:31:13,260
 is good for the task it will get a high

4060
01:31:07,940 --> 01:31:13,260
 

4061
01:31:07,950 --> 01:31:14,730
 rating kind of the similar to here so

4062
01:31:13,250 --> 01:31:14,730
 

4063
01:31:13,260 --> 01:31:16,410
 now you build you use matrix

4064
01:31:14,720 --> 01:31:16,410
 

4065
01:31:14,730 --> 01:31:19,890
 factorization as you do before so you

4066
01:31:16,400 --> 01:31:19,890
 

4067
01:31:16,410 --> 01:31:21,240
 start out with this matrix P you fill it

4068
01:31:19,880 --> 01:31:21,240
 

4069
01:31:19,890 --> 01:31:23,820
 with all the evaluations that you have

4070
01:31:21,230 --> 01:31:23,820
 

4071
01:31:21,240 --> 01:31:26,400
 this will be very sparse matrix then you

4072
01:31:23,810 --> 01:31:26,400
 

4073
01:31:23,820 --> 01:31:32,160
 use matrix factorization to learn latent

4074
01:31:26,390 --> 01:31:32,160
 

4075
01:31:26,400 --> 01:31:35,820
 representation for your your your tasks

4076
01:31:32,150 --> 01:31:35,820
 

4077
01:31:32,160 --> 01:31:37,560
 and your configurations and that means

4078
01:31:35,810 --> 01:31:37,560
 

4079
01:31:35,820 --> 01:31:39,900
 that now every configuration every

4080
01:31:37,550 --> 01:31:39,900
 

4081
01:31:37,560 --> 01:31:43,530
 pipeline Avenue architecture you have is

4082
01:31:39,890 --> 01:31:43,530
 

4083
01:31:39,900 --> 01:31:45,780
 one point in this latent space like so

4084
01:31:43,520 --> 01:31:45,780
 

4085
01:31:43,530 --> 01:31:47,520
 now you compressed the information from

4086
01:31:45,770 --> 01:31:47,520
 

4087
01:31:45,780 --> 01:31:49,680
 a very large mission space to a very

4088
01:31:47,510 --> 01:31:49,680
 

4089
01:31:47,520 --> 01:31:52,490
 small dimensional space in its small

4090
01:31:49,670 --> 01:31:52,490
 

4091
01:31:49,680 --> 01:31:55,230
 rational space you can easily fit a

4092
01:31:52,480 --> 01:31:55,230
 

4093
01:31:52,490 --> 01:31:59,250
 bayesian optimizer intubation

4094
01:31:55,220 --> 01:31:59,250
 

4095
01:31:55,230 --> 01:32:02,670
 optimization and you can use that to

4096
01:31:59,240 --> 01:32:02,670
 

4097
01:31:59,250 --> 01:32:04,020
 find better pipelines in your space and

4098
01:32:02,660 --> 01:32:04,020
 

4099
01:32:02,670 --> 01:32:05,580
 of course you need publicity

4100
01:32:04,010 --> 01:32:05,580
 

4101
01:32:04,020 --> 01:32:06,900
 communicator ization because you need

4102
01:32:05,570 --> 01:32:06,900
 

4103
01:32:05,580 --> 01:32:08,370
 the insert machine here but otherwise

4104
01:32:06,890 --> 01:32:08,370
 

4105
01:32:06,900 --> 01:32:12,780
 it's that's it it's a very cool

4106
01:32:08,360 --> 01:32:12,780
 

4107
01:32:08,370 --> 01:32:14,880
 technique as well okay then you can also

4108
01:32:12,770 --> 01:32:14,880
 

4109
01:32:12,780 --> 01:32:17,790
 directly learn a mapping between the

4110
01:32:14,870 --> 01:32:17,790
 

4111
01:32:14,880 --> 01:32:20,760
 meta features and what you want to use

4112
01:32:17,780 --> 01:32:20,760
 

4113
01:32:17,790 --> 01:32:22,800
 first is you want to you can do similar

4114
01:32:20,750 --> 01:32:22,800
 

4115
01:32:20,760 --> 01:32:25,050
 things as before like instead of warm

4116
01:32:22,790 --> 01:32:25,050
 

4117
01:32:22,800 --> 01:32:27,570
 starting by looking at a similar task

4118
01:32:25,040 --> 01:32:27,570
 

4119
01:32:25,050 --> 01:32:30,210
 you can build a metal ER and it takes

4120
01:32:27,560 --> 01:32:30,210
 

4121
01:32:27,570 --> 01:32:34,080
 the meta features and then gives you a

4122
01:32:30,200 --> 01:32:34,080
 

4123
01:32:30,210 --> 01:32:36,540
 ranking of well performing

4124
01:32:34,070 --> 01:32:36,540
 

4125
01:32:34,080 --> 01:32:39,000
 configurations right so this is

4126
01:32:36,530 --> 01:32:39,000
 

4127
01:32:36,540 --> 01:32:41,280
 basically placing the k10 kind the K & N

4128
01:32:38,990 --> 01:32:41,280
 

4129
01:32:39,000 --> 01:32:42,790
 type approach of artists coloring with

4130
01:32:41,270 --> 01:32:42,790
 

4131
01:32:41,280 --> 01:32:44,020
 metal Molech uses

4132
01:32:42,780 --> 01:32:44,020
 

4133
01:32:42,790 --> 01:32:45,580
 when a forest or actually boost

4134
01:32:44,010 --> 01:32:45,580
 

4135
01:32:44,020 --> 01:32:49,300
 something you can learn more complex

4136
01:32:45,570 --> 01:32:49,300
 

4137
01:32:45,580 --> 01:32:50,770
 patterns you can also use this to to

4138
01:32:49,290 --> 01:32:50,770
 

4139
01:32:49,300 --> 01:32:52,600
 look at your meta features and then say

4140
01:32:50,760 --> 01:32:52,600
 

4141
01:32:50,770 --> 01:32:55,180
 okay now for this data that you need to

4142
01:32:52,590 --> 01:32:55,180
 

4143
01:32:52,600 --> 01:32:57,850
 tune these high parameters or you can

4144
01:32:55,170 --> 01:32:57,850
 

4145
01:32:55,180 --> 01:32:59,500
 configure your search phase this way or

4146
01:32:57,840 --> 01:32:59,500
 

4147
01:32:57,850 --> 01:33:02,320
 you can even train meta learners that

4148
01:32:59,490 --> 01:33:02,320
 

4149
01:32:59,500 --> 01:33:04,840
 take the meta features and configuration

4150
01:33:02,310 --> 01:33:04,840
 

4151
01:33:02,320 --> 01:33:08,640
 and then predict the performance on this

4152
01:33:04,830 --> 01:33:08,640
 

4153
01:33:04,840 --> 01:33:10,690
 task this method will perform this well

4154
01:33:08,630 --> 01:33:10,690
 

4155
01:33:08,640 --> 01:33:12,550
 this is sort of like a surrogate model

4156
01:33:10,680 --> 01:33:12,550
 

4157
01:33:10,690 --> 01:33:14,830
 but more general you can use any model

4158
01:33:12,540 --> 01:33:14,830
 

4159
01:33:12,550 --> 01:33:18,160
 for this you can now integrate these

4160
01:33:14,820 --> 01:33:18,160
 

4161
01:33:14,830 --> 01:33:19,780
 meta models in other HTML pipelines was

4162
01:33:18,150 --> 01:33:19,780
 

4163
01:33:18,160 --> 01:33:21,460
 often very useful for instance is to

4164
01:33:19,770 --> 01:33:21,460
 

4165
01:33:19,780 --> 01:33:23,500
 train a meta learner that predicts how

4166
01:33:21,450 --> 01:33:23,500
 

4167
01:33:21,460 --> 01:33:25,390
 long an algorithm will take to run

4168
01:33:23,490 --> 01:33:25,390
 

4169
01:33:23,500 --> 01:33:27,970
 because that way you if you have a

4170
01:33:25,380 --> 01:33:27,970
 

4171
01:33:25,390 --> 01:33:30,580
 choice between two algorithms and one is

4172
01:33:27,960 --> 01:33:30,580
 

4173
01:33:27,970 --> 01:33:32,950
 they're equally performance estimated

4174
01:33:30,570 --> 01:33:32,950
 

4175
01:33:30,580 --> 01:33:35,080
 but one is much faster then you probably

4176
01:33:32,940 --> 01:33:35,080
 

4177
01:33:32,950 --> 01:33:37,540
 want to start with those configurations

4178
01:33:35,070 --> 01:33:37,540
 

4179
01:33:35,080 --> 01:33:39,700
 to those algorithms that are faster it

4180
01:33:37,530 --> 01:33:39,700
 

4181
01:33:37,540 --> 01:33:46,750
 helps you make predictions to search the

4182
01:33:39,690 --> 01:33:46,750
 

4183
01:33:39,700 --> 01:33:49,120
 space more efficiently okay now we sell

4184
01:33:46,740 --> 01:33:49,120
 

4185
01:33:46,750 --> 01:33:53,380
 them learn a complete algorithm from

4186
01:33:49,110 --> 01:33:53,380
 

4187
01:33:49,120 --> 01:33:55,900
 scratch we often decompose the problem

4188
01:33:53,370 --> 01:33:55,900
 

4189
01:33:53,380 --> 01:33:57,850
 into several parts and let me solve

4190
01:33:55,890 --> 01:33:57,850
 

4191
01:33:55,900 --> 01:33:59,560
 those part individually and one way that

4192
01:33:57,840 --> 01:33:59,560
 

4193
01:33:57,850 --> 01:34:01,630
 we do this is using these pipelines

4194
01:33:59,550 --> 01:34:01,630
 

4195
01:33:59,560 --> 01:34:03,730
 right we don't write an elegant from

4196
01:34:01,620 --> 01:34:03,730
 

4197
01:34:01,630 --> 01:34:05,740
 scratch we first pre-process the data

4198
01:34:03,720 --> 01:34:05,740
 

4199
01:34:03,730 --> 01:34:07,390
 using existing algorithms that we learn

4200
01:34:05,730 --> 01:34:07,390
 

4201
01:34:05,740 --> 01:34:09,070
 from data using existing algorithms and

4202
01:34:07,380 --> 01:34:09,070
 

4203
01:34:07,390 --> 01:34:11,050
 this partner structure is something we

4204
01:34:09,060 --> 01:34:11,050
 

4205
01:34:09,070 --> 01:34:16,720
 all use and it helps to significantly

4206
01:34:11,040 --> 01:34:16,720
 

4207
01:34:11,050 --> 01:34:19,750
 reduce our search space so ok but if you

4208
01:34:16,710 --> 01:34:19,750
 

4209
01:34:16,720 --> 01:34:21,100
 want to explore a space that's defined

4210
01:34:19,740 --> 01:34:21,100
 

4211
01:34:19,750 --> 01:34:25,510
 by the pipeline how do you do that

4212
01:34:21,090 --> 01:34:25,510
 

4213
01:34:21,100 --> 01:34:27,790
 efficiently and well if you can do this

4214
01:34:25,500 --> 01:34:27,790
 

4215
01:34:25,510 --> 01:34:29,860
 then it's easier to learn because the

4216
01:34:27,780 --> 01:34:29,860
 

4217
01:34:27,790 --> 01:34:32,860
 individual parts are easier to learn you

4218
01:34:29,850 --> 01:34:32,860
 

4219
01:34:29,860 --> 01:34:34,900
 can transfer information from one from

4220
01:34:32,850 --> 01:34:34,900
 

4221
01:34:32,860 --> 01:34:38,100
 one part onto another pipeline and it's

4222
01:34:34,890 --> 01:34:38,100
 

4223
01:34:34,900 --> 01:34:41,940
 also more robust right so one way of

4224
01:34:38,090 --> 01:34:41,940
 

4225
01:34:38,100 --> 01:34:44,290
 constrain this place is to substitute

4226
01:34:41,930 --> 01:34:44,290
 

4227
01:34:41,940 --> 01:34:48,520
 discretize the space and only try a

4228
01:34:44,280 --> 01:34:48,520
 

4229
01:34:44,290 --> 01:34:49,900
 number of fixed pipelines and this is

4230
01:34:48,510 --> 01:34:49,900
 

4231
01:34:48,520 --> 01:34:52,630
 what first is done in this work by

4232
01:34:49,890 --> 01:34:52,630
 

4233
01:34:49,900 --> 01:34:54,590
 nicola fousey where they in beforehand

4234
01:34:52,620 --> 01:34:54,590
 

4235
01:34:52,630 --> 01:34:58,270
 discretize the space this

4236
01:34:54,580 --> 01:34:58,270
 

4237
01:34:54,590 --> 01:35:01,610
 apparently works quite well sometimes

4238
01:34:58,260 --> 01:35:01,610
 

4239
01:34:58,270 --> 01:35:03,020
 you can also impose a fixed structure on

4240
01:35:01,600 --> 01:35:03,020
 

4241
01:35:01,610 --> 01:35:04,820
 the pipeline this is what artists

4242
01:35:03,010 --> 01:35:04,820
 

4243
01:35:03,020 --> 01:35:07,630
 Killearn does they start with a number

4244
01:35:04,810 --> 01:35:07,630
 

4245
01:35:04,820 --> 01:35:10,520
 of pre-processing steps then number of

4246
01:35:07,620 --> 01:35:10,520
 

4247
01:35:07,630 --> 01:35:13,040
 yeah things like BCA pitches section and

4248
01:35:10,510 --> 01:35:13,040
 

4249
01:35:10,520 --> 01:35:15,980
 then they build a classifier and you

4250
01:35:13,030 --> 01:35:15,980
 

4251
01:35:13,040 --> 01:35:18,020
 know this again constrains the space of

4252
01:35:15,970 --> 01:35:18,020
 

4253
01:35:15,980 --> 01:35:21,080
 what you only these patterns you can

4254
01:35:18,010 --> 01:35:21,080
 

4255
01:35:18,020 --> 01:35:23,150
 build nothing you can do is hierarchical

4256
01:35:21,070 --> 01:35:23,150
 

4257
01:35:21,080 --> 01:35:25,639
 task planning where you break down the

4258
01:35:23,140 --> 01:35:25,639
 

4259
01:35:23,150 --> 01:35:27,110
 tasks into subtasks and again subtasks

4260
01:35:25,629 --> 01:35:27,110
 

4261
01:35:25,639 --> 01:35:29,300
 so for instance beginning you choose

4262
01:35:27,100 --> 01:35:29,300
 

4263
01:35:27,110 --> 01:35:31,909
 where you're gonna classify this using a

4264
01:35:29,290 --> 01:35:31,909
 

4265
01:35:29,300 --> 01:35:33,349
 pipeline or end to end to the neural net

4266
01:35:31,899 --> 01:35:33,349
 

4267
01:35:31,909 --> 01:35:35,510
 and depending on the choice you have

4268
01:35:33,339 --> 01:35:35,510
 

4269
01:35:33,349 --> 01:35:36,530
 other choices and it again helps you to

4270
01:35:35,500 --> 01:35:36,530
 

4271
01:35:35,510 --> 01:35:38,599
 to bait-and-switch

4272
01:35:36,520 --> 01:35:38,599
 

4273
01:35:36,530 --> 01:35:40,369
 and here you can use meta learning again

4274
01:35:38,589 --> 01:35:40,369
 

4275
01:35:38,599 --> 01:35:42,440
 to make decisions you can use worm

4276
01:35:40,359 --> 01:35:42,440
 

4277
01:35:40,369 --> 01:35:44,869
 starting to start with pipelines that

4278
01:35:42,430 --> 01:35:44,869
 

4279
01:35:42,440 --> 01:35:46,820
 worked well before or if you have to

4280
01:35:44,859 --> 01:35:46,820
 

4281
01:35:44,869 --> 01:35:47,290
 make a decision between including other

4282
01:35:46,810 --> 01:35:47,290
 

4283
01:35:46,820 --> 01:35:49,880
 including

4284
01:35:47,280 --> 01:35:49,880
 

4285
01:35:47,290 --> 01:35:51,679
 components or going let's all right in

4286
01:35:49,870 --> 01:35:51,679
 

4287
01:35:49,880 --> 01:35:53,929
 the search tree you can use meta models

4288
01:35:51,669 --> 01:35:53,929
 

4289
01:35:51,679 --> 01:35:56,090
 that advise you what you should do and

4290
01:35:53,919 --> 01:35:56,090
 

4291
01:35:53,929 --> 01:36:00,530
 you can again search the space more

4292
01:35:56,080 --> 01:36:00,530
 

4293
01:35:56,090 --> 01:36:02,270
 efficiently what particular way is using

4294
01:36:00,520 --> 01:36:02,270
 

4295
01:36:00,530 --> 01:36:03,650
 pipelines so there of course she's not

4296
01:36:02,260 --> 01:36:03,650
 

4297
01:36:02,270 --> 01:36:05,330
 gonna star with very complex pipelines

4298
01:36:03,640 --> 01:36:05,330
 

4299
01:36:03,650 --> 01:36:07,610
 you start with very simple pipelines and

4300
01:36:05,320 --> 01:36:07,610
 

4301
01:36:05,330 --> 01:36:09,590
 you evolve them as much as needed

4302
01:36:07,600 --> 01:36:09,590
 

4303
01:36:07,610 --> 01:36:13,699
 right so the complexity is defined by

4304
01:36:09,580 --> 01:36:13,699
 

4305
01:36:09,590 --> 01:36:15,530
 the complexity of the problem and again

4306
01:36:13,689 --> 01:36:15,530
 

4307
01:36:13,699 --> 01:36:17,659
 you can then evolve them the mechanisms

4308
01:36:15,520 --> 01:36:17,659
 

4309
01:36:15,530 --> 01:36:19,610
 you know you can take a pipeline and

4310
01:36:17,649 --> 01:36:19,610
 

4311
01:36:17,659 --> 01:36:21,080
 then you can mutate it so you can change

4312
01:36:19,600 --> 01:36:21,080
 

4313
01:36:19,610 --> 01:36:22,909
 a component okay you'll move it or you

4314
01:36:21,070 --> 01:36:22,909
 

4315
01:36:21,080 --> 01:36:25,429
 can add one or you can tune the high

4316
01:36:22,899 --> 01:36:25,429
 

4317
01:36:22,909 --> 01:36:27,290
 parameters or you can do cross over

4318
01:36:25,419 --> 01:36:27,290
 

4319
01:36:25,429 --> 01:36:29,300
 where you take two pipelines and then

4320
01:36:27,280 --> 01:36:29,300
 

4321
01:36:27,290 --> 01:36:31,130
 you cross them over you get two new

4322
01:36:29,290 --> 01:36:31,130
 

4323
01:36:29,300 --> 01:36:34,869
 pipelines and then you hope that these

4324
01:36:31,120 --> 01:36:34,869
 

4325
01:36:31,130 --> 01:36:38,060
 do better then and once you have before

4326
01:36:34,859 --> 01:36:38,060
 

4327
01:36:34,869 --> 01:36:39,469
 this again allows you a more efficient

4328
01:36:38,050 --> 01:36:39,469
 

4329
01:36:38,060 --> 01:36:40,820
 space and also if you discovered

4330
01:36:39,459 --> 01:36:40,820
 

4331
01:36:39,469 --> 01:36:43,849
 something like a pipeline it does a

4332
01:36:40,810 --> 01:36:43,849
 

4333
01:36:40,820 --> 01:36:46,190
 crucial step in your process this this

4334
01:36:43,839 --> 01:36:46,190
 

4335
01:36:43,849 --> 01:36:48,080
 this part of pipeline gets populated

4336
01:36:46,180 --> 01:36:48,080
 

4337
01:36:46,190 --> 01:36:51,520
 more and more often so if this

4338
01:36:48,070 --> 01:36:51,520
 

4339
01:36:48,080 --> 01:36:53,570
 information gets spread and more useful

4340
01:36:51,510 --> 01:36:53,570
 

4341
01:36:51,520 --> 01:36:55,460
 so different approaches for doing this

4342
01:36:53,560 --> 01:36:55,460
 

4343
01:36:53,570 --> 01:36:57,800
 the most well-known is teapots

4344
01:36:55,450 --> 01:36:57,800
 

4345
01:36:55,460 --> 01:37:00,349
 so these dis builds tree based pipelines

4346
01:36:57,790 --> 01:37:00,349
 

4347
01:36:57,800 --> 01:37:03,349
 basically you start with a data set you

4348
01:37:00,339 --> 01:37:03,349
 

4349
01:37:00,349 --> 01:37:06,390
 take multiple copies you build some of

4350
01:37:03,339 --> 01:37:06,390
 

4351
01:37:03,349 --> 01:37:08,910
 pipelines like you do PCA or you do

4352
01:37:06,380 --> 01:37:08,910
 

4353
01:37:06,390 --> 01:37:10,860
 at some point those pipelines can come

4354
01:37:08,900 --> 01:37:10,860
 

4355
01:37:08,910 --> 01:37:12,720
 together in a feature joint so you take

4356
01:37:10,850 --> 01:37:12,720
 

4357
01:37:10,860 --> 01:37:14,970
 these features and these features this

4358
01:37:12,710 --> 01:37:14,970
 

4359
01:37:12,720 --> 01:37:17,130
 becomes your new pipeline and so these

4360
01:37:14,960 --> 01:37:17,130
 

4361
01:37:14,970 --> 01:37:18,420
 pipelines are like tree shapes and this

4362
01:37:17,120 --> 01:37:18,420
 

4363
01:37:17,130 --> 01:37:20,990
 allows you to build very complex

4364
01:37:18,410 --> 01:37:20,990
 

4365
01:37:18,420 --> 01:37:23,910
 pipelines very efficiently

4366
01:37:20,980 --> 01:37:23,910
 

4367
01:37:20,990 --> 01:37:25,950
 another method is gamma very new this is

4368
01:37:23,900 --> 01:37:25,950
 

4369
01:37:23,910 --> 01:37:28,200
 more of the same within a synchronous

4370
01:37:25,940 --> 01:37:28,200
 

4371
01:37:25,950 --> 01:37:31,200
 way so you're not stuck waiting for

4372
01:37:28,190 --> 01:37:31,200
 

4373
01:37:28,200 --> 01:37:35,150
 stragglers in your in your generation

4374
01:37:31,190 --> 01:37:35,150
 

4375
01:37:31,200 --> 01:37:37,950
 this this evolution is synchronously and

4376
01:37:35,140 --> 01:37:37,950
 

4377
01:37:35,150 --> 01:37:40,170
 recipe does a grammar based approach and

4378
01:37:37,940 --> 01:37:40,170
 

4379
01:37:37,950 --> 01:37:42,600
 again you can use meta learning although

4380
01:37:40,160 --> 01:37:42,600
 

4381
01:37:40,170 --> 01:37:45,180
 it's not really done yet so much you can

4382
01:37:42,590 --> 01:37:45,180
 

4383
01:37:42,600 --> 01:37:47,130
 use it efficiently for instance for worm

4384
01:37:45,170 --> 01:37:47,130
 

4385
01:37:45,180 --> 01:37:49,350
 starting the search or for using meta

4386
01:37:47,120 --> 01:37:49,350
 

4387
01:37:47,130 --> 01:37:54,180
 models that make help you choose between

4388
01:37:49,340 --> 01:37:54,180
 

4389
01:37:49,350 --> 01:37:55,980
 component between options another

4390
01:37:54,170 --> 01:37:55,980
 

4391
01:37:54,180 --> 01:38:00,510
 interesting approach is building

4392
01:37:55,970 --> 01:38:00,510
 

4393
01:37:55,980 --> 01:38:01,740
 pipelines to self play so so here again

4394
01:38:00,500 --> 01:38:01,740
 

4395
01:38:00,510 --> 01:38:04,740
 you build pipelines by inserting

4396
01:38:01,730 --> 01:38:04,740
 

4397
01:38:01,740 --> 01:38:07,350
 deleting and so on but you start off

4398
01:38:04,730 --> 01:38:07,350
 

4399
01:38:04,740 --> 01:38:09,660
 with a number of trees dodging wrongly

4400
01:38:07,340 --> 01:38:09,660
 

4401
01:38:07,350 --> 01:38:12,990
 generated you evaluate them and then you

4402
01:38:09,650 --> 01:38:12,990
 

4403
01:38:09,660 --> 01:38:16,380
 train a neural nets to predict how well

4404
01:38:12,980 --> 01:38:16,380
 

4405
01:38:12,990 --> 01:38:18,750
 a configuration is going to work then

4406
01:38:16,370 --> 01:38:18,750
 

4407
01:38:16,380 --> 01:38:20,430
 this new attack also predicts some

4408
01:38:18,740 --> 01:38:20,430
 

4409
01:38:18,750 --> 01:38:22,980
 actions you want to take like adding a

4410
01:38:20,420 --> 01:38:22,980
 

4411
01:38:20,430 --> 01:38:24,750
 component or moving components and you

4412
01:38:22,970 --> 01:38:24,750
 

4413
01:38:22,980 --> 01:38:26,910
 send that information to your Monte

4414
01:38:24,740 --> 01:38:26,910
 

4415
01:38:24,750 --> 01:38:28,590
 Carlo tree search within does a number

4416
01:38:26,900 --> 01:38:28,590
 

4417
01:38:26,910 --> 01:38:31,380
 of simulations so it both number of

4418
01:38:28,580 --> 01:38:31,380
 

4419
01:38:28,590 --> 01:38:34,200
 pipelines goes to network ask is this

4420
01:38:31,370 --> 01:38:34,200
 

4421
01:38:31,380 --> 01:38:36,510
 going to work well or not yes then I

4422
01:38:34,190 --> 01:38:36,510
 

4423
01:38:34,200 --> 01:38:39,720
 pass down or do I do more of these kind

4424
01:38:36,500 --> 01:38:39,720
 

4425
01:38:36,510 --> 01:38:42,060
 of simulations and at the ends it uses

4426
01:38:39,710 --> 01:38:42,060
 

4427
01:38:39,720 --> 01:38:44,970
 the feedback from the the neural network

4428
01:38:42,050 --> 01:38:44,970
 

4429
01:38:42,060 --> 01:38:48,120
 to generate good pipelines and the new

4430
01:38:44,960 --> 01:38:48,120
 

4431
01:38:44,970 --> 01:38:50,430
 network gets evaluated pipelines learns

4432
01:38:48,110 --> 01:38:50,430
 

4433
01:38:48,120 --> 01:38:52,350
 more about which departments work well

4434
01:38:50,420 --> 01:38:52,350
 

4435
01:38:50,430 --> 01:38:54,330
 and this way the algorithm basically

4436
01:38:52,340 --> 01:38:54,330
 

4437
01:38:52,350 --> 01:38:55,800
 does cells play with each other so so

4438
01:38:54,320 --> 01:38:55,800
 

4439
01:38:54,330 --> 01:38:59,370
 there's no input here there's nothing

4440
01:38:55,790 --> 01:38:59,370
 

4441
01:38:55,800 --> 01:39:04,440
 comes from before it just does self play

4442
01:38:59,360 --> 01:39:04,440
 

4443
01:38:59,370 --> 01:39:06,840
 and explores by itself ok then finally

4444
01:39:04,430 --> 01:39:06,840
 

4445
01:39:04,440 --> 01:39:09,600
 you want to learn from train models and

4446
01:39:06,830 --> 01:39:09,600
 

4447
01:39:06,840 --> 01:39:11,700
 so here the model that the tasks have to

4448
01:39:09,590 --> 01:39:11,700
 

4449
01:39:09,600 --> 01:39:14,910
 be very similar but you cannot transfer

4450
01:39:11,690 --> 01:39:14,910
 

4451
01:39:11,700 --> 01:39:17,580
 from very different tasks so here your

4452
01:39:14,900 --> 01:39:17,580
 

4453
01:39:14,910 --> 01:39:18,880
 meta learner gets performances it gets

4454
01:39:17,570 --> 01:39:18,880
 

4455
01:39:17,580 --> 01:39:20,350
 configurations actually

4456
01:39:18,870 --> 01:39:20,350
 

4457
01:39:18,880 --> 01:39:22,060
 how did you build this new net for

4458
01:39:20,340 --> 01:39:22,060
 

4459
01:39:20,350 --> 01:39:24,550
 instance and it gets the model

4460
01:39:22,050 --> 01:39:24,550
 

4461
01:39:22,060 --> 01:39:26,590
 parameters of the trained models and

4462
01:39:24,540 --> 01:39:26,590
 

4463
01:39:24,550 --> 01:39:31,390
 then it has to use those to learn more

4464
01:39:26,580 --> 01:39:31,390
 

4465
01:39:26,590 --> 01:39:32,800
 efficiently the easiest way that we all

4466
01:39:31,380 --> 01:39:32,800
 

4467
01:39:31,390 --> 01:39:35,020
 know is transfer learning so here you

4468
01:39:32,790 --> 01:39:35,020
 

4469
01:39:32,800 --> 01:39:37,600
 have a source task and a target task and

4470
01:39:35,010 --> 01:39:37,600
 

4471
01:39:35,020 --> 01:39:40,000
 you need a way to find which source

4472
01:39:37,590 --> 01:39:40,000
 

4473
01:39:37,600 --> 01:39:43,300
 stars are similar if you do you can

4474
01:39:39,990 --> 01:39:43,300
 

4475
01:39:40,000 --> 01:39:46,020
 transfer models from sauce tasks to the

4476
01:39:43,290 --> 01:39:46,020
 

4477
01:39:43,300 --> 01:39:48,670
 new tasks and thirdly this works by

4478
01:39:46,010 --> 01:39:48,670
 

4479
01:39:46,020 --> 01:39:50,410
 either using that model as a starting

4480
01:39:48,660 --> 01:39:50,410
 

4481
01:39:48,670 --> 01:39:53,140
 point and then you tweak it later on

4482
01:39:50,400 --> 01:39:53,140
 

4483
01:39:50,410 --> 01:39:55,000
 are you free certain parts like you you

4484
01:39:53,130 --> 01:39:55,000
 

4485
01:39:53,140 --> 01:39:57,010
 freeze the architecture but you update

4486
01:39:54,990 --> 01:39:57,010
 

4487
01:39:55,000 --> 01:39:59,260
 the weights and there's different ways

4488
01:39:57,000 --> 01:39:59,260
 

4489
01:39:57,010 --> 01:40:00,640
 of doing this been a lot of work here

4490
01:39:59,250 --> 01:40:00,640
 

4491
01:39:59,260 --> 01:40:02,200
 for instance in Bayesian networks where

4492
01:40:00,630 --> 01:40:02,200
 

4493
01:40:00,640 --> 01:40:04,660
 you start with the trained Bayesian nets

4494
01:40:02,190 --> 01:40:04,660
 

4495
01:40:02,200 --> 01:40:07,210
 on a similar task and you do minor

4496
01:40:04,650 --> 01:40:07,210
 

4497
01:40:04,660 --> 01:40:08,440
 changes until you find a new patient net

4498
01:40:07,200 --> 01:40:08,440
 

4499
01:40:07,210 --> 01:40:10,390
 that works well for your task or

4500
01:40:08,430 --> 01:40:10,390
 

4501
01:40:08,440 --> 01:40:12,970
 reinforcement learning you can go from

4502
01:40:10,380 --> 01:40:12,970
 

4503
01:40:10,390 --> 01:40:15,900
 one task to a sinner task by taking a

4504
01:40:12,960 --> 01:40:15,900
 

4505
01:40:12,970 --> 01:40:17,980
 policy and only changing it a little bit

4506
01:40:15,890 --> 01:40:17,980
 

4507
01:40:15,900 --> 01:40:20,190
 now something we've all been doing

4508
01:40:17,970 --> 01:40:20,190
 

4509
01:40:17,980 --> 01:40:23,050
 probably in our universities these days

4510
01:40:20,180 --> 01:40:23,050
 

4511
01:40:20,190 --> 01:40:26,820
 is transfer learning which you know that

4512
01:40:23,040 --> 01:40:26,820
 

4513
01:40:23,050 --> 01:40:30,220
 works because there it works very well

4514
01:40:26,810 --> 01:40:30,220
 

4515
01:40:26,820 --> 01:40:32,400
 this first the thing we all do in chaos

4516
01:40:30,210 --> 01:40:32,400
 

4517
01:40:30,220 --> 01:40:35,110
 for instance we take a neural network

4518
01:40:32,390 --> 01:40:35,110
 

4519
01:40:32,400 --> 01:40:38,050
 coalition neural Nets we build layers

4520
01:40:35,100 --> 01:40:38,050
 

4521
01:40:35,110 --> 01:40:40,270
 and if you look at the filters at each

4522
01:40:38,040 --> 01:40:40,270
 

4523
01:40:38,050 --> 01:40:42,610
 block here in this convolutional nets

4524
01:40:40,260 --> 01:40:42,610
 

4525
01:40:40,270 --> 01:40:47,080
 you can see that first it learns simple

4526
01:40:42,600 --> 01:40:47,080
 

4527
01:40:42,610 --> 01:40:48,850
 things like like its lines and colors

4528
01:40:47,070 --> 01:40:48,850
 

4529
01:40:47,080 --> 01:40:51,040
 then you get more complex things like

4530
01:40:48,840 --> 01:40:51,040
 

4531
01:40:48,850 --> 01:40:53,920
 dots and later on you detect eyes and

4532
01:40:51,030 --> 01:40:53,920
 

4533
01:40:51,040 --> 01:40:58,450
 shapes and so on right so you can use

4534
01:40:53,910 --> 01:40:58,450
 

4535
01:40:53,920 --> 01:41:01,660
 this as the part that learns useful

4536
01:40:58,440 --> 01:41:01,660
 

4537
01:40:58,450 --> 01:41:05,050
 features and then you can learn other

4538
01:41:01,650 --> 01:41:05,050
 

4539
01:41:01,660 --> 01:41:06,790
 top tasks just by adding to it and so

4540
01:41:05,040 --> 01:41:06,790
 

4541
01:41:05,050 --> 01:41:10,030
 here's very important to see how similar

4542
01:41:06,780 --> 01:41:10,030
 

4543
01:41:06,790 --> 01:41:12,430
 tasks are if they're very similar then

4544
01:41:10,020 --> 01:41:12,430
 

4545
01:41:10,030 --> 01:41:13,960
 you can well and if it's small you can

4546
01:41:12,420 --> 01:41:13,960
 

4547
01:41:12,430 --> 01:41:15,790
 typically just freeze the network and

4548
01:41:13,950 --> 01:41:15,790
 

4549
01:41:13,960 --> 01:41:18,070
 then add some laser the end and you just

4550
01:41:15,780 --> 01:41:18,070
 

4551
01:41:15,790 --> 01:41:19,930
 change you just training new layers if

4552
01:41:18,060 --> 01:41:19,930
 

4553
01:41:18,070 --> 01:41:22,120
 the desert is large and similar you just

4554
01:41:19,920 --> 01:41:22,120
 

4555
01:41:19,930 --> 01:41:24,220
 add lays at the end you we trained and

4556
01:41:22,110 --> 01:41:24,220
 

4557
01:41:22,120 --> 01:41:27,130
 joined and if it's large the difference

4558
01:41:24,210 --> 01:41:27,130
 

4559
01:41:24,220 --> 01:41:29,560
 you want to unfreeze part of the network

4560
01:41:27,120 --> 01:41:29,560
 

4561
01:41:27,130 --> 01:41:30,699
 and we trained it again but you can you

4562
01:41:29,550 --> 01:41:30,699
 

4563
01:41:29,560 --> 01:41:32,640
 need more data

4564
01:41:30,689 --> 01:41:32,640
 

4565
01:41:30,699 --> 01:41:35,469
 but if you would all these approaches

4566
01:41:32,630 --> 01:41:35,469
 

4567
01:41:32,640 --> 01:41:37,449
 this will fail if your tasks are not

4568
01:41:35,459 --> 01:41:37,449
 

4569
01:41:35,469 --> 01:41:38,650
 similar enough this is something we

4570
01:41:37,439 --> 01:41:38,650
 

4571
01:41:37,449 --> 01:41:41,170
 don't really understand right now we

4572
01:41:38,640 --> 01:41:41,170
 

4573
01:41:38,650 --> 01:41:43,390
 don't know when a task is similar enough

4574
01:41:41,160 --> 01:41:43,390
 

4575
01:41:41,170 --> 01:41:49,000
 for transferring or not we can just try

4576
01:41:43,380 --> 01:41:49,000
 

4577
01:41:43,390 --> 01:41:51,160
 it and we adapt when it doesn't work so

4578
01:41:48,990 --> 01:41:51,160
 

4579
01:41:49,000 --> 01:41:53,199
 there are a lot less all work in real

4580
01:41:51,150 --> 01:41:53,199
 

4581
01:41:51,160 --> 01:41:58,540
 learning to learn learning to learn new

4582
01:41:53,189 --> 01:41:58,540
 

4583
01:41:53,199 --> 01:42:01,630
 algorithms to data so one critical

4584
01:41:58,530 --> 01:42:01,630
 

4585
01:41:58,540 --> 01:42:05,380
 example is the one from Joshua and Japan

4586
01:42:01,620 --> 01:42:05,380
 

4587
01:42:01,630 --> 01:42:06,940
 geo who actually well they consider the

4588
01:42:05,370 --> 01:42:06,940
 

4589
01:42:05,380 --> 01:42:09,250
 brains probably down to backdrops so we

4590
01:42:06,930 --> 01:42:09,250
 

4591
01:42:06,940 --> 01:42:13,230
 couldn't replace the way it's simple

4592
01:42:09,240 --> 01:42:13,230
 

4593
01:42:09,250 --> 01:42:17,020
 parametric update rule that was inspired

4594
01:42:13,220 --> 01:42:17,020
 

4595
01:42:13,230 --> 01:42:20,350
 from a neurology so it's update rule we

4596
01:42:17,010 --> 01:42:20,350
 

4597
01:42:17,020 --> 01:42:23,500
 have the learning rate some pre synaptic

4598
01:42:20,340 --> 01:42:23,500
 

4599
01:42:20,350 --> 01:42:25,810
 activity then we forced a signal and you

4600
01:42:23,490 --> 01:42:25,810
 

4601
01:42:23,500 --> 01:42:28,989
 penetrate furniture eyes this and then

4602
01:42:25,800 --> 01:42:28,989
 

4603
01:42:25,810 --> 01:42:30,580
 you you learn basically what the optimal

4604
01:42:28,979 --> 01:42:30,580
 

4605
01:42:28,989 --> 01:42:31,810
 settings are for this update rule and

4606
01:42:30,570 --> 01:42:31,810
 

4607
01:42:30,580 --> 01:42:35,610
 you can you can search for different

4608
01:42:31,800 --> 01:42:35,610
 

4609
01:42:31,810 --> 01:42:37,929
 update rules this way later on

4610
01:42:35,600 --> 01:42:37,929
 

4611
01:42:35,610 --> 01:42:38,830
 provenance on and Johnson they replace

4612
01:42:37,919 --> 01:42:38,830
 

4613
01:42:37,929 --> 01:42:41,350
 it with a neural net

4614
01:42:38,820 --> 01:42:41,350
 

4615
01:42:38,830 --> 01:42:43,120
 so here the the weights of funeral have

4616
01:42:41,340 --> 01:42:43,120
 

4617
01:42:41,350 --> 01:42:45,610
 become your high parameters and then you

4618
01:42:43,110 --> 01:42:45,610
 

4619
01:42:43,120 --> 01:42:47,530
 kind of optimize those so after

4620
01:42:45,600 --> 01:42:47,530
 

4621
01:42:45,610 --> 01:42:50,980
 optimization you end up with a new

4622
01:42:47,520 --> 01:42:50,980
 

4623
01:42:47,530 --> 01:42:52,690
 update rule that you can use and then

4624
01:42:50,970 --> 01:42:52,690
 

4625
01:42:50,980 --> 01:42:55,989
 afterwards this was replaced again by an

4626
01:42:52,680 --> 01:42:55,989
 

4627
01:42:52,690 --> 01:42:59,410
 LST M all right so you use to LST m to

4628
01:42:55,979 --> 01:42:59,410
 

4629
01:42:55,989 --> 01:43:03,070
 learn a good update rule for Fortran

4630
01:42:59,400 --> 01:43:03,070
 

4631
01:42:59,410 --> 01:43:04,360
 neural nets that proved out to be not so

4632
01:43:03,060 --> 01:43:04,360
 

4633
01:43:03,070 --> 01:43:06,370
 scalable and then nothing happened for

4634
01:43:04,350 --> 01:43:06,370
 

4635
01:43:04,360 --> 01:43:07,890
 well not not so much happens in the

4636
01:43:06,360 --> 01:43:07,890
 

4637
01:43:06,370 --> 01:43:11,830
 meantime

4638
01:43:07,880 --> 01:43:11,830
 

4639
01:43:07,890 --> 01:43:17,650
 but then marshland and reach of each

4640
01:43:11,820 --> 01:43:17,650
 

4641
01:43:11,830 --> 01:43:21,989
 colleagues at deep mind they replaced

4642
01:43:17,640 --> 01:43:21,989
 

4643
01:43:17,650 --> 01:43:24,190
 this Alice TM with accordin twice LCM

4644
01:43:21,979 --> 01:43:24,190
 

4645
01:43:21,989 --> 01:43:27,449
 which is much more scalable much more

4646
01:43:24,180 --> 01:43:27,449
 

4647
01:43:24,190 --> 01:43:30,340
 flexible the rest the green walk here

4648
01:43:27,439 --> 01:43:30,340
 

4649
01:43:27,449 --> 01:43:34,600
 and then you visit your meta learner a

4650
01:43:30,330 --> 01:43:34,600
 

4651
01:43:30,340 --> 01:43:37,390
 metal model the optimizer and this has

4652
01:43:34,590 --> 01:43:37,390
 

4653
01:43:34,600 --> 01:43:41,679
 to learn the update rule for a model

4654
01:43:37,380 --> 01:43:41,679
 

4655
01:43:37,390 --> 01:43:45,219
 deal to my Z to have a new word so

4656
01:43:41,669 --> 01:43:45,219
 

4657
01:43:41,679 --> 01:43:47,110
 at some point this lsdm gets an error

4658
01:43:45,209 --> 01:43:47,110
 

4659
01:43:45,219 --> 01:43:49,150
 signal from the Altima see from the

4660
01:43:47,100 --> 01:43:49,150
 

4661
01:43:47,110 --> 01:43:50,890
 model and has this current state and

4662
01:43:49,140 --> 01:43:50,890
 

4663
01:43:49,150 --> 01:43:53,380
 then use that to make a prediction about

4664
01:43:50,880 --> 01:43:53,380
 

4665
01:43:50,890 --> 01:43:54,250
 how the way it should be updated just

4666
01:43:53,370 --> 01:43:54,250
 

4667
01:43:53,380 --> 01:43:57,190
 this GG here

4668
01:43:54,240 --> 01:43:57,190
 

4669
01:43:54,250 --> 01:44:01,780
 so here the axis this time again so the

4670
01:43:57,180 --> 01:44:01,780
 

4671
01:43:57,190 --> 01:44:03,969
 original weights come in then you add

4672
01:44:01,770 --> 01:44:03,969
 

4673
01:44:01,780 --> 01:44:06,580
 the u8 update to it you get new weights

4674
01:44:03,959 --> 01:44:06,580
 

4675
01:44:03,969 --> 01:44:09,370
 you can Error error signal the lsdm

4676
01:44:06,570 --> 01:44:09,370
 

4677
01:44:06,580 --> 01:44:11,710
 gives you a new update and you move all

4678
01:44:09,360 --> 01:44:11,710
 

4679
01:44:09,370 --> 01:44:14,110
 the way to the right at some point there

4680
01:44:11,700 --> 01:44:14,110
 

4681
01:44:11,710 --> 01:44:17,230
 is evaluation you get a loss function

4682
01:44:14,100 --> 01:44:17,230
 

4683
01:44:14,110 --> 01:44:19,949
 you back propagate your time the error

4684
01:44:17,220 --> 01:44:19,949
 

4685
01:44:17,230 --> 01:44:23,199
 and that way this awesome gets trained

4686
01:44:19,939 --> 01:44:23,199
 

4687
01:44:19,949 --> 01:44:26,440
 this means that this STM is now able to

4688
01:44:23,189 --> 01:44:26,440
 

4689
01:44:23,199 --> 01:44:28,480
 learn a new update rule there

4690
01:44:26,430 --> 01:44:28,480
 

4691
01:44:26,440 --> 01:44:31,570
 specifically good for the type the type

4692
01:44:28,470 --> 01:44:31,570
 

4693
01:44:28,480 --> 01:44:33,850
 of tasks it's trying to solve and you

4694
01:44:31,560 --> 01:44:33,850
 

4695
01:44:31,570 --> 01:44:35,890
 can then remove this Optimus G replaced

4696
01:44:33,840 --> 01:44:35,890
 

4697
01:44:33,850 --> 01:44:39,520
 with another koozie or use a new task

4698
01:44:35,880 --> 01:44:39,520
 

4699
01:44:35,890 --> 01:44:42,610
 and you can train this optimizer to be

4700
01:44:39,510 --> 01:44:42,610
 

4701
01:44:39,520 --> 01:44:46,239
 very good at doing the weight updates

4702
01:44:42,600 --> 01:44:46,239
 

4703
01:44:42,610 --> 01:44:48,010
 for a range of similar tasks also nice

4704
01:44:46,229 --> 01:44:48,010
 

4705
01:44:46,239 --> 01:44:51,030
 that this is now in one single network

4706
01:44:48,000 --> 01:44:51,030
 

4707
01:44:48,010 --> 01:44:51,030
 like the rain

4708
01:44:53,149 --> 01:44:53,149
 

4709
01:44:53,159 --> 01:44:57,940
 of course if you want to transfer

4710
01:44:55,709 --> 01:44:57,940
 

4711
01:44:55,719 --> 01:45:00,219
 information from previous tasks it will

4712
01:44:57,930 --> 01:45:00,219
 

4713
01:44:57,940 --> 01:45:03,659
 be cool if you can now also use this to

4714
01:45:00,209 --> 01:45:03,659
 

4715
01:45:00,219 --> 01:45:05,290
 learn from very little data because

4716
01:45:03,649 --> 01:45:05,290
 

4717
01:45:03,659 --> 01:45:07,600
 typically take it all the way at the

4718
01:45:05,280 --> 01:45:07,600
 

4719
01:45:05,290 --> 01:45:10,060
 Train what can we do to train from very

4720
01:45:07,590 --> 01:45:10,060
 

4721
01:45:07,600 --> 01:45:12,239
 few examples and this is an image from

4722
01:45:10,050 --> 01:45:12,239
 

4723
01:45:10,060 --> 01:45:16,000
 Google our shell is actually giving a

4724
01:45:12,229 --> 01:45:16,000
 

4725
01:45:12,239 --> 01:45:17,409
 talk and meta learning workshop about

4726
01:45:15,990 --> 01:45:17,409
 

4727
01:45:16,000 --> 01:45:17,730
 you should learning I think this will be

4728
01:45:17,399 --> 01:45:17,730
 

4729
01:45:17,409 --> 01:45:20,650
 very

4730
01:45:17,720 --> 01:45:20,650
 

4731
01:45:17,730 --> 01:45:23,710
 accurse you all to go there so the

4732
01:45:20,640 --> 01:45:23,710
 

4733
01:45:20,650 --> 01:45:26,500
 problem here is you're giving one task

4734
01:45:23,700 --> 01:45:26,500
 

4735
01:45:23,710 --> 01:45:29,500
 and this one task simply consists of

4736
01:45:26,490 --> 01:45:29,500
 

4737
01:45:26,500 --> 01:45:32,350
 five classes you get one example of each

4738
01:45:29,490 --> 01:45:32,350
 

4739
01:45:29,500 --> 01:45:34,449
 class and then you have to make

4740
01:45:32,340 --> 01:45:34,449
 

4741
01:45:32,350 --> 01:45:36,909
 predictions so if to predict which

4742
01:45:34,439 --> 01:45:36,909
 

4743
01:45:34,449 --> 01:45:38,890
 classes is which class this is but you

4744
01:45:36,899 --> 01:45:38,890
 

4745
01:45:36,909 --> 01:45:42,640
 don't get just one example of this tasks

4746
01:45:38,880 --> 01:45:42,640
 

4747
01:45:38,890 --> 01:45:47,080
 you get a lot of these examples and your

4748
01:45:42,630 --> 01:45:47,080
 

4749
01:45:42,640 --> 01:45:48,909
 meta model now has to learn a set of way

4750
01:45:47,070 --> 01:45:48,909
 

4751
01:45:47,080 --> 01:45:51,190
 so well you need to you need to

4752
01:45:48,899 --> 01:45:51,190
 

4753
01:45:48,909 --> 01:45:53,440
 parameterize your model and then you

4754
01:45:51,180 --> 01:45:53,440
 

4755
01:45:51,190 --> 01:45:55,150
 need to learn the parameters so this

4756
01:45:53,430 --> 01:45:55,150
 

4757
01:45:53,440 --> 01:45:58,270
 model can solve this task

4758
01:45:55,140 --> 01:45:58,270
 

4759
01:45:55,150 --> 01:46:01,540
 efficiently and then this you look at

4760
01:45:58,260 --> 01:46:01,540
 

4761
01:45:58,270 --> 01:46:04,060
 the loss of this model and then you look

4762
01:46:01,530 --> 01:46:04,060
 

4763
01:46:01,540 --> 01:46:06,310
 at what is this loss over a number of

4764
01:46:04,050 --> 01:46:06,310
 

4765
01:46:04,060 --> 01:46:08,440
 tasks and that's your error signal for

4766
01:46:06,300 --> 01:46:08,440
 

4767
01:46:06,310 --> 01:46:11,560
 that that's your cost for the metamodel

4768
01:46:08,430 --> 01:46:11,560
 

4769
01:46:08,440 --> 01:46:13,900
 and the meta mode uses that to then find

4770
01:46:11,550 --> 01:46:13,900
 

4771
01:46:11,560 --> 01:46:16,870
 better parameters and then that way

4772
01:46:13,890 --> 01:46:16,870
 

4773
01:46:13,900 --> 01:46:18,420
 learns how to build a model for this

4774
01:46:16,860 --> 01:46:18,420
 

4775
01:46:16,870 --> 01:46:20,739
 task

4776
01:46:18,410 --> 01:46:20,739
 

4777
01:46:18,420 --> 01:46:23,679
 there's a whole range of techniques to

4778
01:46:20,729 --> 01:46:23,679
 

4779
01:46:20,739 --> 01:46:25,690
 solve this problem and different ways of

4780
01:46:23,669 --> 01:46:25,690
 

4781
01:46:23,679 --> 01:46:27,100
 building metal models so one thing you

4782
01:46:25,680 --> 01:46:27,100
 

4783
01:46:25,690 --> 01:46:28,860
 can do you can start with existing

4784
01:46:27,090 --> 01:46:28,860
 

4785
01:46:27,100 --> 01:46:32,489
 algorithm as a metaluna

4786
01:46:28,850 --> 01:46:32,489
 

4787
01:46:28,860 --> 01:46:34,690
 you can't princess start with an LCM and

4788
01:46:32,479 --> 01:46:34,690
 

4789
01:46:32,489 --> 01:46:36,520
 you do great descent I will talk more

4790
01:46:34,680 --> 01:46:36,520
 

4791
01:46:34,690 --> 01:46:38,620
 about this soon another very popular

4792
01:46:36,510 --> 01:46:38,620
 

4793
01:46:36,520 --> 01:46:40,960
 approach is mammal where you just learn

4794
01:46:38,610 --> 01:46:40,960
 

4795
01:46:38,620 --> 01:46:42,429
 the initialization of your network and I

4796
01:46:40,950 --> 01:46:42,429
 

4797
01:46:40,960 --> 01:46:43,540
 need a great inter sense and then

4798
01:46:42,419 --> 01:46:43,540
 

4799
01:46:42,429 --> 01:46:45,580
 there's a whole bunch of techniques I

4800
01:46:43,530 --> 01:46:45,580
 

4801
01:46:43,540 --> 01:46:46,989
 can't go into but they all have some

4802
01:46:45,570 --> 01:46:46,989
 

4803
01:46:45,580 --> 01:46:51,580
 kind of memory component so they

4804
01:46:46,979 --> 01:46:51,580
 

4805
01:46:46,989 --> 01:46:54,030
 remember instances from previous tasks

4806
01:46:51,570 --> 01:46:54,030
 

4807
01:46:51,580 --> 01:46:56,380
 and they use them in the new tasks and

4808
01:46:54,020 --> 01:46:56,380
 

4809
01:46:54,030 --> 01:46:57,940
 these are either K and I liked it nice

4810
01:46:56,370 --> 01:46:57,940
 

4811
01:46:56,380 --> 01:47:00,449
 or you learn embedding in an acacia fire

4812
01:46:57,930 --> 01:47:00,449
 

4813
01:46:57,940 --> 01:47:02,830
 or you also have this black box models

4814
01:47:00,439 --> 01:47:02,830
 

4815
01:47:00,449 --> 01:47:05,590
 so typically some neural network that

4816
01:47:02,820 --> 01:47:05,590
 

4817
01:47:02,830 --> 01:47:07,300
 has memory components that can use the

4818
01:47:05,580 --> 01:47:07,300
 

4819
01:47:05,590 --> 01:47:09,370
 memory from previous tasks and apply the

4820
01:47:07,290 --> 01:47:09,370
 

4821
01:47:07,300 --> 01:47:15,429
 new tasks and you saw actually stair dr

4822
01:47:09,360 --> 01:47:15,429
 

4823
01:47:09,370 --> 01:47:17,620
 right now yeah so one thing you can do

4824
01:47:15,419 --> 01:47:17,620
 

4825
01:47:15,429 --> 01:47:18,850
 it's very general it's not the most

4826
01:47:17,610 --> 01:47:18,850
 

4827
01:47:17,620 --> 01:47:21,580
 performant algorithm it is very

4828
01:47:18,840 --> 01:47:21,580
 

4829
01:47:18,850 --> 01:47:24,969
 generally useful and very elegant is

4830
01:47:21,570 --> 01:47:24,969
 

4831
01:47:21,580 --> 01:47:28,870
 again you use the SDM for this and so if

4832
01:47:24,959 --> 01:47:28,870
 

4833
01:47:24,969 --> 01:47:31,780
 you ask a methyl donor here and you

4834
01:47:28,860 --> 01:47:31,780
 

4835
01:47:28,870 --> 01:47:33,670
 don't you make the observation that if

4836
01:47:31,770 --> 01:47:33,670
 

4837
01:47:31,780 --> 01:47:36,520
 you look at the the current update rule

4838
01:47:33,660 --> 01:47:36,520
 

4839
01:47:33,670 --> 01:47:39,760
 right your weights are equal previous

4840
01:47:36,510 --> 01:47:39,760
 

4841
01:47:36,520 --> 01:47:42,100
 weights - learning rate times the

4842
01:47:39,750 --> 01:47:42,100
 

4843
01:47:39,760 --> 01:47:45,070
 gradient and if you look at what the

4844
01:47:42,090 --> 01:47:45,070
 

4845
01:47:42,100 --> 01:47:47,920
 lsdm cell update does so USDM state your

4846
01:47:45,060 --> 01:47:47,920
 

4847
01:47:45,070 --> 01:47:51,040
 memory equals the previous state times

4848
01:47:47,910 --> 01:47:51,040
 

4849
01:47:47,920 --> 01:47:56,110
 the forgetting gate times the hidden

4850
01:47:51,030 --> 01:47:56,110
 

4851
01:47:51,040 --> 01:47:59,320
 state times the input gate so if you now

4852
01:47:56,100 --> 01:47:59,320
 

4853
01:47:56,110 --> 01:48:02,080
 say okay let's just equal this data to

4854
01:47:59,310 --> 01:48:02,080
 

4855
01:47:59,320 --> 01:48:05,760
 this city and this beam of this one and

4856
01:48:02,070 --> 01:48:05,760
 

4857
01:48:02,080 --> 01:48:07,570
 this one to this one then I can train my

4858
01:48:05,750 --> 01:48:07,570
 

4859
01:48:05,760 --> 01:48:09,130
 LCM

4860
01:48:07,560 --> 01:48:09,130
 

4861
01:48:07,570 --> 01:48:12,190
 if I've trained that and I also have my

4862
01:48:09,120 --> 01:48:12,190
 

4863
01:48:09,130 --> 01:48:13,840
 update rule right I think that's very

4864
01:48:12,180 --> 01:48:13,840
 

4865
01:48:12,190 --> 01:48:15,550
 cool so then you can solve this problem

4866
01:48:13,830 --> 01:48:15,550
 

4867
01:48:13,840 --> 01:48:18,190
 by starting with any random

4868
01:48:15,540 --> 01:48:18,190
 

4869
01:48:15,550 --> 01:48:20,860
 initialization you train your model on

4870
01:48:18,180 --> 01:48:20,860
 

4871
01:48:18,190 --> 01:48:24,310
 the first task you get an error back

4872
01:48:20,850 --> 01:48:24,310
 

4873
01:48:20,860 --> 01:48:28,239
 then the ostium uses its current hidden

4874
01:48:24,300 --> 01:48:28,239
 

4875
01:48:24,310 --> 01:48:30,489
 States you generate a new new weights

4876
01:48:28,229 --> 01:48:30,489
 

4877
01:48:28,239 --> 01:48:32,829
 you train a model you get an error back

4878
01:48:30,479 --> 01:48:32,829
 

4879
01:48:30,489 --> 01:48:35,340
 and you keep doing this until you end up

4880
01:48:32,819 --> 01:48:35,340
 

4881
01:48:32,829 --> 01:48:39,400
 in your chess set then you can evaluate

4882
01:48:35,330 --> 01:48:39,400
 

4883
01:48:35,340 --> 01:48:41,320
 the cost of this Alice TM then you can

4884
01:48:39,390 --> 01:48:41,320
 

4885
01:48:39,400 --> 01:48:44,079
 back propagate this to time you update

4886
01:48:41,310 --> 01:48:44,079
 

4887
01:48:41,320 --> 01:48:47,110
 all the settings of these STM's you have

4888
01:48:44,069 --> 01:48:47,110
 

4889
01:48:44,079 --> 01:48:48,790
 a new theta 0 you go back and forth back

4890
01:48:47,100 --> 01:48:48,790
 

4891
01:48:47,110 --> 01:48:52,290
 and forth back and forth and so you

4892
01:48:48,780 --> 01:48:52,290
 

4893
01:48:48,790 --> 01:48:56,290
 train your LCM to be very good at

4894
01:48:52,280 --> 01:48:56,290
 

4895
01:48:52,290 --> 01:48:58,719
 updating the weights of these these base

4896
01:48:56,280 --> 01:48:58,719
 

4897
01:48:56,290 --> 01:49:02,440
 models and you also learn good in

4898
01:48:58,709 --> 01:49:02,440
 

4899
01:48:58,719 --> 01:49:05,170
 association the same time another

4900
01:49:02,430 --> 01:49:05,170
 

4901
01:49:02,440 --> 01:49:08,770
 approach very popular is this model

4902
01:49:05,160 --> 01:49:08,770
 

4903
01:49:05,170 --> 01:49:11,020
 agnostic meta learning so here you you

4904
01:49:08,760 --> 01:49:11,020
 

4905
01:49:08,770 --> 01:49:14,560
 don't bother with building complex

4906
01:49:11,010 --> 01:49:14,560
 

4907
01:49:11,020 --> 01:49:16,540
 RCMP's you you're going to learn new

4908
01:49:14,550 --> 01:49:16,540
 

4909
01:49:14,560 --> 01:49:19,329
 skills quickly by just starting from a

4910
01:49:16,530 --> 01:49:19,329
 

4911
01:49:16,540 --> 01:49:21,310
 very well initialized neural nets right

4912
01:49:19,319 --> 01:49:21,310
 

4913
01:49:19,329 --> 01:49:24,130
 the goal is here to train a neural net

4914
01:49:21,300 --> 01:49:24,130
 

4915
01:49:21,310 --> 01:49:26,199
 on a bunch of similar tasks so that it

4916
01:49:24,120 --> 01:49:26,199
 

4917
01:49:24,130 --> 01:49:28,480
 is a very good installation and whenever

4918
01:49:26,189 --> 01:49:28,480
 

4919
01:49:26,199 --> 01:49:30,670
 it needs to solve a similar task it can

4920
01:49:28,470 --> 01:49:30,670
 

4921
01:49:28,480 --> 01:49:34,500
 start with 2 from that initialization

4922
01:49:30,660 --> 01:49:34,500
 

4923
01:49:30,670 --> 01:49:37,060
 and get get optimal models much faster

4924
01:49:34,490 --> 01:49:37,060
 

4925
01:49:34,500 --> 01:49:41,020
 so if you look here you have this

4926
01:49:37,050 --> 01:49:41,020
 

4927
01:49:37,060 --> 01:49:43,060
 securitization data then you look at in

4928
01:49:41,010 --> 01:49:43,060
 

4929
01:49:41,020 --> 01:49:45,250
 this case three tasks for each of these

4930
01:49:43,050 --> 01:49:45,250
 

4931
01:49:43,060 --> 01:49:48,699
 three tasks you take key examples

4932
01:49:45,240 --> 01:49:48,699
 

4933
01:49:45,250 --> 01:49:51,400
 evaluate the gradients then you yearly

4934
01:49:48,689 --> 01:49:51,400
 

4935
01:49:48,699 --> 01:49:55,349
 the weights for each of these subtasks

4936
01:49:51,390 --> 01:49:55,349
 

4937
01:49:51,400 --> 01:49:58,690
 and then you update this gradient theta

4938
01:49:55,339 --> 01:49:58,690
 

4939
01:49:55,349 --> 01:50:02,320
 to minimize the sum of these per task

4940
01:49:58,680 --> 01:50:02,320
 

4941
01:49:58,690 --> 01:50:05,199
 losses and it that brings you to a new

4942
01:50:02,310 --> 01:50:05,199
 

4943
01:50:02,320 --> 01:50:07,450
 initialization which is hopefully closer

4944
01:50:05,189 --> 01:50:07,450
 

4945
01:50:05,199 --> 01:50:10,440
 to the optimal

4946
01:50:07,440 --> 01:50:10,440
 

4947
01:50:07,450 --> 01:50:11,590
[Music]

4948
01:50:10,430 --> 01:50:11,590
 

4949
01:50:10,440 --> 01:50:14,800
 weights

4950
01:50:11,580 --> 01:50:14,800
 

4951
01:50:11,590 --> 01:50:16,550
 theta 1 2 3 here it's a very cool album

4952
01:50:14,790 --> 01:50:16,550
 

4953
01:50:14,800 --> 01:50:21,430
 so you just you learn the

4954
01:50:16,540 --> 01:50:21,430
 

4955
01:50:16,550 --> 01:50:23,780
 the insulation for a number of tasks and

4956
01:50:21,420 --> 01:50:23,780
 

4957
01:50:21,430 --> 01:50:26,540
 whenever you have to solve a scimitar so

4958
01:50:23,770 --> 01:50:26,540
 

4959
01:50:23,780 --> 01:50:29,330
 you can just start from that optimized

4960
01:50:26,530 --> 01:50:29,330
 

4961
01:50:26,540 --> 01:50:30,890
 initialization this has been shown to be

4962
01:50:29,320 --> 01:50:30,890
 

4963
01:50:29,330 --> 01:50:32,360
 very resilient to overfitting it also

4964
01:50:30,880 --> 01:50:32,360
 

4965
01:50:30,890 --> 01:50:34,240
 generalized better than the Austrian

4966
01:50:32,350 --> 01:50:34,240
 

4967
01:50:32,360 --> 01:50:36,770
 approaches and is even approve of

4968
01:50:34,230 --> 01:50:36,770
 

4969
01:50:34,240 --> 01:50:39,560
 universality where they can prove that

4970
01:50:36,760 --> 01:50:39,560
 

4971
01:50:36,770 --> 01:50:42,080
 there are no theoretical downsides to

4972
01:50:39,550 --> 01:50:42,080
 

4973
01:50:39,560 --> 01:50:44,690
 doing this so it's if you just learn a

4974
01:50:42,070 --> 01:50:44,690
 

4975
01:50:42,080 --> 01:50:47,090
 good initialization and use green to

4976
01:50:44,680 --> 01:50:47,090
 

4977
01:50:44,690 --> 01:50:51,260
 sense this will never be worse than

4978
01:50:47,080 --> 01:50:51,260
 

4979
01:50:47,090 --> 01:50:54,350
 learning a very complex no not to to

4980
01:50:51,250 --> 01:50:54,350
 

4981
01:50:51,260 --> 01:50:56,720
 learn a bit rule and more recently

4982
01:50:54,340 --> 01:50:56,720
 

4983
01:50:54,350 --> 01:50:58,400
 reptile was released that's a more

4984
01:50:56,710 --> 01:50:58,400
 

4985
01:50:56,720 --> 01:51:00,320
 scalable version of this that uses

4986
01:50:58,390 --> 01:51:00,320
 

4987
01:50:58,400 --> 01:51:02,810
 circuitry great descent so instead of

4988
01:51:00,310 --> 01:51:02,810
 

4989
01:51:00,320 --> 01:51:07,910
 actually doing this thing every step

4990
01:51:02,800 --> 01:51:07,910
 

4991
01:51:02,810 --> 01:51:09,680
 it's does K steps from one task and only

4992
01:51:07,900 --> 01:51:09,680
 

4993
01:51:07,910 --> 01:51:15,530
 then updates the weights so it's a bit

4994
01:51:09,670 --> 01:51:15,530
 

4995
01:51:09,680 --> 01:51:16,910
 more scalable okay then also another

4996
01:51:15,520 --> 01:51:16,910
 

4997
01:51:15,530 --> 01:51:18,740
 useful tool for meta learning is

4998
01:51:16,900 --> 01:51:18,740
 

4999
01:51:16,910 --> 01:51:22,580
 reinforced learning this comes very

5000
01:51:18,730 --> 01:51:22,580
 

5001
01:51:18,740 --> 01:51:25,520
 natural you can just have a

5002
01:51:22,570 --> 01:51:25,520
 

5003
01:51:22,580 --> 01:51:28,460
 reinforcement learner that has to create

5004
01:51:25,510 --> 01:51:28,460
 

5005
01:51:25,520 --> 01:51:30,110
 a new algorithm or new model and then we

5006
01:51:28,450 --> 01:51:30,110
 

5007
01:51:28,460 --> 01:51:32,750
 can just evaluate the model and use the

5008
01:51:30,100 --> 01:51:32,750
 

5009
01:51:30,110 --> 01:51:34,490
 performance as a reward for

5010
01:51:32,740 --> 01:51:34,490
 

5011
01:51:32,750 --> 01:51:36,200
 reinforcement learner and then the

5012
01:51:34,480 --> 01:51:36,200
 

5013
01:51:34,490 --> 01:51:40,520
 reversal torrent has to learn over time

5014
01:51:36,190 --> 01:51:40,520
 

5015
01:51:36,200 --> 01:51:44,270
 how to build off the model and main goal

5016
01:51:40,510 --> 01:51:44,270
 

5017
01:51:40,520 --> 01:51:45,650
 here is to actually solve the first

5018
01:51:44,260 --> 01:51:45,650
 

5019
01:51:44,270 --> 01:51:47,300
 learning problems much faster than

5020
01:51:45,640 --> 01:51:47,300
 

5021
01:51:45,650 --> 01:51:51,350
 before because humans are to be very

5022
01:51:47,290 --> 01:51:51,350
 

5023
01:51:47,300 --> 01:51:53,860
 good at playing new games faster than

5024
01:51:51,340 --> 01:51:53,860
 

5025
01:51:51,350 --> 01:51:57,470
 then reinforcement learning albums are

5026
01:51:53,850 --> 01:51:57,470
 

5027
01:51:53,860 --> 01:51:58,970
 so the idea here is that you build a

5028
01:51:57,460 --> 01:51:58,970
 

5029
01:51:57,470 --> 01:52:04,820
 metal reinforcement learning algorithm

5030
01:51:58,960 --> 01:52:04,820
 

5031
01:51:58,970 --> 01:52:06,860
 that's integrated deep RNN and you train

5032
01:52:04,810 --> 01:52:06,860
 

5033
01:52:04,820 --> 01:52:11,200
 it on a large number of environments and

5034
01:52:06,850 --> 01:52:11,200
 

5035
01:52:06,860 --> 01:52:14,150
 this algn then has to implement an

5036
01:52:11,190 --> 01:52:14,150
 

5037
01:52:11,200 --> 01:52:16,460
 agent that then for similar environments

5038
01:52:14,140 --> 01:52:16,460
 

5039
01:52:14,150 --> 01:52:18,590
 has to learn a policy you look at the

5040
01:52:16,450 --> 01:52:18,590
 

5041
01:52:16,460 --> 01:52:21,110
 performance you give the performance

5042
01:52:18,580 --> 01:52:21,110
 

5043
01:52:18,590 --> 01:52:23,510
 back then that way it can learn a policy

5044
01:52:21,100 --> 01:52:23,510
 

5045
01:52:21,110 --> 01:52:25,820
 over many environments and so this way

5046
01:52:23,500 --> 01:52:25,820
 

5047
01:52:23,510 --> 01:52:27,530
 this we fortunate learning agents learns

5048
01:52:25,810 --> 01:52:27,530
 

5049
01:52:25,820 --> 01:52:31,100
 how to

5050
01:52:27,520 --> 01:52:31,100
 

5051
01:52:27,530 --> 01:52:36,190
 create a new report learning agent it's

5052
01:52:31,090 --> 01:52:36,190
 

5053
01:52:31,100 --> 01:52:38,420
 much faster at solving similar tasks and

5054
01:52:36,180 --> 01:52:38,420
 

5055
01:52:36,190 --> 01:52:44,060
 I should also work for future learning

5056
01:52:38,410 --> 01:52:44,060
 

5057
01:52:38,420 --> 01:52:46,310
 it's a more recent paper by by g1 so

5058
01:52:44,050 --> 01:52:46,310
 

5059
01:52:44,060 --> 01:52:48,380
 here you you do future learning by not

5060
01:52:46,300 --> 01:52:48,380
 

5061
01:52:46,310 --> 01:52:51,199
 conditioning only on the observation but

5062
01:52:48,370 --> 01:52:51,199
 

5063
01:52:48,380 --> 01:52:52,760
 also on the upcoming demonstration right

5064
01:52:51,189 --> 01:52:52,760
 

5065
01:52:51,199 --> 01:52:57,380
 so if you have a bunch of a

5066
01:52:52,750 --> 01:52:57,380
 

5067
01:52:52,760 --> 01:53:03,100
 demonstration sport ask so you learn you

5068
01:52:57,370 --> 01:53:03,100
 

5069
01:52:57,380 --> 01:53:05,780
 train this meta learner to build and

5070
01:53:03,090 --> 01:53:05,780
 

5071
01:53:03,100 --> 01:53:08,060
 we're fortunate learning agents not

5072
01:53:05,770 --> 01:53:08,060
 

5073
01:53:05,780 --> 01:53:09,949
 knowing what the actual demonstration is

5074
01:53:08,050 --> 01:53:09,949
 

5075
01:53:08,060 --> 01:53:12,230
 going to be but it will be make sure it

5076
01:53:09,939 --> 01:53:12,230
 

5077
01:53:09,949 --> 01:53:14,960
 could be maximally prepared for solving

5078
01:53:12,220 --> 01:53:14,960
 

5079
01:53:12,230 --> 01:53:17,210
 tasks that are similar to the ones he's

5080
01:53:14,950 --> 01:53:17,210
 

5081
01:53:14,960 --> 01:53:22,760
 been trained on so it learns how to

5082
01:53:17,200 --> 01:53:22,760
 

5083
01:53:17,210 --> 01:53:24,679
 build RL agents for similar tasks and in

5084
01:53:22,750 --> 01:53:24,679
 

5085
01:53:22,760 --> 01:53:27,530
 this whole range of other tasks that you

5086
01:53:24,669 --> 01:53:27,530
 

5087
01:53:24,679 --> 01:53:29,449
 can solve with with meta learning one

5088
01:53:27,520 --> 01:53:29,449
 

5089
01:53:27,530 --> 01:53:30,739
 thing is active learning so here you can

5090
01:53:29,439 --> 01:53:30,739
 

5091
01:53:29,449 --> 01:53:32,480
 build a deep network that learns

5092
01:53:30,729 --> 01:53:32,480
 

5093
01:53:30,739 --> 01:53:34,850
 representation of your data and then the

5094
01:53:32,470 --> 01:53:34,850
 

5095
01:53:32,480 --> 01:53:36,739
 policy network and so whenever it

5096
01:53:34,840 --> 01:53:36,739
 

5097
01:53:34,850 --> 01:53:38,840
 receives a state and reward it just

5098
01:53:36,729 --> 01:53:38,840
 

5099
01:53:36,739 --> 01:53:42,170
 tells you which lot of points you query

5100
01:53:38,830 --> 01:53:42,170
 

5101
01:53:38,840 --> 01:53:45,980
 next that's and you then you use metal

5102
01:53:42,160 --> 01:53:45,980
 

5103
01:53:42,170 --> 01:53:48,890
 or metal learning to train these the

5104
01:53:45,970 --> 01:53:48,890
 

5105
01:53:45,980 --> 01:53:51,469
 network and so kinda post network you

5106
01:53:48,880 --> 01:53:51,469
 

5107
01:53:48,890 --> 01:53:53,120
 can also do density estimation by just

5108
01:53:51,459 --> 01:53:53,120
 

5109
01:53:51,469 --> 01:53:55,660
 learning a distribution over small set

5110
01:53:53,110 --> 01:53:55,660
 

5111
01:53:53,120 --> 01:53:59,420
 of images and then you use memo

5112
01:53:55,650 --> 01:53:59,420
 

5113
01:53:55,660 --> 01:54:01,010
 metal-based future learner to two

5114
01:53:59,410 --> 01:54:01,010
 

5115
01:53:59,420 --> 01:54:03,410
 learned entities much faster than

5116
01:54:01,000 --> 01:54:03,410
 

5117
01:54:01,010 --> 01:54:05,060
 otherwise possible or you can also you

5118
01:54:03,400 --> 01:54:05,060
 

5119
01:54:03,410 --> 01:54:07,880
 can also do matrix factorization this

5120
01:54:05,050 --> 01:54:07,880
 

5121
01:54:05,060 --> 01:54:11,210
 way the bottom line here is that you can

5122
01:54:07,870 --> 01:54:11,210
 

5123
01:54:07,880 --> 01:54:12,770
 take basically any algorithm on active

5124
01:54:11,200 --> 01:54:12,770
 

5125
01:54:11,210 --> 01:54:14,660
 learning dense estimation matrix

5126
01:54:12,760 --> 01:54:14,660
 

5127
01:54:12,770 --> 01:54:17,480
 factorization and you can replace

5128
01:54:14,650 --> 01:54:17,480
 

5129
01:54:14,660 --> 01:54:21,080
 existing algorithms by a new album that

5130
01:54:17,470 --> 01:54:21,080
 

5131
01:54:17,480 --> 01:54:24,710
 you learned is very powerful you can

5132
01:54:21,070 --> 01:54:24,710
 

5133
01:54:21,080 --> 01:54:28,460
 replace these handcrafted organs by

5134
01:54:24,700 --> 01:54:28,460
 

5135
01:54:24,710 --> 01:54:30,710
 learned ones okay that's the end so

5136
01:54:28,450 --> 01:54:30,710
 

5137
01:54:28,460 --> 01:54:32,660
 finally maybe you wondering yeah I want

5138
01:54:30,700 --> 01:54:32,660
 

5139
01:54:30,710 --> 01:54:35,690
 to do metal learning but how do I get

5140
01:54:32,650 --> 01:54:35,690
 

5141
01:54:32,660 --> 01:54:37,850
 useful metadata to work with to train my

5142
01:54:35,680 --> 01:54:37,850
 

5143
01:54:35,690 --> 01:54:40,390
 models well we'll be working on stat

5144
01:54:37,840 --> 01:54:40,390
 

5145
01:54:37,850 --> 01:54:42,970
 form called open ml so you can go

5146
01:54:40,380 --> 01:54:42,970
 

5147
01:54:40,390 --> 01:54:45,880
 oh man org it has thousands of uniform

5148
01:54:42,960 --> 01:54:45,880
 

5149
01:54:42,970 --> 01:54:47,920
 datasets you can download them all on

5150
01:54:45,870 --> 01:54:47,920
 

5151
01:54:45,880 --> 01:54:49,840
 Mars and they're the same formatting so

5152
01:54:47,910 --> 01:54:49,840
 

5153
01:54:47,920 --> 01:54:52,150
 you can run the experiments and all all

5154
01:54:49,830 --> 01:54:52,150
 

5155
01:54:49,840 --> 01:54:53,800
 their assets simultaneously if the

5156
01:54:52,140 --> 01:54:53,800
 

5157
01:54:52,150 --> 01:54:55,120
 effort gets 100 of these meta features

5158
01:54:53,790 --> 01:54:55,120
 

5159
01:54:53,800 --> 01:54:57,970
 so you can measure distances between

5160
01:54:55,110 --> 01:54:57,970
 

5161
01:54:55,120 --> 01:55:00,190
 tasks you get millions of evaluated runs

5162
01:54:57,960 --> 01:55:00,190
 

5163
01:54:57,970 --> 01:55:03,070
 and these are all evaluated with same

5164
01:55:00,180 --> 01:55:03,070
 

5165
01:55:00,190 --> 01:55:04,690
 splits with different metrics and for a

5166
01:55:03,060 --> 01:55:04,690
 

5167
01:55:03,070 --> 01:55:08,230
 large bunch of them you also get the

5168
01:55:04,680 --> 01:55:08,230
 

5169
01:55:04,690 --> 01:55:10,300
 traces whatever the models optimized you

5170
01:55:08,220 --> 01:55:10,300
 

5171
01:55:08,230 --> 01:55:12,790
 get the all the sub models that are

5172
01:55:10,290 --> 01:55:12,790
 

5173
01:55:10,300 --> 01:55:14,740
 optimized running up to that and the

5174
01:55:12,780 --> 01:55:14,740
 

5175
01:55:12,790 --> 01:55:17,770
 models that were built for some of them

5176
01:55:14,730 --> 01:55:17,770
 

5177
01:55:14,740 --> 01:55:19,990
 not for all of them and then you have

5178
01:55:17,760 --> 01:55:19,990
 

5179
01:55:17,770 --> 01:55:21,550
 API is in Python or in Java so here you

5180
01:55:19,980 --> 01:55:21,550
 

5181
01:55:19,990 --> 01:55:23,950
 see the the Pythian APR for instance

5182
01:55:21,540 --> 01:55:23,950
 

5183
01:55:21,550 --> 01:55:26,770
 it's a very simple interface you

5184
01:55:23,940 --> 01:55:26,770
 

5185
01:55:23,950 --> 01:55:28,990
 download the tasks with an ID then you

5186
01:55:26,760 --> 01:55:28,990
 

5187
01:55:26,770 --> 01:55:31,270
 build whatever classify you want in this

5188
01:55:28,980 --> 01:55:31,270
 

5189
01:55:28,990 --> 01:55:33,400
 case just a cycler classifier you

5190
01:55:31,260 --> 01:55:33,400
 

5191
01:55:31,270 --> 01:55:36,850
 transform that to flow flows a

5192
01:55:33,390 --> 01:55:36,850
 

5193
01:55:33,400 --> 01:55:40,360
 representation of a pipeline or learning

5194
01:55:36,840 --> 01:55:40,360
 

5195
01:55:36,850 --> 01:55:41,890
 object then you run the flow on the task

5196
01:55:40,350 --> 01:55:41,890
 

5197
01:55:40,360 --> 01:55:43,390
 then you get a run which you can store

5198
01:55:41,880 --> 01:55:43,390
 

5199
01:55:41,890 --> 01:55:46,120
 locally or you can also publish it so

5200
01:55:43,380 --> 01:55:46,120
 

5201
01:55:43,390 --> 01:55:48,550
 these five lines allow you to download

5202
01:55:46,110 --> 01:55:48,550
 

5203
01:55:46,120 --> 01:55:51,130
 lots of datasets relative models and

5204
01:55:48,540 --> 01:55:51,130
 

5205
01:55:48,550 --> 01:55:53,440
 also share those moles again with

5206
01:55:51,120 --> 01:55:53,440
 

5207
01:55:51,130 --> 01:55:55,840
 anybody and always run locally you

5208
01:55:53,430 --> 01:55:55,840
 

5209
01:55:53,440 --> 01:55:59,230
 download data you run albums locally and

5210
01:55:55,830 --> 01:55:59,230
 

5211
01:55:55,840 --> 01:56:01,420
 you share results globally and this

5212
01:55:59,220 --> 01:56:01,420
 

5213
01:55:59,230 --> 01:56:04,180
 allows you to do neverending learning

5214
01:56:01,410 --> 01:56:04,180
 

5215
01:56:01,420 --> 01:56:06,190
 you can build an algorithm that works

5216
01:56:04,170 --> 01:56:06,190
 

5217
01:56:04,180 --> 01:56:09,580
 against this API downloads datasets

5218
01:56:06,180 --> 01:56:09,580
 

5219
01:56:06,190 --> 01:56:12,700
 learns how to learn all my tasks shares

5220
01:56:09,570 --> 01:56:12,700
 

5221
01:56:09,580 --> 01:56:15,520
 its result it's Malo with the server and

5222
01:56:12,690 --> 01:56:15,520
 

5223
01:56:12,700 --> 01:56:17,080
 then other agents other BOTS can then

5224
01:56:15,510 --> 01:56:17,080
 

5225
01:56:15,520 --> 01:56:18,820
 download the small again and use them so

5226
01:56:17,070 --> 01:56:18,820
 

5227
01:56:17,080 --> 01:56:21,400
 this gives you a platform where you can

5228
01:56:18,810 --> 01:56:21,400
 

5229
01:56:18,820 --> 01:56:23,530
 exchange information about meta learning

5230
01:56:21,390 --> 01:56:23,530
 

5231
01:56:21,400 --> 01:56:25,180
 and you can also easily build you if you

5232
01:56:23,520 --> 01:56:25,180
 

5233
01:56:23,530 --> 01:56:26,320
 want experiment with meta learning this

5234
01:56:25,170 --> 01:56:26,320
 

5235
01:56:25,180 --> 01:56:30,400
 is the best way to start so you can

5236
01:56:26,310 --> 01:56:30,400
 

5237
01:56:26,320 --> 01:56:33,070
 easily write your own BOTS to to learn

5238
01:56:30,390 --> 01:56:33,070
 

5239
01:56:30,400 --> 01:56:34,720
 well over large number of tasks and we

5240
01:56:33,060 --> 01:56:34,720
 

5241
01:56:33,070 --> 01:56:37,300
 also have benchmarks so here you can see

5242
01:56:34,710 --> 01:56:37,300
 

5243
01:56:34,720 --> 01:56:39,760
 for instance the visualization so every

5244
01:56:37,290 --> 01:56:39,760
 

5245
01:56:37,300 --> 01:56:42,130
 W is a model this is performance higher

5246
01:56:39,750 --> 01:56:42,130
 

5247
01:56:39,760 --> 01:56:43,780
 better with this time and you can see

5248
01:56:42,120 --> 01:56:43,780
 

5249
01:56:42,130 --> 01:56:46,540
 all these color dots are dots built by

5250
01:56:43,770 --> 01:56:46,540
 

5251
01:56:43,780 --> 01:56:49,120
 humans and these orange dots are dots

5252
01:56:46,530 --> 01:56:49,120
 

5253
01:56:46,540 --> 01:56:50,710
 built by robots and it's robot they can

5254
01:56:49,110 --> 01:56:50,710
 

5255
01:56:49,120 --> 01:56:51,980
 just learn by themselves or they can

5256
01:56:50,700 --> 01:56:51,980
 

5257
01:56:50,710 --> 01:56:54,800
 actually look at the

5258
01:56:51,970 --> 01:56:54,800
 

5259
01:56:51,980 --> 01:56:58,400
 models from on previous robots or the

5260
01:56:54,790 --> 01:56:58,400
 

5261
01:56:54,800 --> 01:56:59,930
 previous humans and then use that to do

5262
01:56:58,390 --> 01:56:59,930
 

5263
01:56:58,400 --> 01:57:02,000
 meta learning and build new models

5264
01:56:59,920 --> 01:57:02,000
 

5265
01:56:59,930 --> 01:57:04,160
 faster and by the way we had some

5266
01:57:01,990 --> 01:57:04,160
 

5267
01:57:02,000 --> 01:57:06,740
 openings for programmer and a teaching

5268
01:57:04,150 --> 01:57:06,740
 

5269
01:57:04,160 --> 01:57:11,470
 PhD if you're interested in building is

5270
01:57:06,730 --> 01:57:11,470
 

5271
01:57:06,740 --> 01:57:13,790
 very cool platform ok and a final note I

5272
01:57:11,460 --> 01:57:13,790
 

5273
01:57:11,470 --> 01:57:16,760
 think we have made a lot of progress in

5274
01:57:13,780 --> 01:57:16,760
 

5275
01:57:13,790 --> 01:57:18,860
 the last year's I think we're made a lot

5276
01:57:16,750 --> 01:57:18,860
 

5277
01:57:16,760 --> 01:57:21,250
 of progress in to making a real

5278
01:57:18,850 --> 01:57:21,250
 

5279
01:57:18,860 --> 01:57:24,770
 human-like learning to learn happen I

5280
01:57:21,240 --> 01:57:24,770
 

5281
01:57:21,250 --> 01:57:27,830
 think it is very a crucial aspect of of

5282
01:57:24,760 --> 01:57:27,830
 

5283
01:57:24,770 --> 01:57:29,840
 science because this gives you a

5284
01:57:27,820 --> 01:57:29,840
 

5285
01:57:27,830 --> 01:57:31,970
 significant advantage if you can do if

5286
01:57:29,830 --> 01:57:31,970
 

5287
01:57:29,840 --> 01:57:34,310
 you can solve one task IRL that's fine

5288
01:57:31,960 --> 01:57:34,310
 

5289
01:57:31,970 --> 01:57:36,860
 but if you can learn how to solve any

5290
01:57:34,300 --> 01:57:36,860
 

5291
01:57:34,310 --> 01:57:38,960
 task that's much more powerful right and

5292
01:57:36,850 --> 01:57:38,960
 

5293
01:57:36,860 --> 01:57:41,000
 something we should definitely put more

5294
01:57:38,950 --> 01:57:41,000
 

5295
01:57:38,960 --> 01:57:43,040
 effort into it's also a universal as

5296
01:57:40,990 --> 01:57:43,040
 

5297
01:57:41,000 --> 01:57:45,980
 part of life matches of humans trees

5298
01:57:43,030 --> 01:57:45,980
 

5299
01:57:43,040 --> 01:57:48,470
 learn you know the university well life

5300
01:57:45,970 --> 01:57:48,470
 

5301
01:57:45,980 --> 01:57:51,410
 in general learns all the time so it's

5302
01:57:48,460 --> 01:57:51,410
 

5303
01:57:48,470 --> 01:57:52,310
 the time we understand this process it's

5304
01:57:51,400 --> 01:57:52,310
 

5305
01:57:51,410 --> 01:57:53,870
 a very exciting field

5306
01:57:52,300 --> 01:57:53,870
 

5307
01:57:52,310 --> 01:57:56,390
 there's many unexplored possibilities

5308
01:57:53,860 --> 01:57:56,390
 

5309
01:57:53,870 --> 01:57:58,280
 and many aspects are completely not

5310
01:57:56,380 --> 01:57:58,280
 

5311
01:57:56,390 --> 01:58:01,550
 understood for instance stuff similarity

5312
01:57:58,270 --> 01:58:01,550
 

5313
01:57:58,280 --> 01:58:03,680
 when can I transfer a task or model from

5314
01:58:01,540 --> 01:58:03,680
 

5315
01:58:01,550 --> 01:58:06,290
 previous tasks to new tasks we just

5316
01:58:03,670 --> 01:58:06,290
 

5317
01:58:03,680 --> 01:58:07,700
 don't really know I think we did not a

5318
01:58:06,280 --> 01:58:07,700
 

5319
01:58:06,290 --> 01:58:09,770
 lot more experiments a lot more science

5320
01:58:07,690 --> 01:58:09,770
 

5321
01:58:07,700 --> 01:58:13,250
 going into that so the brings me to

5322
01:58:09,760 --> 01:58:13,250
 

5323
01:58:09,770 --> 01:58:14,780
 final challenge can you actually start

5324
01:58:13,240 --> 01:58:14,780
 

5325
01:58:13,250 --> 01:58:16,820
 building learners that never stop

5326
01:58:14,770 --> 01:58:16,820
 

5327
01:58:14,780 --> 01:58:18,920
 learning that go from task to task the

5328
01:58:16,810 --> 01:58:18,920
 

5329
01:58:16,820 --> 01:58:21,110
 task some tasks can be very similar

5330
01:58:18,910 --> 01:58:21,110
 

5331
01:58:18,920 --> 01:58:23,420
 sometimes can be very different it has

5332
01:58:21,100 --> 01:58:23,420
 

5333
01:58:21,110 --> 01:58:25,670
 to never stop learning it has to learn

5334
01:58:23,410 --> 01:58:25,670
 

5335
01:58:23,420 --> 01:58:28,160
 across these tasks and it also has to

5336
01:58:25,660 --> 01:58:28,160
 

5337
01:58:25,670 --> 01:58:30,800
 learn these agents has to learn from

5338
01:58:28,150 --> 01:58:30,800
 

5339
01:58:28,160 --> 01:58:33,380
 each other why did one learn one learner

5340
01:58:30,790 --> 01:58:33,380
 

5341
01:58:30,800 --> 01:58:36,650
 learns a very good speech very good this

5342
01:58:33,370 --> 01:58:36,650
 

5343
01:58:33,380 --> 01:58:39,980
 was a very good representation or a very

5344
01:58:36,640 --> 01:58:39,980
 

5345
01:58:36,650 --> 01:58:41,570
 good pipeline some other learner should

5346
01:58:39,970 --> 01:58:41,570
 

5347
01:58:39,980 --> 01:58:44,500
 be able to reuse the pipeline or to

5348
01:58:41,560 --> 01:58:44,500
 

5349
01:58:41,570 --> 01:58:48,320
 reuse that that feature representation

5350
01:58:44,490 --> 01:58:48,320
 

5351
01:58:44,500 --> 01:58:50,450
 and we can build a global memory where

5352
01:58:48,310 --> 01:58:50,450
 

5353
01:58:48,320 --> 01:58:52,970
 these systems can can talk to each other

5354
01:58:50,440 --> 01:58:52,970
 

5355
01:58:50,450 --> 01:58:54,560
 and can share the information and then

5356
01:58:52,960 --> 01:58:54,560
 

5357
01:58:52,970 --> 01:58:56,570
 if you have that then you can let an

5358
01:58:54,550 --> 01:58:56,570
 

5359
01:58:54,560 --> 01:59:01,040
 explore by themselves the new active

5360
01:58:56,560 --> 01:59:01,040
 

5361
01:58:56,570 --> 01:59:04,190
 learning to actually use what's their

5362
01:59:01,030 --> 01:59:04,190
 

5363
01:59:01,040 --> 01:59:05,719
 use what what what we know what

5364
01:59:04,180 --> 01:59:05,719
 

5365
01:59:04,190 --> 01:59:07,969
 and to build new malls and that will

5366
01:59:05,709 --> 01:59:07,969
 

5367
01:59:05,719 --> 01:59:11,210
 bring us to true automatic machine

5368
01:59:07,959 --> 01:59:11,210
 

5369
01:59:07,969 --> 01:59:12,949
 learning where these models efficiently

5370
01:59:11,200 --> 01:59:12,949
 

5371
01:59:11,210 --> 01:59:15,789
 use any information that it had before

5372
01:59:12,939 --> 01:59:15,789
 

5373
01:59:12,949 --> 01:59:18,030
 to build new models for new problems

5374
01:59:15,779 --> 01:59:18,030
 

5375
01:59:15,789 --> 01:59:26,760
 thank you

5376
01:59:18,020 --> 01:59:26,760
 

5377
01:59:18,030 --> 01:59:26,760
[Applause]

5378
01:59:30,190 --> 01:59:30,190
 

5379
01:59:30,200 --> 01:59:35,170
 all right

5380
01:59:33,060 --> 01:59:35,170
 

5381
01:59:33,070 --> 01:59:38,139
 questions you can queue beyond the

5382
01:59:35,160 --> 01:59:38,139
 

5383
01:59:35,170 --> 01:59:40,800
 microphones around the central aisle if

5384
01:59:38,129 --> 01:59:40,800
 

5385
01:59:38,139 --> 01:59:40,800
 you have any questions

5386
01:59:43,369 --> 01:59:43,369
 

5387
01:59:43,379 --> 01:59:47,429
 okay so thank you very much for your

5388
01:59:45,169 --> 01:59:47,429
 

5389
01:59:45,179 --> 01:59:50,969
 inspiring talk my question is about

5390
01:59:47,419 --> 01:59:50,969
 

5391
01:59:47,429 --> 01:59:53,039
 tasks so I think I know that by using

5392
01:59:50,959 --> 01:59:53,039
 

5393
01:59:50,969 --> 01:59:54,809
 meta learning or transport learning you

5394
01:59:53,029 --> 01:59:54,809
 

5395
01:59:53,039 --> 01:59:57,599
 know the agent can actually generalize

5396
01:59:54,799 --> 01:59:57,599
 

5397
01:59:54,809 --> 02:00:00,449
 the previously then analogous to a

5398
01:59:57,589 --> 02:00:00,449
 

5399
01:59:57,599 --> 02:00:01,949
 nesting situations or tasks but you know

5400
02:00:00,439 --> 02:00:01,949
 

5401
02:00:00,449 --> 02:00:05,550
 when you look at human beings you know

5402
02:00:01,939 --> 02:00:05,550
 

5403
02:00:01,949 --> 02:00:08,579
 their behaviors might be driven by like

5404
02:00:05,540 --> 02:00:08,579
 

5405
02:00:05,550 --> 02:00:10,379
 you know curiosity or maybe you know

5406
02:00:08,569 --> 02:00:10,379
 

5407
02:00:08,579 --> 02:00:13,379
 just desires where there is no specific

5408
02:00:10,369 --> 02:00:13,379
 

5409
02:00:10,379 --> 02:00:14,789
 goals to achieve so if we continue

5410
02:00:13,369 --> 02:00:14,789
 

5411
02:00:13,379 --> 02:00:16,199
 working on transfer and in your meta

5412
02:00:14,779 --> 02:00:16,199
 

5413
02:00:14,789 --> 02:00:18,719
 learning if you really think that we can

5414
02:00:16,189 --> 02:00:18,719
 

5415
02:00:16,199 --> 02:00:22,199
 actually create truly intelligent agent

5416
02:00:18,709 --> 02:00:22,199
 

5417
02:00:18,719 --> 02:00:26,519
 you know which channel you know

5418
02:00:22,189 --> 02:00:26,519
 

5419
02:00:22,199 --> 02:00:29,909
 spontaneously create some you know

5420
02:00:26,509 --> 02:00:29,909
 

5421
02:00:26,519 --> 02:00:31,619
 behaviors or something like that I think

5422
02:00:29,899 --> 02:00:31,619
 

5423
02:00:29,909 --> 02:00:35,519
 that's this notion of creativity is

5424
02:00:31,609 --> 02:00:35,519
 

5425
02:00:31,619 --> 02:00:38,849
 super importance if he is this also

5426
02:00:35,509 --> 02:00:38,849
 

5427
02:00:35,519 --> 02:00:41,159
 defines us as humans and intelligent

5428
02:00:38,839 --> 02:00:41,159
 

5429
02:00:38,849 --> 02:00:42,510
 beings is that we seek out more

5430
02:00:41,149 --> 02:00:42,510
 

5431
02:00:41,159 --> 02:00:45,059
 information we don't know something we

5432
02:00:42,500 --> 02:00:45,059
 

5433
02:00:42,510 --> 02:00:46,800
 are curious about and we either start

5434
02:00:45,049 --> 02:00:46,800
 

5435
02:00:45,059 --> 02:00:48,659
 reading up or we learn more or we start

5436
02:00:46,790 --> 02:00:48,659
 

5437
02:00:46,800 --> 02:00:50,579
 experimenting and gathering more

5438
02:00:48,649 --> 02:00:50,579
 

5439
02:00:48,659 --> 02:00:53,309
 information I think will be very good if

5440
02:00:50,569 --> 02:00:53,309
 

5441
02:00:50,579 --> 02:01:01,260
 we can build algorithms that have this

5442
02:00:53,299 --> 02:01:01,260
 

5443
02:00:53,309 --> 02:01:03,510
 curiosity and learn actively might be

5444
02:01:01,250 --> 02:01:03,510
 

5445
02:01:01,260 --> 02:01:05,519
 used for the Future past or something

5446
02:01:03,500 --> 02:01:05,519
 

5447
02:01:03,510 --> 02:01:07,679
 and then they don't know maybe the agent

5448
02:01:05,509 --> 02:01:07,679
 

5449
02:01:05,519 --> 02:01:09,320
 don't know how to use this experience to

5450
02:01:07,669 --> 02:01:09,320
 

5451
02:01:07,679 --> 02:01:12,030
 escape sometimes but maybe in the future

5452
02:01:09,310 --> 02:01:12,030
 

5453
02:01:09,320 --> 02:01:13,530
 they can do something so maybe it's you

5454
02:01:12,020 --> 02:01:13,530
 

5455
02:01:12,030 --> 02:01:17,099
 know as you said maybe it's important to

5456
02:01:13,520 --> 02:01:17,099
 

5457
02:01:13,530 --> 02:01:19,199
 stores and memories exactly we have the

5458
02:01:17,089 --> 02:01:19,199
 

5459
02:01:17,099 --> 02:01:21,030
 Internet we should give machine learning

5460
02:01:19,189 --> 02:01:21,030
 

5461
02:01:19,199 --> 02:01:23,579
 agents like an Internet where they can

5462
02:01:21,020 --> 02:01:23,579
 

5463
02:01:21,030 --> 02:01:24,989
 find tasks and moles and things in an

5464
02:01:23,569 --> 02:01:24,989
 

5465
02:01:23,579 --> 02:01:25,349
 organized way that they can learn from

5466
02:01:24,979 --> 02:01:25,349
 

5467
02:01:24,989 --> 02:01:27,919
 them

5468
02:01:25,339 --> 02:01:27,919
 

5469
02:01:25,349 --> 02:01:27,919
 thank you

5470
02:01:29,289 --> 02:01:29,289
 

5471
02:01:29,299 --> 02:01:35,599
 Oh crashing I was bobbing do you think

5472
02:01:33,489 --> 02:01:35,599
 

5473
02:01:33,499 --> 02:01:38,209
 in the distant future the other way just

5474
02:01:35,589 --> 02:01:38,209
 

5475
02:01:35,599 --> 02:01:40,519
 be one the intelligent model that as a

5476
02:01:38,199 --> 02:01:40,519
 

5477
02:01:38,209 --> 02:01:42,349
 way of all tasks what do you think I

5478
02:01:40,509 --> 02:01:42,349
 

5479
02:01:40,519 --> 02:01:45,579
 used to really be and I've many

5480
02:01:42,339 --> 02:01:45,579
 

5481
02:01:42,349 --> 02:01:49,039
 dedicated models for a specific path

5482
02:01:45,569 --> 02:01:49,039
 

5483
02:01:45,579 --> 02:01:52,389
 well you can't really have one model for

5484
02:01:49,029 --> 02:01:52,389
 

5485
02:01:49,039 --> 02:01:55,609
 all tasks but I think you can have a

5486
02:01:52,379 --> 02:01:55,609
 

5487
02:01:52,389 --> 02:01:58,159
 system or meta learner that knows so

5488
02:01:55,599 --> 02:01:58,159
 

5489
02:01:55,609 --> 02:02:00,769
 much about the world that it can build

5490
02:01:58,149 --> 02:02:00,769
 

5491
02:01:58,159 --> 02:02:03,639
 it model efficiently so you don't seem

5492
02:02:00,759 --> 02:02:03,639
 

5493
02:02:00,769 --> 02:02:05,959
 to be one either one system that

5494
02:02:03,629 --> 02:02:05,959
 

5495
02:02:03,639 --> 02:02:09,319
 validated by yourself and as well for

5496
02:02:05,949 --> 02:02:09,319
 

5497
02:02:05,959 --> 02:02:10,519
 many many tasks or a same time well

5498
02:02:09,309 --> 02:02:10,519
 

5499
02:02:09,319 --> 02:02:13,549
 there's this thing called the no free

5500
02:02:10,509 --> 02:02:13,549
 

5501
02:02:10,519 --> 02:02:16,879
 lunch theorem right so you can you can

5502
02:02:13,539 --> 02:02:16,879
 

5503
02:02:13,549 --> 02:02:19,249
 learn good models for tasks you can you

5504
02:02:16,869 --> 02:02:19,249
 

5505
02:02:16,879 --> 02:02:21,949
 can meta learn good models for a group

5506
02:02:19,239 --> 02:02:21,949
 

5507
02:02:19,249 --> 02:02:24,529
 of tasks but one mole that solves all

5508
02:02:21,939 --> 02:02:24,529
 

5509
02:02:21,949 --> 02:02:26,479
 tasks is kind of not possible because

5510
02:02:24,519 --> 02:02:26,479
 

5511
02:02:24,529 --> 02:02:28,969
 you don't have the necessary negative

5512
02:02:26,469 --> 02:02:28,969
 

5513
02:02:26,479 --> 02:02:30,529
 bias to learn efficiently or maybe it's

5514
02:02:28,959 --> 02:02:30,529
 

5515
02:02:28,969 --> 02:02:32,209
 directly possible it's not efficient to

5516
02:02:30,519 --> 02:02:32,209
 

5517
02:02:30,529 --> 02:02:36,439
 build one it's much much more efficient

5518
02:02:32,199 --> 02:02:36,439
 

5519
02:02:32,209 --> 02:02:44,419
 to learn how to learn models for certain

5520
02:02:36,429 --> 02:02:44,419
 

5521
02:02:36,439 --> 02:02:51,369
 types of tasks thank you previous

5522
02:02:44,409 --> 02:02:51,369
 

5523
02:02:44,419 --> 02:02:54,699
 speaker are there any techniques to

5524
02:02:51,359 --> 02:02:54,699
 

5525
02:02:51,369 --> 02:02:54,699
 high-performance variance

5526
02:02:55,060 --> 02:02:55,060
 

5527
02:02:55,070 --> 02:03:01,250
 model that gives high variance from

5528
02:02:59,230 --> 02:03:01,250
 

5529
02:02:59,240 --> 02:03:03,800
 training to train and

5530
02:03:01,240 --> 02:03:03,800
 

5531
02:03:01,250 --> 02:03:06,890
 what type of optimization the competitor

5532
02:03:03,790 --> 02:03:06,890
 

5533
02:03:03,800 --> 02:03:11,200
 will like Bayesian optimization become

5534
02:03:06,880 --> 02:03:11,200
 

5535
02:03:06,890 --> 02:03:11,200
 good under the setting of high variance

5536
02:03:13,950 --> 02:03:13,950
 

5537
02:03:13,960 --> 02:03:18,350
 sorry can you repeat the question I

5538
02:03:16,000 --> 02:03:18,350
 

5539
02:03:16,010 --> 02:03:19,880
 understood it's about high variance and

5540
02:03:18,340 --> 02:03:19,880
 

5541
02:03:18,350 --> 02:03:24,860
 based optimization but that's about all

5542
02:03:19,870 --> 02:03:24,860
 

5543
02:03:19,880 --> 02:03:29,630
 that my question is the performance has

5544
02:03:24,850 --> 02:03:29,630
 

5545
02:03:24,860 --> 02:03:33,260
 high variance are you going to do yes

5546
02:03:29,620 --> 02:03:33,260
 

5547
02:03:29,630 --> 02:03:36,500
 okay so if you have a high variance

5548
02:03:33,250 --> 02:03:36,500
 

5549
02:03:33,260 --> 02:03:39,740
 black box that in repeated runs give you

5550
02:03:36,490 --> 02:03:39,740
 

5551
02:03:36,500 --> 02:03:42,110
 very different performance then well you

5552
02:03:39,730 --> 02:03:42,110
 

5553
02:03:39,740 --> 02:03:44,210
 need well one approach that you can for

5554
02:03:42,100 --> 02:03:44,210
 

5555
02:03:42,110 --> 02:03:49,100
 example user is again the multi fidelity

5556
02:03:44,200 --> 02:03:49,100
 

5557
02:03:44,210 --> 02:03:52,460
 approach where you evaluate only once

5558
02:03:49,090 --> 02:03:52,460
 

5559
02:03:49,100 --> 02:03:54,080
 for kind of all configurations but for

5560
02:03:52,450 --> 02:03:54,080
 

5561
02:03:52,460 --> 02:03:56,120
 the best configurations you actually

5562
02:03:54,070 --> 02:03:56,120
 

5563
02:03:54,080 --> 02:03:58,130
 want to evaluate more and more so the

5564
02:03:56,110 --> 02:03:58,130
 

5565
02:03:56,120 --> 02:03:59,630
 the thing that you return you actually

5566
02:03:58,120 --> 02:03:59,630
 

5567
02:03:58,130 --> 02:04:01,220
 want to be very sure about so for

5568
02:03:59,620 --> 02:04:01,220
 

5569
02:03:59,630 --> 02:04:03,020
 example in reinforcement learning you

5570
02:04:01,210 --> 02:04:03,020
 

5571
02:04:01,220 --> 02:04:06,590
 want to do multiple trials because there

5572
02:04:03,010 --> 02:04:06,590
 

5573
02:04:03,020 --> 02:04:08,750
 is a lot of uncertainty a lot of

5574
02:04:06,580 --> 02:04:08,750
 

5575
02:04:06,590 --> 02:04:10,100
 variance across runs and so for the best

5576
02:04:08,740 --> 02:04:10,100
 

5577
02:04:08,750 --> 02:04:12,160
 configurations you actually want to do a

5578
02:04:10,090 --> 02:04:12,160
 

5579
02:04:10,100 --> 02:04:14,720
 lot of runs but you don't want to waste

5580
02:04:12,150 --> 02:04:14,720
 

5581
02:04:12,160 --> 02:04:17,630
 doing for example 100 runs for every

5582
02:04:14,710 --> 02:04:17,630
 

5583
02:04:14,720 --> 02:04:19,580
 configuration so yeah you can use a

5584
02:04:17,620 --> 02:04:19,580
 

5585
02:04:17,630 --> 02:04:25,730
 standard multi fidelity approach for

5586
02:04:19,570 --> 02:04:25,730
 

5587
02:04:19,580 --> 02:04:27,890
 that so this all ml models could be

5588
02:04:25,720 --> 02:04:27,890
 

5589
02:04:25,730 --> 02:04:29,270
 pretty complicated so how do you make

5590
02:04:27,880 --> 02:04:29,270
 

5591
02:04:27,890 --> 02:04:37,730
 sure that the model actually learn

5592
02:04:29,260 --> 02:04:37,730
 

5593
02:04:29,270 --> 02:04:39,950
 something that makes sense you you set

5594
02:04:37,720 --> 02:04:39,950
 

5595
02:04:37,730 --> 02:04:43,190
 up a testing environment where you test

5596
02:04:39,940 --> 02:04:43,190
 

5597
02:04:39,950 --> 02:04:45,920
 for what you wanted to do yeah and of

5598
02:04:43,180 --> 02:04:45,920
 

5599
02:04:43,190 --> 02:04:47,990
 course you need a generic way of having

5600
02:04:45,910 --> 02:04:47,990
 

5601
02:04:45,920 --> 02:04:50,900
 lots of benchmarks you don't want to

5602
02:04:47,980 --> 02:04:50,900
 

5603
02:04:47,990 --> 02:04:52,790
 over fit to one particular benchmark but

5604
02:04:50,890 --> 02:04:52,790
 

5605
02:04:50,900 --> 02:04:56,020
 but those are standard questions of

5606
02:04:52,780 --> 02:04:56,020
 

5607
02:04:52,790 --> 02:04:59,990
 experimental science right how about

5608
02:04:56,010 --> 02:04:59,990
 

5609
02:04:56,020 --> 02:05:01,690
 problems like dimensions like your model

5610
02:04:59,980 --> 02:05:01,690
 

5611
02:04:59,990 --> 02:05:04,350
 is fit

5612
02:05:01,680 --> 02:05:04,350
 

5613
02:05:01,690 --> 02:05:07,750
 data of a certain time period but going

5614
02:05:04,340 --> 02:05:07,750
 

5615
02:05:04,350 --> 02:05:11,110
 forward in future maybe teens don't look

5616
02:05:07,740 --> 02:05:11,110
 

5617
02:05:07,750 --> 02:05:12,970
 like that way yeah this is scope tons of

5618
02:05:11,100 --> 02:05:12,970
 

5619
02:05:11,110 --> 02:05:15,940
 drift so there's different ways of

5620
02:05:12,960 --> 02:05:15,940
 

5621
02:05:12,970 --> 02:05:17,770
 solving this you have some L games that

5622
02:05:15,930 --> 02:05:17,770
 

5623
02:05:15,940 --> 02:05:20,650
 naturally adapt to that like most

5624
02:05:17,760 --> 02:05:20,650
 

5625
02:05:17,770 --> 02:05:22,000
 grading based algorithms they did well

5626
02:05:20,640 --> 02:05:22,000
 

5627
02:05:20,650 --> 02:05:23,950
 they do go into send until some point

5628
02:05:21,990 --> 02:05:23,950
 

5629
02:05:22,000 --> 02:05:26,050
 and if the data changes then service

5630
02:05:23,940 --> 02:05:26,050
 

5631
02:05:23,950 --> 02:05:27,760
 changes and it optimizes further so

5632
02:05:26,040 --> 02:05:27,760
 

5633
02:05:26,050 --> 02:05:29,890
 those kind of a debt automatically other

5634
02:05:27,750 --> 02:05:29,890
 

5635
02:05:27,760 --> 02:05:32,740
 elegance you just have to retrain every

5636
02:05:29,880 --> 02:05:32,740
 

5637
02:05:29,890 --> 02:05:36,310
 time periods so to adapt to new kind of

5638
02:05:32,730 --> 02:05:36,310
 

5639
02:05:32,740 --> 02:05:38,080
 data and you will over fit to the types

5640
02:05:36,300 --> 02:05:38,080
 

5641
02:05:36,310 --> 02:05:40,000
 of datasets that you will see that you

5642
02:05:38,070 --> 02:05:40,000
 

5643
02:05:38,080 --> 02:05:44,140
 see if you see very different datasets

5644
02:05:39,990 --> 02:05:44,140
 

5645
02:05:40,000 --> 02:05:46,090
 going forward then well you can well

5646
02:05:44,130 --> 02:05:46,090
 

5647
02:05:44,140 --> 02:05:48,640
 have a component that looks at the data

5648
02:05:46,080 --> 02:05:48,640
 

5649
02:05:46,090 --> 02:05:51,810
 set and for these types of data set uses

5650
02:05:48,630 --> 02:05:51,810
 

5651
02:05:48,640 --> 02:05:51,810
 different types of

5652
02:05:54,550 --> 02:05:54,550
 

5653
02:05:54,560 --> 02:05:57,699
[Music]

5654
02:06:06,060 --> 02:06:06,060
 

5655
02:06:06,070 --> 02:06:09,280
[Music]

5656
02:06:09,330 --> 02:06:09,330
 

5657
02:06:09,340 --> 02:06:15,520
 is there any process that I can create

5658
02:06:11,550 --> 02:06:15,520
 

5659
02:06:11,560 --> 02:06:19,870
 metadata for object detection test I'm

5660
02:06:15,510 --> 02:06:19,870
 

5661
02:06:15,520 --> 02:06:22,420
 not aware is there metadata for object

5662
02:06:19,860 --> 02:06:22,420
 

5663
02:06:19,870 --> 02:06:25,890
 detection are there are lots of datasets

5664
02:06:22,410 --> 02:06:25,890
 

5665
02:06:22,420 --> 02:06:31,540
 for it where you can train on I am we

5666
02:06:25,880 --> 02:06:31,540
 

5667
02:06:25,890 --> 02:06:32,740
 open l has a lot of these well not

5668
02:06:31,530 --> 02:06:32,740
 

5669
02:06:31,540 --> 02:06:37,270
 things like like image net or something

5670
02:06:32,730 --> 02:06:37,270
 

5671
02:06:32,740 --> 02:06:41,650
 else or maybe it's a call to arms to the

5672
02:06:37,260 --> 02:06:41,650
 

5673
02:06:37,270 --> 02:06:44,050
 computation community to create image it

5674
02:06:41,640 --> 02:06:44,050
 

5675
02:06:41,650 --> 02:06:46,060
 might be some specification definitely

5676
02:06:44,040 --> 02:06:46,060
 

5677
02:06:44,050 --> 02:06:48,520
 in object recognition I'm not entirely

5678
02:06:46,050 --> 02:06:48,520
 

5679
02:06:46,060 --> 02:06:52,090
 sure I'll look into it and get back to

5680
02:06:48,510 --> 02:06:52,090
 

5681
02:06:48,520 --> 02:06:53,950
 you all right

5682
02:06:52,080 --> 02:06:53,950
 

5683
02:06:52,090 --> 02:06:56,530
 so this concludes the tutorial on auto

5684
02:06:53,940 --> 02:06:56,530
 

5685
02:06:53,950 --> 02:07:01,510
 ml and let's thank the speakers again

5686
02:06:56,520 --> 02:07:01,510
 

5687
02:06:56,530 --> 02:07:01,510
[Applause]

5688
02:07:03,130 --> 02:07:03,130
 

5689
02:07:03,140 --> 02:07:06,359
[Music]

5690
02:07:08,380 --> 02:07:08,380
 

5691
02:07:08,390 --> 02:08:06,509
[Music]

5692
02:08:08,580 --> 02:08:08,580
 

5693
02:08:08,590 --> 02:09:08,999
[Music]

5694
02:09:14,190 --> 02:09:14,190
 

5695
02:09:14,200 --> 02:09:47,930
[Music]