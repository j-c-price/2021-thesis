1
00:00:03,220 --> 00:00:05,270
[Music]

2
00:00:05,270 --> 00:00:07,140
yeah I think next a lot for the

3
00:00:07,140 --> 00:00:09,059
invitation I apologize in advance I've

4
00:00:09,059 --> 00:00:10,710
had this sort of lingered lingering

5
00:00:10,710 --> 00:00:12,059
respiratory thing so I might have

6
00:00:12,059 --> 00:00:15,269
coughing fits throughout so to entertain

7
00:00:15,269 --> 00:00:18,119
you so so please come up at as it goes

8
00:00:18,119 --> 00:00:20,369
and ask questions that will kind of

9
00:00:20,369 --> 00:00:22,170
improve thanks for everything everyone I

10
00:00:22,170 --> 00:00:24,060
think okay so I'm gonna be talking about

11
00:00:24,060 --> 00:00:26,430
scalable Bayesian inference so I'll

12
00:00:26,430 --> 00:00:27,810
start out with the motivation and

13
00:00:27,810 --> 00:00:30,150
background and then I'm gonna have like

14
00:00:30,150 --> 00:00:32,880
two sub parts the the first part is on

15
00:00:32,880 --> 00:00:36,510
big n or big sample size and so you know

16
00:00:36,510 --> 00:00:39,210
you can very roughly think about two

17
00:00:39,210 --> 00:00:41,010
different types of scalability the one

18
00:00:41,010 --> 00:00:42,899
would be where you have a data set

19
00:00:42,899 --> 00:00:44,910
that's really an enormous sample size

20
00:00:44,910 --> 00:00:48,360
but the you know the the complexity per

21
00:00:48,360 --> 00:00:50,340
subject is not that high and so you

22
00:00:50,340 --> 00:00:51,840
might have you might be trying to do a

23
00:00:51,840 --> 00:00:55,020
logistic regression or something like

24
00:00:55,020 --> 00:00:57,780
that and and you might have millions of

25
00:00:57,780 --> 00:01:00,090
subjects or hundreds of millions of

26
00:01:00,090 --> 00:01:01,770
subjects and you want to do something

27
00:01:01,770 --> 00:01:03,210
like logistic regression that's not that

28
00:01:03,210 --> 00:01:05,549
complicated okay and so that'll be the

29
00:01:05,549 --> 00:01:08,369
kind of first chunk of the talk and then

30
00:01:08,369 --> 00:01:10,350
probably more after the break I'll focus

31
00:01:10,350 --> 00:01:12,960
more on high dimensional data which

32
00:01:12,960 --> 00:01:16,740
means that you have a lot of dimension

33
00:01:16,740 --> 00:01:20,180
per or complexity per replicator

34
00:01:20,180 --> 00:01:22,740
subjective it's a biomedical study that

35
00:01:22,740 --> 00:01:26,610
could also be a big model and so if you

36
00:01:26,610 --> 00:01:28,500
had like a deep deep neural network or

37
00:01:28,500 --> 00:01:30,840
some nonparametric Bayesian model with

38
00:01:30,840 --> 00:01:32,970
with many many many many parameters that

39
00:01:32,970 --> 00:01:35,070
would essentially be a kind of big P

40
00:01:35,070 --> 00:01:36,659
problem and so we can roughly think

41
00:01:36,659 --> 00:01:39,299
about big n and big P and big and I

42
00:01:39,299 --> 00:01:41,659
would say would be an area that's much

43
00:01:41,659 --> 00:01:44,520
more well addressed by the existing

44
00:01:44,520 --> 00:01:46,320
literature and big P is more of an

45
00:01:46,320 --> 00:01:49,320
emerging area for which there's some

46
00:01:49,320 --> 00:01:51,210
methods in specific particular cases but

47
00:01:51,210 --> 00:01:53,159
not it's sort of a lack of overarching

48
00:01:53,159 --> 00:01:54,509
methods that can deal with broad

49
00:01:54,509 --> 00:01:58,680
problems okay so um so one thing I'd

50
00:01:58,680 --> 00:02:01,590
like to do partly in this tutorial is to

51
00:02:01,590 --> 00:02:03,630
motivate more of you to work on Bayesian

52
00:02:03,630 --> 00:02:05,189
methods because I think that there is

53
00:02:05,189 --> 00:02:08,220
sort of vast open problems and the huge

54
00:02:08,220 --> 00:02:10,080
potential for new research that can have

55
00:02:10,080 --> 00:02:13,780
a really big impact in broad variety of

56
00:02:13,780 --> 00:02:17,970
areas in in this industry science policy

57
00:02:17,970 --> 00:02:21,130
etc okay and so obviously there's a

58
00:02:21,130 --> 00:02:23,770
really a huge literature already focused

59
00:02:23,770 --> 00:02:26,170
on big data but but I'd say most of the

60
00:02:26,170 --> 00:02:28,060
focus has obviously been on optimization

61
00:02:28,060 --> 00:02:30,280
methods and so as a result there's

62
00:02:30,280 --> 00:02:32,319
there's potentially I would say orders

63
00:02:32,319 --> 00:02:34,239
of magnitude more people working on

64
00:02:34,239 --> 00:02:37,930
trying to do optimization in really

65
00:02:37,930 --> 00:02:40,450
large date complicated data context and

66
00:02:40,450 --> 00:02:42,730
there are people working on Bayesian

67
00:02:42,730 --> 00:02:45,069
methods that do appropriate uncertainty

68
00:02:45,069 --> 00:02:47,620
quantification and and I think that you

69
00:02:47,620 --> 00:02:48,760
know that's really the core of

70
00:02:48,760 --> 00:02:50,680
statistics and really important in a lot

71
00:02:50,680 --> 00:02:52,480
of machine learning applications is we

72
00:02:52,480 --> 00:02:55,600
need to think more about uncertainty and

73
00:02:55,600 --> 00:02:57,970
doing valid inferences in the presence

74
00:02:57,970 --> 00:03:00,090
of uncertainty and so if we have some

75
00:03:00,090 --> 00:03:02,739
algorithm that produces what I'd call a

76
00:03:02,739 --> 00:03:04,690
point estimate it might be a point

77
00:03:04,690 --> 00:03:07,360
estimate for a prediction for an

78
00:03:07,360 --> 00:03:09,400
input-output relationship I think that

79
00:03:09,400 --> 00:03:11,080
that really kind of falls short in a lot

80
00:03:11,080 --> 00:03:12,970
of applications and we can't be just

81
00:03:12,970 --> 00:03:15,370
doing that we need to be actually

82
00:03:15,370 --> 00:03:17,080
characterizing uncertainty in our

83
00:03:17,080 --> 00:03:19,810
predictions and importantly not just in

84
00:03:19,810 --> 00:03:21,250
a black box for prediction but in

85
00:03:21,250 --> 00:03:23,260
something that's really interpretable so

86
00:03:23,260 --> 00:03:25,600
that's really key I think in science and

87
00:03:25,600 --> 00:03:27,430
in other application areas and so we

88
00:03:27,430 --> 00:03:29,140
want to go beyond optimization methods

89
00:03:29,140 --> 00:03:31,900
so again so most of the literature I

90
00:03:31,900 --> 00:03:34,780
would say on scalability is on rapidly

91
00:03:34,780 --> 00:03:36,519
obtaining what I might call a point

92
00:03:36,519 --> 00:03:37,930
estimate it might be a point estimate

93
00:03:37,930 --> 00:03:39,610
for parameters or a point estimate for

94
00:03:39,610 --> 00:03:42,459
an input-output predictive model okay

95
00:03:42,459 --> 00:03:44,470
and we'd like to be able to do that even

96
00:03:44,470 --> 00:03:46,660
when the sample size n and the overall

97
00:03:46,660 --> 00:03:49,720
size of the data is immense I would also

98
00:03:49,720 --> 00:03:52,690
advocate for people working less in a

99
00:03:52,690 --> 00:03:55,359
bandwagon II way I think is I would say

100
00:03:55,359 --> 00:03:56,920
that there's been a huge focus and

101
00:03:56,920 --> 00:03:58,530
statistics and machine learning on

102
00:03:58,530 --> 00:04:01,269
people working on very similar things on

103
00:04:01,269 --> 00:04:04,209
for example linear regression certain

104
00:04:04,209 --> 00:04:06,880
types of imaging problems etc but

105
00:04:06,880 --> 00:04:09,430
there's actually huge open problems that

106
00:04:09,430 --> 00:04:11,560
remain essentially untouched where we

107
00:04:11,560 --> 00:04:13,239
have almost no methods that are

108
00:04:13,239 --> 00:04:14,829
reasonable at all for these types of

109
00:04:14,829 --> 00:04:16,899
problems and so many of those problems

110
00:04:16,899 --> 00:04:19,029
fall into the into the sciences for

111
00:04:19,029 --> 00:04:21,250
example we might want to do something

112
00:04:21,250 --> 00:04:24,060
like precision medicine so we have

113
00:04:24,060 --> 00:04:26,650
millions of observations observed on

114
00:04:26,650 --> 00:04:27,310
each subject

115
00:04:27,310 --> 00:04:29,650
for different biomarkers they might be

116
00:04:29,650 --> 00:04:31,540
genomic they might be neuroscience

117
00:04:31,540 --> 00:04:33,940
biomarkers and we'd like to use that to

118
00:04:33,940 --> 00:04:36,210
predict patient health and to target

119
00:04:36,210 --> 00:04:38,740
treatment for different diseases okay

120
00:04:38,740 --> 00:04:40,150
we'd like to do something like that and

121
00:04:40,150 --> 00:04:41,560
we have no really no idea how to do it

122
00:04:41,560 --> 00:04:44,380
currently okay so I would again advocate

123
00:04:44,380 --> 00:04:46,990
for people working on on those types of

124
00:04:46,990 --> 00:04:48,550
problems on different types of problems

125
00:04:48,550 --> 00:04:50,139
try to avoid working on the same

126
00:04:50,139 --> 00:04:51,810
problems everyone else is working on

127
00:04:51,810 --> 00:04:54,550
okay so my focus is in general on

128
00:04:54,550 --> 00:04:56,470
probability models this is obviously

129
00:04:56,470 --> 00:04:58,780
obviously a tutorial on scalable

130
00:04:58,780 --> 00:05:00,550
Bayesian inference per se but I would

131
00:05:00,550 --> 00:05:03,970
say it's often useful to take the

132
00:05:03,970 --> 00:05:05,889
Bayesian paradigm which is really nice

133
00:05:05,889 --> 00:05:07,900
in terms of using probability to

134
00:05:07,900 --> 00:05:09,729
characterize learning from complicated

135
00:05:09,729 --> 00:05:12,400
data and and take a little step away

136
00:05:12,400 --> 00:05:14,950
sometimes and so the main thing is that

137
00:05:14,950 --> 00:05:17,110
we have a really nice approach for

138
00:05:17,110 --> 00:05:18,639
learning from different types of data

139
00:05:18,639 --> 00:05:21,430
and we're characterizing uncertainty in

140
00:05:21,430 --> 00:05:23,200
that learning and in our inferences and

141
00:05:23,200 --> 00:05:26,530
predictions in a valid way and and

142
00:05:26,530 --> 00:05:28,180
that's really useful if we're using a

143
00:05:28,180 --> 00:05:30,070
probability model and Bayesian

144
00:05:30,070 --> 00:05:32,229
statistics is one type of formulation of

145
00:05:32,229 --> 00:05:32,620
that

146
00:05:32,620 --> 00:05:34,900
okay so we'd like to have some sort of

147
00:05:34,900 --> 00:05:36,370
general probabilistic inference

148
00:05:36,370 --> 00:05:39,490
algorithm from from complex data we'd

149
00:05:39,490 --> 00:05:40,900
like to be able to handle arbitrarily

150
00:05:40,900 --> 00:05:43,720
complex probability models and so you

151
00:05:43,720 --> 00:05:45,310
know often nowadays data are really

152
00:05:45,310 --> 00:05:47,620
increasingly complicated and we need to

153
00:05:47,620 --> 00:05:49,330
be able to model the complexity of the

154
00:05:49,330 --> 00:05:49,690
data

155
00:05:49,690 --> 00:05:51,490
I've got mentioning this precision

156
00:05:51,490 --> 00:05:53,650
medicine application you know you could

157
00:05:53,650 --> 00:05:55,470
think of data from patients coming into

158
00:05:55,470 --> 00:05:58,150
healthcare clinic and there's a lot of

159
00:05:58,150 --> 00:06:00,880
censoring observations collected over

160
00:06:00,880 --> 00:06:04,389
time selection bias which is something

161
00:06:04,389 --> 00:06:06,310
which is incredibly important to take

162
00:06:06,310 --> 00:06:08,620
into account we can have missing data

163
00:06:08,620 --> 00:06:10,870
that's informatively missing and if we

164
00:06:10,870 --> 00:06:12,070
don't characterize this sort of

165
00:06:12,070 --> 00:06:13,960
complexity if we just kind of throw some

166
00:06:13,960 --> 00:06:15,789
sort of neural network or some algorithm

167
00:06:15,789 --> 00:06:18,370
at it putting all this data together

168
00:06:18,370 --> 00:06:20,680
I'm not taking to account the selection

169
00:06:20,680 --> 00:06:22,770
that occurs of the data that we observe

170
00:06:22,770 --> 00:06:24,580
relative to the data in the general

171
00:06:24,580 --> 00:06:26,590
population then we're going to get kind

172
00:06:26,590 --> 00:06:28,539
of bogus garbage basically out of the

173
00:06:28,539 --> 00:06:31,030
out of the results okay so we'd like to

174
00:06:31,030 --> 00:06:32,919
the algorithms to be scalable to huge

175
00:06:32,919 --> 00:06:35,789
data potentially using many computers

176
00:06:35,789 --> 00:06:38,680
okay and an accurate uncertainty

177
00:06:38,680 --> 00:06:40,720
quantification is a critical issue I

178
00:06:40,720 --> 00:06:41,260
would say

179
00:06:41,260 --> 00:06:43,120
you know a lot of this scalable Bayes

180
00:06:43,120 --> 00:06:44,650
inference and I'll touch on this has

181
00:06:44,650 --> 00:06:46,270
been focused on algorithms such as

182
00:06:46,270 --> 00:06:48,610
variational Bayes where you might

183
00:06:48,610 --> 00:06:50,800
actually not characterize uncertainty

184
00:06:50,800 --> 00:06:53,290
very well at all and so you might have

185
00:06:53,290 --> 00:06:55,420
an estimate of a posterior distribution

186
00:06:55,420 --> 00:06:57,730
that is much much much more concentrated

187
00:06:57,730 --> 00:07:01,210
than is realistic given the actual

188
00:07:01,210 --> 00:07:02,710
uncertainty you having a problem and so

189
00:07:02,710 --> 00:07:04,060
we'd like to be able to actually

190
00:07:04,060 --> 00:07:07,270
accurately characterize uncertainty also

191
00:07:07,270 --> 00:07:10,270
an issue that I'm going to touch on and

192
00:07:10,270 --> 00:07:12,610
in a chunk of this talk is on robustness

193
00:07:12,610 --> 00:07:14,620
of inferences and so one one thing

194
00:07:14,620 --> 00:07:17,020
that's really interesting is that okay

195
00:07:17,020 --> 00:07:18,430
we take a Bayesian approach well a

196
00:07:18,430 --> 00:07:20,500
Bayesian approach is one example of a

197
00:07:20,500 --> 00:07:22,720
model based approach to inference and by

198
00:07:22,720 --> 00:07:24,670
model I mean that we need to specify a

199
00:07:24,670 --> 00:07:26,740
likelihood function the generative

200
00:07:26,740 --> 00:07:29,020
likelihood function for the data it

201
00:07:29,020 --> 00:07:30,400
could be that that likelihood function

202
00:07:30,400 --> 00:07:32,860
is very complicated and intricate to

203
00:07:32,860 --> 00:07:35,650
allow for a lot of flexibility but then

204
00:07:35,650 --> 00:07:37,330
what we're worried about is this sort of

205
00:07:37,330 --> 00:07:39,130
robustness of inferences and so that

206
00:07:39,130 --> 00:07:41,770
that can be a particularly large problem

207
00:07:41,770 --> 00:07:44,230
as the sample size gets really big and

208
00:07:44,230 --> 00:07:45,910
so it's it's kind of well-known and

209
00:07:45,910 --> 00:07:47,740
statistics for example let's say I have

210
00:07:47,740 --> 00:07:49,930
a likelihood or something based model

211
00:07:49,930 --> 00:07:51,760
and I might want to be doing some type

212
00:07:51,760 --> 00:07:53,710
of hypothesis testing or variable

213
00:07:53,710 --> 00:07:56,500
selection or or some other type of types

214
00:07:56,500 --> 00:07:58,240
of inferences on the model structure

215
00:07:58,240 --> 00:08:00,670
well what often happens is well if this

216
00:08:00,670 --> 00:08:02,170
sample size gets bigger and bigger and

217
00:08:02,170 --> 00:08:04,030
bigger and bigger then everything

218
00:08:04,030 --> 00:08:05,770
becomes significant and so we have

219
00:08:05,770 --> 00:08:07,750
millions of observations all your

220
00:08:07,750 --> 00:08:11,020
p-value ZAR significant your model gets

221
00:08:11,020 --> 00:08:12,700
bigger and bigger and bigger kind of

222
00:08:12,700 --> 00:08:15,580
without bound and we become very non

223
00:08:15,580 --> 00:08:17,230
robust in the sense and so we might have

224
00:08:17,230 --> 00:08:19,990
a model that's really good that could be

225
00:08:19,990 --> 00:08:22,660
very nice at interpreting the data but

226
00:08:22,660 --> 00:08:24,490
then that the actual data you observe

227
00:08:24,490 --> 00:08:26,140
might not be coming exactly from that

228
00:08:26,140 --> 00:08:27,610
model because all models are wrong and

229
00:08:27,610 --> 00:08:28,570
might be coming from a slight

230
00:08:28,570 --> 00:08:30,880
perturbation of that model and that that

231
00:08:30,880 --> 00:08:33,130
that becomes a big issue when the sample

232
00:08:33,130 --> 00:08:35,530
size is really large and so we end up

233
00:08:35,530 --> 00:08:37,900
having sort of less robust inferences as

234
00:08:37,900 --> 00:08:40,510
ample size becomes really really big and

235
00:08:40,510 --> 00:08:42,220
if we just care about some sort of

236
00:08:42,220 --> 00:08:44,590
predictive input/output blackbox which

237
00:08:44,590 --> 00:08:46,540
is a lot of machine learning then we

238
00:08:46,540 --> 00:08:49,240
might not care so much but if we want to

239
00:08:49,240 --> 00:08:50,590
actually do kind of scientific

240
00:08:50,590 --> 00:08:52,420
inferences based on our model based

241
00:08:52,420 --> 00:08:54,670
inferences that then it's a very big

242
00:08:54,670 --> 00:08:55,120
issue

243
00:08:55,120 --> 00:08:58,449
and I'll discuss that okay so that's the

244
00:08:58,449 --> 00:09:01,120
general focus obviously we're gonna be

245
00:09:01,120 --> 00:09:03,220
focusing on Bayesian approaches I would

246
00:09:03,220 --> 00:09:04,689
say Bayesian methods offer an attractive

247
00:09:04,689 --> 00:09:06,879
general approach for for modeling

248
00:09:06,879 --> 00:09:09,819
complicated data and so to just just

249
00:09:09,819 --> 00:09:11,589
sort of set the stage here many of you

250
00:09:11,589 --> 00:09:13,600
of course will be familiar with basic

251
00:09:13,600 --> 00:09:15,189
Bayesian inference but probably not all

252
00:09:15,189 --> 00:09:17,439
of you and so here we're gonna say well

253
00:09:17,439 --> 00:09:19,870
we have some sort of likelihood model

254
00:09:19,870 --> 00:09:21,069
and so this is some generative

255
00:09:21,069 --> 00:09:22,689
probability model that could generate

256
00:09:22,689 --> 00:09:24,459
the dataview observe and I'm gonna say

257
00:09:24,459 --> 00:09:26,439
that's why and I'm gonna put a

258
00:09:26,439 --> 00:09:30,009
superscript in on that why to index the

259
00:09:30,009 --> 00:09:32,649
the sample size okay and so um I might

260
00:09:32,649 --> 00:09:34,180
be dropping some of that that fine

261
00:09:34,180 --> 00:09:36,459
notation as we go but let's just say the

262
00:09:36,459 --> 00:09:38,079
like this is the likelihood of some

263
00:09:38,079 --> 00:09:40,480
giant collection of data Y superscript n

264
00:09:40,480 --> 00:09:42,370
and it's parameterised by some

265
00:09:42,370 --> 00:09:44,740
parameters theta okay so this might be

266
00:09:44,740 --> 00:09:47,649
some mixture model it could be some sort

267
00:09:47,649 --> 00:09:49,749
of generative neural network it could be

268
00:09:49,749 --> 00:09:51,360
a Gaussian process it could be anything

269
00:09:51,360 --> 00:09:53,529
and so that's our data here's our

270
00:09:53,529 --> 00:09:55,660
parameters and so if we take a Bayesian

271
00:09:55,660 --> 00:09:56,980
approach then we need to put down a

272
00:09:56,980 --> 00:09:59,170
prior probability distribution on these

273
00:09:59,170 --> 00:10:00,670
parameters and I'll call that PI of

274
00:10:00,670 --> 00:10:02,949
theta okay and so we have a prior PI of

275
00:10:02,949 --> 00:10:05,439
theta that sort of encodes information

276
00:10:05,439 --> 00:10:07,569
and in the parameters prior to observing

277
00:10:07,569 --> 00:10:10,059
the current the current data and now the

278
00:10:10,059 --> 00:10:12,160
likelihood is encoding information in

279
00:10:12,160 --> 00:10:14,139
the data about those parameters and then

280
00:10:14,139 --> 00:10:15,550
we kind of put it into our Bayes rule

281
00:10:15,550 --> 00:10:19,059
okay so here's the prior here's the

282
00:10:19,059 --> 00:10:21,279
likelihood and now we we need to turn

283
00:10:21,279 --> 00:10:23,050
this into a probability distribution and

284
00:10:23,050 --> 00:10:25,120
so we normalize it so we divide by the

285
00:10:25,120 --> 00:10:26,620
integral of the prior times the

286
00:10:26,620 --> 00:10:27,879
likelihood over the parameter space

287
00:10:27,879 --> 00:10:30,279
that's often known as the marginal

288
00:10:30,279 --> 00:10:32,829
likelihood or the evidence okay and then

289
00:10:32,829 --> 00:10:34,480
we get this posterior probability

290
00:10:34,480 --> 00:10:36,279
distribution so the Bayesian updating

291
00:10:36,279 --> 00:10:38,410
occurs when we update the information in

292
00:10:38,410 --> 00:10:39,939
the prior which might be a relatively

293
00:10:39,939 --> 00:10:42,399
flat probability distribution with the

294
00:10:42,399 --> 00:10:44,290
information in the likelihood and then

295
00:10:44,290 --> 00:10:46,660
we're doing a type of learning okay and

296
00:10:46,660 --> 00:10:48,249
that's one really nice thing in that we

297
00:10:48,249 --> 00:10:50,410
can we can keep doing that over and over

298
00:10:50,410 --> 00:10:52,269
again as we get more more in different

299
00:10:52,269 --> 00:10:54,100
types and disparate sources of

300
00:10:54,100 --> 00:10:55,749
information we can kind of keep plugging

301
00:10:55,749 --> 00:10:58,089
in and doing this Bayesian updating to

302
00:10:58,089 --> 00:11:00,300
learn ever more about this kind of

303
00:11:00,300 --> 00:11:02,350
posterior distribution there's more data

304
00:11:02,350 --> 00:11:06,519
become available okay the posterior is

305
00:11:06,519 --> 00:11:07,779
really nice and characterizing

306
00:11:07,779 --> 00:11:08,740
uncertainty in the

307
00:11:08,740 --> 00:11:12,850
rameters and and also I would say in any

308
00:11:12,850 --> 00:11:14,709
functional of interest and because we

309
00:11:14,709 --> 00:11:16,930
might not be interested directly in

310
00:11:16,930 --> 00:11:18,760
these parameters theta these are just

311
00:11:18,760 --> 00:11:20,050
parameters that are convenient to

312
00:11:20,050 --> 00:11:21,790
specify our likelihood function you know

313
00:11:21,790 --> 00:11:24,670
and so we might have a model like a some

314
00:11:24,670 --> 00:11:26,380
sort of generative deep neural network

315
00:11:26,380 --> 00:11:28,149
with a lot of uninterpretable parameters

316
00:11:28,149 --> 00:11:30,160
but we might be interested actually in

317
00:11:30,160 --> 00:11:33,160
some sort of function of the parameters

318
00:11:33,160 --> 00:11:35,560
that might be for example the mean

319
00:11:35,560 --> 00:11:38,560
output or some sort of percentile of an

320
00:11:38,560 --> 00:11:40,390
output given an input something like

321
00:11:40,390 --> 00:11:42,339
that okay and so that might be our

322
00:11:42,339 --> 00:11:44,589
functional of interest parameters might

323
00:11:44,589 --> 00:11:45,940
not be interpreted but the functional

324
00:11:45,940 --> 00:11:48,160
might be interpreted okay and we might

325
00:11:48,160 --> 00:11:49,570
also be interested in predictive

326
00:11:49,570 --> 00:11:51,339
distributions even marginalizing out the

327
00:11:51,339 --> 00:11:53,440
parameters as a sort of nuisance we

328
00:11:53,440 --> 00:11:54,970
would like to get the predictive

329
00:11:54,970 --> 00:11:56,920
distribution for a new observation given

330
00:11:56,920 --> 00:11:59,740
features on that new observation using

331
00:11:59,740 --> 00:12:01,480
all the information we have so far okay

332
00:12:01,480 --> 00:12:03,010
and we can do all that sort of thing

333
00:12:03,010 --> 00:12:05,890
using this kind of Bayesian calculus in

334
00:12:05,890 --> 00:12:08,770
some sense okay so um so what's the

335
00:12:08,770 --> 00:12:13,180
problem well I'm you know if you look at

336
00:12:13,180 --> 00:12:14,980
this equation often the theta is

337
00:12:14,980 --> 00:12:17,079
moderate to high dimensional and if you

338
00:12:17,079 --> 00:12:18,670
have an interesting model not some

339
00:12:18,670 --> 00:12:20,620
really simple thing then then this

340
00:12:20,620 --> 00:12:22,450
integral down here is gonna be some

341
00:12:22,450 --> 00:12:25,440
nasty beast and we're not very good at

342
00:12:25,440 --> 00:12:27,339
approximating high dimensional integrals

343
00:12:27,339 --> 00:12:28,690
there's been a lot of literature on that

344
00:12:28,690 --> 00:12:31,060
many many different methods are proposed

345
00:12:31,060 --> 00:12:33,399
to kind of approximate this so-called

346
00:12:33,399 --> 00:12:35,589
marginal likelihood in the denominator

347
00:12:35,589 --> 00:12:37,870
but many of these methods aren't very

348
00:12:37,870 --> 00:12:40,589
good okay and so we run into problems

349
00:12:40,589 --> 00:12:42,000
computationally and kind of

350
00:12:42,000 --> 00:12:44,020
approximating this integral and hence

351
00:12:44,020 --> 00:12:46,000
approximating the posterior because

352
00:12:46,000 --> 00:12:47,529
often we have the prior and the

353
00:12:47,529 --> 00:12:49,450
likelihood sometimes we don't and so

354
00:12:49,450 --> 00:12:51,550
called doubly intractable problems but

355
00:12:51,550 --> 00:12:52,600
often we have the prior in the

356
00:12:52,600 --> 00:12:53,950
likelihood but we can't we can't

357
00:12:53,950 --> 00:12:57,310
evaluate this this integral okay okay so

358
00:12:57,310 --> 00:12:59,200
then the question is well in interesting

359
00:12:59,200 --> 00:13:01,180
models the posterior is not available

360
00:13:01,180 --> 00:13:04,240
analytically what the heck do we do okay

361
00:13:04,240 --> 00:13:08,200
well one thing we could do well if we

362
00:13:08,200 --> 00:13:09,880
had a simple model that simple models

363
00:13:09,880 --> 00:13:11,079
they're often called conjugate

364
00:13:11,079 --> 00:13:12,910
well--what's conjugates so if the model

365
00:13:12,910 --> 00:13:14,680
is conjugate then we can write

366
00:13:14,680 --> 00:13:16,240
everything down in a really simple form

367
00:13:16,240 --> 00:13:17,529
so that means that the prior

368
00:13:17,529 --> 00:13:20,500
distribution PI of theta and the like

369
00:13:20,500 --> 00:13:22,570
the the posterior distribution

370
00:13:22,570 --> 00:13:24,880
pi of theta given why are in the same

371
00:13:24,880 --> 00:13:26,500
form and so maybe we start out with a

372
00:13:26,500 --> 00:13:28,960
Gaussian prior and we update with a

373
00:13:28,960 --> 00:13:31,060
linear model Gaussian linear model

374
00:13:31,060 --> 00:13:33,370
likelihood and now the posterior is

375
00:13:33,370 --> 00:13:36,490
still Gaussian okay or it's still a t or

376
00:13:36,490 --> 00:13:37,660
normal inverse gamma distribution

377
00:13:37,660 --> 00:13:39,670
there's some sort of exponential family

378
00:13:39,670 --> 00:13:42,250
form okay and and so if we have that

379
00:13:42,250 --> 00:13:45,040
type of conjugacy then life is good we

380
00:13:45,040 --> 00:13:46,510
can write down the posterior

381
00:13:46,510 --> 00:13:48,370
analytically and we can do a lot of

382
00:13:48,370 --> 00:13:51,040
things okay and so that that's that's

383
00:13:51,040 --> 00:13:53,080
often true even and somewhat interesting

384
00:13:53,080 --> 00:13:55,080
models if you write down a linear model

385
00:13:55,080 --> 00:13:57,730
where you can have a lot of flexibility

386
00:13:57,730 --> 00:13:59,740
through some type of basis expansion you

387
00:13:59,740 --> 00:14:03,220
can still get conjugacy okay

388
00:14:03,220 --> 00:14:05,380
so in a more complicated settings what

389
00:14:05,380 --> 00:14:06,820
we could do is we could potentially

390
00:14:06,820 --> 00:14:08,770
approximate the posterior using some

391
00:14:08,770 --> 00:14:11,350
tractable class of distributions okay

392
00:14:11,350 --> 00:14:15,580
and one one one popular class which it's

393
00:14:15,580 --> 00:14:18,520
interesting to me I think that you know

394
00:14:18,520 --> 00:14:20,440
machine learning often people will focus

395
00:14:20,440 --> 00:14:22,870
on variational Bayes approximations but

396
00:14:22,870 --> 00:14:25,030
there are actually these kind of

397
00:14:25,030 --> 00:14:26,500
classical approximations that might

398
00:14:26,500 --> 00:14:28,360
might even often do better than

399
00:14:28,360 --> 00:14:29,740
variational Bayes approximations

400
00:14:29,740 --> 00:14:31,750
particularly in terms of uncertainty

401
00:14:31,750 --> 00:14:33,520
quantification and one is this just sort

402
00:14:33,520 --> 00:14:36,070
of good old large sample Gaussian

403
00:14:36,070 --> 00:14:40,570
approximation okay what happens is if

404
00:14:40,570 --> 00:14:42,040
you have under some regularity

405
00:14:42,040 --> 00:14:44,050
conditions that I'll sketch sketch real

406
00:14:44,050 --> 00:14:46,630
quickly if you have a big enough sample

407
00:14:46,630 --> 00:14:48,910
size you know and so that's what we're

408
00:14:48,910 --> 00:14:50,710
worried about we have a huge sample size

409
00:14:50,710 --> 00:14:53,200
how do we do computation well often if

410
00:14:53,200 --> 00:14:55,630
the models not that complicated then the

411
00:14:55,630 --> 00:14:57,100
posterior is gonna be approximately

412
00:14:57,100 --> 00:14:59,020
Gaussian anyway by via what's called the

413
00:14:59,020 --> 00:15:00,970
Bayesian central limit theorem often

414
00:15:00,970 --> 00:15:03,070
known as Bernstein von Mises theorem or

415
00:15:03,070 --> 00:15:05,890
BVM theorems are well as the sample size

416
00:15:05,890 --> 00:15:08,340
get big we get in approximately a

417
00:15:08,340 --> 00:15:11,710
Gaussian elimination and we can easily

418
00:15:11,710 --> 00:15:13,420
calculate the mean and the covariance of

419
00:15:13,420 --> 00:15:16,720
the Gaussian okay and so this is relying

420
00:15:16,720 --> 00:15:18,640
on the sample size n being large

421
00:15:18,640 --> 00:15:20,560
relative to the number of parameters P

422
00:15:20,560 --> 00:15:22,930
um the likely it has to be smooth and

423
00:15:22,930 --> 00:15:24,760
differentiable particularly around some

424
00:15:24,760 --> 00:15:26,620
sort of true value theta not in the

425
00:15:26,620 --> 00:15:28,570
interior of the parameter space and that

426
00:15:28,570 --> 00:15:30,160
really doesn't need to be the true value

427
00:15:30,160 --> 00:15:32,320
it needs to be essentially it's going to

428
00:15:32,320 --> 00:15:34,150
be concentrating around a theta naught

429
00:15:34,150 --> 00:15:36,170
which is the best value

430
00:15:36,170 --> 00:15:38,810
of theta in the parametric class under

431
00:15:38,810 --> 00:15:40,340
consideration and so if we had some

432
00:15:40,340 --> 00:15:41,510
likelihood function that we're

433
00:15:41,510 --> 00:15:44,240
parameterizing L of Y given theta it

434
00:15:44,240 --> 00:15:45,830
might be that the truth is outside of

435
00:15:45,830 --> 00:15:47,630
that class but there's some minimal KL

436
00:15:47,630 --> 00:15:50,690
point outside of the class I mean to the

437
00:15:50,690 --> 00:15:52,400
true data generate model and then that

438
00:15:52,400 --> 00:15:55,310
theta not within the class that's at the

439
00:15:55,310 --> 00:15:56,870
minimal Cal point that's where the

440
00:15:56,870 --> 00:15:58,580
posterior is going to concentrate okay

441
00:15:58,580 --> 00:16:00,740
and it's gonna be eventually Gaussian

442
00:16:00,740 --> 00:16:02,180
under some of these types of conditions

443
00:16:02,180 --> 00:16:03,310
okay

444
00:16:03,310 --> 00:16:06,110
I'd say like I would I would say in most

445
00:16:06,110 --> 00:16:08,060
of the literature that I've looked at on

446
00:16:08,060 --> 00:16:10,370
scalable Bayesian inference it's

447
00:16:10,370 --> 00:16:12,320
probably on all of these conditions

448
00:16:12,320 --> 00:16:14,030
essentially hold and we might have a

449
00:16:14,030 --> 00:16:16,550
large sample Gaussian where we did some

450
00:16:16,550 --> 00:16:18,590
sort of you know divide and conquer

451
00:16:18,590 --> 00:16:20,600
matrix tricks to calculate the mean and

452
00:16:20,600 --> 00:16:22,850
covariance might actually be competitive

453
00:16:22,850 --> 00:16:25,540
or better than many of the methods okay

454
00:16:25,540 --> 00:16:28,100
another class of approximations which is

455
00:16:28,100 --> 00:16:31,280
really closely related to the to the

456
00:16:31,280 --> 00:16:33,620
large sample Gaussian approximation is

457
00:16:33,620 --> 00:16:35,450
the Laplace approximation and so we

458
00:16:35,450 --> 00:16:38,090
could take this posterior back here and

459
00:16:38,090 --> 00:16:39,950
then we'd say oh well here's a hard

460
00:16:39,950 --> 00:16:42,440
integral that we can't calculate really

461
00:16:42,440 --> 00:16:44,360
easily and so we could use Laplace's

462
00:16:44,360 --> 00:16:46,190
method to approximate that integral and

463
00:16:46,190 --> 00:16:48,260
we end up with essentially a large

464
00:16:48,260 --> 00:16:50,330
sample Gaussian type approximation to

465
00:16:50,330 --> 00:16:52,100
the posterior and that's called the

466
00:16:52,100 --> 00:16:54,200
Laplace of proximation and there's a

467
00:16:54,200 --> 00:16:55,700
really big literature on laplace

468
00:16:55,700 --> 00:16:58,310
approximations often they do remarkably

469
00:16:58,310 --> 00:17:00,050
well they do a good job it's sort of

470
00:17:00,050 --> 00:17:01,610
characterizing the first and second

471
00:17:01,610 --> 00:17:03,380
moment of the posterior whereas many

472
00:17:03,380 --> 00:17:05,660
variational methods might butcher the

473
00:17:05,660 --> 00:17:08,030
the second moment you know they're not

474
00:17:08,030 --> 00:17:09,500
going to be do a good job at all if the

475
00:17:09,500 --> 00:17:11,449
posterior is like multimodal or skewed

476
00:17:11,449 --> 00:17:14,240
or weird but you know large samples if

477
00:17:14,240 --> 00:17:15,890
we have a you know not too complicated

478
00:17:15,890 --> 00:17:17,869
model then then this kind of good old

479
00:17:17,869 --> 00:17:19,640
Laplace approximation which has been

480
00:17:19,640 --> 00:17:22,040
around for a really long time can do can

481
00:17:22,040 --> 00:17:23,810
do remarkably well and I would encourage

482
00:17:23,810 --> 00:17:25,730
people working in scalable Bayes

483
00:17:25,730 --> 00:17:27,709
inference to compare to the Laplace

484
00:17:27,709 --> 00:17:29,660
approximation which often they don't and

485
00:17:29,660 --> 00:17:31,610
probably I could define a scalable

486
00:17:31,610 --> 00:17:33,260
Laplace approximation that beats your

487
00:17:33,260 --> 00:17:35,140
variational proximation

488
00:17:35,140 --> 00:17:38,390
okay so that's the kind of really good

489
00:17:38,390 --> 00:17:40,910
classic kind of old-school Bayesian

490
00:17:40,910 --> 00:17:42,710
approximations and large sample sizes

491
00:17:42,710 --> 00:17:44,630
okay and we could certainly define

492
00:17:44,630 --> 00:17:47,790
scalable versions of them

493
00:17:47,790 --> 00:17:50,440
so what could we do instead well it's an

494
00:17:50,440 --> 00:17:53,040
alternative to these kind of really old

495
00:17:53,040 --> 00:17:55,360
approximations we could we could think

496
00:17:55,360 --> 00:17:57,460
of defining some type of approximating

497
00:17:57,460 --> 00:17:59,890
class q of theta so our posterior

498
00:17:59,890 --> 00:18:01,990
distribution apply is PI of theta given

499
00:18:01,990 --> 00:18:05,620
Y and we we that's intractable and so

500
00:18:05,620 --> 00:18:08,380
we'd like to get close close to it so

501
00:18:08,380 --> 00:18:09,760
let's say well let's approximate it with

502
00:18:09,760 --> 00:18:12,550
some some some you know tractable class

503
00:18:12,550 --> 00:18:15,460
q of theta and then q of theta might be

504
00:18:15,460 --> 00:18:17,500
something like a product of exponential

505
00:18:17,500 --> 00:18:19,210
family distributions parameterize by

506
00:18:19,210 --> 00:18:22,210
some you know working parameters i which

507
00:18:22,210 --> 00:18:23,710
is just an algorithmic parameter

508
00:18:23,710 --> 00:18:25,030
controlling the accuracy of the

509
00:18:25,030 --> 00:18:27,610
approximation and then we could think of

510
00:18:27,610 --> 00:18:29,320
to define some type of discrepancy

511
00:18:29,320 --> 00:18:32,620
between Q and and pi the the target

512
00:18:32,620 --> 00:18:35,410
posterior distribution and and if we can

513
00:18:35,410 --> 00:18:36,790
then define some type of optimization

514
00:18:36,790 --> 00:18:39,760
problem to minimize this discrepancy the

515
00:18:39,760 --> 00:18:42,070
resulting q hat might give us a decent

516
00:18:42,070 --> 00:18:44,530
approximation and in variational Bayes

517
00:18:44,530 --> 00:18:47,200
is one if one flavor of this but but

518
00:18:47,200 --> 00:18:49,630
also expectation propagation and related

519
00:18:49,630 --> 00:18:50,800
methods there's been a really

520
00:18:50,800 --> 00:18:52,260
interesting recent literature

521
00:18:52,260 --> 00:18:55,750
generalizing variational Bayes to to use

522
00:18:55,750 --> 00:18:58,330
Bregman divergences of alpha divergences

523
00:18:58,330 --> 00:19:00,640
instead of coal-black lobular and I'm

524
00:19:00,640 --> 00:19:01,870
not going to talk about that stuff very

525
00:19:01,870 --> 00:19:03,610
much today but it's a quite interesting

526
00:19:03,610 --> 00:19:06,520
area in terms of variational days I

527
00:19:06,520 --> 00:19:08,770
would I would encourage you to look at

528
00:19:08,770 --> 00:19:12,510
tamra Broderick's really nice ICML 2018

529
00:19:12,510 --> 00:19:14,730
tutorial where she gives us sort of

530
00:19:14,730 --> 00:19:16,990
state-of-the-art overview on on

531
00:19:16,990 --> 00:19:19,780
variational methods okay and so the

532
00:19:19,780 --> 00:19:21,160
variational methods are based on

533
00:19:21,160 --> 00:19:22,990
maximizing a lower bound discarding an

534
00:19:22,990 --> 00:19:24,610
intractable term in the coal-black libor

535
00:19:24,610 --> 00:19:27,250
divergence the reason that I'm not going

536
00:19:27,250 --> 00:19:29,020
to talk about variational Bayes methods

537
00:19:29,020 --> 00:19:31,480
today is that to me the whole the whole

538
00:19:31,480 --> 00:19:34,090
beauty and and you know reason that I'm

539
00:19:34,090 --> 00:19:35,980
interested in this Bayesian formulation

540
00:19:35,980 --> 00:19:37,590
is I'd like to do a really good job

541
00:19:37,590 --> 00:19:40,120
characterizing uncertainty okay

542
00:19:40,120 --> 00:19:41,890
and if I have a variational Bayes

543
00:19:41,890 --> 00:19:43,450
approximation I've thrown out an

544
00:19:43,450 --> 00:19:45,940
intractable term and then I do all this

545
00:19:45,940 --> 00:19:47,830
then then I don't know what I have I

546
00:19:47,830 --> 00:19:50,200
have some sort of approximation but it's

547
00:19:50,200 --> 00:19:52,870
to me I'm I'm you know a mathematical

548
00:19:52,870 --> 00:19:55,540
statistician partly and I'd like to know

549
00:19:55,540 --> 00:19:57,760
if I if I'm calling an approximation I

550
00:19:57,760 --> 00:19:59,470
have to have some sort of guarantees

551
00:19:59,470 --> 00:20:01,130
that is actually approximating

552
00:20:01,130 --> 00:20:02,570
otherwise it's like just something else

553
00:20:02,570 --> 00:20:05,330
so I have something else which is a Q of

554
00:20:05,330 --> 00:20:07,790
theta which is in no sense really an

555
00:20:07,790 --> 00:20:09,170
approximation to a posterior

556
00:20:09,170 --> 00:20:11,210
distribution and is really no sense

557
00:20:11,210 --> 00:20:12,770
Bayesian it might be useful from a

558
00:20:12,770 --> 00:20:14,720
machine learning perspective but in most

559
00:20:14,720 --> 00:20:16,670
settings I've actually no clue at all

560
00:20:16,670 --> 00:20:17,930
how well it's doing

561
00:20:17,930 --> 00:20:20,840
I could I could see but 99% of the time

562
00:20:20,840 --> 00:20:22,430
when people use it they don't even look

563
00:20:22,430 --> 00:20:23,690
at how well it's doing in terms of

564
00:20:23,690 --> 00:20:25,670
uncertainty quantification so I become

565
00:20:25,670 --> 00:20:26,870
quite skeptical of these kind of

566
00:20:26,870 --> 00:20:28,730
variational Bayes bay based AI methods

567
00:20:28,730 --> 00:20:30,440
for those reasons because we don't know

568
00:20:30,440 --> 00:20:35,150
that they're accurate at all okay there

569
00:20:35,150 --> 00:20:37,370
is some not really nice literature Mike

570
00:20:37,370 --> 00:20:40,370
Jordan and Tamra Broderick and Giordano

571
00:20:40,370 --> 00:20:42,350
have this nice approach I really like

572
00:20:42,350 --> 00:20:44,960
which kind of fixes up the uncertainty

573
00:20:44,960 --> 00:20:46,490
to a kind of using kind of linear

574
00:20:46,490 --> 00:20:48,830
approximation locally so you can think

575
00:20:48,830 --> 00:20:50,630
of it it's almost like taking

576
00:20:50,630 --> 00:20:53,060
variational Bayes and then taking the

577
00:20:53,060 --> 00:20:54,770
Laplace approximation and losing a

578
00:20:54,770 --> 00:20:56,510
Laplace approximation to kind of fix up

579
00:20:56,510 --> 00:20:58,550
variational base it's not exactly that

580
00:20:58,550 --> 00:21:00,410
but that's the kind of flavor of what

581
00:21:00,410 --> 00:21:03,590
what you can do so these types of fix

582
00:21:03,590 --> 00:21:04,820
ups can improve the variance

583
00:21:04,820 --> 00:21:08,120
characterization in a local mode but you

584
00:21:08,120 --> 00:21:10,490
don't know other than that I'd also like

585
00:21:10,490 --> 00:21:13,190
to highlight a recent article maybe I'm

586
00:21:13,190 --> 00:21:14,960
biased towards this article since it's

587
00:21:14,960 --> 00:21:17,360
from three of my former students deputy

588
00:21:17,360 --> 00:21:18,920
patty on our blonde Bhattacharyya and

589
00:21:18,920 --> 00:21:21,170
yin yang have this really nice article

590
00:21:21,170 --> 00:21:23,840
showing really strong theoretical

591
00:21:23,840 --> 00:21:26,240
guarantees on statistical optimality of

592
00:21:26,240 --> 00:21:28,010
variational Bayes but they're doing it

593
00:21:28,010 --> 00:21:30,860
from a point estimation perspective and

594
00:21:30,860 --> 00:21:33,110
so they can show that a variational

595
00:21:33,110 --> 00:21:35,060
posterior will actually concentrate in a

596
00:21:35,060 --> 00:21:37,240
very optimal way giving you a wonderful

597
00:21:37,240 --> 00:21:40,370
point estimate but not showing that the

598
00:21:40,370 --> 00:21:43,130
variance is right actually they find

599
00:21:43,130 --> 00:21:44,600
that the variance is often very wrong

600
00:21:44,600 --> 00:21:47,420
okay okay so there's no theory on

601
00:21:47,420 --> 00:21:50,330
accuracy of UQ and so for this reason

602
00:21:50,330 --> 00:21:52,340
I'm gonna spend most of the talk really

603
00:21:52,340 --> 00:21:54,800
on on Markov chain Monte Carlo methods

604
00:21:54,800 --> 00:21:56,570
and it's been interesting to me that

605
00:21:56,570 --> 00:21:59,750
Markov chain Monte Carlo is this sort of

606
00:21:59,750 --> 00:22:01,760
that now and kind of old-school method

607
00:22:01,760 --> 00:22:03,920
for doing Bayesian computation and I

608
00:22:03,920 --> 00:22:05,840
often find I hear a lot of talks where

609
00:22:05,840 --> 00:22:07,610
people will just discount it entirely

610
00:22:07,610 --> 00:22:09,080
they'll say well I'm simply is not

611
00:22:09,080 --> 00:22:10,750
scalable so I'm going to use this

612
00:22:10,750 --> 00:22:12,620
variational approximation that I know

613
00:22:12,620 --> 00:22:14,270
nothing about but at least I can compute

614
00:22:14,270 --> 00:22:14,870
it

615
00:22:14,870 --> 00:22:17,059
but actually there's now a really really

616
00:22:17,059 --> 00:22:19,370
rich and beautiful literature emerging

617
00:22:19,370 --> 00:22:21,620
on how we can scale up Markov chain

618
00:22:21,620 --> 00:22:23,720
Monte Carlo and I'd say it's just not

619
00:22:23,720 --> 00:22:26,059
the case anymore you you should not be

620
00:22:26,059 --> 00:22:27,380
making the statement the Markov chain

621
00:22:27,380 --> 00:22:29,180
Monte Carlo is not scalable

622
00:22:29,180 --> 00:22:31,100
it's like saying anything else is not

623
00:22:31,100 --> 00:22:33,140
scalable oh well the EM algorithm is not

624
00:22:33,140 --> 00:22:34,610
scalable well that's because you're not

625
00:22:34,610 --> 00:22:37,130
using a scalable version okay so if you

626
00:22:37,130 --> 00:22:38,900
just use the naive version of Markov

627
00:22:38,900 --> 00:22:40,430
chain Monte Carlo yeah it can be really

628
00:22:40,430 --> 00:22:42,559
bloody slow but now we have a lot of

629
00:22:42,559 --> 00:22:45,110
tricks for scaling up Markov chain Monte

630
00:22:45,110 --> 00:22:47,120
Carlo and there's an emerging beautiful

631
00:22:47,120 --> 00:22:49,400
theoretical and impractical literature

632
00:22:49,400 --> 00:22:51,320
that that I hope I can inspire some of

633
00:22:51,320 --> 00:22:54,110
you to to add to in the in the coming

634
00:22:54,110 --> 00:22:54,860
years okay

635
00:22:54,860 --> 00:22:57,559
and so um the one reason that Markov

636
00:22:57,559 --> 00:22:58,970
chain Monte Carlo is kind of remained

637
00:22:58,970 --> 00:23:01,460
popular is that these analytic

638
00:23:01,460 --> 00:23:04,280
approximations like variational Bayes we

639
00:23:04,280 --> 00:23:05,840
can't really show that they do well and

640
00:23:05,840 --> 00:23:07,580
they they might don't not do well after

641
00:23:07,580 --> 00:23:10,030
out of narrow outsider narrow settings

642
00:23:10,030 --> 00:23:12,590
so Markov chain Monte Carlo and other

643
00:23:12,590 --> 00:23:14,090
posterior sampling algorithms provide

644
00:23:14,090 --> 00:23:17,300
provide an alternative okay and so I'm

645
00:23:17,300 --> 00:23:19,820
so MCMC what the game is and the kind of

646
00:23:19,820 --> 00:23:21,800
beauty behind it in some sense is that

647
00:23:21,800 --> 00:23:25,760
we can take we can take sampling we have

648
00:23:25,760 --> 00:23:28,100
a sampling algorithm and we set up a

649
00:23:28,100 --> 00:23:29,900
remarkably simple sampling algorithm

650
00:23:29,900 --> 00:23:31,940
where the the the stationary

651
00:23:31,940 --> 00:23:33,920
distribution of the samples is the true

652
00:23:33,920 --> 00:23:36,050
posterior distribution exactly often

653
00:23:36,050 --> 00:23:37,309
though we might put in approximations

654
00:23:37,309 --> 00:23:40,040
and so we can we want to the goal is to

655
00:23:40,040 --> 00:23:42,679
draw samples not get an analytic

656
00:23:42,679 --> 00:23:44,630
approximation but draw samples of PI of

657
00:23:44,630 --> 00:23:46,280
theta given Y the posterior distribution

658
00:23:46,280 --> 00:23:48,740
and the one reason this is really nice

659
00:23:48,740 --> 00:23:51,290
is that let's say we have a complicated

660
00:23:51,290 --> 00:23:53,179
analytic approximation of pi of theta

661
00:23:53,179 --> 00:23:55,490
given Y well that's really actually not

662
00:23:55,490 --> 00:23:58,130
that useful to me because you know theta

663
00:23:58,130 --> 00:24:00,650
might be a thousand dimensional I can't

664
00:24:00,650 --> 00:24:02,480
visualize it I can't do anything with it

665
00:24:02,480 --> 00:24:04,640
and so really I'm not ever usually

666
00:24:04,640 --> 00:24:05,900
interested in the full posterior

667
00:24:05,900 --> 00:24:08,120
distribution I'm interested in some

668
00:24:08,120 --> 00:24:09,530
functionals I want a one a one

669
00:24:09,530 --> 00:24:11,179
dimensional functional for example I

670
00:24:11,179 --> 00:24:12,740
might want the predictive distribution

671
00:24:12,740 --> 00:24:16,429
of Y for a new patient coming in I'm

672
00:24:16,429 --> 00:24:17,960
given their features or something like

673
00:24:17,960 --> 00:24:19,940
that I'd like that you know where I'd

674
00:24:19,940 --> 00:24:21,679
like I'd like to know something about

675
00:24:21,679 --> 00:24:23,929
particular functionals of the parameters

676
00:24:23,929 --> 00:24:25,910
that are interpretable okay but if I

677
00:24:25,910 --> 00:24:27,620
have an analytic approximation I can't

678
00:24:27,620 --> 00:24:28,519
get that at all

679
00:24:28,519 --> 00:24:30,799
but if I had samples from this then I

680
00:24:30,799 --> 00:24:32,419
would have wonderful that I could do a

681
00:24:32,419 --> 00:24:34,190
lot I could just draw a bunch of samples

682
00:24:34,190 --> 00:24:36,559
I could apply the functional to each

683
00:24:36,559 --> 00:24:38,719
sample and then I can calculate any

684
00:24:38,719 --> 00:24:40,219
posterior summary I want I can calculate

685
00:24:40,219 --> 00:24:42,830
the mean I can calculate an interval I

686
00:24:42,830 --> 00:24:45,109
can calculate a kernel smooth density

687
00:24:45,109 --> 00:24:46,669
estimate for that for that one

688
00:24:46,669 --> 00:24:47,869
dimensional functional that I'm

689
00:24:47,869 --> 00:24:49,489
interested in okay and that's usually

690
00:24:49,489 --> 00:24:51,379
how people do things in practice and so

691
00:24:51,379 --> 00:24:53,659
so samples I don't see that there's a

692
00:24:53,659 --> 00:24:55,299
really good substitute for samples

693
00:24:55,299 --> 00:24:57,799
samples are not inherently slow or

694
00:24:57,799 --> 00:24:59,929
unscalable and they give us an enormous

695
00:24:59,929 --> 00:25:01,820
amount of flexibility and type in terms

696
00:25:01,820 --> 00:25:03,229
of the types of inferences that we can

697
00:25:03,229 --> 00:25:09,200
do okay okay so um one of the beautiful

698
00:25:09,200 --> 00:25:11,239
things about Markov chain Monte Carlo is

699
00:25:11,239 --> 00:25:13,219
that bypasses the need to approximate

700
00:25:13,219 --> 00:25:14,749
the marginal likelihood so that's this

701
00:25:14,749 --> 00:25:16,820
dude in the denominator down here he's a

702
00:25:16,820 --> 00:25:19,309
horrible creature and so there's all

703
00:25:19,309 --> 00:25:20,929
these papers on approximating the

704
00:25:20,929 --> 00:25:22,969
marginal likelihood and basically all

705
00:25:22,969 --> 00:25:25,639
the algorithms are not very good and and

706
00:25:25,639 --> 00:25:27,979
and I'd like to just avoid approximating

707
00:25:27,979 --> 00:25:30,289
this entirely and MCMC cleverly does

708
00:25:30,289 --> 00:25:32,089
that so we can only we only need to

709
00:25:32,089 --> 00:25:34,609
evaluate the the numerator and we the

710
00:25:34,609 --> 00:25:39,289
denominator cancels okay yeah and so

711
00:25:39,289 --> 00:25:40,789
this is just the point that the samples

712
00:25:40,789 --> 00:25:42,469
are often more useful than an analytic

713
00:25:42,469 --> 00:25:48,589
form anyway okay okay so MCMC what are

714
00:25:48,589 --> 00:25:50,570
we doing so we gonna get MC MC based

715
00:25:50,570 --> 00:25:51,829
summaries of the posterior for any

716
00:25:51,829 --> 00:25:54,200
functional F of theta as the number of

717
00:25:54,200 --> 00:25:56,149
samples increases these summaries become

718
00:25:56,149 --> 00:25:57,950
more accurate okay and so they're

719
00:25:57,950 --> 00:25:59,209
converging to the true posterior

720
00:25:59,209 --> 00:26:03,200
summaries so as sad as MC MC work so MC

721
00:26:03,200 --> 00:26:05,419
MC constructs a Markov chain with a

722
00:26:05,419 --> 00:26:07,159
stationary distribution that's a true

723
00:26:07,159 --> 00:26:09,919
posterior distribution and and to

724
00:26:09,919 --> 00:26:11,239
construct the Markov chain we need

725
00:26:11,239 --> 00:26:12,859
what's called a transition kernel and

726
00:26:12,859 --> 00:26:14,570
that transition kernel needs to follow

727
00:26:14,570 --> 00:26:16,729
some types of rules and one of the

728
00:26:16,729 --> 00:26:19,099
classes of rules are most of them are

729
00:26:19,099 --> 00:26:20,809
within what's called a metropolis

730
00:26:20,809 --> 00:26:23,419
Hastings algorithm and the metropolis

731
00:26:23,419 --> 00:26:25,039
Hastings algorithm to just give a bit of

732
00:26:25,039 --> 00:26:27,469
theory certainly one of the most

733
00:26:27,469 --> 00:26:29,749
beautiful algorithms ever devised in my

734
00:26:29,749 --> 00:26:31,219
view I'm certainly in the in the last

735
00:26:31,219 --> 00:26:33,440
hundred years and and the metropolis

736
00:26:33,440 --> 00:26:35,179
Hastings algorithm was originally

737
00:26:35,179 --> 00:26:37,879
published in the paper by metropolis a

738
00:26:37,879 --> 00:26:39,619
tall and in the 1950s

739
00:26:39,619 --> 00:26:41,710
of people working on the on the ball

740
00:26:41,710 --> 00:26:43,150
and then there was this beautiful paper

741
00:26:43,150 --> 00:26:47,020
by Hastings in 1970 which which kind of

742
00:26:47,020 --> 00:26:48,790
took it and made it much broader and

743
00:26:48,790 --> 00:26:50,530
that's the kind of give us giving us the

744
00:26:50,530 --> 00:26:51,730
kind of modern class a metropolis

745
00:26:51,730 --> 00:26:53,650
Hastings algorithms and and they're

746
00:26:53,650 --> 00:26:54,460
gonna have they're gonna have a

747
00:26:54,460 --> 00:26:56,590
celebration of the metropolis Hastings

748
00:26:56,590 --> 00:26:58,150
algorithm which was published in

749
00:26:58,150 --> 00:27:01,720
biometrika in in 1970 the 50th year

750
00:27:01,720 --> 00:27:03,610
anniversary I'm gonna be writing a kind

751
00:27:03,610 --> 00:27:05,440
of special paper I'm highlighting that

752
00:27:05,440 --> 00:27:07,750
and biometrika which should appear next

753
00:27:07,750 --> 00:27:09,700
year and so what the metropolis Hastings

754
00:27:09,700 --> 00:27:12,190
algorithm does so it's quite simple and

755
00:27:12,190 --> 00:27:14,170
broad and so what we do is we would say

756
00:27:14,170 --> 00:27:17,140
let's draw a candidate so let's say

757
00:27:17,140 --> 00:27:20,410
theta star is drawn from some sort of G

758
00:27:20,410 --> 00:27:26,560
of theta t minus one and so the t minus

759
00:27:26,560 --> 00:27:29,560
one is denoting the the t is indexing

760
00:27:29,560 --> 00:27:31,570
the the MCMC iterate and so we're

761
00:27:31,570 --> 00:27:33,730
running an iterative algorithm t minus

762
00:27:33,730 --> 00:27:35,770
one is the theta t minus one is the last

763
00:27:35,770 --> 00:27:39,070
iteration okay and theta T is the sample

764
00:27:39,070 --> 00:27:42,040
of step T so we draw a candidate for

765
00:27:42,040 --> 00:27:44,740
theta T we call theta star from some

766
00:27:44,740 --> 00:27:46,840
candidate generating density G which

767
00:27:46,840 --> 00:27:49,390
might depend on the previous value of

768
00:27:49,390 --> 00:27:51,040
the parameter and on the on the data

769
00:27:51,040 --> 00:27:51,760
okay

770
00:27:51,760 --> 00:27:53,410
but it doesn't depend on farther away

771
00:27:53,410 --> 00:27:55,720
and that's why it's a Markov chain okay

772
00:27:55,720 --> 00:27:57,640
and then we accept the proposal by

773
00:27:57,640 --> 00:27:59,530
letting theta T equal theta star with

774
00:27:59,530 --> 00:28:00,970
this probability which is just the

775
00:28:00,970 --> 00:28:03,580
minimum of one and throwing out this

776
00:28:03,580 --> 00:28:06,190
part just a ratio of the prior times the

777
00:28:06,190 --> 00:28:08,290
likelihood at the candidate value

778
00:28:08,290 --> 00:28:10,120
divided by the prior times the

779
00:28:10,120 --> 00:28:12,670
likelihood at the previous value okay

780
00:28:12,670 --> 00:28:14,080
and then the G just gives you an

781
00:28:14,080 --> 00:28:16,690
adjustment for asymmetry and it's a

782
00:28:16,690 --> 00:28:18,340
metropolis algorithm if there's no G

783
00:28:18,340 --> 00:28:20,230
because we were limited to using a

784
00:28:20,230 --> 00:28:22,720
symmetric G in using metropolis okay

785
00:28:22,720 --> 00:28:24,760
so this is really simple if we have some

786
00:28:24,760 --> 00:28:26,440
G we can always sort of do this as long

787
00:28:26,440 --> 00:28:27,850
as we can evaluate the prior in the

788
00:28:27,850 --> 00:28:32,260
likelihood okay so um so what what's the

789
00:28:32,260 --> 00:28:34,480
game then while the the the if we wanted

790
00:28:34,480 --> 00:28:35,770
to do something efficient and

791
00:28:35,770 --> 00:28:38,050
particularly formally scalable to very

792
00:28:38,050 --> 00:28:40,900
large problem sizes large an and P then

793
00:28:40,900 --> 00:28:42,850
we need to choose the good proposal G

794
00:28:42,850 --> 00:28:46,030
okay so G can depend on the previous

795
00:28:46,030 --> 00:28:47,500
value of theta and on the data but not

796
00:28:47,500 --> 00:28:49,960
on farther back samples except in what's

797
00:28:49,960 --> 00:28:51,940
called a adaptive metropolis Hastings

798
00:28:51,940 --> 00:28:54,400
and so the reason is the Markov chain is

799
00:28:54,400 --> 00:28:55,540
that we're just sampling

800
00:28:55,540 --> 00:28:57,070
based on the previous value of the

801
00:28:57,070 --> 00:28:59,650
sample but not 20 samples ago so it's

802
00:28:59,650 --> 00:29:02,770
like a first-order Markov chain so I'm

803
00:29:02,770 --> 00:29:04,900
special cases the Gibbs sample or what

804
00:29:04,900 --> 00:29:06,670
the gift sampler does is we if we just

805
00:29:06,670 --> 00:29:08,590
say all the parameters are theta it's a

806
00:29:08,590 --> 00:29:10,630
big giant vector parameters we have

807
00:29:10,630 --> 00:29:13,060
theta 1 to theta P then we can draw

808
00:29:13,060 --> 00:29:14,680
subsets of theta from their exact

809
00:29:14,680 --> 00:29:16,090
conditional posterior distributions

810
00:29:16,090 --> 00:29:17,410
fixing the others we have a Gibbs

811
00:29:17,410 --> 00:29:19,690
sampler and so what we might do there is

812
00:29:19,690 --> 00:29:21,430
it might be that while theta wine is

813
00:29:21,430 --> 00:29:23,440
like if we fix all the other parameters

814
00:29:23,440 --> 00:29:24,940
and go through the algebra

815
00:29:24,940 --> 00:29:26,080
well that might just be a Gaussian

816
00:29:26,080 --> 00:29:29,260
distribution theta 2 fixing all the

817
00:29:29,260 --> 00:29:31,450
parameters might be a gamma distribution

818
00:29:31,450 --> 00:29:34,240
at Zeta 3 might be a Poisson

819
00:29:34,240 --> 00:29:35,890
distribution and we might just iterate

820
00:29:35,890 --> 00:29:38,170
through sampling from those conditional

821
00:29:38,170 --> 00:29:39,640
distributions that would be a Gibbs

822
00:29:39,640 --> 00:29:43,360
sampler okay a random that'll be a

823
00:29:43,360 --> 00:29:46,810
special case in which the G cancels with

824
00:29:46,810 --> 00:29:48,400
the other part and we always accept

825
00:29:48,400 --> 00:29:50,710
another special K important special case

826
00:29:50,710 --> 00:29:52,660
is what's called a random walk and so

827
00:29:52,660 --> 00:29:54,370
there way we would start out we're

828
00:29:54,370 --> 00:29:57,250
sitting there at theta t minus 1 we

829
00:29:57,250 --> 00:29:59,500
proposed a proposal around theta t minus

830
00:29:59,500 --> 00:30:00,910
1 just to say from a Gaussian

831
00:30:00,910 --> 00:30:03,340
distribution and then we accept or

832
00:30:03,340 --> 00:30:04,720
reject and that would be a random walk

833
00:30:04,720 --> 00:30:06,490
ok and there's a quite interesting

834
00:30:06,490 --> 00:30:08,830
literature on how much are these types

835
00:30:08,830 --> 00:30:10,600
of random walks scalable to large

836
00:30:10,600 --> 00:30:14,950
dimensional problems ok some more fancy

837
00:30:14,950 --> 00:30:16,360
things that people have been using in

838
00:30:16,360 --> 00:30:18,220
recent years or like Hamiltonian

839
00:30:18,220 --> 00:30:20,380
Montecarlo or launch event algorithms

840
00:30:20,380 --> 00:30:22,330
what they would do is they would try to

841
00:30:22,330 --> 00:30:25,450
try to design a really good G by

842
00:30:25,450 --> 00:30:27,700
exploiting gradient information and so

843
00:30:27,700 --> 00:30:31,390
that we can get a draw from that's a

844
00:30:31,390 --> 00:30:32,800
really good draw but far from the

845
00:30:32,800 --> 00:30:40,150
current draw so what we worry about what

846
00:30:40,150 --> 00:30:43,390
we worry about is is having a G which

847
00:30:43,390 --> 00:30:46,120
gives you a value really close but then

848
00:30:46,120 --> 00:30:47,710
it's accepted with high probability but

849
00:30:47,710 --> 00:30:49,600
then but then it doesn't move far enough

850
00:30:49,600 --> 00:30:51,280
and that can create inefficiency so

851
00:30:51,280 --> 00:30:53,260
these HMC or lines event algorithms are

852
00:30:53,260 --> 00:30:56,560
designed to overcome that ok so let's

853
00:30:56,560 --> 00:30:59,440
get a little bit into the scalability so

854
00:30:59,440 --> 00:31:01,960
what about why why is MCMC thought to be

855
00:31:01,960 --> 00:31:06,760
slow well you know potentially the time

856
00:31:06,760 --> 00:31:08,410
per iteration might increase with the

857
00:31:08,410 --> 00:31:08,960
number Prem

858
00:31:08,960 --> 00:31:11,720
our unknowns and can also increase with

859
00:31:11,720 --> 00:31:13,820
the sample size okay this would also be

860
00:31:13,820 --> 00:31:15,770
true for just about every optimization

861
00:31:15,770 --> 00:31:18,049
algorithm in the world as well but we

862
00:31:18,049 --> 00:31:21,110
can start there okay so um so what

863
00:31:21,110 --> 00:31:22,880
happens is that we have to we have to

864
00:31:22,880 --> 00:31:24,860
sample and so what we're doing is we

865
00:31:24,860 --> 00:31:26,390
have to have the cost of sampling the

866
00:31:26,390 --> 00:31:28,730
proposal and also calculating the

867
00:31:28,730 --> 00:31:31,039
acceptance probability and so if we you

868
00:31:31,039 --> 00:31:32,450
know if we think about just a simple

869
00:31:32,450 --> 00:31:34,490
random walk we're drawing from a

870
00:31:34,490 --> 00:31:36,169
multivariate Gaussian distribution well

871
00:31:36,169 --> 00:31:38,480
that might be not be very expensive but

872
00:31:38,480 --> 00:31:40,580
then we have to accept or reject well

873
00:31:40,580 --> 00:31:42,080
that's a ratio of the prior times the

874
00:31:42,080 --> 00:31:43,429
likelihood so we have to evaluate the

875
00:31:43,429 --> 00:31:45,289
likelihood okay well that might be some

876
00:31:45,289 --> 00:31:46,880
sort of polynomial in the sample size

877
00:31:46,880 --> 00:31:49,490
evaluating the likelihood okay and we

878
00:31:49,490 --> 00:31:50,690
have to do that to calculate the

879
00:31:50,690 --> 00:31:51,980
acceptance probability and so that's

880
00:31:51,980 --> 00:31:54,289
going to be slow potentially or if we

881
00:31:54,289 --> 00:31:55,789
are sampling directly using Gibbs

882
00:31:55,789 --> 00:31:57,529
sampling we might need to also sample

883
00:31:57,529 --> 00:31:59,870
from some sort of big Gaussian with a

884
00:31:59,870 --> 00:32:01,760
complicated covariance which is hard to

885
00:32:01,760 --> 00:32:03,320
invert and that might be some sort of

886
00:32:03,320 --> 00:32:05,620
polynomial in the sample size as well

887
00:32:05,620 --> 00:32:08,240
okay so I would say that you know really

888
00:32:08,240 --> 00:32:10,880
the time per iteration cost is quite

889
00:32:10,880 --> 00:32:14,210
similar to Casa occur in many or most

890
00:32:14,210 --> 00:32:16,490
optimization algorithms and we can do do

891
00:32:16,490 --> 00:32:18,590
similar things in the MCMC context even

892
00:32:18,590 --> 00:32:20,179
though we're not converging to a point

893
00:32:20,179 --> 00:32:22,039
estimate but we're sampling and we're

894
00:32:22,039 --> 00:32:24,760
converging to a stationary distribution

895
00:32:24,760 --> 00:32:27,500
yeah so an example is that the

896
00:32:27,500 --> 00:32:28,909
computational bottleneck might be

897
00:32:28,909 --> 00:32:30,950
attributable to gradient evaluations and

898
00:32:30,950 --> 00:32:33,140
so we need to in running Hamiltonian

899
00:32:33,140 --> 00:32:34,520
Monte Carlo we need to evaluate the

900
00:32:34,520 --> 00:32:35,899
gradient a bunch of times and well

901
00:32:35,899 --> 00:32:38,299
what's the most popular algorithm in

902
00:32:38,299 --> 00:32:40,730
recent years for doing optimization one

903
00:32:40,730 --> 00:32:42,529
of the most popular tricks is to cast a

904
00:32:42,529 --> 00:32:43,940
gradient and so we could do that here as

905
00:32:43,940 --> 00:32:46,850
well okay so um so that's the first type

906
00:32:46,850 --> 00:32:48,890
of computational bottleneck is the time

907
00:32:48,890 --> 00:32:50,539
per iteration okay the time per

908
00:32:50,539 --> 00:32:52,640
iteration cost and so that that's very

909
00:32:52,640 --> 00:32:54,260
similar to the same thing in

910
00:32:54,260 --> 00:32:56,960
optimization algorithms MCMC also has a

911
00:32:56,960 --> 00:32:58,940
potential second bottleneck which which

912
00:32:58,940 --> 00:33:00,830
might create problems it's also similar

913
00:33:00,830 --> 00:33:02,960
to you know things that affect the

914
00:33:02,960 --> 00:33:04,250
convergence rate in optimization

915
00:33:04,250 --> 00:33:06,830
algorithms because MCMC doesn't produce

916
00:33:06,830 --> 00:33:08,360
independent samples from the posterior

917
00:33:08,360 --> 00:33:12,919
distribution so the draws are Auto

918
00:33:12,919 --> 00:33:14,779
correlated what I mean by that is that

919
00:33:14,779 --> 00:33:17,240
the draw at theta T is going to be kind

920
00:33:17,240 --> 00:33:19,370
of correlated with theta T minus 1 and

921
00:33:19,370 --> 00:33:21,320
that's going to create sort of redundant

922
00:33:21,320 --> 00:33:22,880
information and so if

923
00:33:22,880 --> 00:33:24,530
we could get independent draws we might

924
00:33:24,530 --> 00:33:26,000
be getting much more information than we

925
00:33:26,000 --> 00:33:28,040
get out of the MCMC drawers okay and so

926
00:33:28,040 --> 00:33:30,290
that makes it so that we need to draw

927
00:33:30,290 --> 00:33:33,620
more samples than we would like because

928
00:33:33,620 --> 00:33:35,600
of this kind of what autocorrelation or

929
00:33:35,600 --> 00:33:38,660
slow mixing problem so so the slow

930
00:33:38,660 --> 00:33:40,610
mixing Markov chain problem is what

931
00:33:40,610 --> 00:33:41,810
we're really worried about in these

932
00:33:41,810 --> 00:33:43,880
these high dimensional cases is

933
00:33:43,880 --> 00:33:46,340
increasing autocorrelation as the

934
00:33:46,340 --> 00:33:47,960
problem size increases and this can

935
00:33:47,960 --> 00:33:53,390
occur both in N and in P okay so

936
00:33:53,390 --> 00:33:55,460
well-designed MCMC algorithm with a good

937
00:33:55,460 --> 00:33:58,040
proposal should ideally be kind of

938
00:33:58,040 --> 00:34:00,800
scalable in the sense of the the mixing

939
00:34:00,800 --> 00:34:02,480
rate not getting worse and worse in the

940
00:34:02,480 --> 00:34:04,760
problem size you know we don't want the

941
00:34:04,760 --> 00:34:08,300
mixing rate to kind of be exploding and

942
00:34:08,300 --> 00:34:10,760
the because that what that means is

943
00:34:10,760 --> 00:34:12,500
basically the correlation is going up

944
00:34:12,500 --> 00:34:15,290
and the samples and so we get we need to

945
00:34:15,290 --> 00:34:16,879
run we need to run more and more

946
00:34:16,879 --> 00:34:19,699
iterations of the larger and larger the

947
00:34:19,699 --> 00:34:22,129
problem okay and then we also have a

948
00:34:22,129 --> 00:34:24,679
greater per iteration computational cost

949
00:34:24,679 --> 00:34:27,168
and that's where MCMC will grind to a

950
00:34:27,168 --> 00:34:29,480
halt okay and we need to be clever in

951
00:34:29,480 --> 00:34:32,570
improving it yes otherwise the Monte

952
00:34:32,570 --> 00:34:34,219
Carlo Erin post your summaries might be

953
00:34:34,219 --> 00:34:36,290
high and what we like to target in terms

954
00:34:36,290 --> 00:34:38,899
of scalability is let's say we run the

955
00:34:38,899 --> 00:34:40,070
algorithm for some particular

956
00:34:40,070 --> 00:34:42,620
computational clock time or something we

957
00:34:42,620 --> 00:34:44,870
run it for five minutes on a particular

958
00:34:44,870 --> 00:34:47,840
platform then we would like the the

959
00:34:47,840 --> 00:34:49,760
error and the Pope Monte Carlo error and

960
00:34:49,760 --> 00:34:51,770
postera summaries of interest to be as

961
00:34:51,770 --> 00:34:53,510
small as possible okay we'd like to

962
00:34:53,510 --> 00:34:55,489
design the algorithm targeted towards

963
00:34:55,489 --> 00:34:58,250
small Monte Carlo error in a given in

964
00:34:58,250 --> 00:35:01,730
giving computational time okay so often

965
00:35:01,730 --> 00:35:03,260
the mixing gets worse as a problem size

966
00:35:03,260 --> 00:35:06,530
grows data dimension or sample size so

967
00:35:06,530 --> 00:35:08,210
so in some cases we might have a double

968
00:35:08,210 --> 00:35:10,640
bottleneck is a worsening mixing and we

969
00:35:10,640 --> 00:35:12,460
can think of mixing it's similar to

970
00:35:12,460 --> 00:35:15,440
optimization problems with convergence

971
00:35:15,440 --> 00:35:17,810
rate and so our problem is too much got

972
00:35:17,810 --> 00:35:20,390
a correlation it's sort of related to a

973
00:35:20,390 --> 00:35:23,420
problem of slow convergence rate and we

974
00:35:23,420 --> 00:35:25,450
have a worsening time for iteration

975
00:35:25,450 --> 00:35:27,890
another issue that we run into is that

976
00:35:27,890 --> 00:35:31,070
MCMC is inherently a serial algorithm so

977
00:35:31,070 --> 00:35:33,170
not a naive implementation might require

978
00:35:33,170 --> 00:35:35,240
storing and processing all of the data

979
00:35:35,240 --> 00:35:36,180
on one machine

980
00:35:36,180 --> 00:35:38,190
and so if we'd like just take all the

981
00:35:38,190 --> 00:35:39,869
data on one machine and it's just

982
00:35:39,869 --> 00:35:42,150
enormous amounts of crap and memory and

983
00:35:42,150 --> 00:35:43,950
then we're trying to do like this kind

984
00:35:43,950 --> 00:35:45,450
of iterative algorithm and like one

985
00:35:45,450 --> 00:35:47,010
processor then you know everything's

986
00:35:47,010 --> 00:35:48,630
gonna be horrible it's that of course

987
00:35:48,630 --> 00:35:50,779
and so we need to be smarter than that

988
00:35:50,779 --> 00:35:53,309
but it also that it limits the ease at

989
00:35:53,309 --> 00:35:54,869
which divine conquer strategies can be

990
00:35:54,869 --> 00:35:58,289
applied okay so so these are the reasons

991
00:35:58,289 --> 00:36:00,930
that it's commonly dis stated that MCMC

992
00:36:00,930 --> 00:36:04,260
is it's just simply not scalable but but

993
00:36:04,260 --> 00:36:05,460
i would say that each of the above

994
00:36:05,460 --> 00:36:06,809
problems can be addressed there's

995
00:36:06,809 --> 00:36:09,000
already methods in the literature and

996
00:36:09,000 --> 00:36:10,680
there's gonna be a kind of rich emerging

997
00:36:10,680 --> 00:36:13,920
literature in this area i think and i

998
00:36:13,920 --> 00:36:15,450
would say that this is particularly

999
00:36:15,450 --> 00:36:18,180
promising because there's at least an

1000
00:36:18,180 --> 00:36:19,859
order of magnitude if not orders of

1001
00:36:19,859 --> 00:36:22,819
magnitude more people working on

1002
00:36:22,819 --> 00:36:24,690
optimization and trying to develop

1003
00:36:24,690 --> 00:36:26,640
scalable optimization algorithms and

1004
00:36:26,640 --> 00:36:28,799
there are working on scalable sampling

1005
00:36:28,799 --> 00:36:30,270
algorithms there's really not only a

1006
00:36:30,270 --> 00:36:31,529
handful of people working on these

1007
00:36:31,529 --> 00:36:34,140
problems and even given that there's

1008
00:36:34,140 --> 00:36:35,640
already been some really quite quite

1009
00:36:35,640 --> 00:36:37,470
nice developments and i think that

1010
00:36:37,470 --> 00:36:40,980
that's quite promising okay so for an

1011
00:36:40,980 --> 00:36:43,230
MCMC algorithm to be scalable the monte

1012
00:36:43,230 --> 00:36:44,880
carlo and the posterior marie's based on

1013
00:36:44,880 --> 00:36:46,200
running for some time tau should not

1014
00:36:46,200 --> 00:36:49,289
explode with dimensionality some popular

1015
00:36:49,289 --> 00:36:50,640
algorithms have been shown to not be

1016
00:36:50,640 --> 00:36:52,680
scalable why others can be made scalable

1017
00:36:52,680 --> 00:36:54,359
and that's one interesting thing like we

1018
00:36:54,359 --> 00:36:56,849
were working recently you know one of

1019
00:36:56,849 --> 00:36:58,740
the most popular algorithms for bayesian

1020
00:36:58,740 --> 00:37:00,630
computation and categorical data models

1021
00:37:00,630 --> 00:37:02,819
is data augmentation where you will

1022
00:37:02,819 --> 00:37:04,920
throw in a bunch of latent data you'll

1023
00:37:04,920 --> 00:37:06,779
you'll sample a bunch of latent data you

1024
00:37:06,779 --> 00:37:08,940
want to run a Gibb sampler instead of

1025
00:37:08,940 --> 00:37:10,380
having to tune some metropolis Hastings

1026
00:37:10,380 --> 00:37:12,930
algorithm and so so you'll you'll sample

1027
00:37:12,930 --> 00:37:14,700
a bunch of latent data okay and then

1028
00:37:14,700 --> 00:37:16,109
given the latent data then you can

1029
00:37:16,109 --> 00:37:18,809
sample all the Thetas from close form

1030
00:37:18,809 --> 00:37:20,400
simple full conditionals that might be

1031
00:37:20,400 --> 00:37:22,109
normal or something so we impute a bunch

1032
00:37:22,109 --> 00:37:24,630
of late data from truncated normals and

1033
00:37:24,630 --> 00:37:26,460
then we sample a bunch of parameters

1034
00:37:26,460 --> 00:37:29,039
from a Gaussian okay well actually you

1035
00:37:29,039 --> 00:37:30,510
can show that often that's not very

1036
00:37:30,510 --> 00:37:32,579
scalable like in a theoretical way and

1037
00:37:32,579 --> 00:37:34,400
so maybe that's bad and so just by using

1038
00:37:34,400 --> 00:37:37,470
the simple theory well not so simple

1039
00:37:37,470 --> 00:37:39,660
theory arguments you can you can see

1040
00:37:39,660 --> 00:37:41,250
well this class of algorithms is very

1041
00:37:41,250 --> 00:37:43,680
not scalable in this case therefore we

1042
00:37:43,680 --> 00:37:45,180
should just avoid them theoretic for

1043
00:37:45,180 --> 00:37:46,619
theoretical reasons and then we can

1044
00:37:46,619 --> 00:37:48,420
focus on another class of algorithms and

1045
00:37:48,420 --> 00:37:49,000
I would

1046
00:37:49,000 --> 00:37:51,130
say that this is an amazingly

1047
00:37:51,130 --> 00:37:52,960
interesting area that I would encourage

1048
00:37:52,960 --> 00:37:56,290
people to work on much more is how can

1049
00:37:56,290 --> 00:37:58,150
we develop more of that type of theory

1050
00:37:58,150 --> 00:38:00,640
so we can use some sort of theory lens

1051
00:38:00,640 --> 00:38:03,190
to target which algorithms that we can

1052
00:38:03,190 --> 00:38:04,900
use in practice in which practical

1053
00:38:04,900 --> 00:38:07,060
algorithms can be used in certain types

1054
00:38:07,060 --> 00:38:10,810
of large problems okay so anyway so um

1055
00:38:10,810 --> 00:38:13,090
so that's my my very long introduction

1056
00:38:13,090 --> 00:38:15,460
in some sense but I'm gonna focus on

1057
00:38:15,460 --> 00:38:17,410
highlighting some of the relevant recent

1058
00:38:17,410 --> 00:38:19,540
work starting by focusing on big end

1059
00:38:19,540 --> 00:38:21,070
problems and then transitioning the big

1060
00:38:21,070 --> 00:38:22,660
P and I'm gonna give a disclaimer and

1061
00:38:22,660 --> 00:38:25,060
that I'm just you know given the time

1062
00:38:25,060 --> 00:38:26,950
I'm not providing like an overview of

1063
00:38:26,950 --> 00:38:28,990
the literature but I'm just focusing on

1064
00:38:28,990 --> 00:38:30,310
some some of the work we've done and

1065
00:38:30,310 --> 00:38:31,810
I'll give references and then we've

1066
00:38:31,810 --> 00:38:33,400
cited the broader literature and our

1067
00:38:33,400 --> 00:38:36,580
papers okay so now we're on the big end

1068
00:38:36,580 --> 00:38:38,710
and and I'll give a little bit before we

1069
00:38:38,710 --> 00:38:41,620
break okay so some solutions to the the

1070
00:38:41,620 --> 00:38:43,360
big end problem the first is what's

1071
00:38:43,360 --> 00:38:46,030
called embarrassingly parallel Markov

1072
00:38:46,030 --> 00:38:47,620
chain Monte Carlo one of the types of

1073
00:38:47,620 --> 00:38:49,090
solutions that I I'm not going to talk

1074
00:38:49,090 --> 00:38:51,430
about is is to just take any MCMC

1075
00:38:51,430 --> 00:38:52,960
algorithm and then say well maybe I can

1076
00:38:52,960 --> 00:38:54,820
use GPUs or something to break up the

1077
00:38:54,820 --> 00:38:57,340
computation that I think is somewhat

1078
00:38:57,340 --> 00:38:58,780
limited in scope because of the

1079
00:38:58,780 --> 00:39:01,060
communication cost and so if we really

1080
00:39:01,060 --> 00:39:03,730
want to scale up I'd say well we'd like

1081
00:39:03,730 --> 00:39:05,530
to be able to have a giant data set

1082
00:39:05,530 --> 00:39:07,660
stored on different computers not be

1083
00:39:07,660 --> 00:39:09,430
sending it around not be sticking stuff

1084
00:39:09,430 --> 00:39:12,310
on one computer and limit communication

1085
00:39:12,310 --> 00:39:13,930
cost so maybe we can take the data and

1086
00:39:13,930 --> 00:39:15,730
throw shards of the data on different

1087
00:39:15,730 --> 00:39:18,580
computers run em same same parallel for

1088
00:39:18,580 --> 00:39:20,050
different subsets of the data limit

1089
00:39:20,050 --> 00:39:21,550
communication costs and then combine

1090
00:39:21,550 --> 00:39:24,040
okay and that's called EPM CMC and

1091
00:39:24,040 --> 00:39:25,060
there's some really quite nice

1092
00:39:25,060 --> 00:39:28,480
algorithms in in that class by us and

1093
00:39:28,480 --> 00:39:31,240
other groups as well another type of

1094
00:39:31,240 --> 00:39:32,920
class of algorithms is what I might call

1095
00:39:32,920 --> 00:39:34,840
approximate Markov chain Monte Carlo

1096
00:39:34,840 --> 00:39:36,700
then we could just look at a Markov

1097
00:39:36,700 --> 00:39:38,260
chain Monte Carlo algorithm be like well

1098
00:39:38,260 --> 00:39:40,270
geez that gradient calculation is really

1099
00:39:40,270 --> 00:39:42,370
bloody expensive let's use stochastic

1100
00:39:42,370 --> 00:39:44,770
gradient there or over here or let's use

1101
00:39:44,770 --> 00:39:46,840
some subset of the data or let's use

1102
00:39:46,840 --> 00:39:47,740
some sort of low-rank approximation

1103
00:39:47,740 --> 00:39:50,170
linear algebra trick here and then we

1104
00:39:50,170 --> 00:39:52,920
can free up the bottlenecks by taking a

1105
00:39:52,920 --> 00:39:54,850
transition kernel that will converge to

1106
00:39:54,850 --> 00:39:56,770
exactly the right target distribution

1107
00:39:56,770 --> 00:39:59,020
and replacing it with some type of alum

1108
00:39:59,020 --> 00:40:00,790
provably accurate approximation now I

1109
00:40:00,790 --> 00:40:03,010
call that approximate MCM

1110
00:40:03,010 --> 00:40:05,380
and these two can be used together as

1111
00:40:05,380 --> 00:40:08,260
well of course something that I haven't

1112
00:40:08,260 --> 00:40:12,280
been talking about yet but I will focus

1113
00:40:12,280 --> 00:40:14,410
on a bit is that we'd like to also

1114
00:40:14,410 --> 00:40:16,690
develop robust methods kind of

1115
00:40:16,690 --> 00:40:18,400
regardless of the scalability issue we'd

1116
00:40:18,400 --> 00:40:20,140
like to also have methods that are more

1117
00:40:20,140 --> 00:40:22,390
inherently robust in large sample sizes

1118
00:40:22,390 --> 00:40:24,400
and we have a such a method which I will

1119
00:40:24,400 --> 00:40:27,430
highlight soon I'm something I find

1120
00:40:27,430 --> 00:40:29,320
really promising as well as to do type

1121
00:40:29,320 --> 00:40:31,540
of hybrid algorithms and so instead of

1122
00:40:31,540 --> 00:40:33,760
instead of running MCMC for everything

1123
00:40:33,760 --> 00:40:35,710
maybe we can use some sort of fast

1124
00:40:35,710 --> 00:40:37,660
estimate for some of the parameters and

1125
00:40:37,660 --> 00:40:39,370
then run MCMC for a subset of the

1126
00:40:39,370 --> 00:40:41,620
parameters we've had a lot of luck with

1127
00:40:41,620 --> 00:40:43,570
this type of method making it like very

1128
00:40:43,570 --> 00:40:45,670
very much scalable for but very flexible

1129
00:40:45,670 --> 00:40:48,520
models you can scale as best as well as

1130
00:40:48,520 --> 00:40:50,830
like you know state of the art a lasso

1131
00:40:50,830 --> 00:40:53,440
algorithms for very complicated problems

1132
00:40:53,440 --> 00:40:55,180
by doing a bit of optimization and then

1133
00:40:55,180 --> 00:40:58,060
sampling for the rest okay so I'm going

1134
00:40:58,060 --> 00:41:01,840
to start out on this EP MCMC thread and

1135
00:41:01,840 --> 00:41:03,520
so the idea is that we have some big

1136
00:41:03,520 --> 00:41:06,670
data set with really large n we'd like

1137
00:41:06,670 --> 00:41:08,650
to divide the data into subsets and

1138
00:41:08,650 --> 00:41:10,600
sometimes I find we're really kind of

1139
00:41:10,600 --> 00:41:13,120
trying to work seriously on really large

1140
00:41:13,120 --> 00:41:14,860
data problems for example we have a

1141
00:41:14,860 --> 00:41:16,960
collaboration with Alibaba where they

1142
00:41:16,960 --> 00:41:18,700
have really like billions and billions

1143
00:41:18,700 --> 00:41:21,340
of users in different domains and we'd

1144
00:41:21,340 --> 00:41:22,900
like to be kind of applying these types

1145
00:41:22,900 --> 00:41:24,340
of methods and so we're kind of working

1146
00:41:24,340 --> 00:41:26,650
seriously in these types of domains also

1147
00:41:26,650 --> 00:41:29,110
with SAS and we find that often you

1148
00:41:29,110 --> 00:41:30,550
can't really move the data around and

1149
00:41:30,550 --> 00:41:32,440
cap much control of the subsets and so

1150
00:41:32,440 --> 00:41:33,940
that's an interesting area is to

1151
00:41:33,940 --> 00:41:35,680
actually where you just kind of stock

1152
00:41:35,680 --> 00:41:37,570
with data on some machine from a

1153
00:41:37,570 --> 00:41:39,310
particular client or some subset of

1154
00:41:39,310 --> 00:41:41,710
users we can't move things around that

1155
00:41:41,710 --> 00:41:43,690
much and so we have these data subsets

1156
00:41:43,690 --> 00:41:46,060
here I'm going to focus on more more

1157
00:41:46,060 --> 00:41:47,970
intelligently constructed data subsets

1158
00:41:47,970 --> 00:41:51,070
okay so we we take big data we develop

1159
00:41:51,070 --> 00:41:53,590
we break it into shards and then based

1160
00:41:53,590 --> 00:41:55,420
on each shard of data we get a subset

1161
00:41:55,420 --> 00:41:57,190
posterior then we somehow combine them

1162
00:41:57,190 --> 00:41:59,380
magically into a beautiful accurate

1163
00:41:59,380 --> 00:42:02,440
posterior approximation okay so we're

1164
00:42:02,440 --> 00:42:04,600
gonna be drawing samples from each

1165
00:42:04,600 --> 00:42:07,570
subset posterior and parallel and then

1166
00:42:07,570 --> 00:42:09,700
then we get sort of atoms or a bunch of

1167
00:42:09,700 --> 00:42:11,380
samples and then based on the samples

1168
00:42:11,380 --> 00:42:14,380
we'd like to combine so how can we do

1169
00:42:14,380 --> 00:42:15,790
that well here here's this got a toy

1170
00:42:15,790 --> 00:42:16,700
example that

1171
00:42:16,700 --> 00:42:18,560
use a lot of logistic regression and so

1172
00:42:18,560 --> 00:42:20,930
we might say you know what's the label

1173
00:42:20,930 --> 00:42:23,810
on the ice example given the vector

1174
00:42:23,810 --> 00:42:26,869
features X I 1 through X IP and some

1175
00:42:26,869 --> 00:42:29,390
parameters theta okay so that's just a

1176
00:42:29,390 --> 00:42:31,670
logistic regression and and well here

1177
00:42:31,670 --> 00:42:33,650
here shows a bunch of subsets posteriors

1178
00:42:33,650 --> 00:42:34,970
and so we get a bunch of noisy

1179
00:42:34,970 --> 00:42:36,859
approximations the full data posterior

1180
00:42:36,859 --> 00:42:39,410
and you know you think usually often

1181
00:42:39,410 --> 00:42:41,060
divide and conquer algorithms we have a

1182
00:42:41,060 --> 00:42:43,339
bunch of chunks of the data and we get

1183
00:42:43,339 --> 00:42:46,010
like some some estimate of theta hat or

1184
00:42:46,010 --> 00:42:47,510
something on each subset and then we

1185
00:42:47,510 --> 00:42:49,420
take the median or we combine some how

1186
00:42:49,420 --> 00:42:52,369
well here the challenge is that we have

1187
00:42:52,369 --> 00:42:54,650
a probability distribution and so we

1188
00:42:54,650 --> 00:42:56,690
have a full probability distribution for

1189
00:42:56,690 --> 00:42:59,570
each subset and you know will this give

1190
00:42:59,570 --> 00:43:00,920
it only the subset it should be like

1191
00:43:00,920 --> 00:43:03,619
it's too it's too variable you know

1192
00:43:03,619 --> 00:43:04,970
relative to the full data posterior is

1193
00:43:04,970 --> 00:43:06,290
really this really concentrated thing

1194
00:43:06,290 --> 00:43:07,700
around some value maybe it's even

1195
00:43:07,700 --> 00:43:09,829
multimodal and then the subset posterior

1196
00:43:09,829 --> 00:43:11,510
czar like to two variable but they're

1197
00:43:11,510 --> 00:43:13,670
all probability distributions and so we

1198
00:43:13,670 --> 00:43:15,170
end up with this really big problem and

1199
00:43:15,170 --> 00:43:18,230
how do we how do we combine them and so

1200
00:43:18,230 --> 00:43:20,329
here I just shown some results some

1201
00:43:20,329 --> 00:43:22,220
where the contour plots were just two

1202
00:43:22,220 --> 00:43:25,040
the parameters the the based on MCMC

1203
00:43:25,040 --> 00:43:26,960
which we ran for a week or something is

1204
00:43:26,960 --> 00:43:29,660
is here in blue we have an approach

1205
00:43:29,660 --> 00:43:32,930
called wasp which is in red which gives

1206
00:43:32,930 --> 00:43:35,180
you a very accurate approximation and in

1207
00:43:35,180 --> 00:43:36,890
green shows the contours for the subset

1208
00:43:36,890 --> 00:43:40,520
posteriors okay so so basically what

1209
00:43:40,520 --> 00:43:41,780
we're going to do is use this type of

1210
00:43:41,780 --> 00:43:43,880
stochastic approximation to the full

1211
00:43:43,880 --> 00:43:46,400
data posterior distribution based on

1212
00:43:46,400 --> 00:43:48,170
each subset and we're going to average

1213
00:43:48,170 --> 00:43:51,020
in some type of way geometrically to

1214
00:43:51,020 --> 00:43:52,700
reduce noise okay so it's a type of

1215
00:43:52,700 --> 00:43:54,470
stochastic approximation algorithm that

1216
00:43:54,470 --> 00:43:57,980
produces this ok so we start out we have

1217
00:43:57,980 --> 00:43:59,839
the full data posterior distribution PI

1218
00:43:59,839 --> 00:44:03,140
of theta given Y and this is just you

1219
00:44:03,140 --> 00:44:05,119
know some notation for that we're gonna

1220
00:44:05,119 --> 00:44:08,060
divide the full data Y n into K subsets

1221
00:44:08,060 --> 00:44:11,630
of size M okay break up all the data

1222
00:44:11,630 --> 00:44:13,369
into shards throw them on different

1223
00:44:13,369 --> 00:44:16,339
machines or processors and then based on

1224
00:44:16,339 --> 00:44:20,960
each chunk of the data Y Y subscript J

1225
00:44:20,960 --> 00:44:23,690
in brackets we define some sort of

1226
00:44:23,690 --> 00:44:25,700
subset posterior and the goal here is

1227
00:44:25,700 --> 00:44:27,109
we'd like to take a subset of the data

1228
00:44:27,109 --> 00:44:29,839
to approximate give us a noisy

1229
00:44:29,839 --> 00:44:30,450
approximate

1230
00:44:30,450 --> 00:44:32,190
the full data posterior distribution

1231
00:44:32,190 --> 00:44:33,839
okay and then we're going to sort of

1232
00:44:33,839 --> 00:44:35,460
average those to get a really nice

1233
00:44:35,460 --> 00:44:37,560
approximation of the full data posterior

1234
00:44:37,560 --> 00:44:40,109
reducing noise and so how we do that is

1235
00:44:40,109 --> 00:44:42,060
we take the light this is a likelihood

1236
00:44:42,060 --> 00:44:44,880
for one observation we take that all

1237
00:44:44,880 --> 00:44:46,740
those observations in that shard and

1238
00:44:46,740 --> 00:44:48,540
we're gonna raise those likelihood to

1239
00:44:48,540 --> 00:44:50,880
some power gamma and so it's a type of

1240
00:44:50,880 --> 00:44:53,280
gamma as this power chosen to minimize

1241
00:44:53,280 --> 00:44:56,280
approximation error okay but over all

1242
00:44:56,280 --> 00:44:57,930
the subsets posterior just looks just

1243
00:44:57,930 --> 00:44:59,550
like a posterior distribution just with

1244
00:44:59,550 --> 00:45:02,420
this tweak with this power gamma okay

1245
00:45:02,420 --> 00:45:05,190
otherwise we could just do MCMC within

1246
00:45:05,190 --> 00:45:07,980
subset note no problem okay so um so

1247
00:45:07,980 --> 00:45:10,320
then that what we're gonna do is we're

1248
00:45:10,320 --> 00:45:12,109
gonna use this notion of a Wasserstein

1249
00:45:12,109 --> 00:45:14,430
barycentre which has become increasingly

1250
00:45:14,430 --> 00:45:16,140
popular i think a lot of people are

1251
00:45:16,140 --> 00:45:17,700
interested in the machine learning and

1252
00:45:17,700 --> 00:45:19,170
statistics and optimal transport

1253
00:45:19,170 --> 00:45:21,390
problems and so we're kind of defining

1254
00:45:21,390 --> 00:45:23,130
things as a type of optimal transport

1255
00:45:23,130 --> 00:45:24,990
problem and so we're getting a bunch of

1256
00:45:24,990 --> 00:45:26,940
noisy approximations to the full data

1257
00:45:26,940 --> 00:45:29,550
posterior based on subsets and we would

1258
00:45:29,550 --> 00:45:30,930
like to kind of take some sort of

1259
00:45:30,930 --> 00:45:34,440
central approximation or barycenter and

1260
00:45:34,440 --> 00:45:35,550
we're gonna use that as our

1261
00:45:35,550 --> 00:45:38,160
approximation okay and so if we can

1262
00:45:38,160 --> 00:45:39,690
define some sort of distance between

1263
00:45:39,690 --> 00:45:41,369
probability measures and we'll use a

1264
00:45:41,369 --> 00:45:43,800
Wasserstein distance then we can kind of

1265
00:45:43,800 --> 00:45:45,510
solve this optimization problem to get a

1266
00:45:45,510 --> 00:45:46,800
really nice approximation of the

1267
00:45:46,800 --> 00:45:49,859
posterior okay and so that's this kind

1268
00:45:49,859 --> 00:45:52,290
of wasp method or Wasserstein barycenter

1269
00:45:52,290 --> 00:45:54,420
subset posteriors we define a

1270
00:45:54,420 --> 00:45:57,540
Wasserstein distance between probability

1271
00:45:57,540 --> 00:46:00,750
distributions and then we can just solve

1272
00:46:00,750 --> 00:46:02,790
a type of optimization problem using

1273
00:46:02,790 --> 00:46:05,369
this Wasserstein very center which is a

1274
00:46:05,369 --> 00:46:07,109
well known quantity in the optimization

1275
00:46:07,109 --> 00:46:10,260
literature okay and so what one nice

1276
00:46:10,260 --> 00:46:12,060
thing is that what we would do is we

1277
00:46:12,060 --> 00:46:13,740
would take each of these subsets

1278
00:46:13,740 --> 00:46:16,380
posteriors and we would get samples from

1279
00:46:16,380 --> 00:46:18,030
them and so in parallel on each machine

1280
00:46:18,030 --> 00:46:20,609
we have a subset of the pub set of the

1281
00:46:20,609 --> 00:46:23,640
data we run MCMC that's easy because now

1282
00:46:23,640 --> 00:46:25,410
the problem size isn't big because we

1283
00:46:25,410 --> 00:46:26,940
have a small sample size on each machine

1284
00:46:26,940 --> 00:46:28,980
we just run in parallel using whatever

1285
00:46:28,980 --> 00:46:32,069
MCMC algorithm we take those samples we

1286
00:46:32,069 --> 00:46:33,750
plug them into this discrete optimal

1287
00:46:33,750 --> 00:46:35,069
transport problem to calculate the

1288
00:46:35,069 --> 00:46:37,230
barycenter it's a sparse linear program

1289
00:46:37,230 --> 00:46:38,520
that we can calculate it very quickly

1290
00:46:38,520 --> 00:46:42,000
okay and that's all we do okay it also

1291
00:46:42,000 --> 00:46:43,980
then gives you an atomic approximation

1292
00:46:43,980 --> 00:46:44,369
and

1293
00:46:44,369 --> 00:46:45,809
because it's going to take those samples

1294
00:46:45,809 --> 00:46:47,759
and reweighed them and then now at the

1295
00:46:47,759 --> 00:46:49,769
end of the day we're like oh we have

1296
00:46:49,769 --> 00:46:52,109
waited samples from the full posterior

1297
00:46:52,109 --> 00:46:54,630
distribution given the whole data and we

1298
00:46:54,630 --> 00:46:56,430
can use that to calculate any post ear

1299
00:46:56,430 --> 00:46:58,470
summaries we want for any functional

1300
00:46:58,470 --> 00:47:03,059
just quite easily okay yeah we were

1301
00:47:03,059 --> 00:47:04,710
supposed to break but I'll just finish

1302
00:47:04,710 --> 00:47:08,099
up this this this shard of the talk okay

1303
00:47:08,099 --> 00:47:09,749
so the minimizing Wasserstein is a

1304
00:47:09,749 --> 00:47:11,640
solution to a discrete optimal transport

1305
00:47:11,640 --> 00:47:13,619
problem and we could say oh well like a

1306
00:47:13,619 --> 00:47:15,960
pair of subsets posteriors we can write

1307
00:47:15,960 --> 00:47:19,049
as like on this atomic form they're like

1308
00:47:19,049 --> 00:47:20,700
weights on different atoms the atoms

1309
00:47:20,700 --> 00:47:23,400
we're taking by sampling from MCMC and

1310
00:47:23,400 --> 00:47:25,200
we have a couple of probability

1311
00:47:25,200 --> 00:47:26,970
distributions and then we can define a

1312
00:47:26,970 --> 00:47:28,890
matrix of squared differences in the

1313
00:47:28,890 --> 00:47:31,920
atoms we can define a what's called an

1314
00:47:31,920 --> 00:47:34,529
optimal transportation polytope and we

1315
00:47:34,529 --> 00:47:36,569
can then solve this type of optimization

1316
00:47:36,569 --> 00:47:38,849
problem using you know just stuff that's

1317
00:47:38,849 --> 00:47:40,559
already been developed in the literature

1318
00:47:40,559 --> 00:47:43,470
for solving these types of optimal

1319
00:47:43,470 --> 00:47:45,119
transport problems using sparse LP

1320
00:47:45,119 --> 00:47:48,509
solver solvers okay and we won one nice

1321
00:47:48,509 --> 00:47:50,930
thing is that then we get this wasp

1322
00:47:50,930 --> 00:47:53,819
posterior approximation and we can prove

1323
00:47:53,819 --> 00:47:55,650
lots of things about this approximation

1324
00:47:55,650 --> 00:47:58,440
that actually you know is an accurate

1325
00:47:58,440 --> 00:48:01,109
approximation and it converges nicely

1326
00:48:01,109 --> 00:48:03,630
about some optimal value of the

1327
00:48:03,630 --> 00:48:06,089
parameters and so I'm going to stop here

1328
00:48:06,089 --> 00:48:08,160
because it's break time because this is

1329
00:48:08,160 --> 00:48:10,230
actually a simple vert simpler version

1330
00:48:10,230 --> 00:48:12,809
that's I think much more useful in

1331
00:48:12,809 --> 00:48:14,220
practice because we can actually bypass

1332
00:48:14,220 --> 00:48:16,410
doing all of that optimal transport part

1333
00:48:16,410 --> 00:48:18,029
which may which makes things a lot

1334
00:48:18,029 --> 00:48:19,650
easier but I'll talk about that after

1335
00:48:19,650 --> 00:48:21,779
the break I think we have ten minutes

1336
00:48:21,779 --> 00:48:27,619
come back at 9:30 okay

1337
00:48:31,380 --> 00:48:34,300
so this is another variant that's some

1338
00:48:34,300 --> 00:48:36,940
kind of similar but that we just had

1339
00:48:36,940 --> 00:48:39,730
published last year in biometrika so the

1340
00:48:39,730 --> 00:48:42,790
idea here is that usually when we're

1341
00:48:42,790 --> 00:48:45,760
doing Bayesian inference we're almost

1342
00:48:45,760 --> 00:48:47,800
always focusing on one-dimensional

1343
00:48:47,800 --> 00:48:49,390
functionals and so we have this joint

1344
00:48:49,390 --> 00:48:50,920
posterior distribution which might be

1345
00:48:50,920 --> 00:48:54,340
four five thousand parameters but we're

1346
00:48:54,340 --> 00:48:56,170
usually focusing on one-dimensional

1347
00:48:56,170 --> 00:48:58,170
things when we're reporting the results

1348
00:48:58,170 --> 00:49:00,600
so the one-dimensional thing might be

1349
00:49:00,600 --> 00:49:03,670
the probability of a class label or a

1350
00:49:03,670 --> 00:49:06,490
predictive distribution or you know a

1351
00:49:06,490 --> 00:49:08,380
parameter of interest or functional the

1352
00:49:08,380 --> 00:49:09,490
parameters of interest it's almost

1353
00:49:09,490 --> 00:49:11,530
always a one-dimensional thing that we

1354
00:49:11,530 --> 00:49:14,010
we're poor it at the end of the day okay

1355
00:49:14,010 --> 00:49:16,450
so we're reporting point in interval

1356
00:49:16,450 --> 00:49:18,120
estimates for different 1d functionals

1357
00:49:18,120 --> 00:49:22,120
okay so keep that in mind so so if we do

1358
00:49:22,120 --> 00:49:23,680
that and we think about well the the

1359
00:49:23,680 --> 00:49:26,860
Wasserstein barycentre of the subsets it

1360
00:49:26,860 --> 00:49:28,840
has an explicit relationship with the

1361
00:49:28,840 --> 00:49:31,440
subset posteriors in 1d okay in

1362
00:49:31,440 --> 00:49:35,170
particular the quantiles of our simple

1363
00:49:35,170 --> 00:49:36,910
averages of quantiles of the subsets

1364
00:49:36,910 --> 00:49:40,090
posteriors okay and so that this leads

1365
00:49:40,090 --> 00:49:41,920
to a really trivial algorithm and so the

1366
00:49:41,920 --> 00:49:45,160
algorithm is super super trivial and so

1367
00:49:45,160 --> 00:49:46,990
what we do is we have a giant data set

1368
00:49:46,990 --> 00:49:49,240
we break it up into shards we throw

1369
00:49:49,240 --> 00:49:51,940
those on different machines we run MCMC

1370
00:49:51,940 --> 00:49:53,590
in parallel with the likelyhood raised

1371
00:49:53,590 --> 00:49:55,930
to that power that I mentioned and

1372
00:49:55,930 --> 00:49:57,820
that's really easy and okay now what do

1373
00:49:57,820 --> 00:50:00,130
we do so now we're interested in some

1374
00:50:00,130 --> 00:50:02,740
posterior functional f of theta or some

1375
00:50:02,740 --> 00:50:06,480
predictive distribution F of Y given X

1376
00:50:06,480 --> 00:50:10,170
okay well we we keep track of that

1377
00:50:10,170 --> 00:50:14,050
samples from that on each machine we we

1378
00:50:14,050 --> 00:50:16,750
calculate a percentile of the posterior

1379
00:50:16,750 --> 00:50:18,610
distribution on each machine in parallel

1380
00:50:18,610 --> 00:50:20,530
we feed those back to the central

1381
00:50:20,530 --> 00:50:22,990
processor and we average them okay and

1382
00:50:22,990 --> 00:50:25,630
then we can based on that we can get we

1383
00:50:25,630 --> 00:50:27,570
can get a approvable super accurate

1384
00:50:27,570 --> 00:50:31,090
approximate 1d posterior approximation

1385
00:50:31,090 --> 00:50:34,510
for any functional of interest and have

1386
00:50:34,510 --> 00:50:36,400
very very very strong theoretical

1387
00:50:36,400 --> 00:50:39,250
guarantees and and good at performance

1388
00:50:39,250 --> 00:50:40,750
on those and and it's really really easy

1389
00:50:40,750 --> 00:50:43,810
all we do is just break the data up put

1390
00:50:43,810 --> 00:50:44,849
it on different machines

1391
00:50:44,849 --> 00:50:46,829
I'm sensing parallel calculate a

1392
00:50:46,829 --> 00:50:48,299
percentile or quantile for any

1393
00:50:48,299 --> 00:50:50,849
functional of interest send those back

1394
00:50:50,849 --> 00:50:52,440
to the central processor and average

1395
00:50:52,440 --> 00:50:55,559
them okay we didn't realize this at the

1396
00:50:55,559 --> 00:50:57,690
time but it's sort of reminiscent of a

1397
00:50:57,690 --> 00:50:59,970
paper that Mike Jordan and collaborators

1398
00:50:59,970 --> 00:51:01,710
came up with called bag of little

1399
00:51:01,710 --> 00:51:03,720
bootstraps so it's sort of almost like

1400
00:51:03,720 --> 00:51:07,200
an MCMC version of that okay so we have

1401
00:51:07,200 --> 00:51:08,940
quite nice theory showing accuracy of

1402
00:51:08,940 --> 00:51:11,160
these approximations you can potentially

1403
00:51:11,160 --> 00:51:13,019
implement it and Stan which is a sort of

1404
00:51:13,019 --> 00:51:14,759
probabilistic programming language for

1405
00:51:14,759 --> 00:51:16,950
Bayesian inference which because they

1406
00:51:16,950 --> 00:51:20,160
allow powered likelihoods okay I don't

1407
00:51:20,160 --> 00:51:21,869
want to get too much into the theory

1408
00:51:21,869 --> 00:51:26,249
today but but did you just say that too

1409
00:51:26,249 --> 00:51:28,079
you know as the subset sample size

1410
00:51:28,079 --> 00:51:30,029
increases it doesn't have to it's not

1411
00:51:30,029 --> 00:51:32,009
really dependent on the subset it's

1412
00:51:32,009 --> 00:51:35,579
getting large but it as they grow really

1413
00:51:35,579 --> 00:51:37,710
rapidly we can we can have good good

1414
00:51:37,710 --> 00:51:38,999
good

1415
00:51:38,999 --> 00:51:41,970
accurate approximations okay and so the

1416
00:51:41,970 --> 00:51:43,739
bias variance is quantiles only

1417
00:51:43,739 --> 00:51:45,059
different higher orders of the total

1418
00:51:45,059 --> 00:51:47,460
sample size okay

1419
00:51:47,460 --> 00:51:48,869
so we've implemented this in a really

1420
00:51:48,869 --> 00:51:51,299
broad variety of data and models I don't

1421
00:51:51,299 --> 00:51:53,160
have time to show results for this part

1422
00:51:53,160 --> 00:51:55,469
today but logistic linear random effects

1423
00:51:55,469 --> 00:51:57,049
models mixtures models matrix

1424
00:51:57,049 --> 00:51:59,339
factorizations for recommender systems

1425
00:51:59,339 --> 00:52:01,670
Gaussian process regression

1426
00:52:01,670 --> 00:52:05,400
nonparametric models etc so we can

1427
00:52:05,400 --> 00:52:08,670
compare to long runs of MCMC and VB it's

1428
00:52:08,670 --> 00:52:10,859
much much much faster than MCMC doing

1429
00:52:10,859 --> 00:52:12,630
this and you get this sort of a win-win

1430
00:52:12,630 --> 00:52:15,359
because you think well the the time per

1431
00:52:15,359 --> 00:52:17,400
iteration is too slow as the sample size

1432
00:52:17,400 --> 00:52:19,950
increases and also the mixing might get

1433
00:52:19,950 --> 00:52:21,479
worse with the sample size but we're

1434
00:52:21,479 --> 00:52:23,400
we're breaking up the data and running

1435
00:52:23,400 --> 00:52:25,380
it in parallel and so we're exploiting

1436
00:52:25,380 --> 00:52:27,450
parallelization to improve the whole

1437
00:52:27,450 --> 00:52:30,089
mixing issue and also massively improve

1438
00:52:30,089 --> 00:52:33,660
the time for iteration okay I found that

1439
00:52:33,660 --> 00:52:35,039
in many of these problems that there

1440
00:52:35,039 --> 00:52:36,719
there are actually quite quite good

1441
00:52:36,719 --> 00:52:38,819
variational Bayes implementations that

1442
00:52:38,819 --> 00:52:43,289
were competitive okay so I'm you know

1443
00:52:43,289 --> 00:52:45,210
kind of given the time and the what

1444
00:52:45,210 --> 00:52:46,799
people were interested in over the break

1445
00:52:46,799 --> 00:52:48,450
I might just kind of skim through this

1446
00:52:48,450 --> 00:52:50,910
part but you remember that like in the

1447
00:52:50,910 --> 00:52:52,619
the big end case there's these different

1448
00:52:52,619 --> 00:52:56,099
types of methods the one is EPM CMC the

1449
00:52:56,099 --> 00:52:58,299
the second type of approach is

1450
00:52:58,299 --> 00:53:00,429
approximate Markov chain Monte Carlo and

1451
00:53:00,429 --> 00:53:02,380
and and there's a rich literature on

1452
00:53:02,380 --> 00:53:03,999
this I just kind of listed this one

1453
00:53:03,999 --> 00:53:05,769
archive paper by a really great student

1454
00:53:05,769 --> 00:53:07,359
a former student of mine James John drew

1455
00:53:07,359 --> 00:53:10,509
but the idea here is that we have some

1456
00:53:10,509 --> 00:53:13,599
MCMC algorithm that it that converges to

1457
00:53:13,599 --> 00:53:15,339
exactly the right thing but it's too too

1458
00:53:15,339 --> 00:53:18,369
slow it's too expensive to to evaluate

1459
00:53:18,369 --> 00:53:21,219
the transition kernel and so we might

1460
00:53:21,219 --> 00:53:23,469
use instead an approximation for example

1461
00:53:23,469 --> 00:53:24,699
we might approximate a conditional

1462
00:53:24,699 --> 00:53:26,559
distribution and give sampling with a

1463
00:53:26,559 --> 00:53:28,660
gaussian or using a sub sample of the

1464
00:53:28,660 --> 00:53:30,479
data and so that gives us a type of

1465
00:53:30,479 --> 00:53:33,099
faster or thing that we can sample from

1466
00:53:33,099 --> 00:53:35,109
more more quickly and approximate the

1467
00:53:35,109 --> 00:53:38,380
actual conditional distribution so using

1468
00:53:38,380 --> 00:53:40,539
this trick we can potentially vastly

1469
00:53:40,539 --> 00:53:42,099
speed-up MCMC sampling in high

1470
00:53:42,099 --> 00:53:44,349
dimensional settings one one example is

1471
00:53:44,349 --> 00:53:46,150
that there's this huge literature on you

1472
00:53:46,150 --> 00:53:48,969
know Gaussian processes where if I if I

1473
00:53:48,969 --> 00:53:50,469
don't use an approximation then I have

1474
00:53:50,469 --> 00:53:52,119
an order n cube matrix inversion

1475
00:53:52,119 --> 00:53:54,729
bottleneck and so then instead I use one

1476
00:53:54,729 --> 00:53:56,650
of a hundred different approximations

1477
00:53:56,650 --> 00:53:57,759
that have been proposed in the

1478
00:53:57,759 --> 00:53:59,890
literature that that are low rank or

1479
00:53:59,890 --> 00:54:01,660
nearest neighbor Gaussian process

1480
00:54:01,660 --> 00:54:03,880
approximations or whatever okay and so

1481
00:54:03,880 --> 00:54:07,119
that would be a type of a MCMC so the

1482
00:54:07,119 --> 00:54:09,729
original MCMC sampler converges to the

1483
00:54:09,729 --> 00:54:11,949
exact posterior distribution but you

1484
00:54:11,949 --> 00:54:13,269
know it's not necessarily clear what

1485
00:54:13,269 --> 00:54:14,859
happens when we take that one and we

1486
00:54:14,859 --> 00:54:16,959
perturb it so we like oh we have this

1487
00:54:16,959 --> 00:54:18,459
transition kernel now we've like use

1488
00:54:18,459 --> 00:54:20,589
this approximation well does the thing

1489
00:54:20,589 --> 00:54:22,209
even converge then what does it converts

1490
00:54:22,209 --> 00:54:26,079
to excetera so but but this method is

1491
00:54:26,079 --> 00:54:28,059
used routinely anyway and often there's

1492
00:54:28,059 --> 00:54:30,039
a rich empirical evidence that the

1493
00:54:30,039 --> 00:54:32,019
methods do do quite well in terms of

1494
00:54:32,019 --> 00:54:35,229
uncertainty quantification okay so but

1495
00:54:35,229 --> 00:54:36,819
we can we can develop some theory in

1496
00:54:36,819 --> 00:54:39,549
this case so we're gonna define an exact

1497
00:54:39,549 --> 00:54:41,259
MCMC algorithm which is computationally

1498
00:54:41,259 --> 00:54:43,689
tractable but has good mixing and we're

1499
00:54:43,689 --> 00:54:45,640
gonna leverage on that okay and so we're

1500
00:54:45,640 --> 00:54:46,869
gonna approximate with a more

1501
00:54:46,869 --> 00:54:48,459
computationally tractable alternative

1502
00:54:48,459 --> 00:54:50,979
and then we can kind of come up with a

1503
00:54:50,979 --> 00:54:52,900
type of theory framework that we call

1504
00:54:52,900 --> 00:54:54,880
cop minimax and so you might have a

1505
00:54:54,880 --> 00:54:56,650
particular computational budget and then

1506
00:54:56,650 --> 00:54:59,019
you want to have the optimal algorithm

1507
00:54:59,019 --> 00:55:00,489
within some class of algorithms to

1508
00:55:00,489 --> 00:55:02,079
produce the lowest error in a particular

1509
00:55:02,079 --> 00:55:04,869
computational time and so we can then

1510
00:55:04,869 --> 00:55:07,089
get get you know tight finite sample

1511
00:55:07,089 --> 00:55:09,249
bounds on l2 error for a broad class of

1512
00:55:09,249 --> 00:55:11,559
of approximate Markov chain Monte Carlo

1513
00:55:11,559 --> 00:55:12,010
algorithm

1514
00:55:12,010 --> 00:55:14,290
and you can see while which types of

1515
00:55:14,290 --> 00:55:17,910
algorithms work well often having a

1516
00:55:17,910 --> 00:55:20,140
non-negligible approximation error works

1517
00:55:20,140 --> 00:55:22,420
for low computational budgets but as the

1518
00:55:22,420 --> 00:55:23,920
computational budget increases you want

1519
00:55:23,920 --> 00:55:25,920
to have a more accurate approximation

1520
00:55:25,920 --> 00:55:28,780
okay so I'm just gonna skip over this I

1521
00:55:28,780 --> 00:55:30,369
could you could do it for subsets you

1522
00:55:30,369 --> 00:55:32,290
can do it for binary outcomes you can do

1523
00:55:32,290 --> 00:55:34,630
it for mixer models you can do it for

1524
00:55:34,630 --> 00:55:39,580
GPS etc so let's um I want to skip to

1525
00:55:39,580 --> 00:55:42,820
the robustness problem okay so so you

1526
00:55:42,820 --> 00:55:45,730
know this a MCMC EP MCMC these are just

1527
00:55:45,730 --> 00:55:47,770
these are these are focused on providing

1528
00:55:47,770 --> 00:55:50,320
algorithms for scaling up MCMC in a

1529
00:55:50,320 --> 00:55:52,660
formal sense and a practical sense to

1530
00:55:52,660 --> 00:55:55,330
large data problems but then you know

1531
00:55:55,330 --> 00:55:57,340
we're trying to approximate the exact

1532
00:55:57,340 --> 00:55:59,680
posterior distribution and one thing we

1533
00:55:59,680 --> 00:56:01,420
might worry about is that well the exact

1534
00:56:01,420 --> 00:56:03,040
posterior distribution might not be very

1535
00:56:03,040 --> 00:56:05,350
robust in large sample problems and

1536
00:56:05,350 --> 00:56:06,940
because in standard Bayesian inference

1537
00:56:06,940 --> 00:56:09,670
as in other model-based inferences it's

1538
00:56:09,670 --> 00:56:11,320
it's typically assumed that the model is

1539
00:56:11,320 --> 00:56:14,530
correct okay and so what we worry about

1540
00:56:14,530 --> 00:56:16,690
is the one thing that having a model is

1541
00:56:16,690 --> 00:56:18,280
nice and that the parameters might be

1542
00:56:18,280 --> 00:56:19,780
interpretable we'd like to kind of do

1543
00:56:19,780 --> 00:56:21,130
inferences on these parameters they

1544
00:56:21,130 --> 00:56:23,980
might have physical meaning etc but but

1545
00:56:23,980 --> 00:56:25,780
small violations of the assumption that

1546
00:56:25,780 --> 00:56:27,910
the model is correct can sometimes have

1547
00:56:27,910 --> 00:56:29,320
a large impact particularly in large

1548
00:56:29,320 --> 00:56:33,760
data sets you know it's very famous

1549
00:56:33,760 --> 00:56:35,980
quote is that all models are wrong and

1550
00:56:35,980 --> 00:56:37,990
the ability to carefully check modeling

1551
00:56:37,990 --> 00:56:39,880
assumptions can kind of decrease for big

1552
00:56:39,880 --> 00:56:43,210
and complicated datasets so what we were

1553
00:56:43,210 --> 00:56:44,980
interested in doing and this is joint

1554
00:56:44,980 --> 00:56:47,290
work with a former postdoc a brilliant

1555
00:56:47,290 --> 00:56:49,810
guy who worked with me Jeff Miller is

1556
00:56:49,810 --> 00:56:51,160
now at now at Harvard and biostatistics

1557
00:56:51,160 --> 00:56:53,619
and and we came up with this idea that

1558
00:56:53,619 --> 00:56:55,810
you could maybe potentially tweak the

1559
00:56:55,810 --> 00:56:57,670
Bayesian paradigm to be inherently more

1560
00:56:57,670 --> 00:57:00,550
robust and I'll give an example of the

1561
00:57:00,550 --> 00:57:03,460
non robustness here so um let's say we

1562
00:57:03,460 --> 00:57:05,260
wanted to use a mixture of gaussians for

1563
00:57:05,260 --> 00:57:07,000
clustering and so clustering is like you

1564
00:57:07,000 --> 00:57:09,369
know unsupervised the big unsupervised

1565
00:57:09,369 --> 00:57:11,859
learning problem model-based clustering

1566
00:57:11,859 --> 00:57:13,810
is an incredibly popular type of method

1567
00:57:13,810 --> 00:57:16,450
and.and you know one type of very

1568
00:57:16,450 --> 00:57:18,190
popular model-based clustering method is

1569
00:57:18,190 --> 00:57:22,720
to mix Gaussian okay let's say that the

1570
00:57:22,720 --> 00:57:24,340
true data distributions like this okay

1571
00:57:24,340 --> 00:57:25,870
so we have a

1572
00:57:25,870 --> 00:57:27,970
a mixture of gaussians so we're gonna

1573
00:57:27,970 --> 00:57:30,640
take a mixture of gaussians and then

1574
00:57:30,640 --> 00:57:32,440
we're gonna say the true model is not

1575
00:57:32,440 --> 00:57:34,060
exactly mixture gaussians we're just

1576
00:57:34,060 --> 00:57:37,390
gonna change the distribution system ixr

1577
00:57:37,390 --> 00:57:38,500
of gaussians would still be a nice

1578
00:57:38,500 --> 00:57:40,150
approximation that we would might want

1579
00:57:40,150 --> 00:57:42,160
to use and then we're gonna see what

1580
00:57:42,160 --> 00:57:46,570
happens as n increases so the the going

1581
00:57:46,570 --> 00:57:48,520
down here we go from N equals 200 to N

1582
00:57:48,520 --> 00:57:50,830
equals 20,000 and now we fit a model

1583
00:57:50,830 --> 00:57:52,630
that that allows the the number of

1584
00:57:52,630 --> 00:57:54,160
mixture components or clusters to be

1585
00:57:54,160 --> 00:57:57,040
unknown what we see is that if we just

1586
00:57:57,040 --> 00:57:58,990
even slightly perturb the gaussians you

1587
00:57:58,990 --> 00:58:00,760
can see the red line versus the blue

1588
00:58:00,760 --> 00:58:02,500
line that we're gonna keep adding

1589
00:58:02,500 --> 00:58:04,840
mixture components without bound as the

1590
00:58:04,840 --> 00:58:06,430
sample size goes up so we're sort of

1591
00:58:06,430 --> 00:58:08,080
letting making the model be more and

1592
00:58:08,080 --> 00:58:10,270
more complicated even though this

1593
00:58:10,270 --> 00:58:11,830
mixture of two gaussians provides a

1594
00:58:11,830 --> 00:58:13,600
really accurate approximation we would

1595
00:58:13,600 --> 00:58:16,060
like to do that but we instead add more

1596
00:58:16,060 --> 00:58:17,770
components unnecessarily as the sample

1597
00:58:17,770 --> 00:58:21,100
size increases yes so the point is that

1598
00:58:21,100 --> 00:58:22,840
that's the right answer from a patient

1599
00:58:22,840 --> 00:58:25,180
perspective is that we're allowing the

1600
00:58:25,180 --> 00:58:27,970
model to be flexible and it can figure

1601
00:58:27,970 --> 00:58:29,920
out which model to use using beta

1602
00:58:29,920 --> 00:58:31,690
bayesian model selection and averaging

1603
00:58:31,690 --> 00:58:33,640
but it's going to then introduce more

1604
00:58:33,640 --> 00:58:35,680
and more components as the sample size

1605
00:58:35,680 --> 00:58:38,710
increases to fit the data and then the

1606
00:58:38,710 --> 00:58:40,180
interpretability the clusters breaks

1607
00:58:40,180 --> 00:58:42,160
down in large sample sizes and this is a

1608
00:58:42,160 --> 00:58:44,290
very broad problem it can occur we've

1609
00:58:44,290 --> 00:58:45,910
found and met many many models

1610
00:58:45,910 --> 00:58:48,820
particularly when we have a model where

1611
00:58:48,820 --> 00:58:50,590
the models flexible and so we're doing

1612
00:58:50,590 --> 00:58:52,060
something like model averaging or

1613
00:58:52,060 --> 00:58:54,220
there's a set of models and we have some

1614
00:58:54,220 --> 00:58:55,720
sort of parameter comparing the

1615
00:58:55,720 --> 00:58:57,580
controlling the complexity of the model

1616
00:58:57,580 --> 00:59:01,030
in general as we add sample size the

1617
00:59:01,030 --> 00:59:02,680
complexity will be forced to increase

1618
00:59:02,680 --> 00:59:05,190
because the model isn't exactly right

1619
00:59:05,190 --> 00:59:07,990
here's a nice example to flow cytometry

1620
00:59:07,990 --> 00:59:11,290
clustering so each sample has 3 to 20

1621
00:59:11,290 --> 00:59:12,940
dimensional measurements on tens of

1622
00:59:12,940 --> 00:59:15,550
thousands of cells and here's like just

1623
00:59:15,550 --> 00:59:18,100
kind of two dimensions so shown and so

1624
00:59:18,100 --> 00:59:19,810
here's like clusters these are actually

1625
00:59:19,810 --> 00:59:21,730
correspond to concretely two different

1626
00:59:21,730 --> 00:59:26,050
cell types and this is based on manual

1627
00:59:26,050 --> 00:59:27,880
gating and so somebody goes in and

1628
00:59:27,880 --> 00:59:30,940
labels the data and then here if we use

1629
00:59:30,940 --> 00:59:32,830
a mixture of gaussians it'll break up

1630
00:59:32,830 --> 00:59:34,570
these like very non Gaussian looking but

1631
00:59:34,570 --> 00:59:36,330
you know may be approximately Gaussian

1632
00:59:36,330 --> 00:59:38,440
into different sub clusters and over

1633
00:59:38,440 --> 00:59:39,960
cluster

1634
00:59:39,960 --> 00:59:42,400
okay so we'd like to do it automatically

1635
00:59:42,400 --> 00:59:44,860
but then these are the kind of usual

1636
00:59:44,860 --> 00:59:46,840
model based clustering methods don't

1637
00:59:46,840 --> 00:59:48,850
don't work very well and distance based

1638
00:59:48,850 --> 00:59:51,030
clustering also doesn't work very well

1639
00:59:51,030 --> 00:59:54,730
okay so what can we do so if the model

1640
00:59:54,730 --> 00:59:56,080
is wrong why don't we just fix it well

1641
00:59:56,080 --> 00:59:58,030
what we could do in this case is like

1642
00:59:58,030 --> 00:59:59,560
well let's instead of using a mixture of

1643
00:59:59,560 --> 01:00:00,820
gaussians that come up with like a

1644
01:00:00,820 --> 01:00:02,470
mixture of some strange kernel and

1645
01:00:02,470 --> 01:00:04,750
there's a literature on that but it's

1646
01:00:04,750 --> 01:00:06,250
kind of hard to do that you know come up

1647
01:00:06,250 --> 01:00:07,960
with this weird kind of complicated

1648
01:00:07,960 --> 01:00:09,880
flexible kernels and then we run into

1649
01:00:09,880 --> 01:00:12,550
computational problems for you know

1650
01:00:12,550 --> 01:00:13,930
implementing these models with weird

1651
01:00:13,930 --> 01:00:15,520
flexible kernels and so we maybe don't

1652
01:00:15,520 --> 01:00:17,560
want to do that

1653
01:00:17,560 --> 01:00:23,380
yes also the simple model might be

1654
01:00:23,380 --> 01:00:25,690
inappropriate more appropriate even if

1655
01:00:25,690 --> 01:00:28,480
it's slightly wrong you know it might be

1656
01:00:28,480 --> 01:00:30,340
that that you know the data are always

1657
01:00:30,340 --> 01:00:31,810
noisy it might even be that the mixture

1658
01:00:31,810 --> 01:00:33,190
of gaussians is right but we just have

1659
01:00:33,190 --> 01:00:35,110
some noise in the data contamination in

1660
01:00:35,110 --> 01:00:37,570
the data somebody's you know typed in

1661
01:00:37,570 --> 01:00:38,770
some observations that are slightly

1662
01:00:38,770 --> 01:00:40,300
wrong or there's some machine

1663
01:00:40,300 --> 01:00:42,520
calibration issue and so we'd like to

1664
01:00:42,520 --> 01:00:44,110
like allow first light did good degree

1665
01:00:44,110 --> 01:00:47,680
of can take contamination okay so what

1666
01:00:47,680 --> 01:00:49,660
we proposed this paper just come in

1667
01:00:49,660 --> 01:00:52,480
Jassa is as follows and so we call it C

1668
01:00:52,480 --> 01:00:55,300
Bayes or a course and posterior maybe

1669
01:00:55,300 --> 01:00:57,000
it's not a very sexy name but see Bayes

1670
01:00:57,000 --> 01:00:59,530
so here's the kind of setting and so

1671
01:00:59,530 --> 01:01:01,810
we're gonna assume a model P of theta in

1672
01:01:01,810 --> 01:01:05,140
a prior PI of theta okay so we're gonna

1673
01:01:05,140 --> 01:01:07,150
say theta I represents the idealized

1674
01:01:07,150 --> 01:01:08,920
distribution of the data so that's over

1675
01:01:08,920 --> 01:01:14,200
here so theta I is the true state of

1676
01:01:14,200 --> 01:01:16,150
nature about which one's interested in

1677
01:01:16,150 --> 01:01:19,480
making inferences okay but then you know

1678
01:01:19,480 --> 01:01:22,930
we're gonna say that the the x1 to xn or

1679
01:01:22,930 --> 01:01:25,180
let's just say for now or iid you don't

1680
01:01:25,180 --> 01:01:26,590
have to have iid in any of these methods

1681
01:01:26,590 --> 01:01:31,630
or unobserved idealized data okay but

1682
01:01:31,630 --> 01:01:34,240
but the observed data are actually

1683
01:01:34,240 --> 01:01:35,950
slightly corrupted version and so our

1684
01:01:35,950 --> 01:01:37,450
you know our observed data are never

1685
01:01:37,450 --> 01:01:39,010
perfect so we're saying like well

1686
01:01:39,010 --> 01:01:40,360
there's these idealized data that we

1687
01:01:40,360 --> 01:01:42,790
could have generated but it that would

1688
01:01:42,790 --> 01:01:44,410
be under these very perfect conditions

1689
01:01:44,410 --> 01:01:46,330
in reality we have these like

1690
01:01:46,330 --> 01:01:48,040
error-prone conditions where we observe

1691
01:01:48,040 --> 01:01:50,860
data little X 1 xn instead of idealized

1692
01:01:50,860 --> 01:01:51,720
data at

1693
01:01:51,720 --> 01:01:55,970
x1 to xn okay but you know we think that

1694
01:01:55,970 --> 01:01:59,099
that the the statistics um deviance or

1695
01:01:59,099 --> 01:02:02,340
statistical distribution its are closed

1696
01:02:02,340 --> 01:02:06,330
for the observed data little x1 xn and

1697
01:02:06,330 --> 01:02:09,300
the corrupted data okay and so that's

1698
01:02:09,300 --> 01:02:11,520
the game so we're gonna say if there

1699
01:02:11,520 --> 01:02:12,690
were no corruption we should use the

1700
01:02:12,690 --> 01:02:14,400
standard posterior that would be PI of

1701
01:02:14,400 --> 01:02:17,099
theta given given big ax is equal to

1702
01:02:17,099 --> 01:02:20,070
little X okay but due to corruption that

1703
01:02:20,070 --> 01:02:21,270
would be clearly incorrect and that

1704
01:02:21,270 --> 01:02:23,190
causes the problems with with with you

1705
01:02:23,190 --> 01:02:24,599
know too many mixture components and the

1706
01:02:24,599 --> 01:02:26,160
model growing in complexity with sample

1707
01:02:26,160 --> 01:02:29,310
size so instead what we could do is we

1708
01:02:29,310 --> 01:02:31,170
could condition on what is known which

1709
01:02:31,170 --> 01:02:33,690
is just PI of theta given some sort of

1710
01:02:33,690 --> 01:02:37,290
the distribution of the data the

1711
01:02:37,290 --> 01:02:38,730
idealized data and the observed data

1712
01:02:38,730 --> 01:02:40,410
those two things are close they're

1713
01:02:40,410 --> 01:02:42,390
within R so we can define some sort of

1714
01:02:42,390 --> 01:02:44,700
discrepancy between the empirical

1715
01:02:44,700 --> 01:02:46,800
distributions of the of the data

1716
01:02:46,800 --> 01:02:49,440
observed and the data idealized data

1717
01:02:49,440 --> 01:02:52,770
generated from the assumed model okay so

1718
01:02:52,770 --> 01:02:54,150
this seems kind of complicated but it'll

1719
01:02:54,150 --> 01:02:55,470
it'll actually turn into something quite

1720
01:02:55,470 --> 01:02:58,740
simple so R might be different difficult

1721
01:02:58,740 --> 01:03:00,780
to choose and so R is a sort of tuning

1722
01:03:00,780 --> 01:03:03,030
parameter controlling the kind of model

1723
01:03:03,030 --> 01:03:05,070
mismatch or model MS specification in

1724
01:03:05,070 --> 01:03:06,720
some sense and we could put a prior and

1725
01:03:06,720 --> 01:03:09,359
there are and and then we could have

1726
01:03:09,359 --> 01:03:10,980
some sort of class of posterior

1727
01:03:10,980 --> 01:03:14,550
distributions okay so what happens is

1728
01:03:14,550 --> 01:03:16,260
that that we could you well you know to

1729
01:03:16,260 --> 01:03:17,760
do this in practice we need to choose

1730
01:03:17,760 --> 01:03:21,030
the D a discrepancy as well as the R so

1731
01:03:21,030 --> 01:03:23,430
if we if we do something in particular

1732
01:03:23,430 --> 01:03:25,770
choosing relative entropy and we go

1733
01:03:25,770 --> 01:03:27,599
through some mathematical calculations

1734
01:03:27,599 --> 01:03:30,540
then we can show that the the court the

1735
01:03:30,540 --> 01:03:32,609
C posterior distribution which is

1736
01:03:32,609 --> 01:03:34,800
conditioning on on this kind of event it

1737
01:03:34,800 --> 01:03:37,830
is is it's very close to this guy okay

1738
01:03:37,830 --> 01:03:40,080
and so this guy is what so this is the

1739
01:03:40,080 --> 01:03:41,880
the PI of theta that's the prior

1740
01:03:41,880 --> 01:03:44,010
distribution of theta and now this is

1741
01:03:44,010 --> 01:03:45,240
what we have in place of the likelihood

1742
01:03:45,240 --> 01:03:48,480
and like we have you know there would be

1743
01:03:48,480 --> 01:03:50,460
no no no that wouldn't be there at all

1744
01:03:50,460 --> 01:03:51,930
if we just had the regular likelihood

1745
01:03:51,930 --> 01:03:53,640
and if the model was perfectly specified

1746
01:03:53,640 --> 01:03:55,650
but instead due to the due to the miss

1747
01:03:55,650 --> 01:03:57,180
specification we've raised to some power

1748
01:03:57,180 --> 01:04:00,180
Zeta n okay each of the observations and

1749
01:04:00,180 --> 01:04:02,880
the Zeta n is just alpha over alpha plus

1750
01:04:02,880 --> 01:04:05,000
ad okay and so this we

1751
01:04:05,000 --> 01:04:06,830
taking this likelihood we've raised it

1752
01:04:06,830 --> 01:04:09,020
to a power and raising it to the power

1753
01:04:09,020 --> 01:04:10,790
is gonna automatically give us more

1754
01:04:10,790 --> 01:04:12,800
robustness okay if we use this power

1755
01:04:12,800 --> 01:04:15,620
okay and then we can you know as in the

1756
01:04:15,620 --> 01:04:18,500
EPM CMC algorithms where we had a power

1757
01:04:18,500 --> 01:04:20,600
but the power was motivated by getting a

1758
01:04:20,600 --> 01:04:23,210
good posterior proximation and parallel

1759
01:04:23,210 --> 01:04:25,610
inference here we instead have a power

1760
01:04:25,610 --> 01:04:28,490
that's motivated by robustness in large

1761
01:04:28,490 --> 01:04:32,600
sample sizes okay so the power posterior

1762
01:04:32,600 --> 01:04:33,980
labels inference using standard

1763
01:04:33,980 --> 01:04:35,900
techniques we can if we can use

1764
01:04:35,900 --> 01:04:38,300
conjugate priors or we can also use MCMC

1765
01:04:38,300 --> 01:04:40,310
just using this some likelihood raised

1766
01:04:40,310 --> 01:04:43,190
to a particular power okay so i'll

1767
01:04:43,190 --> 01:04:44,810
illustrate this kind of model miss

1768
01:04:44,810 --> 01:04:46,460
specification issue with just this

1769
01:04:46,460 --> 01:04:48,590
really simple coin flipping example so

1770
01:04:48,590 --> 01:04:50,990
let's say our interest was in testing

1771
01:04:50,990 --> 01:04:54,800
whether or not the coin was unbiased or

1772
01:04:54,800 --> 01:04:56,840
not so we we flip the coin a lot of

1773
01:04:56,840 --> 01:04:58,970
times and our null hypothesis is that

1774
01:04:58,970 --> 01:05:01,730
theta is 0.5 so we have a 50% chance of

1775
01:05:01,730 --> 01:05:04,970
a head okay but then we say well the

1776
01:05:04,970 --> 01:05:06,680
date and practice are a little corrupted

1777
01:05:06,680 --> 01:05:08,510
and so they might say they behave like

1778
01:05:08,510 --> 01:05:11,840
Bernoulli 0.51 and said of point 5 so

1779
01:05:11,840 --> 01:05:13,520
the null hypothesis was exactly true

1780
01:05:13,520 --> 01:05:16,430
would be 0.5 but it's 0.5 1 and our data

1781
01:05:16,430 --> 01:05:19,490
we observe so our C posterior is robust

1782
01:05:19,490 --> 01:05:21,200
to this but the standard posterior is

1783
01:05:21,200 --> 01:05:24,170
not you know and so the on the on the

1784
01:05:24,170 --> 01:05:26,810
x-axis is the sample size so the sample

1785
01:05:26,810 --> 01:05:29,300
size is blowing up and on the y-axis is

1786
01:05:29,300 --> 01:05:31,010
the posterior probability of the null

1787
01:05:31,010 --> 01:05:33,350
hypothesis given the data okay

1788
01:05:33,350 --> 01:05:34,970
and so and technically the null

1789
01:05:34,970 --> 01:05:37,520
hypothesis isn't true because it's 0.5 1

1790
01:05:37,520 --> 01:05:40,070
but it's really close to true and so

1791
01:05:40,070 --> 01:05:42,680
what we get is we get that you know as

1792
01:05:42,680 --> 01:05:44,180
the sample size increases initially

1793
01:05:44,180 --> 01:05:46,250
we're getting up the troop the exact

1794
01:05:46,250 --> 01:05:47,630
posteriors hat looks like it has

1795
01:05:47,630 --> 01:05:49,760
evidence building up for the null

1796
01:05:49,760 --> 01:05:51,530
hypothesis and then that but then at

1797
01:05:51,530 --> 01:05:54,410
some point it drops off okay because

1798
01:05:54,410 --> 01:05:55,940
then the data are big enough to show

1799
01:05:55,940 --> 01:05:58,640
that well actually 0.5 1 isn't isn't 0.5

1800
01:05:58,640 --> 01:06:04,610
ok and we get this kind of problem we

1801
01:06:04,610 --> 01:06:06,890
also want wanted to see whether we could

1802
01:06:06,890 --> 01:06:08,600
do the same thing in mixture models to

1803
01:06:08,600 --> 01:06:10,370
many other models and so let's say we

1804
01:06:10,370 --> 01:06:13,070
have a model x1 through xn they're iid

1805
01:06:13,070 --> 01:06:15,530
from some kernel mixture and we do a

1806
01:06:15,530 --> 01:06:17,240
Bayesian thing and so we put a prior on

1807
01:06:17,240 --> 01:06:18,510
the different parameters

1808
01:06:18,510 --> 01:06:20,610
and then we have this see posterior

1809
01:06:20,610 --> 01:06:22,650
approximation okay and then we can run

1810
01:06:22,650 --> 01:06:25,530
MCMC so we could run MC MC for a mixture

1811
01:06:25,530 --> 01:06:27,960
model for for model based clustering and

1812
01:06:27,960 --> 01:06:31,410
compare our RC posterior or remote more

1813
01:06:31,410 --> 01:06:34,260
robust posterior to the exact one and we

1814
01:06:34,260 --> 01:06:36,000
have an M stamps algorithm that we could

1815
01:06:36,000 --> 01:06:38,610
easily implement it for quite large data

1816
01:06:38,610 --> 01:06:40,770
sets and then we could further scale up

1817
01:06:40,770 --> 01:06:42,560
using our previous tricks

1818
01:06:42,560 --> 01:06:45,390
okay so here's just some results and so

1819
01:06:45,390 --> 01:06:47,220
here I go back to that perturb mixture

1820
01:06:47,220 --> 01:06:49,590
of gaussians where the true data

1821
01:06:49,590 --> 01:06:51,510
generating model we've simulated is very

1822
01:06:51,510 --> 01:06:54,660
close to a mixture of two gaussians but

1823
01:06:54,660 --> 01:06:56,010
they're slightly perturbed and then if

1824
01:06:56,010 --> 01:06:58,080
we use a standard posterior we keep

1825
01:06:58,080 --> 01:06:59,820
adding Gaussian components as the sample

1826
01:06:59,820 --> 01:07:02,880
size goes up but if we use the the the C

1827
01:07:02,880 --> 01:07:04,560
posterior then that doesn't happen at

1828
01:07:04,560 --> 01:07:07,350
all actually we just we get consistent

1829
01:07:07,350 --> 01:07:09,000
inferences as the sample size goes up we

1830
01:07:09,000 --> 01:07:10,680
we're always choosing two components

1831
01:07:10,680 --> 01:07:15,150
okay I mean that also happens it happens

1832
01:07:15,150 --> 01:07:17,160
if we use four components and we perturb

1833
01:07:17,160 --> 01:07:18,840
those as well the same the same kind of

1834
01:07:18,840 --> 01:07:22,230
deal and we also got beautiful results

1835
01:07:22,230 --> 01:07:24,330
in this quite important applied flow

1836
01:07:24,330 --> 01:07:27,060
cytometry clustering data becomes much

1837
01:07:27,060 --> 01:07:28,800
more robust the clustering to the shape

1838
01:07:28,800 --> 01:07:30,270
of the kernel miss specification and the

1839
01:07:30,270 --> 01:07:33,150
shape of the kernel okay and we we

1840
01:07:33,150 --> 01:07:36,150
actually had a label data set and from

1841
01:07:36,150 --> 01:07:37,830
people going in manually and doing this

1842
01:07:37,830 --> 01:07:40,320
manually gate gating to label the data

1843
01:07:40,320 --> 01:07:42,240
and we had a whole bunch of different

1844
01:07:42,240 --> 01:07:44,390
label data sets 7 through 12 here and

1845
01:07:44,390 --> 01:07:46,860
looking at an F measure we did you know

1846
01:07:46,860 --> 01:07:49,050
dramatically better than using a usual

1847
01:07:49,050 --> 01:07:51,630
um Bayesian posterior distribution in

1848
01:07:51,630 --> 01:08:01,170
terms of clustering okay so this C Bayes

1849
01:08:01,170 --> 01:08:02,490
provides a framework for improving

1850
01:08:02,490 --> 01:08:04,350
robustness to model miss specification

1851
01:08:04,350 --> 01:08:06,750
it's particularly useful when when

1852
01:08:06,750 --> 01:08:08,700
interest is in model-based inferences

1853
01:08:08,700 --> 01:08:10,470
instead of some black box we want to

1854
01:08:10,470 --> 01:08:12,180
interpret the parameters for example and

1855
01:08:12,180 --> 01:08:15,840
the sample size is large it can be

1856
01:08:15,840 --> 01:08:17,970
implemented quite easily using MCMC and

1857
01:08:17,970 --> 01:08:19,439
those scalable MCMC tricks we talked

1858
01:08:19,439 --> 01:08:21,410
about earlier okay

1859
01:08:21,410 --> 01:08:24,359
okay I'm just gonna skip that okay so um

1860
01:08:24,359 --> 01:08:26,970
so in the remaining time I'd like to

1861
01:08:26,970 --> 01:08:28,950
just transition to talking about high

1862
01:08:28,950 --> 01:08:31,649
dimensional data okay or big P

1863
01:08:31,649 --> 01:08:35,049
so so we focused on so far on solving

1864
01:08:35,049 --> 01:08:36,819
computational robustness problems

1865
01:08:36,819 --> 01:08:39,309
arising in large n and I would say

1866
01:08:39,309 --> 01:08:41,649
really many ways these problems are

1867
01:08:41,649 --> 01:08:43,538
quite easier to deal with than then

1868
01:08:43,538 --> 01:08:45,339
issues with high dimensional complicated

1869
01:08:45,339 --> 01:08:47,979
data so that that is really I would say

1870
01:08:47,979 --> 01:08:50,380
the most important problem moving

1871
01:08:50,380 --> 01:08:52,389
forward I think in statistics and

1872
01:08:52,389 --> 01:08:54,399
machine learning is how the hell do we

1873
01:08:54,399 --> 01:08:56,049
deal with all these complicated data

1874
01:08:56,049 --> 01:08:58,149
where we don't have a giant sample size

1875
01:08:58,149 --> 01:08:59,679
you know all this stuff with deep

1876
01:08:59,679 --> 01:09:00,908
learning and everything's been very

1877
01:09:00,908 --> 01:09:02,769
exciting but usually in those settings

1878
01:09:02,769 --> 01:09:04,359
we have really structured data and we

1879
01:09:04,359 --> 01:09:05,948
have a live we have a big sample size we

1880
01:09:05,948 --> 01:09:08,710
have a lot of labels etc well what if we

1881
01:09:08,710 --> 01:09:10,509
have you know data that aren't so

1882
01:09:10,509 --> 01:09:12,279
structured or the structure is not so

1883
01:09:12,279 --> 01:09:14,649
clear and the sample size is enormous I

1884
01:09:14,649 --> 01:09:16,179
mean the sample size isn't that big but

1885
01:09:16,179 --> 01:09:17,769
the dimension of the data is enormous

1886
01:09:17,769 --> 01:09:19,988
and so for each patient in the study

1887
01:09:19,988 --> 01:09:22,839
I've measured you know a billion

1888
01:09:22,839 --> 01:09:24,908
different omics things about them or

1889
01:09:24,908 --> 01:09:28,328
I've you know taken brain scans and

1890
01:09:28,328 --> 01:09:29,948
neuroscience and measured like their

1891
01:09:29,948 --> 01:09:32,408
entire brain connectome you know and we

1892
01:09:32,408 --> 01:09:34,089
keep getting more and more and more

1893
01:09:34,089 --> 01:09:36,309
higher resolution and better measuring

1894
01:09:36,309 --> 01:09:38,710
technologies for massive dimensional

1895
01:09:38,710 --> 01:09:40,439
data but we really don't have

1896
01:09:40,439 --> 01:09:42,670
statistical tools for making sense of it

1897
01:09:42,670 --> 01:09:44,859
and often what happens is this is

1898
01:09:44,859 --> 01:09:46,809
complete garbage in my mind in some

1899
01:09:46,809 --> 01:09:49,089
sense is that okay well I have some data

1900
01:09:49,089 --> 01:09:51,158
set I have some outcome Y and I have

1901
01:09:51,158 --> 01:09:52,899
some features X and the X is really high

1902
01:09:52,899 --> 01:09:54,639
dimensional and I don't have that

1903
01:09:54,639 --> 01:09:57,369
biggest sample size and I'm doing basic

1904
01:09:57,369 --> 01:09:58,750
science or I'm doing medical research

1905
01:09:58,750 --> 01:10:01,869
I'm doing neuroscience and I go in and I

1906
01:10:01,869 --> 01:10:03,940
and I fit some machine learning

1907
01:10:03,940 --> 01:10:06,280
algorithm random forests or a neural

1908
01:10:06,280 --> 01:10:08,559
network or I do lasso or something or

1909
01:10:08,559 --> 01:10:10,719
elastic net and I get a lot of variables

1910
01:10:10,719 --> 01:10:13,090
that are spit out and the scientists are

1911
01:10:13,090 --> 01:10:15,340
gonna like interpret those variables you

1912
01:10:15,340 --> 01:10:17,559
know and then really like that's really

1913
01:10:17,559 --> 01:10:19,480
unreliable and we're running into like a

1914
01:10:19,480 --> 01:10:22,179
replicability crisis and science and so

1915
01:10:22,179 --> 01:10:25,300
we'd like to be able to say in a lot of

1916
01:10:25,300 --> 01:10:26,980
cases I think that we'd like to be able

1917
01:10:26,980 --> 01:10:29,920
to say that actually you know scientists

1918
01:10:29,920 --> 01:10:31,690
we can't do what you'd like us to do

1919
01:10:31,690 --> 01:10:34,360
you've given us a billion predictors and

1920
01:10:34,360 --> 01:10:37,260
you've run this study on ten mice or

1921
01:10:37,260 --> 01:10:40,420
twenty patients we can't actually decide

1922
01:10:40,420 --> 01:10:41,650
which of those predictors are the

1923
01:10:41,650 --> 01:10:43,690
important ones that's just not going to

1924
01:10:43,690 --> 01:10:45,370
work there's an

1925
01:10:45,370 --> 01:10:47,230
we're on the wrong side of some horrible

1926
01:10:47,230 --> 01:10:49,270
face transition but a lot of the

1927
01:10:49,270 --> 01:10:51,220
algorithms in the literature is focusing

1928
01:10:51,220 --> 01:10:52,870
on giving us positive results you know

1929
01:10:52,870 --> 01:10:54,340
like I stick it in this optimization

1930
01:10:54,340 --> 01:10:56,530
algorithm here's my features and it

1931
01:10:56,530 --> 01:10:59,290
doesn't like flag and say actually all

1932
01:10:59,290 --> 01:11:00,760
your features are probably almost surely

1933
01:11:00,760 --> 01:11:03,310
wrong you know like you've got all false

1934
01:11:03,310 --> 01:11:04,780
pause in the false negatives because the

1935
01:11:04,780 --> 01:11:06,430
problems too big you have too high

1936
01:11:06,430 --> 01:11:08,620
correlation the dimension is too high

1937
01:11:08,620 --> 01:11:11,290
and so I want better ways to deal with

1938
01:11:11,290 --> 01:11:12,820
uncertainty quantification and give me

1939
01:11:12,820 --> 01:11:14,320
negative results give me realistic

1940
01:11:14,320 --> 01:11:17,140
results give me ways to like course and

1941
01:11:17,140 --> 01:11:19,780
the questions being asked so that we can

1942
01:11:19,780 --> 01:11:21,460
get something much more reliable and

1943
01:11:21,460 --> 01:11:23,200
reproducible in these high dimensional

1944
01:11:23,200 --> 01:11:25,000
settings there's there a way to like

1945
01:11:25,000 --> 01:11:26,380
learn Labette or low dimensional

1946
01:11:26,380 --> 01:11:28,270
structures and more reliably okay

1947
01:11:28,270 --> 01:11:29,800
and so that's some really beyond

1948
01:11:29,800 --> 01:11:32,110
Bayesian inference but I would just like

1949
01:11:32,110 --> 01:11:34,060
give a plea for many of you to kind of

1950
01:11:34,060 --> 01:11:35,830
try to work on these types of problems

1951
01:11:35,830 --> 01:11:39,310
with me not with me particularly but you

1952
01:11:39,310 --> 01:11:42,670
do it and then tell me how to do it okay

1953
01:11:42,670 --> 01:11:44,950
so that's the yeah so these are types of

1954
01:11:44,950 --> 01:11:47,080
things I work on all the time and so we

1955
01:11:47,080 --> 01:11:48,700
have very few label data relative to

1956
01:11:48,700 --> 01:11:51,580
data dimensionality and and it's really

1957
01:11:51,580 --> 01:11:53,560
important as well that you know even

1958
01:11:53,560 --> 01:11:54,790
though you're like oh I'm in some

1959
01:11:54,790 --> 01:11:56,620
medical context I'd like to say diagnose

1960
01:11:56,620 --> 01:11:58,870
a patient or cell to tell me which which

1961
01:11:58,870 --> 01:12:01,480
treatment to give the patient but we

1962
01:12:01,480 --> 01:12:02,710
don't really want a black box for

1963
01:12:02,710 --> 01:12:04,450
prediction no doctor is gonna use

1964
01:12:04,450 --> 01:12:06,040
something that just says oh we should do

1965
01:12:06,040 --> 01:12:07,450
this through this patient they want to

1966
01:12:07,450 --> 01:12:10,360
know why well how is it working you know

1967
01:12:10,360 --> 01:12:12,310
what features are important etc we often

1968
01:12:12,310 --> 01:12:13,780
want to do some version of variable

1969
01:12:13,780 --> 01:12:15,000
selection

1970
01:12:15,000 --> 01:12:17,410
okay so Bayes for big P I'd say is a

1971
01:12:17,410 --> 01:12:20,350
huge topic I'm just gonna provide some

1972
01:12:20,350 --> 01:12:22,180
vignettes to give a flavor in the

1973
01:12:22,180 --> 01:12:23,770
remaining kind of 30 minutes before we

1974
01:12:23,770 --> 01:12:27,910
run out of time okay so um so one of the

1975
01:12:27,910 --> 01:12:29,830
kind of very popular things in these

1976
01:12:29,830 --> 01:12:32,050
types of scientific fields that I've

1977
01:12:32,050 --> 01:12:33,670
just been talking about is to do some

1978
01:12:33,670 --> 01:12:35,140
type of variable or a feature selection

1979
01:12:35,140 --> 01:12:39,160
that's all often the focus okay so um

1980
01:12:39,160 --> 01:12:41,440
you know one example would be I have

1981
01:12:41,440 --> 01:12:43,690
some phenotype a response variable for a

1982
01:12:43,690 --> 01:12:46,180
patient Y some health response I work a

1983
01:12:46,180 --> 01:12:47,830
lot on cancer genomics so this might be

1984
01:12:47,830 --> 01:12:50,440
your subtype of cancer and then I have a

1985
01:12:50,440 --> 01:12:53,260
lot of genetic variants XJ associated

1986
01:12:53,260 --> 01:12:55,180
with that response okay but there might

1987
01:12:55,180 --> 01:12:56,770
be you know now I have all of these like

1988
01:12:56,770 --> 01:12:58,810
a whole genome sequencing and all this

1989
01:12:58,810 --> 01:12:59,110
stuff

1990
01:12:59,110 --> 01:13:00,370
and so there might be a lot of these

1991
01:13:00,370 --> 01:13:03,370
guys the sample size is going to be

1992
01:13:03,370 --> 01:13:05,020
modest the number of genetic variants is

1993
01:13:05,020 --> 01:13:07,210
huge so we have this large piece small

1994
01:13:07,210 --> 01:13:09,820
end problem so what do we do so what

1995
01:13:09,820 --> 01:13:12,160
mostly what people do you know beyond

1996
01:13:12,160 --> 01:13:13,930
this kind of Bayes vs. frequentist but

1997
01:13:13,930 --> 01:13:15,880
mostly what people do are two main

1998
01:13:15,880 --> 01:13:17,740
approaches the most popular I would say

1999
01:13:17,740 --> 01:13:19,060
is is what's called independent

2000
01:13:19,060 --> 01:13:21,280
screening and so we might like look at

2001
01:13:21,280 --> 01:13:23,800
some sort of statistical test for

2002
01:13:23,800 --> 01:13:26,710
association between yxj we're gonna do

2003
01:13:26,710 --> 01:13:29,020
that separately for each J I'm gonna get

2004
01:13:29,020 --> 01:13:30,790
a p-value and get a billion people use

2005
01:13:30,790 --> 01:13:33,010
and then threshold them or something the

2006
01:13:33,010 --> 01:13:34,900
other type of approach people use often

2007
01:13:34,900 --> 01:13:36,640
is some sort of penalize a scaleable

2008
01:13:36,640 --> 01:13:39,340
penalized estimation or shrinkage lasso

2009
01:13:39,340 --> 01:13:42,240
elastic net etc being popular examples

2010
01:13:42,240 --> 01:13:45,040
okay so um so we test for association

2011
01:13:45,040 --> 01:13:46,270
between two variables at the time a

2012
01:13:46,270 --> 01:13:48,640
phenotype in a snip we repeat this for

2013
01:13:48,640 --> 01:13:51,130
all possible pairs we get a third number

2014
01:13:51,130 --> 01:13:53,560
of p-values and then we choose some sort

2015
01:13:53,560 --> 01:13:54,670
of p-value threshold

2016
01:13:54,670 --> 01:13:56,500
controlling the what's called the false

2017
01:13:56,500 --> 01:13:58,780
discovery rate I'm using for example

2018
01:13:58,780 --> 01:14:00,310
this Benjen benjamine Ian Hochberg

2019
01:14:00,310 --> 01:14:03,040
threshold okay and then we get a list of

2020
01:14:03,040 --> 01:14:04,570
discoveries so we've taken like this

2021
01:14:04,570 --> 01:14:06,460
huge number of biomarkers and we've

2022
01:14:06,460 --> 01:14:08,440
reduced it to some number and then now

2023
01:14:08,440 --> 01:14:10,030
we can maybe run follow-up Studies on

2024
01:14:10,030 --> 01:14:14,020
that number to verify okay so um so this

2025
01:14:14,020 --> 01:14:15,730
is really really really really commonly

2026
01:14:15,730 --> 01:14:17,680
used it's very appealing in its

2027
01:14:17,680 --> 01:14:19,870
simplicity it's quite scalable we can do

2028
01:14:19,870 --> 01:14:21,340
this in parallel we're just doing a

2029
01:14:21,340 --> 01:14:22,780
simple thing on little pieces of the

2030
01:14:22,780 --> 01:14:25,300
data separately and so it's white quite

2031
01:14:25,300 --> 01:14:27,730
nice in that sense it's really widely

2032
01:14:27,730 --> 01:14:31,000
used there's a lot of false positives

2033
01:14:31,000 --> 01:14:33,370
and negatives obviously for sparse data

2034
01:14:33,370 --> 01:14:35,410
it's a it's a really big problem we

2035
01:14:35,410 --> 01:14:38,550
might have no power to detect anything

2036
01:14:38,550 --> 01:14:40,840
just looking at a pair of variables at a

2037
01:14:40,840 --> 01:14:43,240
time leads to limited insights so let's

2038
01:14:43,240 --> 01:14:44,770
kind of go into the the weeds here a

2039
01:14:44,770 --> 01:14:46,030
little bit and so let's say we have a

2040
01:14:46,030 --> 01:14:48,940
linear regression model okay so we have

2041
01:14:48,940 --> 01:14:51,640
some response Y I on patient I and then

2042
01:14:51,640 --> 01:14:53,410
we just say model the effect of all the

2043
01:14:53,410 --> 01:14:55,720
biomarkers with some X I prime beta a

2044
01:14:55,720 --> 01:14:58,240
bunch of features genetic features and

2045
01:14:58,240 --> 01:15:00,400
then their coefficients or beta and

2046
01:15:00,400 --> 01:15:01,900
let's just put a little Gaussian error

2047
01:15:01,900 --> 01:15:05,440
on there okay and so um if we were you

2048
01:15:05,440 --> 01:15:07,480
know around awhile ago before we started

2049
01:15:07,480 --> 01:15:09,130
making P really big we might just have

2050
01:15:09,130 --> 01:15:10,600
done linear regression or least squares

2051
01:15:10,600 --> 01:15:12,850
okay and we would just get beta hat

2052
01:15:12,850 --> 01:15:14,890
as X prime X inverse X prime Y okay

2053
01:15:14,890 --> 01:15:17,050
everything's good because n is bigger

2054
01:15:17,050 --> 01:15:18,520
than much bigger than P and we can

2055
01:15:18,520 --> 01:15:19,930
estimate these coefficients reliably

2056
01:15:19,930 --> 01:15:23,230
using least squares but what happens as

2057
01:15:23,230 --> 01:15:25,870
P increases or the x x i's become more

2058
01:15:25,870 --> 01:15:27,520
correlated the variance of that beta

2059
01:15:27,520 --> 01:15:28,990
hats going to blow up and it's not going

2060
01:15:28,990 --> 01:15:31,780
to be a very good estimator okay and so

2061
01:15:31,780 --> 01:15:33,190
if P is bigger than n you're that's

2062
01:15:33,190 --> 01:15:35,440
really not even going to exist so we

2063
01:15:35,440 --> 01:15:37,210
need to include some sort of outsider

2064
01:15:37,210 --> 01:15:39,640
prior information and doing this okay

2065
01:15:39,640 --> 01:15:41,740
and so in a Bayesian approach what would

2066
01:15:41,740 --> 01:15:43,210
we do well we would choose a prior

2067
01:15:43,210 --> 01:15:45,510
probability distribution PI of beta

2068
01:15:45,510 --> 01:15:47,350
characterizing our uncertainty in beta

2069
01:15:47,350 --> 01:15:49,240
prior to observing the current data and

2070
01:15:49,240 --> 01:15:52,180
then we would use our Bayes rule that I

2071
01:15:52,180 --> 01:15:53,680
described earlier to update the prior

2072
01:15:53,680 --> 01:15:55,060
with information in the likelihood so

2073
01:15:55,060 --> 01:15:56,470
we'd have a posterior distribution of

2074
01:15:56,470 --> 01:15:59,320
beta given our response data why are our

2075
01:15:59,320 --> 01:16:01,720
features X and we would just plug it

2076
01:16:01,720 --> 01:16:04,810
into Bayes rule as before okay so if we

2077
01:16:04,810 --> 01:16:06,250
did if we have a normal linear

2078
01:16:06,250 --> 01:16:08,070
regression model and we choose a

2079
01:16:08,070 --> 01:16:11,200
Gaussian prior then we have conjugacy

2080
01:16:11,200 --> 01:16:12,730
and we can just write down the posterior

2081
01:16:12,730 --> 01:16:14,670
distribution of beta in this simple form

2082
01:16:14,670 --> 01:16:17,350
it's just a Gaussian distribution and

2083
01:16:17,350 --> 01:16:19,360
the posterior covariance is just going

2084
01:16:19,360 --> 01:16:21,400
to be a part on this part contains

2085
01:16:21,400 --> 01:16:23,350
information in the prior this part

2086
01:16:23,350 --> 01:16:24,640
contains information in the likelihood

2087
01:16:24,640 --> 01:16:25,930
it combines those two sources of

2088
01:16:25,930 --> 01:16:28,900
information kind of shrinking back the

2089
01:16:28,900 --> 01:16:32,200
the the MLE towards towards zero

2090
01:16:32,200 --> 01:16:34,660
it's a type of shrinkage estimator ok so

2091
01:16:34,660 --> 01:16:36,040
then that show that's shown in the

2092
01:16:36,040 --> 01:16:39,430
posterior mean okay so we can get the

2093
01:16:39,430 --> 01:16:43,840
same the same answer by just solving

2094
01:16:43,840 --> 01:16:45,670
this penalized optimization problem

2095
01:16:45,670 --> 01:16:47,440
which many people in this audience I'm

2096
01:16:47,440 --> 01:16:48,700
sure I've seen a million of these types

2097
01:16:48,700 --> 01:16:50,560
of things and so we just have you know

2098
01:16:50,560 --> 01:16:52,480
we're minimizing some least squares plus

2099
01:16:52,480 --> 01:16:55,270
some penalty and if we had a Gaussian

2100
01:16:55,270 --> 01:16:57,250
prior that would correspond to a l2

2101
01:16:57,250 --> 01:17:00,460
penalty so this is known as Ridge or l2

2102
01:17:00,460 --> 01:17:02,890
panelized regression and so it has a

2103
01:17:02,890 --> 01:17:04,570
dual interpretation as a bayesian

2104
01:17:04,570 --> 01:17:06,190
estimator under Gaussian prior centered

2105
01:17:06,190 --> 01:17:08,140
at zero and a least squares estimator

2106
01:17:08,140 --> 01:17:09,940
with a penalty on large coefficients and

2107
01:17:09,940 --> 01:17:13,420
l2 l2 penalty okay so the game here is

2108
01:17:13,420 --> 01:17:16,030
to introduce some bias the maximum

2109
01:17:16,030 --> 01:17:17,710
relative to maximum likelihood estimator

2110
01:17:17,710 --> 01:17:19,450
while reducing the variance a lot to

2111
01:17:19,450 --> 01:17:21,460
improve mean Square and I'll enable

2112
01:17:21,460 --> 01:17:24,849
scaling to higher dimensional problems

2113
01:17:24,849 --> 01:17:27,940
okay so of course we can generalize that

2114
01:17:27,940 --> 01:17:30,130
to a broad class of penalize loss

2115
01:17:30,130 --> 01:17:31,570
functions where we have a least squares

2116
01:17:31,570 --> 01:17:34,270
part for good as a fit plus P of lambda

2117
01:17:34,270 --> 01:17:37,060
of beta and we that's a penalty term and

2118
01:17:37,060 --> 01:17:39,219
we could we could put we previously put

2119
01:17:39,219 --> 01:17:41,380
l2 but we could put on an l1 penalty to

2120
01:17:41,380 --> 01:17:43,630
have a lasso type procedure we could

2121
01:17:43,630 --> 01:17:45,940
week and if we did that there was a

2122
01:17:45,940 --> 01:17:48,639
question at the break about the bayesian

2123
01:17:48,639 --> 01:17:49,270
lasso

2124
01:17:49,270 --> 01:17:50,949
well the Bayesian lasso would just be

2125
01:17:50,949 --> 01:17:52,989
you know doing this but we would put a

2126
01:17:52,989 --> 01:17:54,849
prior on beta that would correspond to a

2127
01:17:54,849 --> 01:17:57,070
double exponential distribution and then

2128
01:17:57,070 --> 01:17:58,570
the mode of the posterior is going to be

2129
01:17:58,570 --> 01:17:59,739
exactly the solution to this

2130
01:17:59,739 --> 01:18:03,159
optimization problem okay the nice thing

2131
01:18:03,159 --> 01:18:04,810
is that that mode is then going to be

2132
01:18:04,810 --> 01:18:07,060
sparse and contains exactly row values

2133
01:18:07,060 --> 01:18:10,659
hence the popularity of the lasso okay

2134
01:18:10,659 --> 01:18:12,670
so um so there's a huge literature

2135
01:18:12,670 --> 01:18:14,849
proposing many different penalties

2136
01:18:14,849 --> 01:18:18,460
adaptive last abuse lasso blah blah blah

2137
01:18:18,460 --> 01:18:21,070
in general the methods only produce a

2138
01:18:21,070 --> 01:18:22,780
sparse point estimate and I would say

2139
01:18:22,780 --> 01:18:24,940
are dangerous scientifically and I work

2140
01:18:24,940 --> 01:18:27,480
a lot in real problems with scientists I

2141
01:18:27,480 --> 01:18:30,909
funded by NIH and working on real trying

2142
01:18:30,909 --> 01:18:33,310
to kind of solve health problems using

2143
01:18:33,310 --> 01:18:35,260
machine learning methods and statistical

2144
01:18:35,260 --> 01:18:38,619
methods and and a lot of the times these

2145
01:18:38,619 --> 01:18:40,210
methods kind of pull you in you do this

2146
01:18:40,210 --> 01:18:42,159
kind of sparse point estimation but then

2147
01:18:42,159 --> 01:18:43,750
you kind of run into problems due to the

2148
01:18:43,750 --> 01:18:46,320
lack of uncertainty quantification okay

2149
01:18:46,320 --> 01:18:48,520
let's say there's a parallel Bayesian

2150
01:18:48,520 --> 01:18:51,639
literature on shrinkage priors Bayes

2151
01:18:51,639 --> 01:18:53,440
lasso is actually not a very good

2152
01:18:53,440 --> 01:18:55,030
shrinkage prior I was talking about that

2153
01:18:55,030 --> 01:18:56,500
over the break it's kind of a really bad

2154
01:18:56,500 --> 01:18:58,929
shrinkage prior if we want to be able to

2155
01:18:58,929 --> 01:19:00,820
deal with a sparse problem where we have

2156
01:19:00,820 --> 01:19:02,860
you know very high dimensional

2157
01:19:02,860 --> 01:19:05,320
predictors then the mode of the

2158
01:19:05,320 --> 01:19:06,550
posterior is not something we

2159
01:19:06,550 --> 01:19:08,110
particularly focus on we care about the

2160
01:19:08,110 --> 01:19:10,150
full posterior distribution then we want

2161
01:19:10,150 --> 01:19:13,030
a prior that actually has concentration

2162
01:19:13,030 --> 01:19:15,010
near zero to shrink away the kind of

2163
01:19:15,010 --> 01:19:17,079
noise parts or the small signals and

2164
01:19:17,079 --> 01:19:19,480
then has heavy tails to avoid over

2165
01:19:19,480 --> 01:19:21,130
shrinking the large signals and Bayes

2166
01:19:21,130 --> 01:19:24,099
lasso has only one parameter controlling

2167
01:19:24,099 --> 01:19:25,329
the tails in the concentration around

2168
01:19:25,329 --> 01:19:27,429
zero so it's going to kind of over

2169
01:19:27,429 --> 01:19:30,159
shrink the the the signals away from

2170
01:19:30,159 --> 01:19:32,710
zero and then kind of could confound the

2171
01:19:32,710 --> 01:19:35,650
kind of number of zero signals with with

2172
01:19:35,650 --> 01:19:37,270
the number of non-zeros and so it

2173
01:19:37,270 --> 01:19:38,389
doesn't have enough

2174
01:19:38,389 --> 01:19:40,820
and some more the state of the art in

2175
01:19:40,820 --> 01:19:43,310
Bayes based rink it's priors for high

2176
01:19:43,310 --> 01:19:44,600
dimensional problems and we can apply

2177
01:19:44,600 --> 01:19:46,610
them much much more broadly than in

2178
01:19:46,610 --> 01:19:48,920
regression I would say horseshoe is the

2179
01:19:48,920 --> 01:19:51,170
most popular I'm and then I'm also

2180
01:19:51,170 --> 01:19:52,850
generalized double pret oh in dear sale

2181
01:19:52,850 --> 01:19:55,610
applause okay and these priors are are

2182
01:19:55,610 --> 01:19:57,020
designed to have this kind of more

2183
01:19:57,020 --> 01:19:59,989
control over the shrinkage at zero

2184
01:19:59,989 --> 01:20:03,560
versus the tails okay so I'm an

2185
01:20:03,560 --> 01:20:05,480
appropriate sanity prior for a high

2186
01:20:05,480 --> 01:20:07,550
dimensional vector of coefficients many

2187
01:20:07,550 --> 01:20:08,900
of the priors can be written in this

2188
01:20:08,900 --> 01:20:11,389
Nick pulled Polson had a nice paper on

2189
01:20:11,389 --> 01:20:13,460
this and this global local scale mixture

2190
01:20:13,460 --> 01:20:15,530
of gaussians framework where the beta

2191
01:20:15,530 --> 01:20:18,409
J's are drawn iid conditionally iid from

2192
01:20:18,409 --> 01:20:21,290
this normal 0 sy J lambda and then we

2193
01:20:21,290 --> 01:20:23,659
put some sort of prior on sy j and on

2194
01:20:23,659 --> 01:20:25,670
lambda so these are the local scales and

2195
01:20:25,670 --> 01:20:27,409
this is a global scale and when we would

2196
01:20:27,409 --> 01:20:29,210
choose an appropriate F and G this is

2197
01:20:29,210 --> 01:20:32,210
the kind of game and we would like to

2198
01:20:32,210 --> 01:20:34,790
choose lambda to be kind of close to

2199
01:20:34,790 --> 01:20:38,420
zero to correspond to global sparsity

2200
01:20:38,420 --> 01:20:40,840
and then then sigh J to have heavy tails

2201
01:20:40,840 --> 01:20:43,340
okay so we have them we have really a

2202
01:20:43,340 --> 01:20:45,530
rich literature now on these methods I'd

2203
01:20:45,530 --> 01:20:47,270
like to kind of just highlight one paper

2204
01:20:47,270 --> 01:20:50,000
which which is a you know biased a

2205
01:20:50,000 --> 01:20:51,530
really excellent student of mine James

2206
01:20:51,530 --> 01:20:53,119
John drew is now a Stein fellow at

2207
01:20:53,119 --> 01:20:55,250
Stanford and on the job market he has

2208
01:20:55,250 --> 01:20:56,810
this paper where he kind of shows that

2209
01:20:56,810 --> 01:21:00,290
you can take a horseshoe prior and you

2210
01:21:00,290 --> 01:21:03,350
can actually use some scalability tricks

2211
01:21:03,350 --> 01:21:05,150
to show that theoretically you can scale

2212
01:21:05,150 --> 01:21:07,070
up a prior to very high dimensional

2213
01:21:07,070 --> 01:21:09,980
problems theoretically and practically

2214
01:21:09,980 --> 01:21:12,199
and so you can now deal with certainly

2215
01:21:12,199 --> 01:21:13,639
hundreds of thousands of predictors

2216
01:21:13,639 --> 01:21:15,230
using these types of Markov chain Monte

2217
01:21:15,230 --> 01:21:17,719
Carlo algorithms there's a really not

2218
01:21:17,719 --> 01:21:20,119
another really nice paper is by Martin

2219
01:21:20,119 --> 01:21:21,949
Wainwright and Mike Jordan and a former

2220
01:21:21,949 --> 01:21:23,360
student of mine you and yang in the

2221
01:21:23,360 --> 01:21:25,280
annals of statistics show it kind of

2222
01:21:25,280 --> 01:21:27,040
have a similar result showing

2223
01:21:27,040 --> 01:21:29,659
scalability for spikes lab priors we

2224
01:21:29,659 --> 01:21:33,350
have a formal mass at zero Mik mixed

2225
01:21:33,350 --> 01:21:35,030
with another another distribution and

2226
01:21:35,030 --> 01:21:36,409
then you're doing a stochastic search in

2227
01:21:36,409 --> 01:21:39,409
a very high dimensional space they show

2228
01:21:39,409 --> 01:21:41,630
that that also can be designed to be to

2229
01:21:41,630 --> 01:21:43,820
be technically scalable and in their CS

2230
01:21:43,820 --> 01:21:48,159
sense okay

2231
01:21:48,260 --> 01:21:51,050
so features of a Bayesian approach so a

2232
01:21:51,050 --> 01:21:52,760
Bayesian approach provides a full

2233
01:21:52,760 --> 01:21:54,320
posterior distribution characterizing

2234
01:21:54,320 --> 01:21:55,850
uncertainty instead of just a sparse

2235
01:21:55,850 --> 01:21:58,070
point estimate beta hat and that I think

2236
01:21:58,070 --> 01:22:01,280
is really really really important and so

2237
01:22:01,280 --> 01:22:05,239
by using MCMC we can get we can get

2238
01:22:05,239 --> 01:22:07,610
credible bands and so those are like

2239
01:22:07,610 --> 01:22:09,170
Bayesian versions of confidence

2240
01:22:09,170 --> 01:22:11,000
intervals and so we can actually

2241
01:22:11,000 --> 01:22:12,560
characterize uncertainty so these going

2242
01:22:12,560 --> 01:22:14,420
to be confidence intervals for the beta

2243
01:22:14,420 --> 01:22:16,220
beta J's and for any functional of

2244
01:22:16,220 --> 01:22:17,210
interest like the predictive

2245
01:22:17,210 --> 01:22:19,460
distribution and it's relatively

2246
01:22:19,460 --> 01:22:20,660
straightforward to incorporate

2247
01:22:20,660 --> 01:22:22,550
extensions to allow all sorts of like

2248
01:22:22,550 --> 01:22:24,470
bells and whistles hierarchical

2249
01:22:24,470 --> 01:22:26,510
dependent structures multivariate

2250
01:22:26,510 --> 01:22:30,920
responses missing data etc etc but I

2251
01:22:30,920 --> 01:22:33,170
would say that um you know a couple of

2252
01:22:33,170 --> 01:22:35,150
caveats and motivations for people who

2253
01:22:35,150 --> 01:22:37,460
might want to do some do some research

2254
01:22:37,460 --> 01:22:40,280
in this area or that um I like still

2255
01:22:40,280 --> 01:22:45,050
don't really trust these methods I'm

2256
01:22:45,050 --> 01:22:46,130
gonna coffee but now I'm doing something

2257
01:22:46,130 --> 01:22:48,080
I don't really don't trust these methods

2258
01:22:48,080 --> 01:22:49,760
very well because what what we what we

2259
01:22:49,760 --> 01:22:52,270
do in practice is in order to get good

2260
01:22:52,270 --> 01:22:54,860
results you know provably good results

2261
01:22:54,860 --> 01:22:57,920
for really big P relative to N then we

2262
01:22:57,920 --> 01:22:59,480
kind of apply really an aggressive

2263
01:22:59,480 --> 01:23:01,970
shrinkage prior okay and so the the

2264
01:23:01,970 --> 01:23:03,410
prior is pretty shrinking really

2265
01:23:03,410 --> 01:23:05,270
aggressively well what if the what if

2266
01:23:05,270 --> 01:23:06,950
the truth is actually not that sparse

2267
01:23:06,950 --> 01:23:10,489
for example well you know then our prior

2268
01:23:10,489 --> 01:23:12,050
is actually really informative we've had

2269
01:23:12,050 --> 01:23:13,640
to made it really informative because

2270
01:23:13,640 --> 01:23:15,350
the dimension of the data is much bigger

2271
01:23:15,350 --> 01:23:17,570
than the sample size if we don't make

2272
01:23:17,570 --> 01:23:19,580
the prior really informative then the

2273
01:23:19,580 --> 01:23:21,020
posterior is just gonna be really vague

2274
01:23:21,020 --> 01:23:22,430
and it's not gonna concentrate around

2275
01:23:22,430 --> 01:23:23,720
anything and then we can't make sense of

2276
01:23:23,720 --> 01:23:25,400
it maybe that would be the appropriate

2277
01:23:25,400 --> 01:23:28,520
uncertainty quantification but you know

2278
01:23:28,520 --> 01:23:30,290
to try to get a good result we've

2279
01:23:30,290 --> 01:23:31,550
because we've sort of put in a lot of

2280
01:23:31,550 --> 01:23:33,110
information in the prior may be too much

2281
01:23:33,110 --> 01:23:34,910
information in the prior and then we

2282
01:23:34,910 --> 01:23:36,230
look at these intervals in there then

2283
01:23:36,230 --> 01:23:38,180
they're really tight and so I get a

2284
01:23:38,180 --> 01:23:39,560
little bit skeptical about those

2285
01:23:39,560 --> 01:23:41,000
intervals because they're so tight and

2286
01:23:41,000 --> 01:23:43,220
they're kind of putting in information

2287
01:23:43,220 --> 01:23:45,350
in the prior as if the problem is really

2288
01:23:45,350 --> 01:23:47,540
sparse and so I think we we all need to

2289
01:23:47,540 --> 01:23:49,040
kind of think think carefully about

2290
01:23:49,040 --> 01:23:51,110
about this kind of issue it gives you

2291
01:23:51,110 --> 01:23:53,180
uncertainty quantification but it gives

2292
01:23:53,180 --> 01:23:55,280
you it under a very informative prior if

2293
01:23:55,280 --> 01:23:56,360
you don't have a really informative

2294
01:23:56,360 --> 01:23:58,750
prior the posterior doesn't concentrate

2295
01:23:58,750 --> 01:24:01,130
okay

2296
01:24:01,130 --> 01:24:03,750
okay so in the in the in the remainder

2297
01:24:03,750 --> 01:24:05,760
I'm gonna give a couple of kind of

2298
01:24:05,760 --> 01:24:07,410
vignettes about you know kind of

2299
01:24:07,410 --> 01:24:09,000
different types of approaches jumping

2300
01:24:09,000 --> 01:24:11,970
into a couple of applications that can

2301
01:24:11,970 --> 01:24:13,860
be made made scalable that I find kind

2302
01:24:13,860 --> 01:24:18,840
of interesting so here's one really cool

2303
01:24:18,840 --> 01:24:20,820
application so DNA methylation arrays

2304
01:24:20,820 --> 01:24:23,070
and so what's DNA methylation and so um

2305
01:24:23,070 --> 01:24:25,110
so we all have like our genome you know

2306
01:24:25,110 --> 01:24:27,930
we have like our sequence but actually

2307
01:24:27,930 --> 01:24:29,880
there's all these CPG sites along our

2308
01:24:29,880 --> 01:24:32,250
genome that can become methylated and so

2309
01:24:32,250 --> 01:24:34,170
like maybe by what we're exposed to I'm

2310
01:24:34,170 --> 01:24:35,790
drinking too much coffee or something

2311
01:24:35,790 --> 01:24:37,890
then I get certain regions methylated

2312
01:24:37,890 --> 01:24:39,810
and then that's going to that's going to

2313
01:24:39,810 --> 01:24:41,940
impact gene expression and so that can

2314
01:24:41,940 --> 01:24:43,710
maybe impact you know my probability of

2315
01:24:43,710 --> 01:24:45,300
getting diseases or other things about

2316
01:24:45,300 --> 01:24:48,120
me okay and so we'd like to we'd like to

2317
01:24:48,120 --> 01:24:49,830
understand the effect of this kind of

2318
01:24:49,830 --> 01:24:52,320
epigenome corresponding to these DNA

2319
01:24:52,320 --> 01:24:53,760
methylation sites okay

2320
01:24:53,760 --> 01:24:55,860
and so now there's all sorts of like

2321
01:24:55,860 --> 01:24:57,900
amazing biotechnology for measuring

2322
01:24:57,900 --> 01:24:59,220
stuff like this and so we could measure

2323
01:24:59,220 --> 01:25:01,680
it across the whole genome and all of

2324
01:25:01,680 --> 01:25:03,720
these different CPG sites and so here

2325
01:25:03,720 --> 01:25:05,630
we're analyzing data where we have

2326
01:25:05,630 --> 01:25:08,190
450,000 different CPG sites there's even

2327
01:25:08,190 --> 01:25:09,630
higher dimensional data available where

2328
01:25:09,630 --> 01:25:10,950
we can look at across the entire genome

2329
01:25:10,950 --> 01:25:14,730
for every individual in a study okay so

2330
01:25:14,730 --> 01:25:19,320
um what we'd like to do is we'd like to

2331
01:25:19,320 --> 01:25:22,680
identify differentially methylated CPG

2332
01:25:22,680 --> 01:25:24,240
sites and there's there's a ton of them

2333
01:25:24,240 --> 01:25:27,180
but if we zoom in at any given location

2334
01:25:27,180 --> 01:25:30,240
like say these are the densities of DNA

2335
01:25:30,240 --> 01:25:33,060
methylation for people with two

2336
01:25:33,060 --> 01:25:35,180
different subtypes of cancer for example

2337
01:25:35,180 --> 01:25:38,160
then we see that the densities are quite

2338
01:25:38,160 --> 01:25:40,200
non Gaussian they're quite multimodal

2339
01:25:40,200 --> 01:25:42,540
and strange-looking okay so how do we

2340
01:25:42,540 --> 01:25:44,130
kind of develop some sort of statists

2341
01:25:44,130 --> 01:25:49,440
callable Bayesian statistical method so

2342
01:25:49,440 --> 01:25:50,820
the you know the met here the

2343
01:25:50,820 --> 01:25:52,470
measurements aren't a zero one interval

2344
01:25:52,470 --> 01:25:54,390
ranging from no methylation to fully

2345
01:25:54,390 --> 01:25:57,750
methylated so the data from the Cancer

2346
01:25:57,750 --> 01:26:01,680
Genome Atlas okay so we observe data

2347
01:26:01,680 --> 01:26:03,740
like this at a huge number of CPG sites

2348
01:26:03,740 --> 01:26:05,970
many distributions share common

2349
01:26:05,970 --> 01:26:08,670
attributes modes etc and so we could we

2350
01:26:08,670 --> 01:26:11,280
could you know what will we do if we

2351
01:26:11,280 --> 01:26:13,170
tried to build like a religious fully

2352
01:26:13,170 --> 01:26:14,100
Bayesian

2353
01:26:14,100 --> 01:26:18,110
model then each individual would have a

2354
01:26:18,110 --> 01:26:20,850
450 thousand dimensional response and

2355
01:26:20,850 --> 01:26:22,980
then we could try to like what would you

2356
01:26:22,980 --> 01:26:25,140
Canada to build a mixture model for a

2357
01:26:25,140 --> 01:26:27,480
450 thousand dimensional response we

2358
01:26:27,480 --> 01:26:32,490
really can't do that and so I'm what I'm

2359
01:26:32,490 --> 01:26:34,080
gonna do is I'm gonna I talked earlier

2360
01:26:34,080 --> 01:26:35,280
about sea bass

2361
01:26:35,280 --> 01:26:38,190
another thing that I really like you've

2362
01:26:38,190 --> 01:26:40,140
s been looked at in the recent

2363
01:26:40,140 --> 01:26:41,730
literature is what's called modular

2364
01:26:41,730 --> 01:26:44,010
Bay's and so that's taking the Bayesian

2365
01:26:44,010 --> 01:26:46,320
framework but within a religious

2366
01:26:46,320 --> 01:26:47,940
Bayesian framework you would have to

2367
01:26:47,940 --> 01:26:49,830
model everything jointly you'd have to

2368
01:26:49,830 --> 01:26:51,840
have like a likelihood you believe for

2369
01:26:51,840 --> 01:26:54,330
everything jointly what modular Bayes

2370
01:26:54,330 --> 01:26:56,970
does is it actually breaks up the data

2371
01:26:56,970 --> 01:26:59,430
in the modules and you can like put a

2372
01:26:59,430 --> 01:27:01,800
Bayesian model within each module but

2373
01:27:01,800 --> 01:27:03,450
the modules don't necessarily talk to

2374
01:27:03,450 --> 01:27:05,640
each other and there there's - there's a

2375
01:27:05,640 --> 01:27:08,010
couple of reasons why that's really nice

2376
01:27:08,010 --> 01:27:10,740
as a type of generalized Bayes procedure

2377
01:27:10,740 --> 01:27:12,540
that does nice and scalable uncertainty

2378
01:27:12,540 --> 01:27:14,970
quantification the first is that because

2379
01:27:14,970 --> 01:27:17,280
the modules don't talk to each other the

2380
01:27:17,280 --> 01:27:19,740
model can be much more robust and so

2381
01:27:19,740 --> 01:27:22,290
sometimes fully base joint models for

2382
01:27:22,290 --> 01:27:25,380
high dimensional data can be brittle to

2383
01:27:25,380 --> 01:27:27,570
model Miss specification okay

2384
01:27:27,570 --> 01:27:30,000
as I've talked about earlier but also

2385
01:27:30,000 --> 01:27:33,330
and if we if we have they're much less

2386
01:27:33,330 --> 01:27:34,830
brittle if we use a modular approach

2387
01:27:34,830 --> 01:27:36,900
where we're modeling pieces of the data

2388
01:27:36,900 --> 01:27:39,690
separately another really big reason of

2389
01:27:39,690 --> 01:27:41,370
course is then scalability because if

2390
01:27:41,370 --> 01:27:43,320
we're modularizing and we have piece of

2391
01:27:43,320 --> 01:27:45,200
models for pieces of the data separately

2392
01:27:45,200 --> 01:27:47,790
then we can potentially do computation

2393
01:27:47,790 --> 01:27:54,120
separately as well so what I'm gonna do

2394
01:27:54,120 --> 01:27:55,200
here is I'm gonna do a type of

2395
01:27:55,200 --> 01:27:57,120
dictionary learning basically so I'd

2396
01:27:57,120 --> 01:28:00,990
like to characterize hundreds or

2397
01:28:00,990 --> 01:28:02,610
thousands or millions of these

2398
01:28:02,610 --> 01:28:05,280
distributions using a small number of

2399
01:28:05,280 --> 01:28:07,380
pieces and then reuse those pieces

2400
01:28:07,380 --> 01:28:09,330
within a Bayesian model so I'm going to

2401
01:28:09,330 --> 01:28:10,770
use what's called a shared kernel

2402
01:28:10,770 --> 01:28:13,290
mixture model or shark okay so we're

2403
01:28:13,290 --> 01:28:14,910
gonna use the same kernels across the

2404
01:28:14,910 --> 01:28:16,470
sites and groups but allow the weights

2405
01:28:16,470 --> 01:28:21,600
to vary okay so so the methylation

2406
01:28:21,600 --> 01:28:24,780
density at site J in Group G so the site

2407
01:28:24,780 --> 01:28:27,360
says different sites along the genome

2408
01:28:27,360 --> 01:28:29,070
group is the different types of cancer

2409
01:28:29,070 --> 01:28:31,320
group okay and so we could say well that

2410
01:28:31,320 --> 01:28:35,940
that density is fjg so fjg of Y that's

2411
01:28:35,940 --> 01:28:38,790
the density of methylation at that that

2412
01:28:38,790 --> 01:28:40,650
site in that group we can write that as

2413
01:28:40,650 --> 01:28:43,710
a kernel mixture okay of H components

2414
01:28:43,710 --> 01:28:44,940
and then we can put down some kernels

2415
01:28:44,940 --> 01:28:46,110
and now we're gonna let the weights vary

2416
01:28:46,110 --> 01:28:48,360
but the kernels are common so when you

2417
01:28:48,360 --> 01:28:51,570
do that we make the kernels common we

2418
01:28:51,570 --> 01:28:53,400
basically have like a ridiculously

2419
01:28:53,400 --> 01:28:55,230
enormous a bit essentially infinite

2420
01:28:55,230 --> 01:28:57,170
sample size for learning the kernels

2421
01:28:57,170 --> 01:28:59,460
because we get this kind of blessing of

2422
01:28:59,460 --> 01:29:01,170
dimensionality across the different CPG

2423
01:29:01,170 --> 01:29:03,210
sites and so we can we don't need to

2424
01:29:03,210 --> 01:29:05,160
characterize uncertainty and learning

2425
01:29:05,160 --> 01:29:07,740
the kernels because our data are so

2426
01:29:07,740 --> 01:29:09,240
immense that if we characterize the

2427
01:29:09,240 --> 01:29:10,590
uncertainty the posterior would be just

2428
01:29:10,590 --> 01:29:12,810
a point mass super concentrated on one

2429
01:29:12,810 --> 01:29:14,970
value anyway and so we're gonna learn a

2430
01:29:14,970 --> 01:29:17,310
fixed set of kernels and then put a

2431
01:29:17,310 --> 01:29:19,320
posterior distribution characterizing

2432
01:29:19,320 --> 01:29:20,910
uncertainty on the weights with the

2433
01:29:20,910 --> 01:29:22,590
weights vary across the different sites

2434
01:29:22,590 --> 01:29:24,450
and groups okay and so that's gonna be

2435
01:29:24,450 --> 01:29:28,470
the game so these weights 1 PI JG are

2436
01:29:28,470 --> 01:29:33,120
weight specific to site J group G and so

2437
01:29:33,120 --> 01:29:35,220
K is a shared kernel so here we use the

2438
01:29:35,220 --> 01:29:37,230
truncated normal we can use any sort of

2439
01:29:37,230 --> 01:29:41,280
like fancy kernel okay so we're gonna

2440
01:29:41,280 --> 01:29:42,750
put a simple hierarchical model on the

2441
01:29:42,750 --> 01:29:44,700
pi PI J G's the dearest lay in each

2442
01:29:44,700 --> 01:29:47,430
group and then we we do a testing

2443
01:29:47,430 --> 01:29:49,440
problem now we can do a scalable testing

2444
01:29:49,440 --> 01:29:51,300
I think testing is some some problem

2445
01:29:51,300 --> 01:29:53,580
that we often really have to do in

2446
01:29:53,580 --> 01:29:55,260
scientific inferences but machine

2447
01:29:55,260 --> 01:29:56,940
learning people haven't focused on as

2448
01:29:56,940 --> 01:29:59,850
much as I'd like so I'm here we have two

2449
01:29:59,850 --> 01:30:01,170
different possibilities need each site

2450
01:30:01,170 --> 01:30:03,420
so the two different cancer subtype

2451
01:30:03,420 --> 01:30:05,400
groups they either have the same

2452
01:30:05,400 --> 01:30:07,890
distribution of methylation or they have

2453
01:30:07,890 --> 01:30:09,120
a different distribution of methylation

2454
01:30:09,120 --> 01:30:10,890
we'd like to identify the sites that are

2455
01:30:10,890 --> 01:30:13,740
differentially methylated okay that's a

2456
01:30:13,740 --> 01:30:16,170
multiple hypothesis testing problem so

2457
01:30:16,170 --> 01:30:18,600
we're gonna put D relate priors on each

2458
01:30:18,600 --> 01:30:21,270
of these probability vectors within this

2459
01:30:21,270 --> 01:30:23,400
hypothesis testing problem and it's

2460
01:30:23,400 --> 01:30:25,230
gonna be automatically adjusting for a

2461
01:30:25,230 --> 01:30:27,750
multiple testing error controlling false

2462
01:30:27,750 --> 01:30:29,610
discovery rate automatically within a

2463
01:30:29,610 --> 01:30:32,180
Bayesian multiple testing framework and

2464
01:30:32,180 --> 01:30:34,800
the computation is extremely fast

2465
01:30:34,800 --> 01:30:37,530
because if we use fixed kernels we get

2466
01:30:37,530 --> 01:30:39,730
conjugacy everywhere else except

2467
01:30:39,730 --> 01:30:41,410
for one little piece where we can just

2468
01:30:41,410 --> 01:30:43,600
use a simple parallel I skip sampler and

2469
01:30:43,600 --> 01:30:46,330
so we can run you know this for hundreds

2470
01:30:46,330 --> 01:30:47,710
and hundreds and hundreds of thousands

2471
01:30:47,710 --> 01:30:49,330
or millions of different sites quite

2472
01:30:49,330 --> 01:30:52,720
quite seamlessly and we have some theory

2473
01:30:52,720 --> 01:30:55,270
I won't talk about okay so um so we're

2474
01:30:55,270 --> 01:30:58,900
gonna we Illustrated this using of 597

2475
01:30:58,900 --> 01:31:01,000
breast cancer samples let's just show it

2476
01:31:01,000 --> 01:31:04,000
for 21,000 CBG sites from this Cancer

2477
01:31:04,000 --> 01:31:07,090
Genome Atlas and here just shows the the

2478
01:31:07,090 --> 01:31:10,030
probability of the null hypothesis at

2479
01:31:10,030 --> 01:31:13,960
each of the sites so there's 21,000 22

2480
01:31:13,960 --> 01:31:15,580
almost thousand sites and so this is

2481
01:31:15,580 --> 01:31:17,530
just a histogram of the estimated

2482
01:31:17,530 --> 01:31:20,440
posterior probabilities that that that

2483
01:31:20,440 --> 01:31:22,300
there's no differential methylation and

2484
01:31:22,300 --> 01:31:24,550
so you see most of most of the sites are

2485
01:31:24,550 --> 01:31:26,470
like there's no difference between the

2486
01:31:26,470 --> 01:31:28,840
cancer subtypes but here there's a

2487
01:31:28,840 --> 01:31:31,150
subset of site sites having a high

2488
01:31:31,150 --> 01:31:33,310
posterior probability of each not

2489
01:31:33,310 --> 01:31:36,190
meaning a you know a low posterior

2490
01:31:36,190 --> 01:31:37,960
probability H not meaning a high post

2491
01:31:37,960 --> 01:31:38,950
your probability that there's a

2492
01:31:38,950 --> 01:31:42,130
difference okay and so we can test

2493
01:31:42,130 --> 01:31:44,170
differences between basal-like and not

2494
01:31:44,170 --> 01:31:46,450
at each site and the global proportion

2495
01:31:46,450 --> 01:31:50,050
is no difference was 0.8 to 1 okay so

2496
01:31:50,050 --> 01:31:51,670
this is pretty cool and then we could we

2497
01:31:51,670 --> 01:31:54,190
can look at relationships but with with

2498
01:31:54,190 --> 01:31:56,980
held-out gene expression data and to

2499
01:31:56,980 --> 01:31:59,440
kind of validate or have the essentially

2500
01:31:59,440 --> 01:32:00,910
labels on the results and when we do

2501
01:32:00,910 --> 01:32:03,130
that we can see that this does massively

2502
01:32:03,130 --> 01:32:05,290
better than than other types of methods

2503
01:32:05,290 --> 01:32:07,120
that people might use just doing

2504
01:32:07,120 --> 01:32:09,040
independent testing and then false

2505
01:32:09,040 --> 01:32:11,170
discovery rate control so there's kind

2506
01:32:11,170 --> 01:32:13,090
of Bayesian it's really kind of Bayesian

2507
01:32:13,090 --> 01:32:14,680
nonparametric multiple testing method

2508
01:32:14,680 --> 01:32:16,600
kind of really has practical

2509
01:32:16,600 --> 01:32:19,660
improvements and the usual multiple

2510
01:32:19,660 --> 01:32:21,190
testing method for frequentist would

2511
01:32:21,190 --> 01:32:23,230
like separately at each site get a

2512
01:32:23,230 --> 01:32:25,450
p-value and here but they wouldn't be

2513
01:32:25,450 --> 01:32:27,310
boring information across the sites and

2514
01:32:27,310 --> 01:32:29,290
so here by using a Bayesian hierarchal

2515
01:32:29,290 --> 01:32:32,800
model you're borrowing information okay

2516
01:32:32,800 --> 01:32:34,989
so i'm gonna give one one last example

2517
01:32:34,989 --> 01:32:37,570
before we run out of time where we can

2518
01:32:37,570 --> 01:32:38,890
do this same thing for really

2519
01:32:38,890 --> 01:32:43,300
complicated outcome data okay so um so

2520
01:32:43,300 --> 01:32:44,470
what we're doing is we're characterizing

2521
01:32:44,470 --> 01:32:46,450
nonparametric lis the distribution

2522
01:32:46,450 --> 01:32:49,450
within each group at each site I'm using

2523
01:32:49,450 --> 01:32:50,980
this kernel mixture model and then we're

2524
01:32:50,980 --> 01:32:52,690
letting the weights vary so we're

2525
01:32:52,690 --> 01:32:53,440
putting all the acts

2526
01:32:53,440 --> 01:32:55,570
the weights to allow for to testing

2527
01:32:55,570 --> 01:32:57,790
differences among groups covariance etc

2528
01:32:57,790 --> 01:32:59,530
so we could do this in a really

2529
01:32:59,530 --> 01:33:02,290
complicated settings so let's say we

2530
01:33:02,290 --> 01:33:05,020
have you know brain connectomes for

2531
01:33:05,020 --> 01:33:07,360
example and so here just shows like this

2532
01:33:07,360 --> 01:33:09,760
is kind of a picture of you know in any

2533
01:33:09,760 --> 01:33:11,620
given individual I've taken a brain scan

2534
01:33:11,620 --> 01:33:13,000
of everyone in the room and then I

2535
01:33:13,000 --> 01:33:15,460
applied all this processing and now I

2536
01:33:15,460 --> 01:33:17,410
get you know the location of all these

2537
01:33:17,410 --> 01:33:19,420
like white matter fiber tract bundles in

2538
01:33:19,420 --> 01:33:21,400
your brain and I have a million of those

2539
01:33:21,400 --> 01:33:24,280
and they're all spatially located and

2540
01:33:24,280 --> 01:33:25,810
that's my like complicated data I would

2541
01:33:25,810 --> 01:33:27,520
like to analyze and I would say that

2542
01:33:27,520 --> 01:33:29,500
that's what the act where the actions

2543
01:33:29,500 --> 01:33:31,900
going to be moving forward is like well

2544
01:33:31,900 --> 01:33:33,699
how the hell do we like analyze this

2545
01:33:33,699 --> 01:33:36,820
kind of amazingly rich data collected

2546
01:33:36,820 --> 01:33:40,000
from this new biotechnology without just

2547
01:33:40,000 --> 01:33:41,290
using kind of simple off-the-shelf

2548
01:33:41,290 --> 01:33:43,180
methods but but having methods that are

2549
01:33:43,180 --> 01:33:45,130
appropriate for really actually

2550
01:33:45,130 --> 01:33:49,510
complicated data okay so um so let's say

2551
01:33:49,510 --> 01:33:51,940
we represent the data as X I which is a

2552
01:33:51,940 --> 01:33:53,890
network or a graph really for each

2553
01:33:53,890 --> 01:33:57,250
individual so x iu b is one if there's

2554
01:33:57,250 --> 01:33:59,230
any connection between regions U and V

2555
01:33:59,230 --> 01:34:02,710
for individual I and x iu v equals zero

2556
01:34:02,710 --> 01:34:05,770
otherwise okay so we need some sort of

2557
01:34:05,770 --> 01:34:07,870
model for nonparametric lee

2558
01:34:07,870 --> 01:34:09,580
characterizing variation in these brain

2559
01:34:09,580 --> 01:34:11,560
networks across individuals okay so

2560
01:34:11,560 --> 01:34:14,050
lacks eyes drawn some from p what the

2561
01:34:14,050 --> 01:34:16,239
hell is P P is some distribution of of

2562
01:34:16,239 --> 01:34:19,120
random graphs okay and they sure as hell

2563
01:34:19,120 --> 01:34:20,590
don't follow some sort of stochastic

2564
01:34:20,590 --> 01:34:22,660
block model or whatever the usual random

2565
01:34:22,660 --> 01:34:26,260
graphs people use so we can define some

2566
01:34:26,260 --> 01:34:28,000
type of model let's say for each brain

2567
01:34:28,000 --> 01:34:31,180
region R and component H we can assign

2568
01:34:31,180 --> 01:34:33,219
some sort of individual specific score

2569
01:34:33,219 --> 01:34:35,350
and then we can put down a model a

2570
01:34:35,350 --> 01:34:37,420
simple hierarchical model latent space

2571
01:34:37,420 --> 01:34:40,210
model for brain networks and so here is

2572
01:34:40,210 --> 01:34:42,610
the the logit of the probability that we

2573
01:34:42,610 --> 01:34:44,920
have a connection between regions U and

2574
01:34:44,920 --> 01:34:48,010
V in the brain of individual I we can

2575
01:34:48,010 --> 01:34:50,230
write as an intercept part that's common

2576
01:34:50,230 --> 01:34:51,460
across the different individuals

2577
01:34:51,460 --> 01:34:53,410
characterizing commonalities and brain

2578
01:34:53,410 --> 01:34:55,510
structure across different people I mean

2579
01:34:55,510 --> 01:34:57,280
then we have this kind of a you know

2580
01:34:57,280 --> 01:34:59,320
almost singular value decomposition or

2581
01:34:59,320 --> 01:35:02,350
principal components type piece the data

2582
01:35:02,350 --> 01:35:04,179
or cement symmetric and so we have like

2583
01:35:04,179 --> 01:35:06,970
a sum from I equals 1 to K

2584
01:35:06,970 --> 01:35:09,490
different latent components lambda IH

2585
01:35:09,490 --> 01:35:12,040
some some weight specific to individual

2586
01:35:12,040 --> 01:35:14,110
I component H and then we have some sort

2587
01:35:14,110 --> 01:35:16,570
of ADA ADA part characterizing them

2588
01:35:16,570 --> 01:35:18,000
specific to the different brain regions

2589
01:35:18,000 --> 01:35:21,010
okay and then we can we can stack all

2590
01:35:21,010 --> 01:35:22,870
the individual specific parameters and

2591
01:35:22,870 --> 01:35:25,930
some theta I vector and and those are

2592
01:35:25,930 --> 01:35:27,700
sort of random effects characterizing

2593
01:35:27,700 --> 01:35:30,100
variation across individuals and so this

2594
01:35:30,100 --> 01:35:32,890
is a sort of an example of Bayesian

2595
01:35:32,890 --> 01:35:34,240
hierarchical modeling for high

2596
01:35:34,240 --> 01:35:36,370
dimensional complicated data so we'd

2597
01:35:36,370 --> 01:35:38,410
write down some model where we're

2598
01:35:38,410 --> 01:35:41,500
characterizing variation and complicated

2599
01:35:41,500 --> 01:35:43,300
data through some latent variables and

2600
01:35:43,300 --> 01:35:45,430
so this is quite quite a common kind of

2601
01:35:45,430 --> 01:35:48,370
canonical game in in Bayesian modeling

2602
01:35:48,370 --> 01:35:50,530
of high dimensional data and then we

2603
01:35:50,530 --> 01:35:51,850
don't know what the distribution of

2604
01:35:51,850 --> 01:35:54,550
those random effects is and so here

2605
01:35:54,550 --> 01:35:55,720
we're going to use Bayesian non /

2606
01:35:55,720 --> 01:35:58,000
metrics to allow to allow Q the

2607
01:35:58,000 --> 01:35:59,380
distribution of the random effects and

2608
01:35:59,380 --> 01:36:01,660
hence P the distribution the brain

2609
01:36:01,660 --> 01:36:03,610
networks across people to be unknown

2610
01:36:03,610 --> 01:36:08,740
okay okay so um so based on this

2611
01:36:08,740 --> 01:36:10,150
framework you're actually clustering

2612
01:36:10,150 --> 01:36:11,440
individuals in terms of their brain

2613
01:36:11,440 --> 01:36:13,480
structure which is pretty cool and we

2614
01:36:13,480 --> 01:36:15,310
can test for relationships between brain

2615
01:36:15,310 --> 01:36:21,280
structure and traits and genotype okay

2616
01:36:21,280 --> 01:36:22,750
so we just allow the weights in our

2617
01:36:22,750 --> 01:36:24,520
mixture model to vary with with with

2618
01:36:24,520 --> 01:36:27,070
traits with these fixed kernels so the

2619
01:36:27,070 --> 01:36:28,510
kernels are characterizing the kind of

2620
01:36:28,510 --> 01:36:31,120
God provided dictionary with which we

2621
01:36:31,120 --> 01:36:34,750
can characterize brain graphs okay and

2622
01:36:34,750 --> 01:36:36,760
so this allows scientific inferences on

2623
01:36:36,760 --> 01:36:38,620
global and local differences in network

2624
01:36:38,620 --> 01:36:41,380
structure with traits and I really think

2625
01:36:41,380 --> 01:36:43,440
we need more of these types of kind of

2626
01:36:43,440 --> 01:36:46,240
scalable methods with uncertainty

2627
01:36:46,240 --> 01:36:47,980
quantification for complex data and so I

2628
01:36:47,980 --> 01:36:49,720
really encourage more people to work on

2629
01:36:49,720 --> 01:36:51,100
the on this type of thing not

2630
01:36:51,100 --> 01:36:52,660
necessarily for brain networks but for

2631
01:36:52,660 --> 01:36:55,570
any kind of complicated data that just

2632
01:36:55,570 --> 01:36:57,700
for a multiple testing as well okay so

2633
01:36:57,700 --> 01:37:00,100
here here are some results and so we we

2634
01:37:00,100 --> 01:37:01,780
applied the model to brain networks of

2635
01:37:01,780 --> 01:37:04,810
36 subjects 19 with high creativity 17

2636
01:37:04,810 --> 01:37:07,990
with low creativity measured with a the

2637
01:37:07,990 --> 01:37:11,350
composite creativity index the it gives

2638
01:37:11,350 --> 01:37:13,450
us an overall probability a posterior

2639
01:37:13,450 --> 01:37:15,250
probability that there's any differences

2640
01:37:15,250 --> 01:37:18,160
in brain structure with with creativity

2641
01:37:18,160 --> 01:37:20,830
and that was 0.995 which is quite

2642
01:37:20,830 --> 01:37:23,950
evidence and it also we also like you

2643
01:37:23,950 --> 01:37:27,340
can see here what we did here is we we

2644
01:37:27,340 --> 01:37:31,030
can flag statistically significant after

2645
01:37:31,030 --> 01:37:33,300
adjusting from multiple testing

2646
01:37:33,300 --> 01:37:36,250
differences in brain structure and so a

2647
01:37:36,250 --> 01:37:39,190
red a red line would mean that people

2648
01:37:39,190 --> 01:37:43,540
with with low creativity have more

2649
01:37:43,540 --> 01:37:46,420
connections in that location and so a

2650
01:37:46,420 --> 01:37:49,180
green line means less connections so so

2651
01:37:49,180 --> 01:37:50,980
that what this green line here means is

2652
01:37:50,980 --> 01:37:53,140
that that the individuals with low

2653
01:37:53,140 --> 01:37:56,950
creativity have significantly less

2654
01:37:56,950 --> 01:37:59,080
connections between these regions okay

2655
01:37:59,080 --> 01:38:01,150
and so we can see a lot of green lines

2656
01:38:01,150 --> 01:38:03,130
that are cross hemisphere connections in

2657
01:38:03,130 --> 01:38:05,110
the frontal lobe of the brain and so

2658
01:38:05,110 --> 01:38:07,930
what that means essentially is that that

2659
01:38:07,930 --> 01:38:09,870
people who are highly create creative

2660
01:38:09,870 --> 01:38:12,700
have more across hemisphere connections

2661
01:38:12,700 --> 01:38:14,680
more more connections that occur in sort

2662
01:38:14,680 --> 01:38:18,610
of unanticipated regions okay it was an

2663
01:38:18,610 --> 01:38:21,430
interesting side comment is we had a we

2664
01:38:21,430 --> 01:38:23,260
in a press release on this result and

2665
01:38:23,260 --> 01:38:25,540
there you know the this left-brain

2666
01:38:25,540 --> 01:38:27,880
right-brain hypothesis this whole thing

2667
01:38:27,880 --> 01:38:29,440
we're like left brain people are like

2668
01:38:29,440 --> 01:38:31,450
more mathematical and right brain people

2669
01:38:31,450 --> 01:38:33,910
are more creative or whatever the the

2670
01:38:33,910 --> 01:38:35,800
guy who came up with this he's deceased

2671
01:38:35,800 --> 01:38:38,740
now but his son contacted us and said

2672
01:38:38,740 --> 01:38:40,990
well actually his dad like ended up not

2673
01:38:40,990 --> 01:38:42,070
believing the right brain left brain

2674
01:38:42,070 --> 01:38:43,990
thing at all and what he ended up

2675
01:38:43,990 --> 01:38:45,400
believing was much more consistent with

2676
01:38:45,400 --> 01:38:47,050
our thing which was there's not left

2677
01:38:47,050 --> 01:38:48,250
brain right brain it's more

2678
01:38:48,250 --> 01:38:50,140
interconnected brains that are more

2679
01:38:50,140 --> 01:38:52,330
creative the left brain right brain

2680
01:38:52,330 --> 01:38:56,800
thing is just bogus okay so um so the

2681
01:38:56,800 --> 01:38:59,650
the idea of just wrapping up is that in

2682
01:38:59,650 --> 01:39:02,280
this kind of modularization strategy

2683
01:39:02,280 --> 01:39:05,470
which is rated through that that that

2684
01:39:05,470 --> 01:39:08,200
that DNA methylation example is we don't

2685
01:39:08,200 --> 01:39:10,060
allow for all the dependencies implied

2686
01:39:10,060 --> 01:39:13,450
by the joint Bayesian model i think i

2687
01:39:13,450 --> 01:39:14,710
kind of said that already

2688
01:39:14,710 --> 01:39:17,140
i think we have like five minutes i'd

2689
01:39:17,140 --> 01:39:18,580
like to i take some questions and so i'm

2690
01:39:18,580 --> 01:39:21,850
just gonna skip this so that i just the

2691
01:39:21,850 --> 01:39:24,670
bottom line here is that we can scale up

2692
01:39:24,670 --> 01:39:27,310
bayesian inferences for genomic problems

2693
01:39:27,310 --> 01:39:30,750
with with tens of millions of snips

2694
01:39:30,750 --> 01:39:33,700
using these types same types of methods

2695
01:39:33,700 --> 01:39:34,659
that i've been talking about

2696
01:39:34,659 --> 01:39:36,940
and so it's not true at all that

2697
01:39:36,940 --> 01:39:38,500
Bayesian inference isn't scalable we can

2698
01:39:38,500 --> 01:39:40,690
deal with hundreds of millions of snaps

2699
01:39:40,690 --> 01:39:43,840
even or features in regression problems

2700
01:39:43,840 --> 01:39:46,170
and we can use these types of methods

2701
01:39:46,170 --> 01:39:50,440
okay so discussion so so I kind of gave

2702
01:39:50,440 --> 01:39:52,000
a brief intro to Bayesian methods for

2703
01:39:52,000 --> 01:39:55,000
large P problems at the end of this talk

2704
01:39:55,000 --> 01:39:56,530
highlighted some recent work using

2705
01:39:56,530 --> 01:39:58,480
shared kernels and modularization I

2706
01:39:58,480 --> 01:40:00,130
would say there's a really a rich

2707
01:40:00,130 --> 01:40:01,929
literature that's just kind of beginning

2708
01:40:01,929 --> 01:40:04,570
an increasing focus on scalability and I

2709
01:40:04,570 --> 01:40:06,130
hope I hope to inspire some people to

2710
01:40:06,130 --> 01:40:09,580
work more more in these areas and one

2711
01:40:09,580 --> 01:40:11,139
important direction is obtain methods

2712
01:40:11,139 --> 01:40:12,310
for assessing when we're attempting

2713
01:40:12,310 --> 01:40:14,110
inferences on to find a scale for our

2714
01:40:14,110 --> 01:40:16,870
data I think that's really important to

2715
01:40:16,870 --> 01:40:18,760
come up with more negative results I

2716
01:40:18,760 --> 01:40:21,070
mean like and these problems work with

2717
01:40:21,070 --> 01:40:22,330
scientists all the time I want to be

2718
01:40:22,330 --> 01:40:24,040
able to tell them like actually the

2719
01:40:24,040 --> 01:40:26,560
questions you're trying to ask are not

2720
01:40:26,560 --> 01:40:28,270
possible given the data you've provided

2721
01:40:28,270 --> 01:40:30,489
me and the prior information that we

2722
01:40:30,489 --> 01:40:32,260
have and so if I'm going to answer your

2723
01:40:32,260 --> 01:40:34,659
questions then I have to put more prior

2724
01:40:34,659 --> 01:40:36,940
information in the analysis than I

2725
01:40:36,940 --> 01:40:39,429
actually have which is probably a bad

2726
01:40:39,429 --> 01:40:43,150
idea and so therefore maybe maybe I can

2727
01:40:43,150 --> 01:40:45,880
come up with a method that will say no

2728
01:40:45,880 --> 01:40:48,820
you can't answer those questions maybe

2729
01:40:48,820 --> 01:40:51,760
it can then even go beyond that almost

2730
01:40:51,760 --> 01:40:53,320
like in an artificial intelligence way

2731
01:40:53,320 --> 01:40:55,630
and say well here are these questions

2732
01:40:55,630 --> 01:40:56,889
you can answer you can have like an

2733
01:40:56,889 --> 01:40:58,719
assistant like a Bayesian assistant that

2734
01:40:58,719 --> 01:41:00,790
can say well the questions you're trying

2735
01:41:00,790 --> 01:41:03,429
to ask are impossible why don't you why

2736
01:41:03,429 --> 01:41:04,929
do you ask these types of questions ease

2737
01:41:04,929 --> 01:41:06,070
are the types of questions you can ask

2738
01:41:06,070 --> 01:41:09,040
you can't identify individual snips that

2739
01:41:09,040 --> 01:41:10,690
are related to the phenotype you can

2740
01:41:10,690 --> 01:41:12,969
only identify regions of the genome for

2741
01:41:12,969 --> 01:41:14,560
example so that would be incredibly

2742
01:41:14,560 --> 01:41:16,179
useful and there's really nothing like

2743
01:41:16,179 --> 01:41:19,630
that out there now so one way is the

2744
01:41:19,630 --> 01:41:21,340
course on the scale of the data here's

2745
01:41:21,340 --> 01:41:24,280
some some references on large and I'm

2746
01:41:24,280 --> 01:41:26,889
gonna I'm happy to provide the slides to

2747
01:41:26,889 --> 01:41:29,080
anyone I can post them on my website and

2748
01:41:29,080 --> 01:41:30,880
so and then please email me if you have

2749
01:41:30,880 --> 01:41:34,000
any comments etc the take-home message

2750
01:41:34,000 --> 01:41:36,880
is just that Bayes is scalable and MCMC

2751
01:41:36,880 --> 01:41:39,010
is scalable but in big and high

2752
01:41:39,010 --> 01:41:40,650
dimensional problems you can't just use

2753
01:41:40,650 --> 01:41:43,270
off-the-shelf algorithms you have to be

2754
01:41:43,270 --> 01:41:45,400
smart about it and we need to think

2755
01:41:45,400 --> 01:41:47,199
carefully about how to exploit parallel

2756
01:41:47,199 --> 01:41:48,310
processing accurate

2757
01:41:48,310 --> 01:41:51,130
approximations to reduce bottlenecks we

2758
01:41:51,130 --> 01:41:52,810
might also need to take a step away from

2759
01:41:52,810 --> 01:41:55,000
fully base frame works by using mod

2760
01:41:55,000 --> 01:41:57,160
realization composite likelihood C Bay's

2761
01:41:57,160 --> 01:42:01,090
etc and we'd like to be able to design

2762
01:42:01,090 --> 01:42:02,920
algorithms and and an inference

2763
01:42:02,920 --> 01:42:05,680
frameworks that are improved in a

2764
01:42:05,680 --> 01:42:07,240
computational way and in terms of

2765
01:42:07,240 --> 01:42:11,070
robustness no it's not there

2766
01:42:11,250 --> 01:42:13,220
[Applause]

2767
01:42:13,220 --> 01:42:14,140
[Music]

2768
01:42:14,140 --> 01:42:15,090
[Applause]

2769
01:42:15,090 --> 01:42:18,530
[Music]

2770
01:42:18,530 --> 01:42:21,120
thanks a lot David very very inspiring

2771
01:42:21,120 --> 01:42:23,310
talk so we have time for a few questions

2772
01:42:23,310 --> 01:42:25,050
if there are any there are microphones

2773
01:42:25,050 --> 01:42:27,720
here in the front so please PLEASE use

2774
01:42:27,720 --> 01:42:29,960
them

2775
01:42:32,270 --> 01:42:36,120
okay there are no questions I'm sorry

2776
01:42:36,120 --> 01:42:39,000
beautiful hi thank you very much it was

2777
01:42:39,000 --> 01:42:41,580
very illuminating talk my question is

2778
01:42:41,580 --> 01:42:44,730
about collaboration so any Bayesian

2779
01:42:44,730 --> 01:42:48,900
framework starts with likelihood

2780
01:42:48,900 --> 01:42:50,790
function which is a condition all of you

2781
01:42:50,790 --> 01:42:52,680
observed data on whatever latent

2782
01:42:52,680 --> 01:42:54,960
parameters theta you choose then you

2783
01:42:54,960 --> 01:42:57,390
start with the priors and then through

2784
01:42:57,390 --> 01:42:59,730
multi-car law whatever you essentially

2785
01:42:59,730 --> 01:43:01,980
can describe mysterious and you can also

2786
01:43:01,980 --> 01:43:05,130
just describe the marginals so and quite

2787
01:43:05,130 --> 01:43:07,590
often it happens that yeah and of course

2788
01:43:07,590 --> 01:43:11,520
this likelihood function is a parametric

2789
01:43:11,520 --> 01:43:14,130
model and you know the real problems are

2790
01:43:14,130 --> 01:43:15,900
very complex it's very hard to actual

2791
01:43:15,900 --> 01:43:18,300
analytically write that likelihood

2792
01:43:18,300 --> 01:43:20,010
function so in the end of the day when

2793
01:43:20,010 --> 01:43:22,380
everything is done and one computes the

2794
01:43:22,380 --> 01:43:25,020
marginal of the data and computes the

2795
01:43:25,020 --> 01:43:27,090
quantiles for example you had an example

2796
01:43:27,090 --> 01:43:28,620
of single dimensional distribution at

2797
01:43:28,620 --> 01:43:30,810
the end marginalized everything and for

2798
01:43:30,810 --> 01:43:32,940
single quantile function then you gets

2799
01:43:32,940 --> 01:43:34,470
the quantiles which are different from

2800
01:43:34,470 --> 01:43:36,090
what you observe in the data so

2801
01:43:36,090 --> 01:43:37,380
essentially the model is miscalibrated

2802
01:43:37,380 --> 01:43:39,930
and because model had like hundreds of

2803
01:43:39,930 --> 01:43:42,420
the parameters so what exactly should be

2804
01:43:42,420 --> 01:43:46,460
the approach to recalibrate model back

2805
01:43:46,460 --> 01:43:48,830
yeah that's a really great question the

2806
01:43:48,830 --> 01:43:51,570
first note that it's really important to

2807
01:43:51,570 --> 01:43:54,330
look at calibration and so I'm often

2808
01:43:54,330 --> 01:43:56,970
using you know out-of-sample assessments

2809
01:43:56,970 --> 01:43:59,160
and then calibration in a sense of not

2810
01:43:59,160 --> 01:44:01,800
just point point estimates but that the

2811
01:44:01,800 --> 01:44:03,600
probability distribution is

2812
01:44:03,600 --> 01:44:06,030
characterizing uncertainty well in terms

2813
01:44:06,030 --> 01:44:08,310
of predictive distributions when the

2814
01:44:08,310 --> 01:44:10,650
model is not well calibrated I often

2815
01:44:10,650 --> 01:44:13,440
think to try to put in more more more

2816
01:44:13,440 --> 01:44:15,150
flexibilities somehow I don't know that

2817
01:44:15,150 --> 01:44:17,850
there's a great answer for you know okay

2818
01:44:17,850 --> 01:44:20,490
well where did I go wrong exactly it's

2819
01:44:20,490 --> 01:44:22,920
in a complicated setting it's not well

2820
01:44:22,920 --> 01:44:25,920
calibrated but the kind of rule of thumb

2821
01:44:25,920 --> 01:44:27,870
in some sense is - well maybe I need

2822
01:44:27,870 --> 01:44:29,940
some more I need some more for the

2823
01:44:29,940 --> 01:44:31,519
flexibility and so then I can

2824
01:44:31,519 --> 01:44:33,260
back and try to do some Diagnostics to

2825
01:44:33,260 --> 01:44:35,269
try to figure out where my model is not

2826
01:44:35,269 --> 01:44:37,279
fitting very well and kind of iterate

2827
01:44:37,279 --> 01:44:44,749
from there thank you hi two short

2828
01:44:44,749 --> 01:44:47,299
questions one question concerning the

2829
01:44:47,299 --> 01:44:49,789
example that you make on the Bernoulli

2830
01:44:49,789 --> 01:44:53,689
case conflicting in that case the test

2831
01:44:53,689 --> 01:44:57,109
is a certain point telling you that your

2832
01:44:57,109 --> 01:44:59,539
hypothesis is false so isn't that just

2833
01:44:59,539 --> 01:45:02,239
the right answer I mean at some point we

2834
01:45:02,239 --> 01:45:04,729
have enough power to review to refute

2835
01:45:04,729 --> 01:45:06,829
the apologies the power the hypothesis

2836
01:45:06,829 --> 01:45:09,499
is false and so that's a great question

2837
01:45:09,499 --> 01:45:11,510
I think that that's one of the

2838
01:45:11,510 --> 01:45:14,029
philosophical issues a lot of people

2839
01:45:14,029 --> 01:45:15,439
don't believe in testing because they

2840
01:45:15,439 --> 01:45:18,379
never believe in precisely that exact

2841
01:45:18,379 --> 01:45:20,089
specification of hypothesis and it's

2842
01:45:20,089 --> 01:45:21,739
related to this model of specification

2843
01:45:21,739 --> 01:45:23,989
it was also doing the right thing in

2844
01:45:23,989 --> 01:45:26,479
terms of the mixture model in adding

2845
01:45:26,479 --> 01:45:28,489
more components because it wasn't

2846
01:45:28,489 --> 01:45:30,319
precisely two components that were true

2847
01:45:30,319 --> 01:45:32,089
when I would say that you know

2848
01:45:32,089 --> 01:45:35,989
practically allowing a tiny bit a small

2849
01:45:35,989 --> 01:45:37,909
amount of miss specification of

2850
01:45:37,909 --> 01:45:40,159
hypothesis in the model is going to give

2851
01:45:40,159 --> 01:45:42,019
you much better performance because in

2852
01:45:42,019 --> 01:45:44,149
reality you know you're never gonna have

2853
01:45:44,149 --> 01:45:47,119
exactly the data you observed or from

2854
01:45:47,119 --> 01:45:49,069
exactly a particular hypothesis or

2855
01:45:49,069 --> 01:45:50,539
exactly a particular model and so you'd

2856
01:45:50,539 --> 01:45:53,089
like to allow some sort of fudge factor

2857
01:45:53,089 --> 01:45:54,919
there to allow them to be close but not

2858
01:45:54,919 --> 01:45:57,769
exactly okay the other question we are

2859
01:45:57,769 --> 01:46:02,089
doing we are applying the sorry merry

2860
01:46:02,089 --> 01:46:05,419
way and repin it they beckon you convoy

2861
01:46:05,419 --> 01:46:08,299
we are applying the Kennedy and Hagin

2862
01:46:08,299 --> 01:46:11,539
framework to a pair so it's Bayesian

2863
01:46:11,539 --> 01:46:14,359
calibration for computer models okay to

2864
01:46:14,359 --> 01:46:17,809
a problem where basically we know that

2865
01:46:17,809 --> 01:46:19,939
Gaussian process regression is not

2866
01:46:19,939 --> 01:46:22,129
appropriate or at least a standard

2867
01:46:22,129 --> 01:46:25,369
Gaussian process because we are sure

2868
01:46:25,369 --> 01:46:28,429
that it's not the model is the variance

2869
01:46:28,429 --> 01:46:30,679
is not isotropic a great problem in this

2870
01:46:30,679 --> 01:46:32,929
computer model emulation literature is

2871
01:46:32,929 --> 01:46:35,329
really cool but often the Gaussian

2872
01:46:35,329 --> 01:46:37,309
process falls flat and so better models

2873
01:46:37,309 --> 01:46:40,449
formulation or is a good important area

2874
01:46:40,449 --> 01:46:43,570
so yeah in that case what could be an

2875
01:46:43,570 --> 01:46:46,179
alternative to using variational bass

2876
01:46:46,179 --> 01:46:48,610
because it's like we have let's say

2877
01:46:48,610 --> 01:46:51,040
alpha million samples and it's a

2878
01:46:51,040 --> 01:46:54,190
fourteen dimensional problem so it's I

2879
01:46:54,190 --> 01:46:57,820
mean not big P but relatively large and

2880
01:46:57,820 --> 01:47:00,580
so instead then trying variational base

2881
01:47:00,580 --> 01:47:02,679
that usually works certainly you could

2882
01:47:02,679 --> 01:47:05,590
use these EPM CMC kind of methods in

2883
01:47:05,590 --> 01:47:07,360
that setting they've been used for

2884
01:47:07,360 --> 01:47:08,920
Gaussian processes but even if the

2885
01:47:08,920 --> 01:47:10,989
Gaussian process wasn't it didn't hold

2886
01:47:10,989 --> 01:47:12,790
exactly you could even use them for

2887
01:47:12,790 --> 01:47:15,070
something like I know you know one of

2888
01:47:15,070 --> 01:47:16,510
our projects is using deep neural

2889
01:47:16,510 --> 01:47:19,270
networks for emulation instead and you

2890
01:47:19,270 --> 01:47:20,949
can even run em Sam Seaver deep deep

2891
01:47:20,949 --> 01:47:23,020
neural networks and you could do that in

2892
01:47:23,020 --> 01:47:23,800
a parallel way

2893
01:47:23,800 --> 01:47:25,900
now that's something like that okay

2894
01:47:25,900 --> 01:47:27,760
thank you

2895
01:47:27,760 --> 01:47:30,580
I was also gonna ask about the sea bass

2896
01:47:30,580 --> 01:47:32,739
I found that quite intriguing and I

2897
01:47:32,739 --> 01:47:35,710
guess it seems like what you've actually

2898
01:47:35,710 --> 01:47:38,080
done there is replace having an explicit

2899
01:47:38,080 --> 01:47:41,050
noise model with something nonparametric

2900
01:47:41,050 --> 01:47:44,170
so is is is actually what this is doing

2901
01:47:44,170 --> 01:47:46,060
is corresponding to having some

2902
01:47:46,060 --> 01:47:48,400
nonparametric noise model but then you

2903
01:47:48,400 --> 01:47:48,940
know it's just

2904
01:47:48,940 --> 01:47:50,560
that's a great question yeah then we

2905
01:47:50,560 --> 01:47:52,510
often get a lot of questions on sea bass

2906
01:47:52,510 --> 01:47:54,280
whether it's like exactly the same as an

2907
01:47:54,280 --> 01:47:56,320
explicit noise model but it it's sort of

2908
01:47:56,320 --> 01:47:58,869
like avoiding specifying the noise model

2909
01:47:58,869 --> 01:48:00,790
because the noise model might also be

2910
01:48:00,790 --> 01:48:03,280
miss specified etc so it is like being

2911
01:48:03,280 --> 01:48:05,199
nonparametric but not nonparametric in

2912
01:48:05,199 --> 01:48:07,000
the Bayesian nonparametric way which

2913
01:48:07,000 --> 01:48:08,920
means a really flexible model but more

2914
01:48:08,920 --> 01:48:10,540
nonparametric in the classical way where

2915
01:48:10,540 --> 01:48:16,960
you avoid modeling something my question

2916
01:48:16,960 --> 01:48:18,850
is mostly referred in terms of

2917
01:48:18,850 --> 01:48:21,190
scalability usually you say that

2918
01:48:21,190 --> 01:48:23,320
probably most of the problems are kind

2919
01:48:23,320 --> 01:48:26,199
of scalable in terms of my robustness in

2920
01:48:26,199 --> 01:48:28,030
which cases you have faced this

2921
01:48:28,030 --> 01:48:30,400
challenge to a scale to make it a scale

2922
01:48:30,400 --> 01:48:33,760
or to where the actual training the full

2923
01:48:33,760 --> 01:48:36,150
training is much much accurate that

2924
01:48:36,150 --> 01:48:39,460
trying to too many bootstrap for example

2925
01:48:39,460 --> 01:48:41,500
yeah that's a great question of it so

2926
01:48:41,500 --> 01:48:44,020
the so this is really a new literature

2927
01:48:44,020 --> 01:48:46,570
and so there are definitely areas that

2928
01:48:46,570 --> 01:48:47,820
are not developed

2929
01:48:47,820 --> 01:48:50,130
and I would say that that two of those

2930
01:48:50,130 --> 01:48:52,890
are you know network structured data and

2931
01:48:52,890 --> 01:48:54,960
time series data and so if you have like

2932
01:48:54,960 --> 01:48:57,390
a really like super long time series and

2933
01:48:57,390 --> 01:48:59,280
you're trying to scale it up like scale

2934
01:48:59,280 --> 01:49:01,560
up these MCMC methods there's almost no

2935
01:49:01,560 --> 01:49:03,600
literature on that there's a tiny bit by

2936
01:49:03,600 --> 01:49:05,370
Emily Fox's group on hidden Markov

2937
01:49:05,370 --> 01:49:07,290
models but and we just posted a paper on

2938
01:49:07,290 --> 01:49:09,570
an archive on hit 4 hidden Markov models

2939
01:49:09,570 --> 01:49:12,000
but there's no like useful general

2940
01:49:12,000 --> 01:49:14,520
methods and that's even more true in the

2941
01:49:14,520 --> 01:49:17,490
network case you know we have billions

2942
01:49:17,490 --> 01:49:19,710
of you know users and products in this

2943
01:49:19,710 --> 01:49:21,930
Alibaba collaboration and we're kind of

2944
01:49:21,930 --> 01:49:23,640
trying to trying to develop scalable

2945
01:49:23,640 --> 01:49:25,950
Bayesian methods in that case is a

2946
01:49:25,950 --> 01:49:27,660
challenge because if you're just kind of

2947
01:49:27,660 --> 01:49:29,370
subsampling of breaking up the data well

2948
01:49:29,370 --> 01:49:30,690
how do you do that if the data really

2949
01:49:30,690 --> 01:49:32,340
dependent and like a network context

2950
01:49:32,340 --> 01:49:35,430
it's an open problem so do you have a

2951
01:49:35,430 --> 01:49:37,530
kind of initial guesses about that

2952
01:49:37,530 --> 01:49:40,860
especially for the time series data sure

2953
01:49:40,860 --> 01:49:42,420
yeah yeah time series data the the

2954
01:49:42,420 --> 01:49:44,820
initial guess would be tube in time you

2955
01:49:44,820 --> 01:49:46,380
know and see if you have a choice chunks

2956
01:49:46,380 --> 01:49:48,540
and of adjacent time points and then you

2957
01:49:48,540 --> 01:49:51,030
can do dumb computation in parallel for

2958
01:49:51,030 --> 01:49:52,980
adjacent time points that then you can

2959
01:49:52,980 --> 01:49:54,630
kind of do a lot in terms of scalability

2960
01:49:54,630 --> 01:49:56,430
doing that I think that that'll end up

2961
01:49:56,430 --> 01:50:02,820
working quite well okay thank hi thanks

2962
01:50:02,820 --> 01:50:04,790
for your talk it's very interesting I

2963
01:50:04,790 --> 01:50:07,670
guess what wasn't really clear to me is

2964
01:50:07,670 --> 01:50:11,130
if we have a lot of data are you really

2965
01:50:11,130 --> 01:50:13,920
getting a different conclusion using

2966
01:50:13,920 --> 01:50:15,780
Bayesian methods versus using some more

2967
01:50:15,780 --> 01:50:18,510
classical methods yeah that's a great

2968
01:50:18,510 --> 01:50:20,610
question I mean I think that uh you

2969
01:50:20,610 --> 01:50:22,320
should hopefully shouldn't you know if

2970
01:50:22,320 --> 01:50:23,940
you have if you're like in a setting

2971
01:50:23,940 --> 01:50:26,850
where you have a ton of data and the

2972
01:50:26,850 --> 01:50:28,590
sort of number of parameters isn't that

2973
01:50:28,590 --> 01:50:30,810
giant so if I have a logistic regression

2974
01:50:30,810 --> 01:50:32,790
and I have maybe a hundred hundred

2975
01:50:32,790 --> 01:50:34,920
predictors but my sample size is under

2976
01:50:34,920 --> 01:50:36,480
million or something hopefully I'm

2977
01:50:36,480 --> 01:50:38,490
getting exactly the same results and and

2978
01:50:38,490 --> 01:50:40,020
we can show that theoretically actually

2979
01:50:40,020 --> 01:50:41,760
with that the Bayesian central limit

2980
01:50:41,760 --> 01:50:43,440
theorem or bernstein von Mises result

2981
01:50:43,440 --> 01:50:45,150
the Bayesian posterior is actually going

2982
01:50:45,150 --> 01:50:47,850
to converge to exactly the around the

2983
01:50:47,850 --> 01:50:49,440
maximum likelihood estimates with the

2984
01:50:49,440 --> 01:50:50,790
covariance being inverse Fisher

2985
01:50:50,790 --> 01:50:52,100
information and so there's this

2986
01:50:52,100 --> 01:50:54,870
well-known equivalence between between

2987
01:50:54,870 --> 01:50:57,540
bays and frequentist methods where where

2988
01:50:57,540 --> 01:50:58,950
the equivalence doesn't hold is when

2989
01:50:58,950 --> 01:50:59,520
you're in these

2990
01:50:59,520 --> 01:51:01,800
challenging problems certainly where you

2991
01:51:01,800 --> 01:51:04,590
have you know you you have uncertainty

2992
01:51:04,590 --> 01:51:07,350
remaining it's not well characterized by

2993
01:51:07,350 --> 01:51:09,890
a large sample Gaussian distribution

2994
01:51:09,890 --> 01:51:12,320
thanks

2995
01:51:12,320 --> 01:51:15,150
you mentioned that there's orders of

2996
01:51:15,150 --> 01:51:16,380
magnitude more people working on

2997
01:51:16,380 --> 01:51:18,140
optimization distribute optimization

2998
01:51:18,140 --> 01:51:20,690
techniques is there any way our

2999
01:51:20,690 --> 01:51:22,950
literature of applying these

3000
01:51:22,950 --> 01:51:24,720
optimization distribute optimization

3001
01:51:24,720 --> 01:51:27,870
techniques to scaleable Bayesian MCMC

3002
01:51:27,870 --> 01:51:29,820
methods that's a great question I mean

3003
01:51:29,820 --> 01:51:31,260
that that's actually what we were trying

3004
01:51:31,260 --> 01:51:34,080
to do and that that wass method or

3005
01:51:34,080 --> 01:51:35,370
something like so it's that we do

3006
01:51:35,370 --> 01:51:38,190
separate sampling but then use modern

3007
01:51:38,190 --> 01:51:39,780
optimization techniques for doing the

3008
01:51:39,780 --> 01:51:42,450
doing the combining but there's also a

3009
01:51:42,450 --> 01:51:44,550
you know a really amazingly cool

3010
01:51:44,550 --> 01:51:45,900
literature that I didn't have time to

3011
01:51:45,900 --> 01:51:47,790
talk about like by people like Yusef

3012
01:51:47,790 --> 01:51:51,090
more Zook and at MIT you know trying to

3013
01:51:51,090 --> 01:51:53,330
do things like oh let's try to sample

3014
01:51:53,330 --> 01:51:55,860
iid from a multivariate Gaussian but

3015
01:51:55,860 --> 01:51:57,750
defined some optimal transportation plan

3016
01:51:57,750 --> 01:51:59,550
to the true posterior distribution and

3017
01:51:59,550 --> 01:52:00,900
then defined some corresponding

3018
01:52:00,900 --> 01:52:02,160
optimization problems solved an

3019
01:52:02,160 --> 01:52:04,140
optimization problem well if I have the

3020
01:52:04,140 --> 01:52:05,940
map that I just you know take those

3021
01:52:05,940 --> 01:52:07,590
independent samples and I transform them

3022
01:52:07,590 --> 01:52:08,580
to get perfect samples from the

3023
01:52:08,580 --> 01:52:10,500
posterior so there's a number of people

3024
01:52:10,500 --> 01:52:12,390
working on that Guangcheng add up Purdue

3025
01:52:12,390 --> 01:52:14,130
is another one yeah it's a great

3026
01:52:14,130 --> 01:52:19,110
question thanks you briefly mentioned

3027
01:52:19,110 --> 01:52:22,410
that sorry I think we need to break for

3028
01:52:22,410 --> 01:52:23,940
the for the coffee break but feel free

3029
01:52:23,940 --> 01:52:25,800
to come come forward that we can chat

3030
01:52:25,800 --> 01:52:27,900
that offline afterwards but let's thank

3031
01:52:27,900 --> 01:52:31,040
David again thanks a lot

3032
01:52:33,220 --> 01:53:04,319
[Music]

