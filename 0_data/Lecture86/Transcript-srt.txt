1
00:06:57,930 --> 00:07:01,030
welcome to nurbs welcome back to

2
00:07:01,030 --> 00:07:03,850
Montreal if you've been here before

3
00:07:03,850 --> 00:07:07,720
live here or welcome if this is your

4
00:07:07,720 --> 00:07:10,990
first time tuner ups or to tuner drips

5
00:07:10,990 --> 00:07:14,590
at Montreal this morning we're going to

6
00:07:14,590 --> 00:07:18,580
have a great tutorial on adversarial

7
00:07:18,580 --> 00:07:22,360
robustness theory and practice and let

8
00:07:22,360 --> 00:07:25,050
me introduce our two speakers today

9
00:07:25,050 --> 00:07:29,220
Aleksandr Madhuri and Zico Colter

10
00:07:29,220 --> 00:07:32,830
Alexander Madhuri is a professor at MIT

11
00:07:32,830 --> 00:07:35,650
at the csail Laboratory and his work

12
00:07:35,650 --> 00:07:39,190
focuses on optimization and robustness

13
00:07:39,190 --> 00:07:41,949
in particular in deep learning Zico

14
00:07:41,949 --> 00:07:45,490
Colter is a professor at Carnegie Mellon

15
00:07:45,490 --> 00:07:48,190
and also the chief scientist at the

16
00:07:48,190 --> 00:07:53,050
Bosch AI Center and his work focuses as

17
00:07:53,050 --> 00:07:55,270
well on optimization and on in

18
00:07:55,270 --> 00:07:58,360
particular rigorous explainable and

19
00:07:58,360 --> 00:08:01,780
robust deep learning algorithms please

20
00:08:01,780 --> 00:08:04,300
welcome the two of them and let's enjoy

21
00:08:04,300 --> 00:08:14,080
the tutorial thank you I'm Alexander

22
00:08:14,080 --> 00:08:16,870
Madrid and together with Zico to talk

23
00:08:16,870 --> 00:08:21,550
about robustness so is something that we

24
00:08:21,550 --> 00:08:24,370
sort of knew what it is but we it still

25
00:08:24,370 --> 00:08:27,010
caught us somehow off-guard and you know

26
00:08:27,010 --> 00:08:29,530
because of that a well we have to now

27
00:08:29,530 --> 00:08:32,320
struggle ok and actually we wanted to

28
00:08:32,320 --> 00:08:33,760
use this opportunity to not only talk

29
00:08:33,760 --> 00:08:35,740
about the solar business but also to

30
00:08:35,740 --> 00:08:38,529
sketch a bit broader picture of the

31
00:08:38,529 --> 00:08:40,240
piece of the field and the challenges

32
00:08:40,240 --> 00:08:42,610
that we face when we try to deploy

33
00:08:42,610 --> 00:08:44,589
machine learning in the real world

34
00:08:44,589 --> 00:08:48,490
ok so let's get started ok so I think

35
00:08:48,490 --> 00:08:50,620
it's safe to say that machine learning

36
00:08:50,620 --> 00:08:53,110
is a success story currently you know if

37
00:08:53,110 --> 00:08:54,910
you work on machine learning there is

38
00:08:54,910 --> 00:08:56,770
plenty of opportunities to feel proud

39
00:08:56,770 --> 00:08:59,140
about yourself over the last decade so

40
00:08:59,140 --> 00:09:01,990
we made progress on some of the flagship

41
00:09:01,990 --> 00:09:04,209
trenches that just a few years before we

42
00:09:04,209 --> 00:09:05,250
did not think

43
00:09:05,250 --> 00:09:08,100
in rich now they are so that's all great

44
00:09:08,100 --> 00:09:12,000
and in particular this kind of bread a

45
00:09:12,000 --> 00:09:14,069
lot of excitement ok so now you can't

46
00:09:14,069 --> 00:09:16,230
open the newspaper without reading about

47
00:09:16,230 --> 00:09:18,930
all the great things that ml or a I will

48
00:09:18,930 --> 00:09:21,389
bring to us and in particular you know

49
00:09:21,389 --> 00:09:23,910
all successes essentially inspired

50
00:09:23,910 --> 00:09:27,000
people to kind of apply or think of

51
00:09:27,000 --> 00:09:28,769
applying machine learning to anything

52
00:09:28,769 --> 00:09:31,769
you do okay in a way no matter what you

53
00:09:31,769 --> 00:09:33,509
do if you don't put machine learning

54
00:09:33,509 --> 00:09:36,480
into it you are doing it wrong okay so

55
00:09:36,480 --> 00:09:38,370
this is all very exciting it definitely

56
00:09:38,370 --> 00:09:41,069
makes this will be very exciting to be

57
00:09:41,069 --> 00:09:45,180
in just you know this also however comes

58
00:09:45,180 --> 00:09:47,160
with certain responsibilities okay

59
00:09:47,160 --> 00:09:51,420
because as exciting as you know trying

60
00:09:51,420 --> 00:09:54,089
to deploy this ml everywhere as exciting

61
00:09:54,089 --> 00:09:57,269
as it is I think it behooves of us as a

62
00:09:57,269 --> 00:10:00,149
community to stop for a moment and think

63
00:10:00,149 --> 00:10:02,790
okay is actually detect the ml

64
00:10:02,790 --> 00:10:05,129
technology the way it is now actually

65
00:10:05,129 --> 00:10:08,519
truly ready for real world applaud okay

66
00:10:08,519 --> 00:10:10,290
and that's somehow the question that we

67
00:10:10,290 --> 00:10:11,309
brought questions we would like to

68
00:10:11,309 --> 00:10:14,100
tackle or specifically you know there is

69
00:10:14,100 --> 00:10:16,980
many way to attack this question and

70
00:10:16,980 --> 00:10:18,870
many in the community try to do it

71
00:10:18,870 --> 00:10:21,240
somehow the angle that we want to focus

72
00:10:21,240 --> 00:10:23,399
on in this talk is something that you

73
00:10:23,399 --> 00:10:25,529
know we kind of phrased as you know can

74
00:10:25,529 --> 00:10:27,629
we truly rely on the machine learning

75
00:10:27,629 --> 00:10:30,269
the way we have it and even that

76
00:10:30,269 --> 00:10:32,189
question has many interpretation you

77
00:10:32,189 --> 00:10:33,990
know one of them would be that you know

78
00:10:33,990 --> 00:10:36,809
we are kind of afraid that the AI will

79
00:10:36,809 --> 00:10:39,120
bring doom to us all and you know this

80
00:10:39,120 --> 00:10:40,949
is definitely a legitimate concern but

81
00:10:40,949 --> 00:10:43,199
that's not what you want to cover in

82
00:10:43,199 --> 00:10:45,870
this talk what you want to focus on is

83
00:10:45,870 --> 00:10:49,079
something actually much more mundane but

84
00:10:49,079 --> 00:10:51,420
because of this actually much more acute

85
00:10:51,420 --> 00:10:54,750
and dangerous is just the fact that

86
00:10:54,750 --> 00:10:57,029
essentially we don't really understand

87
00:10:57,029 --> 00:10:59,910
yet well how machine learning interacts

88
00:10:59,910 --> 00:11:02,160
with other parts of our systems pipeline

89
00:11:02,160 --> 00:11:05,040
okay and in particular kind of whenever

90
00:11:05,040 --> 00:11:06,569
machine learning is embedded into a

91
00:11:06,569 --> 00:11:09,660
system it turns out that it gives a way

92
00:11:09,660 --> 00:11:12,389
for adversaries to manipulate it in a

93
00:11:12,389 --> 00:11:13,139
variety of

94
00:11:13,139 --> 00:11:15,420
okay and have seen plenty examples of

95
00:11:15,420 --> 00:11:17,189
that already but kind of the point here

96
00:11:17,189 --> 00:11:19,170
is to just not allow this to happen

97
00:11:19,170 --> 00:11:21,660
and you know going further okay so

98
00:11:21,660 --> 00:11:23,879
that's when there are some bad guys

99
00:11:23,879 --> 00:11:26,069
trying to manipulate our system that's

100
00:11:26,069 --> 00:11:27,389
already where machine learning becomes

101
00:11:27,389 --> 00:11:28,829
you know like this is a problem for

102
00:11:28,829 --> 00:11:30,989
machine learning but also even if there

103
00:11:30,989 --> 00:11:33,089
is no bad guys in the loop there is

104
00:11:33,089 --> 00:11:35,759
still nature and sort of when we want to

105
00:11:35,759 --> 00:11:38,699
go from 95% reliability to 99 percent

106
00:11:38,699 --> 00:11:41,790
reliability things actually become quite

107
00:11:41,790 --> 00:11:43,799
difficult and you know kind of safety is

108
00:11:43,799 --> 00:11:46,019
still very much an issue for application

109
00:11:46,019 --> 00:11:50,579
of a map okay so this is kind of general

110
00:11:50,579 --> 00:11:52,559
theme that you want to tackle here and I

111
00:11:52,559 --> 00:11:54,809
think it no it behooves to us for us to

112
00:11:54,809 --> 00:11:56,669
just think about okay where are these

113
00:11:56,669 --> 00:11:59,160
problems coming from okay so as you like

114
00:11:59,160 --> 00:12:00,989
why would you worry about all these

115
00:12:00,989 --> 00:12:02,189
questions in the first place because you

116
00:12:02,189 --> 00:12:05,009
know we know that a kind of ml seems to

117
00:12:05,009 --> 00:12:06,959
work really really well on all the

118
00:12:06,959 --> 00:12:10,019
benchmarks that we deploy it okay and in

119
00:12:10,019 --> 00:12:11,790
particular here we have you know one of

120
00:12:11,790 --> 00:12:14,279
the key homeruns of machine learning

121
00:12:14,279 --> 00:12:16,980
namely the image net challenge so as all

122
00:12:16,980 --> 00:12:18,629
of you probably know this is like a

123
00:12:18,629 --> 00:12:20,009
computer vision object recognition

124
00:12:20,009 --> 00:12:22,649
challenge where you know you train on

125
00:12:22,649 --> 00:12:26,549
roughly 1.5 1.5 million of high

126
00:12:26,549 --> 00:12:28,739
resolution images and your goal is to

127
00:12:28,739 --> 00:12:31,230
essentially correctly classify the

128
00:12:31,230 --> 00:12:36,149
corresponding test set and well to place

129
00:12:36,149 --> 00:12:38,399
every photo in a correct category okay

130
00:12:38,399 --> 00:12:40,860
and over here we have a progress over

131
00:12:40,860 --> 00:12:43,079
the years over like you know how well

132
00:12:43,079 --> 00:12:46,019
are we doing on this task okay so in

133
00:12:46,019 --> 00:12:49,129
particular we see this market deep in

134
00:12:49,129 --> 00:12:51,569
2012 that's exactly where the Alex that

135
00:12:51,569 --> 00:12:53,429
paper showed up and essentially showed

136
00:12:53,429 --> 00:12:55,860
how deep learning can be you know

137
00:12:55,860 --> 00:12:57,809
applied extremely successfully in

138
00:12:57,809 --> 00:12:59,639
computer vision and it just started the

139
00:12:59,639 --> 00:13:00,869
computer vision deep learning revolution

140
00:13:00,869 --> 00:13:04,919
and in 2015 something very interesting

141
00:13:04,919 --> 00:13:07,589
happened so essentially the performance

142
00:13:07,589 --> 00:13:09,779
of the best-performing deep learning

143
00:13:09,779 --> 00:13:11,220
system on this challenge actually

144
00:13:11,220 --> 00:13:14,129
outperformed humans or to be more

145
00:13:14,129 --> 00:13:17,429
precise a human named Leandre karpati

146
00:13:17,429 --> 00:13:20,999
and you know well I think energy did a

147
00:13:20,999 --> 00:13:23,220
pretty good job and the fact that you

148
00:13:23,220 --> 00:13:25,570
know the existing existing

149
00:13:25,570 --> 00:13:27,220
vision solutions can do better than that

150
00:13:27,220 --> 00:13:29,440
to definitely give us a pause and is

151
00:13:29,440 --> 00:13:31,120
definitely Express it's a definitely

152
00:13:31,120 --> 00:13:31,720
impressive

153
00:13:31,720 --> 00:13:34,900
okay so that's great but what I would

154
00:13:34,900 --> 00:13:38,200
like to ask to ponder here is what does

155
00:13:38,200 --> 00:13:41,050
this result actually mean okay so it

156
00:13:41,050 --> 00:13:42,940
clearly means that you know computer

157
00:13:42,940 --> 00:13:45,400
vision tools the way we deployed them

158
00:13:45,400 --> 00:13:49,000
now can outperform human on this

159
00:13:49,000 --> 00:13:51,610
particular benchmark but somehow you

160
00:13:51,610 --> 00:13:53,020
know and this is definitely true this is

161
00:13:53,020 --> 00:13:55,060
exactly what this test measures but

162
00:13:55,060 --> 00:13:57,880
somehow in a way is a human with tends

163
00:13:57,880 --> 00:13:59,440
to kind of generalize you know the

164
00:13:59,440 --> 00:14:01,480
things to that we see to things that

165
00:14:01,480 --> 00:14:03,460
maybe are not there but kind of removed

166
00:14:03,460 --> 00:14:05,770
very much like to see okay and this is

167
00:14:05,770 --> 00:14:07,090
definitely the case in computer vision

168
00:14:07,090 --> 00:14:09,550
and there is a good reason for that okay

169
00:14:09,550 --> 00:14:10,780
and the good reason for that is

170
00:14:10,780 --> 00:14:13,690
something about you know the conceptual

171
00:14:13,690 --> 00:14:16,350
framework that we use that somehow we

172
00:14:16,350 --> 00:14:18,970
kind of you know sort of are aware of

173
00:14:18,970 --> 00:14:21,460
but don't really think about it okay so

174
00:14:21,460 --> 00:14:24,100
what am i referring to here so let's

175
00:14:24,100 --> 00:14:27,010
quickly look back to supervised ml

176
00:14:27,010 --> 00:14:28,660
framework and just let's think about how

177
00:14:28,660 --> 00:14:31,030
we think about the process of you know

178
00:14:31,030 --> 00:14:33,190
training you know training a classifier

179
00:14:33,190 --> 00:14:35,260
and the way we evaluate it okay so

180
00:14:35,260 --> 00:14:38,170
usually we always think there is so kind

181
00:14:38,170 --> 00:14:40,180
of distribution in the sky okay that

182
00:14:40,180 --> 00:14:41,770
essentially says this distribution of

183
00:14:41,770 --> 00:14:43,510
all the objects you could ever see and

184
00:14:43,510 --> 00:14:45,520
would ever encounter and you would like

185
00:14:45,520 --> 00:14:48,340
to classify okay and now well the way we

186
00:14:48,340 --> 00:14:50,320
train is that we sample a bunch of

187
00:14:50,320 --> 00:14:52,240
samples from this distribution and we

188
00:14:52,240 --> 00:14:54,070
just plug it into our training algorithm

189
00:14:54,070 --> 00:14:56,050
and this you know this gives us the

190
00:14:56,050 --> 00:14:58,600
model that we trained and then the way

191
00:14:58,600 --> 00:15:01,630
we actually check if we are doing a good

192
00:15:01,630 --> 00:15:03,940
job essentially if you know if our if we

193
00:15:03,940 --> 00:15:06,040
did any good it's essentially what we do

194
00:15:06,040 --> 00:15:07,380
it just will sample some more

195
00:15:07,380 --> 00:15:09,100
independent sample from the distribution

196
00:15:09,100 --> 00:15:12,100
and then we test you know how well our

197
00:15:12,100 --> 00:15:14,290
classifiers does on classifying these

198
00:15:14,290 --> 00:15:16,690
okay so in particular our measure of

199
00:15:16,690 --> 00:15:18,700
performance is in a fraction of mistakes

200
00:15:18,700 --> 00:15:21,700
we made during this test and you know

201
00:15:21,700 --> 00:15:24,580
it's safe to say that we made you know

202
00:15:24,580 --> 00:15:27,040
really really in a beautiful progress on

203
00:15:27,040 --> 00:15:28,930
trying to optimize this measure of

204
00:15:28,930 --> 00:15:32,230
performance okay so everything is great

205
00:15:32,230 --> 00:15:34,630
so far except there is one crucial

206
00:15:34,630 --> 00:15:36,850
assumption here that actually

207
00:15:36,850 --> 00:15:39,699
is not reflected in practice namely this

208
00:15:39,699 --> 00:15:41,290
assumption is that actually in practice

209
00:15:41,290 --> 00:15:44,050
it is not the case that the distribution

210
00:15:44,050 --> 00:15:47,500
that we use to train our emotion in the

211
00:15:47,500 --> 00:15:49,660
model is exactly the distribution that

212
00:15:49,660 --> 00:15:52,149
the model will encounter where we deploy

213
00:15:52,149 --> 00:15:54,370
it in the real world okay so there are

214
00:15:54,370 --> 00:15:56,139
various forms of covariates shift and

215
00:15:56,139 --> 00:15:57,579
essentially I says this almost never

216
00:15:57,579 --> 00:15:59,709
happens that these distributions are are

217
00:15:59,709 --> 00:16:02,230
the same and you may say well oh well

218
00:16:02,230 --> 00:16:04,209
you know like that's how we make sense

219
00:16:04,209 --> 00:16:05,860
of stuff we just make assumptions that

220
00:16:05,860 --> 00:16:07,509
maybe are not exactly reflected in

221
00:16:07,509 --> 00:16:09,730
practice still they lead us to our good

222
00:16:09,730 --> 00:16:12,519
solutions but it turns however there is

223
00:16:12,519 --> 00:16:14,440
not however that in this case this may

224
00:16:14,440 --> 00:16:15,850
be an assumption that actually was

225
00:16:15,850 --> 00:16:18,490
somewhat misleading okay so essentially

226
00:16:18,490 --> 00:16:20,529
what can go wrong in the real world

227
00:16:20,529 --> 00:16:23,230
where this assumption does not work it

228
00:16:23,230 --> 00:16:26,709
does not hold anymore okay so I think

229
00:16:26,709 --> 00:16:30,009
one of the key one of the key

230
00:16:30,009 --> 00:16:32,079
implications of this assumption or lack

231
00:16:32,079 --> 00:16:33,850
of lack of this assumption is the

232
00:16:33,850 --> 00:16:35,649
phenomena that we just observe in our

233
00:16:35,649 --> 00:16:38,319
uniformly is that you know the our

234
00:16:38,319 --> 00:16:40,329
machine learning models the predictions

235
00:16:40,329 --> 00:16:42,689
are actually extremely good on average

236
00:16:42,689 --> 00:16:45,939
but also they are extremely brittle ok

237
00:16:45,939 --> 00:16:49,259
so again one of the like the most

238
00:16:49,259 --> 00:16:51,699
publicized this version of this is

239
00:16:51,699 --> 00:16:53,800
something called a vessel examples over

240
00:16:53,800 --> 00:16:55,930
here we have an image of a peak which

241
00:16:55,930 --> 00:16:58,089
state-of-the-art classifiers correctly

242
00:16:58,089 --> 00:17:00,880
recognizes a pig and so everything is

243
00:17:00,880 --> 00:17:02,980
great so far however what it turns out

244
00:17:02,980 --> 00:17:04,809
is that there is a way for me to add

245
00:17:04,809 --> 00:17:07,510
just a little bit of noise to this image

246
00:17:07,510 --> 00:17:09,939
this noise is not random it's you know

247
00:17:09,939 --> 00:17:11,949
it's chosen in a very specific way and

248
00:17:11,949 --> 00:17:14,169
you know the good thing about this thing

249
00:17:14,169 --> 00:17:15,369
is that this know is essentially

250
00:17:15,369 --> 00:17:17,020
imperceptible okay so after I added this

251
00:17:17,020 --> 00:17:19,390
noise I have an image on the right it's

252
00:17:19,390 --> 00:17:21,010
not much different to the image I

253
00:17:21,010 --> 00:17:23,619
started with okay so to ask humans this

254
00:17:23,619 --> 00:17:26,919
is still very much of a pig however for

255
00:17:26,919 --> 00:17:29,350
some reason now the classifier is

256
00:17:29,350 --> 00:17:31,809
convinced that this is an airliner okay

257
00:17:31,809 --> 00:17:33,700
so my favorite drug here is that

258
00:17:33,700 --> 00:17:35,500
essentially machine learning is truly

259
00:17:35,500 --> 00:17:37,900
magical a magical technology it you know

260
00:17:37,900 --> 00:17:41,710
can make pigs fly so that's that's going

261
00:17:41,710 --> 00:17:42,970
for us

262
00:17:42,970 --> 00:17:45,310
and by the way so this phenomena was

263
00:17:45,310 --> 00:17:46,960
probably if you heard about it before

264
00:17:46,960 --> 00:17:48,760
you help from one of these two papers

265
00:17:48,760 --> 00:17:51,790
but again this is and not a new thing to

266
00:17:51,790 --> 00:17:53,680
our field there was a lot of you know

267
00:17:53,680 --> 00:17:55,630
previous work that identified this kind

268
00:17:55,630 --> 00:17:57,340
of brittleness it just somehow you know

269
00:17:57,340 --> 00:18:00,640
before you know last four years in a

270
00:18:00,640 --> 00:18:02,710
machine learning struggled even without

271
00:18:02,710 --> 00:18:05,500
adverse Nadya noise and kind of this was

272
00:18:05,500 --> 00:18:06,790
not that much of a problem

273
00:18:06,790 --> 00:18:09,220
now once the average case performance is

274
00:18:09,220 --> 00:18:12,130
here this worst case performance is what

275
00:18:12,130 --> 00:18:13,870
we really realize you know has to be

276
00:18:13,870 --> 00:18:17,140
addressed okay so this is the this is

277
00:18:17,140 --> 00:18:18,880
the kind of the example of a brittleness

278
00:18:18,880 --> 00:18:21,250
but but you might say okay you know this

279
00:18:21,250 --> 00:18:22,660
noise is not random it's actually very

280
00:18:22,660 --> 00:18:24,550
carefully chosen and in the lab setting

281
00:18:24,550 --> 00:18:26,650
you for sure can you know figure it out

282
00:18:26,650 --> 00:18:29,260
synthesize it and make things fail but

283
00:18:29,260 --> 00:18:30,760
you know in the real world there is so

284
00:18:30,760 --> 00:18:32,140
many other noise coming from different

285
00:18:32,140 --> 00:18:33,730
sources you don't have such a

286
00:18:33,730 --> 00:18:35,560
fine-grained control over what you do

287
00:18:35,560 --> 00:18:37,660
then you know probably everything is

288
00:18:37,660 --> 00:18:39,790
fine if you just you know if you just do

289
00:18:39,790 --> 00:18:41,530
it in the real world like there is this

290
00:18:41,530 --> 00:18:43,230
brittleness does not exhibit yourself

291
00:18:43,230 --> 00:18:46,090
however well it turns out that you know

292
00:18:46,090 --> 00:18:48,880
there is a number numerous demonstration

293
00:18:48,880 --> 00:18:50,410
by now that this is not the case that

294
00:18:50,410 --> 00:18:53,200
you can actually deploy this kind of a

295
00:18:53,200 --> 00:18:55,690
desire noise in practice and I guess my

296
00:18:55,690 --> 00:18:57,430
favorite examples over here so this is

297
00:18:57,430 --> 00:18:59,860
just for MIT undergrad or they are no

298
00:18:59,860 --> 00:19:01,060
longer undergrad but they used to be

299
00:19:01,060 --> 00:19:03,040
undergrad essentially what they did is

300
00:19:03,040 --> 00:19:05,920
they three dip into the turtle that to

301
00:19:05,920 --> 00:19:07,960
us of course looks like a turtle but to

302
00:19:07,960 --> 00:19:09,430
the state-of-the-art classifier it

303
00:19:09,430 --> 00:19:12,190
identifies as a rifle from very

304
00:19:12,190 --> 00:19:15,130
different angles okay so this is

305
00:19:15,130 --> 00:19:17,020
something you know I could I held in my

306
00:19:17,020 --> 00:19:18,700
hand this is something very physical and

307
00:19:18,700 --> 00:19:20,800
it really does in a full

308
00:19:20,800 --> 00:19:24,130
state-of-the-art pacifiers okay so you

309
00:19:24,130 --> 00:19:25,600
know so this definitely is not something

310
00:19:25,600 --> 00:19:27,250
that is just of academic interest this

311
00:19:27,250 --> 00:19:29,440
is actually how can be happening in the

312
00:19:29,440 --> 00:19:31,510
in the physical world and in a sense

313
00:19:31,510 --> 00:19:33,610
this goes even worse than that because

314
00:19:33,610 --> 00:19:36,160
okay so so far when I talk about this

315
00:19:36,160 --> 00:19:38,110
obvious error noise the noise pattern

316
00:19:38,110 --> 00:19:40,450
that I had to add it was very intricate

317
00:19:40,450 --> 00:19:42,370
and complicated so each pixel had to

318
00:19:42,370 --> 00:19:44,320
have a specific values to make it work

319
00:19:44,320 --> 00:19:46,930
in particular this turtle pattern on the

320
00:19:46,930 --> 00:19:49,810
shell had to be very carefully painted

321
00:19:49,810 --> 00:19:52,060
to kind of achieve this adversely affect

322
00:19:52,060 --> 00:19:54,730
it turns out that you don't even

323
00:19:54,730 --> 00:19:57,130
to go to all these lengths to exhibit

324
00:19:57,130 --> 00:19:59,350
some kind of brittleness of our of our

325
00:19:59,350 --> 00:20:00,850
machine learning systems in particular

326
00:20:00,850 --> 00:20:02,890
it turns out that you just using

327
00:20:02,890 --> 00:20:04,480
something like rotation and translations

328
00:20:04,480 --> 00:20:07,090
is already enough to see this brittle

329
00:20:07,090 --> 00:20:09,370
ice so here is an example so over here

330
00:20:09,370 --> 00:20:11,740
we have just like an image of a rifle

331
00:20:11,740 --> 00:20:14,230
which again correct is Kazakh correctly

332
00:20:14,230 --> 00:20:15,940
classified with high confidence as a

333
00:20:15,940 --> 00:20:18,730
rifle and how everything is great but

334
00:20:18,730 --> 00:20:22,179
now I start you know I start rotating it

335
00:20:22,179 --> 00:20:24,549
and as you see is for some of the angles

336
00:20:24,549 --> 00:20:26,650
I actually have a the top high

337
00:20:26,650 --> 00:20:29,440
confidence classification is a pro is an

338
00:20:29,440 --> 00:20:32,049
agricultural device and not the rifle so

339
00:20:32,049 --> 00:20:35,080
again whatever you do if you're you know

340
00:20:35,080 --> 00:20:36,850
if your object recognition system is not

341
00:20:36,850 --> 00:20:38,740
able to reliably recognize the object if

342
00:20:38,740 --> 00:20:40,600
you rotate it then you know in you and

343
00:20:40,600 --> 00:20:42,820
travel and by the way this is not just

344
00:20:42,820 --> 00:20:45,010
some kind of queer of the pipeline

345
00:20:45,010 --> 00:20:46,750
you know you of course might think of

346
00:20:46,750 --> 00:20:48,370
using data argumentation to try to

347
00:20:48,370 --> 00:20:50,440
correct it and you know and we did try

348
00:20:50,440 --> 00:20:52,000
it and we did evaluate it and the answer

349
00:20:52,000 --> 00:20:53,919
is it helps somewhat but it definitely

350
00:20:53,919 --> 00:20:57,309
does not solve the problem okay so I

351
00:20:57,309 --> 00:21:00,010
think we can kind of satisfactorily

352
00:21:00,010 --> 00:21:02,260
conclude that the brittleness of ml is a

353
00:21:02,260 --> 00:21:04,840
thing and I guess what we should do now

354
00:21:04,840 --> 00:21:06,970
is so we should wonder okay

355
00:21:06,970 --> 00:21:09,820
should we be worried by this or not okay

356
00:21:09,820 --> 00:21:13,090
well needless to say mine and I'm sure

357
00:21:13,090 --> 00:21:15,760
zico's stance is that you know that this

358
00:21:15,760 --> 00:21:17,980
is a problem and for a variety of reason

359
00:21:17,980 --> 00:21:20,470
so you know the most obvious one is

360
00:21:20,470 --> 00:21:22,350
security okay so essentially whenever I

361
00:21:22,350 --> 00:21:24,610
deploy a machine learning system in the

362
00:21:24,610 --> 00:21:26,559
context where someone else might have

363
00:21:26,559 --> 00:21:28,780
incentive to manipulate it well I should

364
00:21:28,780 --> 00:21:30,190
be very worried because if I can make my

365
00:21:30,190 --> 00:21:32,620
system see something different then I

366
00:21:32,620 --> 00:21:34,720
see that's exactly how all of the

367
00:21:34,720 --> 00:21:37,720
bridges really happen okay and security

368
00:21:37,720 --> 00:21:39,700
community had a number of very cool

369
00:21:39,700 --> 00:21:41,380
demos showing that this kind of things

370
00:21:41,380 --> 00:21:43,090
can happen so there is a work from CMU

371
00:21:43,090 --> 00:21:45,549
that shows that you can 3d print glasses

372
00:21:45,549 --> 00:21:47,860
that essentially once you put them on

373
00:21:47,860 --> 00:21:50,260
the state-of-the-art face recognition

374
00:21:50,260 --> 00:21:51,700
system thinks you're a completely

375
00:21:51,700 --> 00:21:52,299
different person

376
00:21:52,299 --> 00:21:53,919
okay so think about like automating

377
00:21:53,919 --> 00:21:56,049
automated passport gates you know now

378
00:21:56,049 --> 00:21:57,520
you can just fool them by just putting

379
00:21:57,520 --> 00:21:59,860
on some weird glasses okay and by the

380
00:21:59,860 --> 00:22:02,110
way even though in this tutorial we

381
00:22:02,110 --> 00:22:05,830
focus mostly on on image signal you know

382
00:22:05,830 --> 00:22:06,370
this

383
00:22:06,370 --> 00:22:08,230
brittleness everything applies to other

384
00:22:08,230 --> 00:22:09,970
type of signal as well in particular

385
00:22:09,970 --> 00:22:11,590
there is a work from from Berkeley by

386
00:22:11,590 --> 00:22:13,750
Colleen Wagner that shows that you can

387
00:22:13,750 --> 00:22:15,490
do the same kind of manipulation to

388
00:22:15,490 --> 00:22:17,500
sound so you can kind of synthesize

389
00:22:17,500 --> 00:22:20,140
music clips but to you sound like music

390
00:22:20,140 --> 00:22:23,440
but to Alexa or you know to Alex or

391
00:22:23,440 --> 00:22:25,990
other voice voice control to voice

392
00:22:25,990 --> 00:22:28,180
control device this sounds like a

393
00:22:28,180 --> 00:22:30,670
perfectly clear voice command so someone

394
00:22:30,670 --> 00:22:32,470
you might be listening to the music on

395
00:22:32,470 --> 00:22:34,300
radio and someone might be reprogramming

396
00:22:34,300 --> 00:22:35,920
your Alexa and you will not even know

397
00:22:35,920 --> 00:22:38,650
about it okay so this is about security

398
00:22:38,650 --> 00:22:40,120
about like when there are some bad guys

399
00:22:40,120 --> 00:22:42,370
to get us but again it even the safety

400
00:22:42,370 --> 00:22:44,140
and reliability is an issue already

401
00:22:44,140 --> 00:22:46,630
so recently just Tesla you know they

402
00:22:46,630 --> 00:22:50,530
just released their kind of data from

403
00:22:50,530 --> 00:22:52,720
the lack object and line recognition

404
00:22:52,720 --> 00:22:54,850
pipeline essentially as a drive through

405
00:22:54,850 --> 00:22:56,500
Paris and again it's actually is doing

406
00:22:56,500 --> 00:22:59,350
pretty amazing job except you see that

407
00:22:59,350 --> 00:23:00,790
these predictions you know they can

408
00:23:00,790 --> 00:23:02,980
actually be quite glittery and kind of

409
00:23:02,980 --> 00:23:05,350
they click clearly struggle a bit and

410
00:23:05,350 --> 00:23:08,440
indeed if you look online you can see

411
00:23:08,440 --> 00:23:10,900
plenty of you know YouTube video where

412
00:23:10,900 --> 00:23:13,330
Tesla drivers they just record one kind

413
00:23:13,330 --> 00:23:15,010
of like situations like here where the

414
00:23:15,010 --> 00:23:17,230
driver assistance system you know it

415
00:23:17,230 --> 00:23:19,090
works great most of the time but over

416
00:23:19,090 --> 00:23:21,580
here is trying to continue just straight

417
00:23:21,580 --> 00:23:23,440
into the divider and the driver has to

418
00:23:23,440 --> 00:23:25,870
take over to avoid the you notify the

419
00:23:25,870 --> 00:23:28,990
like hitting hit and it is even like the

420
00:23:28,990 --> 00:23:30,820
system is not even reporting an error it

421
00:23:30,820 --> 00:23:32,740
just thinks really that this is where

422
00:23:32,740 --> 00:23:35,200
the line goes okay so we clearly

423
00:23:35,200 --> 00:23:37,059
achieved a lot but there is more to go

424
00:23:37,059 --> 00:23:38,470
so this is another reason why we should

425
00:23:38,470 --> 00:23:40,330
care about this brittleness but also

426
00:23:40,330 --> 00:23:42,850
there is one more way my own reason why

427
00:23:42,850 --> 00:23:44,770
we might want to care and I usually call

428
00:23:44,770 --> 00:23:47,410
it like machine learning alignment so

429
00:23:47,410 --> 00:23:49,420
essentially adversely robustness tells

430
00:23:49,420 --> 00:23:51,550
us shows us the failure mode of ml that

431
00:23:51,550 --> 00:23:53,020
kind of are different to our failure

432
00:23:53,020 --> 00:23:55,630
modes and this should drive the point to

433
00:23:55,630 --> 00:23:57,250
us that the way machine learning

434
00:23:57,250 --> 00:23:59,050
solution work and the way they succeed

435
00:23:59,050 --> 00:24:01,600
is very different to the way we work and

436
00:24:01,600 --> 00:24:03,100
we succeed okay and that's I think very

437
00:24:03,100 --> 00:24:05,770
important to keep in mind okay

438
00:24:05,770 --> 00:24:08,230
so so this is all about the brittleness

439
00:24:08,230 --> 00:24:09,640
of prediction of machine learning system

440
00:24:09,640 --> 00:24:10,929
and you may ask okay

441
00:24:10,929 --> 00:24:12,460
is that the only problem in machine

442
00:24:12,460 --> 00:24:15,160
learning that we have and the answer is

443
00:24:15,160 --> 00:24:17,440
emphatically no okay so essentially as I

444
00:24:17,440 --> 00:24:19,290
tell my student every aspect of

445
00:24:19,290 --> 00:24:22,860
is broken currently if you look at it to

446
00:24:22,860 --> 00:24:25,260
the robustness yes okay so so far we

447
00:24:25,260 --> 00:24:27,000
talked about inference and the problem

448
00:24:27,000 --> 00:24:28,620
of missing examples but if you talk

449
00:24:28,620 --> 00:24:30,270
about training there is also a problem

450
00:24:30,270 --> 00:24:32,250
looming there it's called data project

451
00:24:32,250 --> 00:24:34,920
okay so what's going on there well

452
00:24:34,920 --> 00:24:37,890
essentially as beautiful and great as ml

453
00:24:37,890 --> 00:24:40,350
is it's actually infinitely data hungry

454
00:24:40,350 --> 00:24:42,510
we always need more and more data to

455
00:24:42,510 --> 00:24:44,610
have it perform well and the implication

456
00:24:44,610 --> 00:24:46,020
is that essentially if you need so much

457
00:24:46,020 --> 00:24:48,180
data we country afford to be too picky

458
00:24:48,180 --> 00:24:50,310
where it is from so we actually might

459
00:24:50,310 --> 00:24:53,250
you know train on data we have no full

460
00:24:53,250 --> 00:24:55,230
control and we like we can't really

461
00:24:55,230 --> 00:24:57,750
trust and of course the question is what

462
00:24:57,750 --> 00:24:59,880
can go wrong and this is exactly the

463
00:24:59,880 --> 00:25:01,530
regime of something called data

464
00:25:01,530 --> 00:25:04,170
poisoning and in classic settings like

465
00:25:04,170 --> 00:25:05,640
the goal of the data posnick is as the

466
00:25:05,640 --> 00:25:07,350
following is you would like to coconut a

467
00:25:07,350 --> 00:25:09,240
training set by just manipulating some

468
00:25:09,240 --> 00:25:11,670
small fraction of it in a way that it

469
00:25:11,670 --> 00:25:14,100
maintains training accuracy to be small

470
00:25:14,100 --> 00:25:16,230
but it tries to hamper the organization

471
00:25:16,230 --> 00:25:18,750
so this is the particular example I have

472
00:25:18,750 --> 00:25:20,970
two distributions green and blue and

473
00:25:20,970 --> 00:25:23,220
then here is a sub sample some sample

474
00:25:23,220 --> 00:25:24,510
from distribution and that's all I see

475
00:25:24,510 --> 00:25:26,540
any I find a linear classifier

476
00:25:26,540 --> 00:25:28,680
everything work great is a large margin

477
00:25:28,680 --> 00:25:30,420
classifier it generalizes perfectly

478
00:25:30,420 --> 00:25:33,540
that's the success story of ML but now

479
00:25:33,540 --> 00:25:35,790
if I'm at adversary and I also add this

480
00:25:35,790 --> 00:25:38,880
very weird point over here and then I

481
00:25:38,880 --> 00:25:42,180
ask someone to find the best classifier

482
00:25:42,180 --> 00:25:44,730
to classify it well if he or she does no

483
00:25:44,730 --> 00:25:46,080
evil II they will come up with such

484
00:25:46,080 --> 00:25:47,580
classifier and it will just not

485
00:25:47,580 --> 00:25:50,100
generalize okay and this is essentially

486
00:25:50,100 --> 00:25:51,690
like one of the dangers of data

487
00:25:51,690 --> 00:25:53,490
poisoning that that we can get into this

488
00:25:53,490 --> 00:25:55,770
kind of being manipulated in this way so

489
00:25:55,770 --> 00:25:58,080
this is essentially like a fundamental

490
00:25:58,080 --> 00:26:00,600
problem in the classic ml and by classic

491
00:26:00,600 --> 00:26:02,400
I mean kind of three deep learning in

492
00:26:02,400 --> 00:26:03,540
particular is the whole robust

493
00:26:03,540 --> 00:26:05,100
statistics field that tries to tackle

494
00:26:05,100 --> 00:26:07,830
exactly this question however it turns

495
00:26:07,830 --> 00:26:09,780
out to be not so much of a problem in

496
00:26:09,780 --> 00:26:11,910
deep learning because essentially if you

497
00:26:11,910 --> 00:26:14,250
if deep learning classifier is is

498
00:26:14,250 --> 00:26:15,810
confronted with this kind of example

499
00:26:15,810 --> 00:26:17,100
what it really does is essentially

500
00:26:17,100 --> 00:26:18,990
memorizes the world example so

501
00:26:18,990 --> 00:26:20,400
essentially like the classifier you

502
00:26:20,400 --> 00:26:21,810
would come up with would look like this

503
00:26:21,810 --> 00:26:24,150
and actually it would generalize just

504
00:26:24,150 --> 00:26:26,880
fine so for some magical reason that we

505
00:26:26,880 --> 00:26:28,500
still have done this is better for deep

506
00:26:28,500 --> 00:26:30,380
learning this is not a problem

507
00:26:30,380 --> 00:26:33,200
however what is a problem is something a

508
00:26:33,200 --> 00:26:35,030
bit different okay so essentially

509
00:26:35,030 --> 00:26:36,860
instead of thinking of the goal of

510
00:26:36,860 --> 00:26:38,720
trying to hamper the organization across

511
00:26:38,720 --> 00:26:41,900
all of the examples okay what if we just

512
00:26:41,900 --> 00:26:43,790
wanted to manipulate the training set to

513
00:26:43,790 --> 00:26:45,350
just effect predictions on some

514
00:26:45,350 --> 00:26:47,360
particular set of examples particular

515
00:26:47,360 --> 00:26:49,640
set of inputs okay and when we look at

516
00:26:49,640 --> 00:26:52,700
that then things become actually much

517
00:26:52,700 --> 00:26:54,650
much worse so in particular what was

518
00:26:54,650 --> 00:26:56,450
shown is that you can just by

519
00:26:56,450 --> 00:26:58,490
manipulating even single example you can

520
00:26:58,490 --> 00:27:00,020
manipulate prediction across a whole

521
00:27:00,020 --> 00:27:03,800
classes of predictions later on but it

522
00:27:03,800 --> 00:27:06,320
actually gets even worse than that so

523
00:27:06,320 --> 00:27:07,580
what you can actually show is that you

524
00:27:07,580 --> 00:27:09,890
can use the ability to change just a

525
00:27:09,890 --> 00:27:12,260
tiny fraction of the data set to

526
00:27:12,260 --> 00:27:15,140
essentially take full control over the

527
00:27:15,140 --> 00:27:16,730
classifier that will be trained on this

528
00:27:16,730 --> 00:27:19,010
okay so essentially imagine that what I

529
00:27:19,010 --> 00:27:21,260
have as adversary later on is when the

530
00:27:21,260 --> 00:27:22,940
models deploy is that I could can do is

531
00:27:22,940 --> 00:27:24,500
that whenever there is an input that I

532
00:27:24,500 --> 00:27:26,960
want to have it classify the way I want

533
00:27:26,960 --> 00:27:29,120
not the way it should be classified all

534
00:27:29,120 --> 00:27:30,880
I have to do is I have to just add some

535
00:27:30,880 --> 00:27:33,440
planted trigger and whenever this

536
00:27:33,440 --> 00:27:35,390
trigger is present my classifier you

537
00:27:35,390 --> 00:27:37,460
know magically essentially makes the

538
00:27:37,460 --> 00:27:39,710
prediction that I wanted it to make okay

539
00:27:39,710 --> 00:27:42,050
so this is actually quite scary because

540
00:27:42,050 --> 00:27:44,330
you know as a user of the system you

541
00:27:44,330 --> 00:27:45,650
don't see anything wrong until I

542
00:27:45,650 --> 00:27:47,930
exercise this power and you know I think

543
00:27:47,930 --> 00:27:49,100
this is something that you know we

544
00:27:49,100 --> 00:27:50,960
should think much much more about but

545
00:27:50,960 --> 00:27:52,190
you know I will not talk about it

546
00:27:52,190 --> 00:27:52,730
anymore

547
00:27:52,730 --> 00:27:55,190
today today on Wednesday there will be a

548
00:27:55,190 --> 00:27:57,110
poster from my lab that talks about data

549
00:27:57,110 --> 00:27:58,490
poisoning and if you want to learn more

550
00:27:58,490 --> 00:28:00,680
about this you should just go there okay

551
00:28:00,680 --> 00:28:03,170
so this is data poisoning but you might

552
00:28:03,170 --> 00:28:05,030
ask okay so is this training case a

553
00:28:05,030 --> 00:28:05,390
problem

554
00:28:05,390 --> 00:28:06,770
and inferences problem is that the only

555
00:28:06,770 --> 00:28:08,240
problem in machine learning and as

556
00:28:08,240 --> 00:28:10,760
already told you the answer is no so

557
00:28:10,760 --> 00:28:13,880
imagine I train my MA amazing in a

558
00:28:13,880 --> 00:28:15,530
machine learning model and now I want to

559
00:28:15,530 --> 00:28:17,810
deploy it to the to the whole world okay

560
00:28:17,810 --> 00:28:18,830
I would just want to deploy it in the

561
00:28:18,830 --> 00:28:20,570
real world and have people use it maybe

562
00:28:20,570 --> 00:28:23,180
pay me for the usage so everything feels

563
00:28:23,180 --> 00:28:24,440
great because now you know I had my

564
00:28:24,440 --> 00:28:26,420
classifier and you know it sits on my

565
00:28:26,420 --> 00:28:28,100
server it's secure on the server I have

566
00:28:28,100 --> 00:28:29,630
you know everything is encrypted what

567
00:28:29,630 --> 00:28:31,250
has to be encrypted it sounds like

568
00:28:31,250 --> 00:28:32,960
everything is great and safe now

569
00:28:32,960 --> 00:28:35,210
essentially now nothing can harm my

570
00:28:35,210 --> 00:28:37,040
model in particular this adversely

571
00:28:37,040 --> 00:28:38,960
annoys I discussed about earlier

572
00:28:38,960 --> 00:28:41,210
actually to synthesize it in principle I

573
00:28:41,210 --> 00:28:42,680
need to have a full access to the weight

574
00:28:42,680 --> 00:28:43,460
of the model so

575
00:28:43,460 --> 00:28:45,740
you might say that in this setting you

576
00:28:45,740 --> 00:28:47,360
are safe from it because they cannot

577
00:28:47,360 --> 00:28:49,760
access your model directly so you know

578
00:28:49,760 --> 00:28:51,679
that's the limited access that this

579
00:28:51,679 --> 00:28:53,330
corresponds to does having input/output

580
00:28:53,330 --> 00:28:55,309
aspect to the model is actually problem

581
00:28:55,309 --> 00:28:58,100
the answer is no okay and there is

582
00:28:58,100 --> 00:29:00,860
essentially the whole kind of line of

583
00:29:00,860 --> 00:29:02,149
work on so-called black goose attacks

584
00:29:02,149 --> 00:29:04,130
that essentially show that just having

585
00:29:04,130 --> 00:29:05,840
input-output access to your model is

586
00:29:05,840 --> 00:29:08,360
already enough to diagnose so first of

587
00:29:08,360 --> 00:29:11,059
all to reverse-engineer it and also to

588
00:29:11,059 --> 00:29:12,350
black noise like what will be the

589
00:29:12,350 --> 00:29:16,010
perturbations that would mislead okay so

590
00:29:16,010 --> 00:29:17,510
this is not this is not setting as

591
00:29:17,510 --> 00:29:20,240
either so you know and here I will be

592
00:29:20,240 --> 00:29:21,980
talking about this on Friday in one of

593
00:29:21,980 --> 00:29:23,029
the workshops if you were very

594
00:29:23,029 --> 00:29:26,029
interested okay so you know after seeing

595
00:29:26,029 --> 00:29:27,169
all of that you might start wondering

596
00:29:27,169 --> 00:29:29,299
okay so you know we've seen some bad

597
00:29:29,299 --> 00:29:30,890
things so you know can we try to

598
00:29:30,890 --> 00:29:32,720
synthesize you know what are the kind of

599
00:29:32,720 --> 00:29:35,240
commandments of using ml in a

600
00:29:35,240 --> 00:29:36,799
responsible and safe and secure fashion

601
00:29:36,799 --> 00:29:38,570
so what should you do if you wanted to

602
00:29:38,570 --> 00:29:40,669
do that well first of all you should

603
00:29:40,669 --> 00:29:43,220
never train on data you you don't trust

604
00:29:43,220 --> 00:29:45,350
because of a later poisoning second of

605
00:29:45,350 --> 00:29:47,059
all you should never let anyone use your

606
00:29:47,059 --> 00:29:48,919
model or observe a prediction because

607
00:29:48,919 --> 00:29:51,020
they can essentially reverse-engineer it

608
00:29:51,020 --> 00:29:53,419
and synthesize the black box attack and

609
00:29:53,419 --> 00:29:55,730
also in the end you should also not

610
00:29:55,730 --> 00:29:57,830
trust your model yourself because of

611
00:29:57,830 --> 00:30:00,500
attrition examples so I can assure you

612
00:30:00,500 --> 00:30:02,510
that as long as you follow these three

613
00:30:02,510 --> 00:30:04,399
commands you are just great you will

614
00:30:04,399 --> 00:30:05,929
have a safe and secure machine learning

615
00:30:05,929 --> 00:30:08,210
that's that's very you know that's very

616
00:30:08,210 --> 00:30:10,730
relieving but of course the secesh says

617
00:30:10,730 --> 00:30:12,409
that if you want to follow the sentence

618
00:30:12,409 --> 00:30:14,240
machine learning is useless in any of

619
00:30:14,240 --> 00:30:16,850
the worst case scenarios so this might

620
00:30:16,850 --> 00:30:19,820
make us wonder so you know are we doomed

621
00:30:19,820 --> 00:30:22,159
here so maybe you know there is

622
00:30:22,159 --> 00:30:23,840
something about ml that makes it really

623
00:30:23,840 --> 00:30:25,250
really nice where we just care about

624
00:30:25,250 --> 00:30:27,320
kind of average case performance but

625
00:30:27,320 --> 00:30:29,390
whenever worst case performance comes

626
00:30:29,390 --> 00:30:31,130
into play it just fundamentally broken

627
00:30:31,130 --> 00:30:33,200
and there is nothing we can do about

628
00:30:33,200 --> 00:30:36,950
this okay and of course you know I and

629
00:30:36,950 --> 00:30:39,230
you know many of you many people work in

630
00:30:39,230 --> 00:30:39,950
this area

631
00:30:39,950 --> 00:30:41,390
believe that the answer is empathetic we

632
00:30:41,390 --> 00:30:44,330
know that actually ml can still play an

633
00:30:44,330 --> 00:30:46,039
extremely useful and extremely powerful

634
00:30:46,039 --> 00:30:48,409
role here would just needs to happen

635
00:30:48,409 --> 00:30:50,899
though is that we need to go back to

636
00:30:50,899 --> 00:30:53,210
like entry visit all the ways like all

637
00:30:53,210 --> 00:30:54,710
the tools that we use in the context of

638
00:30:54,710 --> 00:30:55,870
ml and

639
00:30:55,870 --> 00:30:57,850
freeing them with this kind of

640
00:30:57,850 --> 00:31:00,760
guarantees of being worst case robust in

641
00:31:00,760 --> 00:31:03,190
mind okay and you know even by the way

642
00:31:03,190 --> 00:31:05,650
if you don't really buy into this kind

643
00:31:05,650 --> 00:31:07,780
of story of you know some bad guys being

644
00:31:07,780 --> 00:31:09,910
out there trying to manipulate our

645
00:31:09,910 --> 00:31:11,559
systems you know you still should care

646
00:31:11,559 --> 00:31:13,450
about this other side robustness because

647
00:31:13,450 --> 00:31:15,700
you can view it as a way to stress test

648
00:31:15,700 --> 00:31:17,800
your system so if there is an input that

649
00:31:17,800 --> 00:31:19,870
makes your system is Debbie's behave you

650
00:31:19,870 --> 00:31:23,650
probably want to know that okay so let's

651
00:31:23,650 --> 00:31:25,059
talk about this essentially like the

652
00:31:25,059 --> 00:31:26,559
rest of this tutorial will be exactly

653
00:31:26,559 --> 00:31:27,970
about this it's just about trying to

654
00:31:27,970 --> 00:31:31,120
come up with models that elevate one of

655
00:31:31,120 --> 00:31:33,550
the one of the problems that we just

656
00:31:33,550 --> 00:31:35,440
identified mainly this problem of

657
00:31:35,440 --> 00:31:37,600
brittle predictions okay so essentially

658
00:31:37,600 --> 00:31:39,700
we would like to find classifiers that

659
00:31:39,700 --> 00:31:42,130
when confronted with any like small

660
00:31:42,130 --> 00:31:44,230
perturbation of a pig they still know

661
00:31:44,230 --> 00:31:46,480
this is a pig not the airbag okay and

662
00:31:46,480 --> 00:31:50,320
that's our goal okay so to answer kind

663
00:31:50,320 --> 00:31:52,990
of you know how you know how can we go

664
00:31:52,990 --> 00:31:55,809
about training models that are robust to

665
00:31:55,809 --> 00:31:57,580
this kind of adversarial perturbation

666
00:31:57,580 --> 00:31:58,900
well we should ask ourselves a question

667
00:31:58,900 --> 00:32:00,820
of you know where do these adverse solid

668
00:32:00,820 --> 00:32:02,650
examples even come from in the first

669
00:32:02,650 --> 00:32:03,160
place

670
00:32:03,160 --> 00:32:05,890
okay and to understand that we have to

671
00:32:05,890 --> 00:32:08,320
go back all the way to the kind of the

672
00:32:08,320 --> 00:32:09,970
basic tenets of our framework and

673
00:32:09,970 --> 00:32:11,559
socially like you know think about what

674
00:32:11,559 --> 00:32:13,270
is the goal of training as an

675
00:32:13,270 --> 00:32:15,130
optimization problem so what we are

676
00:32:15,130 --> 00:32:16,870
trying to do when we do training

677
00:32:16,870 --> 00:32:18,550
essentially we are trying to find the

678
00:32:18,550 --> 00:32:20,350
setting of parameters theta that

679
00:32:20,350 --> 00:32:22,720
minimizes the loss on our training

680
00:32:22,720 --> 00:32:24,340
examples or over here just one example

681
00:32:24,340 --> 00:32:27,070
but you can add the sum to generalize

682
00:32:27,070 --> 00:32:29,500
from this and you know what is nice in

683
00:32:29,500 --> 00:32:30,910
particular and deploring is that you

684
00:32:30,910 --> 00:32:33,340
know our models are differentiable in

685
00:32:33,340 --> 00:32:35,350
this parameters theta so we can use

686
00:32:35,350 --> 00:32:37,750
techniques that great in descent to find

687
00:32:37,750 --> 00:32:39,730
you know good setting of parameters that

688
00:32:39,730 --> 00:32:42,100
will make this loss as small as we as we

689
00:32:42,100 --> 00:32:43,780
would like it to be okay again

690
00:32:43,780 --> 00:32:45,220
there is still much to be understood but

691
00:32:45,220 --> 00:32:47,559
at the basic level efficient practice

692
00:32:47,559 --> 00:32:49,990
that's what seem to be happening so this

693
00:32:49,990 --> 00:32:51,910
is a very convenient thing that this is

694
00:32:51,910 --> 00:32:53,080
the problem we are solving and we can

695
00:32:53,080 --> 00:32:55,000
solve it in a very kind of in a fairly

696
00:32:55,000 --> 00:32:57,700
principled way so unfortunately this is

697
00:32:57,700 --> 00:32:59,410
also like the fact that you know in

698
00:32:59,410 --> 00:33:01,420
particular deep learning networks are so

699
00:33:01,420 --> 00:33:03,790
convenient to optimize in various shapes

700
00:33:03,790 --> 00:33:05,890
and forms is also where kind of you can

701
00:33:05,890 --> 00:33:07,789
this is what you can exploit to see

702
00:33:07,789 --> 00:33:09,470
sighs this illusory perturbations

703
00:33:09,470 --> 00:33:11,299
because essentially to get in a vessel

704
00:33:11,299 --> 00:33:13,249
examples all you have to do is just like

705
00:33:13,249 --> 00:33:14,869
look at this program program again and

706
00:33:14,869 --> 00:33:16,970
just repair amortize it so you just

707
00:33:16,970 --> 00:33:18,139
freeze the parameters let's say you

708
00:33:18,139 --> 00:33:20,389
trained good parameters and you are done

709
00:33:20,389 --> 00:33:22,159
for now and now I would just like to

710
00:33:22,159 --> 00:33:24,649
find a perturbation of my input that

711
00:33:24,649 --> 00:33:26,690
will make the you know this prediction

712
00:33:26,690 --> 00:33:27,320
to be bad

713
00:33:27,320 --> 00:33:28,999
so essentially what it means is that we

714
00:33:28,999 --> 00:33:30,739
want to find the perturbation that will

715
00:33:30,739 --> 00:33:33,259
make the loss to be large as opposed to

716
00:33:33,259 --> 00:33:35,809
small and you know the problem is that

717
00:33:35,809 --> 00:33:38,029
you know as much as you know our system

718
00:33:38,029 --> 00:33:40,129
was kind of our machine learning model

719
00:33:40,129 --> 00:33:42,470
was differentiable in theta it's also

720
00:33:42,470 --> 00:33:44,779
differentiable in Delta so you can use

721
00:33:44,779 --> 00:33:46,820
techniques that gradient descent to find

722
00:33:46,820 --> 00:33:48,229
this path perturbation and that's

723
00:33:48,229 --> 00:33:50,149
exactly how the perturbations I showed

724
00:33:50,149 --> 00:33:52,190
you earlier were you know where were

725
00:33:52,190 --> 00:33:55,429
discovered okay so there's there is

726
00:33:55,429 --> 00:33:57,139
however like an important question to

727
00:33:57,139 --> 00:33:58,580
ask you when we set up an optimization

728
00:33:58,580 --> 00:34:00,619
problem namely you know the question is

729
00:34:00,619 --> 00:34:02,090
what kind of when I do the optimization

730
00:34:02,090 --> 00:34:04,879
what kind of values of Delta should I

731
00:34:04,879 --> 00:34:06,710
allow okay and this is an excellent

732
00:34:06,710 --> 00:34:08,929
question and so far we know essentially

733
00:34:08,929 --> 00:34:11,149
in principle you know if Delta was just

734
00:34:11,149 --> 00:34:12,980
a difference between my current example

735
00:34:12,980 --> 00:34:15,139
and some other example that is from

736
00:34:15,139 --> 00:34:17,210
different class well we wouldn't worry

737
00:34:17,210 --> 00:34:18,619
about that that's that's something that

738
00:34:18,619 --> 00:34:19,940
we would like our machine learning to do

739
00:34:19,940 --> 00:34:21,530
this to change prediction if we change

740
00:34:21,530 --> 00:34:23,389
the input to a different class but of

741
00:34:23,389 --> 00:34:25,219
course so this is only interesting if

742
00:34:25,219 --> 00:34:27,530
Delta is imperceptible or you know

743
00:34:27,530 --> 00:34:29,869
inconsequential in some way and again

744
00:34:29,869 --> 00:34:32,270
now how to formally capture what

745
00:34:32,270 --> 00:34:34,129
imperceptible or inconsequential means

746
00:34:34,129 --> 00:34:36,980
well there is a bunch of a bunch of

747
00:34:36,980 --> 00:34:38,329
attempts but honestly this is like a

748
00:34:38,329 --> 00:34:39,799
really really hard problem essentially

749
00:34:39,799 --> 00:34:41,690
woody particular for vision tasks it

750
00:34:41,690 --> 00:34:43,668
would require us to formalize human

751
00:34:43,668 --> 00:34:45,379
vision which we are nowhere close to be

752
00:34:45,379 --> 00:34:47,449
so you know so this is definitely like a

753
00:34:47,449 --> 00:34:48,829
very important question it's like what

754
00:34:48,829 --> 00:34:50,929
kind of perturbation of the input our

755
00:34:50,929 --> 00:34:52,909
vision system should be kind of robust

756
00:34:52,909 --> 00:34:54,829
to and we don't really go into that I

757
00:34:54,829 --> 00:34:56,270
just want to mention that you know in

758
00:34:56,270 --> 00:34:57,920
the rest of the talk we will focus on

759
00:34:57,920 --> 00:35:00,140
this LP / LP perturbation especially

760
00:35:00,140 --> 00:35:02,059
when we just say that we don't want we

761
00:35:02,059 --> 00:35:04,490
are just bounding the LP norm of the

762
00:35:04,490 --> 00:35:07,010
change of each individual pixel so this

763
00:35:07,010 --> 00:35:09,470
is a very simplistic metric but in some

764
00:35:09,470 --> 00:35:11,089
ways you know it is a metric to

765
00:35:11,089 --> 00:35:13,760
definitely should be robust to before we

766
00:35:13,760 --> 00:35:15,589
get anything more complicated so that's

767
00:35:15,589 --> 00:35:18,010
why this is a good kind of milestone

768
00:35:18,010 --> 00:35:20,770
- like to move towards when we work on

769
00:35:20,770 --> 00:35:24,250
the cell robustus okay so so this is our

770
00:35:24,250 --> 00:35:26,530
goal and now the question is this is

771
00:35:26,530 --> 00:35:27,910
where every cell examples are coming

772
00:35:27,910 --> 00:35:30,340
from and now the question is okay so how

773
00:35:30,340 --> 00:35:32,590
would you go actually about getting

774
00:35:32,590 --> 00:35:35,290
models that aren't are not like that are

775
00:35:35,290 --> 00:35:37,780
not so is so vulnerable to these

776
00:35:37,780 --> 00:35:40,210
perturbations and in particular there

777
00:35:40,210 --> 00:35:42,040
were quite a bunch of people who is

778
00:35:42,040 --> 00:35:43,510
usually claimed when the adversarial

779
00:35:43,510 --> 00:35:45,850
examples became a thing a lot of people

780
00:35:45,850 --> 00:35:47,920
claim that this is just an evidence of

781
00:35:47,920 --> 00:35:49,420
failure of a machine learning of our

782
00:35:49,420 --> 00:35:51,340
machine to tolki that in some way we

783
00:35:51,340 --> 00:35:53,650
failed because we let these other

784
00:35:53,650 --> 00:35:56,680
examples to arise okay and I think the

785
00:35:56,680 --> 00:35:59,109
crucial thing to realize to understand

786
00:35:59,109 --> 00:36:00,609
you know how to get robust models rise

787
00:36:00,609 --> 00:36:04,119
that we did not fail at all okay so at

788
00:36:04,119 --> 00:36:05,770
this area like the lack of detail

789
00:36:05,770 --> 00:36:08,590
robustness is not you know it's not our

790
00:36:08,590 --> 00:36:10,210
failure because it's act like the

791
00:36:10,210 --> 00:36:11,859
existence of the examples is completely

792
00:36:11,859 --> 00:36:14,410
not at odds with our current you know

793
00:36:14,410 --> 00:36:16,390
current machine learning toolkit in

794
00:36:16,390 --> 00:36:19,119
particular what it is that our models

795
00:36:19,119 --> 00:36:21,040
try to achieve most of the time what

796
00:36:21,040 --> 00:36:22,540
they try to achieve is something we call

797
00:36:22,540 --> 00:36:24,220
some the generalization so what I would

798
00:36:24,220 --> 00:36:25,720
like to do is I would like to if I

799
00:36:25,720 --> 00:36:28,270
sample an example from my distribution

800
00:36:28,270 --> 00:36:31,000
that is in the sky I would like the loss

801
00:36:31,000 --> 00:36:33,160
of my classifier on this example to be

802
00:36:33,160 --> 00:36:36,340
small in in expectation okay so in

803
00:36:36,340 --> 00:36:38,080
expectation is great for average case

804
00:36:38,080 --> 00:36:40,210
performance but adversarial examples is

805
00:36:40,210 --> 00:36:43,119
a worst-case notion it's actually it's

806
00:36:43,119 --> 00:36:45,940
of measure measure 0 so this expectation

807
00:36:45,940 --> 00:36:48,910
is completely invariant on existence or

808
00:36:48,910 --> 00:36:52,060
nonexistence of original examples so in

809
00:36:52,060 --> 00:36:54,820
a sense if we never tried to get like

810
00:36:54,820 --> 00:36:57,580
essentially if the goal that all of our

811
00:36:57,580 --> 00:36:58,720
machine learning took its driving

812
00:36:58,720 --> 00:37:00,760
towards its completely invariant other

813
00:37:00,760 --> 00:37:02,500
existence were some examples you should

814
00:37:02,500 --> 00:37:05,680
not be surprised that they do exist okay

815
00:37:05,680 --> 00:37:08,590
so it is essentially if you wanted to

816
00:37:08,590 --> 00:37:11,230
get models where you know you don't get

817
00:37:11,230 --> 00:37:13,210
into trouble for this error examples

818
00:37:13,210 --> 00:37:14,530
what you have to do well you have to

819
00:37:14,530 --> 00:37:16,600
change the goal you are trying to

820
00:37:16,600 --> 00:37:19,270
achieve and the goal that you get that

821
00:37:19,270 --> 00:37:20,680
you should arrived at with something

822
00:37:20,680 --> 00:37:22,630
called know let's say adversary are

823
00:37:22,630 --> 00:37:24,369
robust generalization in which what you

824
00:37:24,369 --> 00:37:26,530
do is saying or what I would like to do

825
00:37:26,530 --> 00:37:26,940
is I

826
00:37:26,940 --> 00:37:29,220
would like to do well in expectation not

827
00:37:29,220 --> 00:37:30,869
only on the sampled example from

828
00:37:30,869 --> 00:37:33,420
distribution but also on the worst case

829
00:37:33,420 --> 00:37:36,060
perturbation of that example and just

830
00:37:36,060 --> 00:37:38,160
like plugging in the smacks inside of

831
00:37:38,160 --> 00:37:40,920
expectation essentially is is the whole

832
00:37:40,920 --> 00:37:43,170
difference between what we were doing so

833
00:37:43,170 --> 00:37:45,480
far and what we need to do for adversary

834
00:37:45,480 --> 00:37:46,109
robustness

835
00:37:46,109 --> 00:37:49,640
okay so this is you know kind of this is

836
00:37:49,640 --> 00:37:52,319
this is the general outlook of like what

837
00:37:52,319 --> 00:37:53,460
is the problem we are trying to tackle

838
00:37:53,460 --> 00:37:55,470
and where we are going from now so now

839
00:37:55,470 --> 00:37:57,359
we know that our goal is the original

840
00:37:57,359 --> 00:37:59,790
robustness now it's time for taking a

841
00:37:59,790 --> 00:38:01,800
deeper dive into this topic in

842
00:38:01,800 --> 00:38:03,990
particular Zika now we'll talk in a bit

843
00:38:03,990 --> 00:38:05,940
more in depth about other examples and

844
00:38:05,940 --> 00:38:08,460
about kind of ways to figure out how to

845
00:38:08,460 --> 00:38:12,060
pro to even check if there exists a list

846
00:38:12,060 --> 00:38:13,470
of examples for our system or no and

847
00:38:13,470 --> 00:38:15,960
then he will talk about actual details

848
00:38:15,960 --> 00:38:17,790
of how do you go about training models

849
00:38:17,790 --> 00:38:19,619
that are robust to it and then I will

850
00:38:19,619 --> 00:38:21,510
come back a bit and just give you a bit

851
00:38:21,510 --> 00:38:23,130
broader perspective again about like

852
00:38:23,130 --> 00:38:25,140
what can we do with the toast to achieve

853
00:38:25,140 --> 00:38:27,770
over here thank you

854
00:38:27,770 --> 00:38:34,739
[Applause]

855
00:38:37,120 --> 00:38:41,080
alright this on good

856
00:38:41,080 --> 00:38:45,830
so as Alex mentioned after that great

857
00:38:45,830 --> 00:38:48,680
sort of broad introduction I'm gonna

858
00:38:48,680 --> 00:38:50,270
have the the fun job of actually going

859
00:38:50,270 --> 00:38:52,040
to some depth about how you do it so

860
00:38:52,040 --> 00:38:53,420
I'll put this down in a second just just

861
00:38:53,420 --> 00:38:55,490
one second one thing I actually do want

862
00:38:55,490 --> 00:38:56,990
to highlight which Alex didn't mention

863
00:38:56,990 --> 00:38:59,030
maybe this try to zoom in here is that

864
00:38:59,030 --> 00:39:00,860
we actually also have an accompanying

865
00:39:00,860 --> 00:39:03,350
website this tutorial so adversarial -

866
00:39:03,350 --> 00:39:07,460
ml - Ettore org has a set of notes that

867
00:39:07,460 --> 00:39:09,620
go through pretty much all the details

868
00:39:09,620 --> 00:39:11,930
when described here and so this is the

869
00:39:11,930 --> 00:39:14,930
website here and the notes here are

870
00:39:14,930 --> 00:39:17,510
actually in as written as Jupiter

871
00:39:17,510 --> 00:39:19,480
notebooks so there's there's notes with

872
00:39:19,480 --> 00:39:22,760
you know pros with examples but also

873
00:39:22,760 --> 00:39:24,140
with code that will do everything I'm

874
00:39:24,140 --> 00:39:25,490
going to talk about so all the examples

875
00:39:25,490 --> 00:39:27,410
I have there's actually code and

876
00:39:27,410 --> 00:39:29,060
walkthroughs on how to actually do this

877
00:39:29,060 --> 00:39:30,650
and you can actually download these

878
00:39:30,650 --> 00:39:32,390
notes as Jupiter notebooks or just read

879
00:39:32,390 --> 00:39:34,850
them on the web and use that to to go

880
00:39:34,850 --> 00:39:37,250
through sort of these these examples so

881
00:39:37,250 --> 00:39:38,900
if anything I do here goes a little bit

882
00:39:38,900 --> 00:39:41,060
fast which it might please just know you

883
00:39:41,060 --> 00:39:42,770
have this resource we'll have it again

884
00:39:42,770 --> 00:39:44,480
at the end it's also I think in our

885
00:39:44,480 --> 00:39:45,920
Twitter post about this and you can you

886
00:39:45,920 --> 00:39:47,960
can use that okay

887
00:39:47,960 --> 00:39:58,580
so I'll look at that lovely display when

888
00:39:58,580 --> 00:40:01,570
I quit my yeah

889
00:40:04,800 --> 00:40:07,400
okay

890
00:40:08,359 --> 00:40:11,749
there we go all right so as Alex

891
00:40:11,749 --> 00:40:13,069
mentioned he starts to behave a Brian

892
00:40:13,069 --> 00:40:15,829
reduction and we are in my port I really

893
00:40:15,829 --> 00:40:17,450
wanna emphasize focusing on this problem

894
00:40:17,450 --> 00:40:21,619
of a test time adversarial attacks so of

895
00:40:21,619 --> 00:40:22,849
course adversary but this is much

896
00:40:22,849 --> 00:40:24,650
broader than this but this is the topic

897
00:40:24,650 --> 00:40:26,960
I'm gonna focus on and particularly I'm

898
00:40:26,960 --> 00:40:29,029
gonna kind of focus on in some sense the

899
00:40:29,029 --> 00:40:31,519
the attack and then the defense so the

900
00:40:31,519 --> 00:40:32,450
first part is going to be about

901
00:40:32,450 --> 00:40:34,519
constructing adversarial examples or

902
00:40:34,519 --> 00:40:36,170
maybe a little bit more broadly also

903
00:40:36,170 --> 00:40:37,880
verifying whether or not these things

904
00:40:37,880 --> 00:40:39,890
exist and the second part is going to be

905
00:40:39,890 --> 00:40:42,259
about training robust models and the

906
00:40:42,259 --> 00:40:44,140
particular first we'll talk about

907
00:40:44,140 --> 00:40:46,190
because certain gave us examples there's

908
00:40:46,190 --> 00:40:47,839
a verification then average shell

909
00:40:47,839 --> 00:40:49,249
training as well as probably robust

910
00:40:49,249 --> 00:40:53,329
training okay so as Alex mentioned the

911
00:40:53,329 --> 00:40:56,059
big picture here is the following we

912
00:40:56,059 --> 00:41:00,700
want to train a model ideally that is

913
00:41:00,700 --> 00:41:03,319
well general as well not just on a fixed

914
00:41:03,319 --> 00:41:05,420
data set but actually on adversarial

915
00:41:05,420 --> 00:41:08,480
perturbations of that data set all right

916
00:41:08,480 --> 00:41:11,329
of course transportation is not you know

917
00:41:11,329 --> 00:41:13,160
it's not always easy to do so of course

918
00:41:13,160 --> 00:41:14,119
what we're going to do in practice is

919
00:41:14,119 --> 00:41:15,499
we're going to have some fun I theta set

920
00:41:15,499 --> 00:41:18,049
that we're going to minimize this

921
00:41:18,049 --> 00:41:20,210
objective over all right so on some data

922
00:41:20,210 --> 00:41:22,609
set s we're going to minimize this

923
00:41:22,609 --> 00:41:25,489
robust objective here and what this

924
00:41:25,489 --> 00:41:27,259
portion the talk is about these sort of

925
00:41:27,259 --> 00:41:31,249
two portions really is part one is gonna

926
00:41:31,249 --> 00:41:34,670
be about the inner maximization so what

927
00:41:34,670 --> 00:41:36,579
that means is it's is about either

928
00:41:36,579 --> 00:41:40,700
finding adversarial examples or actually

929
00:41:40,700 --> 00:41:43,400
somehow otherwise verifying that one

930
00:41:43,400 --> 00:41:45,230
cannot exist I'll talk about what I mean

931
00:41:45,230 --> 00:41:48,349
by that in a second and part two is

932
00:41:48,349 --> 00:41:50,809
going to be about training a robust

933
00:41:50,809 --> 00:41:52,989
classifier so how do we solve the outer

934
00:41:52,989 --> 00:41:55,369
minimization problem now that we've

935
00:41:55,369 --> 00:41:57,289
already broken machine learning will

936
00:41:57,289 --> 00:41:59,299
tell you how at least we might be able

937
00:41:59,299 --> 00:42:04,190
to fix it in some cases all right so

938
00:42:04,190 --> 00:42:08,630
let's dive into Part one now here's a

939
00:42:08,630 --> 00:42:11,089
view kind of of the lost landscape and

940
00:42:11,089 --> 00:42:12,829
the problem that we have all right so we

941
00:42:12,829 --> 00:42:15,170
have some perturbation region Delta

942
00:42:15,170 --> 00:42:17,359
that's shown on the bottom there we have

943
00:42:17,359 --> 00:42:19,190
some initial point Delta zero we have

944
00:42:19,190 --> 00:42:21,529
some loss function that exists for

945
00:42:21,529 --> 00:42:22,200
different perturbed

946
00:42:22,200 --> 00:42:25,710
of delta and so if we want to maximize

947
00:42:25,710 --> 00:42:29,339
this quantity what Alex described is

948
00:42:29,339 --> 00:42:30,210
just sort of well we can just run

949
00:42:30,210 --> 00:42:31,890
grading descent right but what I

950
00:42:31,890 --> 00:42:32,970
actually want to highlight here is

951
00:42:32,970 --> 00:42:34,650
there's other possibilities as well and

952
00:42:34,650 --> 00:42:35,730
actually understanding these things

953
00:42:35,730 --> 00:42:37,020
really helps you understand the nature

954
00:42:37,020 --> 00:42:40,020
of adversarial examples and verification

955
00:42:40,020 --> 00:42:43,380
in in this setting so we really have

956
00:42:43,380 --> 00:42:46,170
three options the first is we can do

957
00:42:46,170 --> 00:42:47,700
local search right we can just do sort

958
00:42:47,700 --> 00:42:49,950
of search to find an adversarial example

959
00:42:49,950 --> 00:42:51,030
and this is actually the most common

960
00:42:51,030 --> 00:42:52,380
thing that's done in practice this is

961
00:42:52,380 --> 00:42:54,300
sort of what people typically talk about

962
00:42:54,300 --> 00:42:56,690
when we talk about adversarial examples

963
00:42:56,690 --> 00:42:59,010
but there are other possibilities too

964
00:42:59,010 --> 00:43:00,150
when I was essentially what I want to

965
00:43:00,150 --> 00:43:01,560
really emphasize here is that there's

966
00:43:01,560 --> 00:43:03,540
other approaches are also possible so

967
00:43:03,540 --> 00:43:06,030
it's also possible actually maybe not

968
00:43:06,030 --> 00:43:07,349
for large models but for relatively

969
00:43:07,349 --> 00:43:09,660
small models to actually solve this

970
00:43:09,660 --> 00:43:11,910
problem exactly using techniques and

971
00:43:11,910 --> 00:43:13,530
combinatorial optimization so weak

972
00:43:13,530 --> 00:43:14,820
actually could not just do local search

973
00:43:14,820 --> 00:43:17,070
but we can actually find the worst case

974
00:43:17,070 --> 00:43:20,089
example for a given classifier and

975
00:43:20,089 --> 00:43:23,970
finally we can also form an upper bound

976
00:43:23,970 --> 00:43:27,329
on this loss function at least over the

977
00:43:27,329 --> 00:43:29,880
over the region of question and if this

978
00:43:29,880 --> 00:43:31,410
upper bound has nice properties like

979
00:43:31,410 --> 00:43:33,180
it's convex or really concave those are

980
00:43:33,180 --> 00:43:35,849
maximizing it we can actually find exact

981
00:43:35,849 --> 00:43:36,930
solutions as well

982
00:43:36,930 --> 00:43:39,750
they will also let us give verifications

983
00:43:39,750 --> 00:43:42,569
of whether or not an adversarial example

984
00:43:42,569 --> 00:43:44,700
exists or not so the first part of the

985
00:43:44,700 --> 00:43:45,780
talk here is really going to be about

986
00:43:45,780 --> 00:43:47,250
running through these three

987
00:43:47,250 --> 00:43:48,869
possibilities and kind of checking

988
00:43:48,869 --> 00:43:51,470
what's possible and what's not possible

989
00:43:51,470 --> 00:43:53,849
one thing I do want to emphasize though

990
00:43:53,849 --> 00:43:56,880
is that all of this work though it's

991
00:43:56,880 --> 00:43:58,710
relatively new in machine learning and

992
00:43:58,710 --> 00:44:00,900
people in particular it actually goes

993
00:44:00,900 --> 00:44:03,150
back a long time to the topic of robust

994
00:44:03,150 --> 00:44:05,670
optimization largely in linear models

995
00:44:05,670 --> 00:44:07,380
and so the only point I want to make

996
00:44:07,380 --> 00:44:10,440
here is that for linear models that

997
00:44:10,440 --> 00:44:11,910
actually turns out you can solve this

998
00:44:11,910 --> 00:44:14,099
problem exactly and all those three

999
00:44:14,099 --> 00:44:15,869
cases I described before actually

1000
00:44:15,869 --> 00:44:17,970
collapse to one so without too much

1001
00:44:17,970 --> 00:44:19,290
detail here if we have actually just in

1002
00:44:19,290 --> 00:44:20,069
this case consider a binary

1003
00:44:20,069 --> 00:44:22,650
classification problem it's actually

1004
00:44:22,650 --> 00:44:24,000
possible to find the worst with with

1005
00:44:24,000 --> 00:44:25,680
with norm bounds for your regions you

1006
00:44:25,680 --> 00:44:27,180
can find the worst case perturbations

1007
00:44:27,180 --> 00:44:28,650
it's just ort of the ones closest to the

1008
00:44:28,650 --> 00:44:31,140
to the decision boundary and actually

1009
00:44:31,140 --> 00:44:32,490
sort of formally what this means is that

1010
00:44:32,490 --> 00:44:34,829
you take your optimization here you can

1011
00:44:34,829 --> 00:44:35,900
actually push the

1012
00:44:35,900 --> 00:44:38,240
the max inside the loss because the loss

1013
00:44:38,240 --> 00:44:39,920
is negative is monotonic decreasing it

1014
00:44:39,920 --> 00:44:42,559
becomes a min and this can be solved

1015
00:44:42,559 --> 00:44:44,240
analytically in terms of the dual norm

1016
00:44:44,240 --> 00:44:46,460
of your perturbation region alright so

1017
00:44:46,460 --> 00:44:47,690
this is actually not I'm not gonna go

1018
00:44:47,690 --> 00:44:48,980
into detail about how this works there

1019
00:44:48,980 --> 00:44:50,299
are of course a lot more details in the

1020
00:44:50,299 --> 00:44:52,190
toriel but I just want to highlight the

1021
00:44:52,190 --> 00:44:53,749
fact that this is not a new

1022
00:44:53,749 --> 00:44:54,920
probabilities in the context of linear

1023
00:44:54,920 --> 00:44:57,460
models this goes back a very long time

1024
00:44:57,460 --> 00:45:00,109
ok so now let me get to this sort of

1025
00:45:00,109 --> 00:45:02,150
these three cases of how we deal or

1026
00:45:02,150 --> 00:45:05,960
think about adversarial examples in the

1027
00:45:05,960 --> 00:45:08,509
first case we talk about local search so

1028
00:45:08,509 --> 00:45:10,430
you know everyone knows of course that

1029
00:45:10,430 --> 00:45:12,829
the lost landscape of deep learning is

1030
00:45:12,829 --> 00:45:14,990
or DB deep classifiers there's it's non

1031
00:45:14,990 --> 00:45:17,180
convex and Nasty's if it's local optima

1032
00:45:17,180 --> 00:45:19,549
but the big the great thing about deep

1033
00:45:19,549 --> 00:45:20,599
learning is we just don't care about

1034
00:45:20,599 --> 00:45:21,920
this anymore right we don't care about

1035
00:45:21,920 --> 00:45:23,690
the fact that we're optimizing non

1036
00:45:23,690 --> 00:45:25,430
convex functions we're just gonna do it

1037
00:45:25,430 --> 00:45:26,839
anyway and we're gonna try to

1038
00:45:26,839 --> 00:45:29,480
empirically find an attack that exists

1039
00:45:29,480 --> 00:45:31,910
within some norm bound so how do we do

1040
00:45:31,910 --> 00:45:33,650
this and there are a lot of ways of

1041
00:45:33,650 --> 00:45:35,029
doing this but the main tool I wanna

1042
00:45:35,029 --> 00:45:36,140
highlight was actually what Alex

1043
00:45:36,140 --> 00:45:39,230
mentioned to our variance of projected

1044
00:45:39,230 --> 00:45:41,599
gradient descent the idea here is very

1045
00:45:41,599 --> 00:45:44,690
simple we want to optimize our

1046
00:45:44,690 --> 00:45:46,579
perturbation bound in some region Delta

1047
00:45:46,579 --> 00:45:48,019
that's represented by the gray region

1048
00:45:48,019 --> 00:45:50,690
there and what we do is we just take a

1049
00:45:50,690 --> 00:45:53,930
step in the direction of the gradient of

1050
00:45:53,930 --> 00:45:56,119
the loss function so we're taking a

1051
00:45:56,119 --> 00:45:56,989
post-up in the positive direction

1052
00:45:56,989 --> 00:45:58,430
because we're trying to maximize the

1053
00:45:58,430 --> 00:46:01,400
loss function and then we just project

1054
00:46:01,400 --> 00:46:03,380
back on to the set so we find the

1055
00:46:03,380 --> 00:46:05,329
closest point in the set to our to our

1056
00:46:05,329 --> 00:46:07,700
projected point and we just repeat this

1057
00:46:07,700 --> 00:46:09,619
process and this is what and then we

1058
00:46:09,619 --> 00:46:12,170
just hopefully will find our way to

1059
00:46:12,170 --> 00:46:16,910
points of high of high cost and actually

1060
00:46:16,910 --> 00:46:18,890
I would say that most attacks not quite

1061
00:46:18,890 --> 00:46:20,539
all of them most tactics that leads in

1062
00:46:20,539 --> 00:46:22,670
practice are based on some variant of

1063
00:46:22,670 --> 00:46:23,630
this but they're often based upon

1064
00:46:23,630 --> 00:46:26,930
simplified variants of this so let me

1065
00:46:26,930 --> 00:46:28,460
describe actually one for the most part

1066
00:46:28,460 --> 00:46:29,900
I'm actually not going I'm gonna talk

1067
00:46:29,900 --> 00:46:31,430
about sort of the methodologies here not

1068
00:46:31,430 --> 00:46:33,109
specific attacks people have proposed in

1069
00:46:33,109 --> 00:46:34,519
the past because I want to follow this

1070
00:46:34,519 --> 00:46:36,170
general framework of predicted gradient

1071
00:46:36,170 --> 00:46:37,700
descent but I think it is worth

1072
00:46:37,700 --> 00:46:39,440
mentioning one particular type of attack

1073
00:46:39,440 --> 00:46:40,700
which was actually one of the ones that

1074
00:46:40,700 --> 00:46:42,230
kind of ignited a lot of the interest in

1075
00:46:42,230 --> 00:46:44,210
this field which is the fast gradient

1076
00:46:44,210 --> 00:46:46,190
sign methods this is work by Ian good

1077
00:46:46,190 --> 00:46:49,880
fellow and others in 2014 so

1078
00:46:49,880 --> 00:46:51,980
the way this arises from gradient

1079
00:46:51,980 --> 00:46:53,750
descent at least in our sort of

1080
00:46:53,750 --> 00:46:56,600
viewpoint is that we can take now our

1081
00:46:56,600 --> 00:46:59,240
perturbation region Delta to be the L

1082
00:46:59,240 --> 00:47:02,200
infinity ball we're thinking about a

1083
00:47:02,200 --> 00:47:04,610
region where the L infinity norm of the

1084
00:47:04,610 --> 00:47:06,050
perturbation is less than Delta so it's

1085
00:47:06,050 --> 00:47:07,520
less than Epsilon ie

1086
00:47:07,520 --> 00:47:09,170
the perturbation each coordinate the

1087
00:47:09,170 --> 00:47:11,000
perturbation is is between negative

1088
00:47:11,000 --> 00:47:14,000
epsilon and positive epsilon in this

1089
00:47:14,000 --> 00:47:15,260
case actually projection is very easy

1090
00:47:15,260 --> 00:47:16,850
you just clip to the ball so the

1091
00:47:16,850 --> 00:47:18,200
friction that's one reason why this is a

1092
00:47:18,200 --> 00:47:19,580
nice norm bound you just clip the

1093
00:47:19,580 --> 00:47:20,420
entries to the ball that's the

1094
00:47:20,420 --> 00:47:22,910
projection operator and if you think

1095
00:47:22,910 --> 00:47:24,920
about what participating descent does in

1096
00:47:24,920 --> 00:47:29,120
this in this setting if you take a big

1097
00:47:29,120 --> 00:47:31,550
enough step size in the direction of the

1098
00:47:31,550 --> 00:47:33,800
gradient you will end up outside the

1099
00:47:33,800 --> 00:47:35,540
ball right you'll end up outside the

1100
00:47:35,540 --> 00:47:36,770
ball always because there's always some

1101
00:47:36,770 --> 00:47:39,620
direction and if you then take a big

1102
00:47:39,620 --> 00:47:40,850
enough step you'll actually and when you

1103
00:47:40,850 --> 00:47:43,040
project back you'll actually always be

1104
00:47:43,040 --> 00:47:45,050
at a corner of the ball right so you

1105
00:47:45,050 --> 00:47:46,550
basically follow this direct gradient

1106
00:47:46,550 --> 00:47:48,230
direction you put it back and for a big

1107
00:47:48,230 --> 00:47:49,370
enough step you can see you'll always

1108
00:47:49,370 --> 00:47:51,800
hit a corner and that corner is just

1109
00:47:51,800 --> 00:47:55,910
equal to epsilon times the sign of the

1110
00:47:55,910 --> 00:47:57,980
gradient in each coordinate right and

1111
00:47:57,980 --> 00:48:00,320
that is exactly the fast gradient sign

1112
00:48:00,320 --> 00:48:02,630
method it's very nice because it sort of

1113
00:48:02,630 --> 00:48:04,160
gives you as big a step as you can

1114
00:48:04,160 --> 00:48:06,620
possibly take within your ball very

1115
00:48:06,620 --> 00:48:07,910
cheaply because only take you only to

1116
00:48:07,910 --> 00:48:09,500
evaluate the greeting at once and then

1117
00:48:09,500 --> 00:48:11,150
you get that that step there so it's

1118
00:48:11,150 --> 00:48:12,650
become a very sort of popular technique

1119
00:48:12,650 --> 00:48:15,080
for doing this all right so let's see

1120
00:48:15,080 --> 00:48:18,680
how this actually looks in practice and

1121
00:48:18,680 --> 00:48:21,230
I'm going to apologize now to everyone

1122
00:48:21,230 --> 00:48:23,450
else because this is 2018 almost does in

1123
00:48:23,450 --> 00:48:25,010
nineteen and you're gonna see almost

1124
00:48:25,010 --> 00:48:28,220
entirely Emma statisti toriel so I'm

1125
00:48:28,220 --> 00:48:29,660
gonna pause it for this in advance the

1126
00:48:29,660 --> 00:48:31,640
reason is actually not because these are

1127
00:48:31,640 --> 00:48:33,620
the best examples to show attacks in

1128
00:48:33,620 --> 00:48:35,030
because we know we can attack as Alex

1129
00:48:35,030 --> 00:48:36,410
Otte imagenet we actually have an

1130
00:48:36,410 --> 00:48:38,810
example of code to attack on a larger

1131
00:48:38,810 --> 00:48:40,790
problem here but for some of the defense

1132
00:48:40,790 --> 00:48:42,200
mechanisms especially the the

1133
00:48:42,200 --> 00:48:45,140
combinatorial optimization methods we're

1134
00:48:45,140 --> 00:48:46,940
still kind of at the scale of imagenet

1135
00:48:46,940 --> 00:48:48,590
here so I want to be very explicit about

1136
00:48:48,590 --> 00:48:50,360
that is that a lot of these defense

1137
00:48:50,360 --> 00:48:52,430
techniques do not yet scale to really

1138
00:48:52,430 --> 00:48:54,500
large problems and so you know so that

1139
00:48:54,500 --> 00:48:56,060
you also have code that can run this all

1140
00:48:56,060 --> 00:48:57,350
and you can run it maybe on one key P

1141
00:48:57,350 --> 00:48:59,180
you not you know in a few minutes not

1142
00:48:59,180 --> 00:49:00,200
days

1143
00:49:00,200 --> 00:49:01,819
we are gonna we are using

1144
00:49:01,819 --> 00:49:03,469
here as an example and in particular

1145
00:49:03,469 --> 00:49:05,509
we're gonna use two networks a two-layer

1146
00:49:05,509 --> 00:49:07,309
fully connected Network and a six layer

1147
00:49:07,309 --> 00:49:09,259
convolutional network to illustrate a

1148
00:49:09,259 --> 00:49:12,380
lot of these points okay so let's see

1149
00:49:12,380 --> 00:49:13,910
what these fast gradients sign methods

1150
00:49:13,910 --> 00:49:15,829
actually do and they're also another

1151
00:49:15,829 --> 00:49:16,819
thing about and this is these things a

1152
00:49:16,819 --> 00:49:17,809
little bit more apparent than they were

1153
00:49:17,809 --> 00:49:20,209
you can't even see them in an image net

1154
00:49:20,209 --> 00:49:21,349
but you actually can see them here at

1155
00:49:21,349 --> 00:49:24,380
MMS okay so we're taking a bigger almost

1156
00:49:24,380 --> 00:49:26,209
like a perceptron and we're gonna take

1157
00:49:26,209 --> 00:49:29,209
adjust the image in a step of the in the

1158
00:49:29,209 --> 00:49:30,380
direction of the sign of the gradient

1159
00:49:30,380 --> 00:49:33,739
using FG SM and we get an image like

1160
00:49:33,739 --> 00:49:35,089
this right so these things are being

1161
00:49:35,089 --> 00:49:36,380
adjusted for now I'm actually not

1162
00:49:36,380 --> 00:49:37,609
worrying about there being negative

1163
00:49:37,609 --> 00:49:39,199
pixels you can do all this the same

1164
00:49:39,199 --> 00:49:40,789
thing if you just clip the absolute

1165
00:49:40,789 --> 00:49:43,190
range of the pixels being between zero

1166
00:49:43,190 --> 00:49:44,390
and one but these are just numbers as

1167
00:49:44,390 --> 00:49:45,619
far as the Fed is concerned so I'm not

1168
00:49:45,619 --> 00:49:48,859
gonna worry much about that do the same

1169
00:49:48,859 --> 00:49:50,180
thing for a confident of course they're

1170
00:49:50,180 --> 00:49:51,499
all differentiable so you can do this

1171
00:49:51,499 --> 00:49:52,699
whatever your classifier is you can

1172
00:49:52,699 --> 00:49:55,039
still do it you get something that looks

1173
00:49:55,039 --> 00:49:57,079
kind of similar maybe a little less sort

1174
00:49:57,079 --> 00:49:58,640
of splotchy little few if it allows

1175
00:49:58,640 --> 00:50:00,789
speckled noise but it looks very similar

1176
00:50:00,789 --> 00:50:03,559
and if we look at the actual test error

1177
00:50:03,559 --> 00:50:05,059
of these things so this is the test

1178
00:50:05,059 --> 00:50:06,559
error on them this with a perturbation

1179
00:50:06,559 --> 00:50:09,170
of size 0.1 which is relatively small

1180
00:50:09,170 --> 00:50:11,239
right that's that's not gonna change our

1181
00:50:11,239 --> 00:50:14,089
perception of the image here then on

1182
00:50:14,089 --> 00:50:16,190
clean accuracy these things both do

1183
00:50:16,190 --> 00:50:17,690
quite well you have you know maybe 3%

1184
00:50:17,690 --> 00:50:19,339
and 1% again these are not even well

1185
00:50:19,339 --> 00:50:20,749
trained comp Nets here we're just sort

1186
00:50:20,749 --> 00:50:23,499
of running something quickly and

1187
00:50:23,499 --> 00:50:26,180
unfortunately though when you run this

1188
00:50:26,180 --> 00:50:28,880
attack method you get a very large error

1189
00:50:28,880 --> 00:50:32,329
for both these things okay so this is

1190
00:50:32,329 --> 00:50:33,559
this is maybe not good but they rarely

1191
00:50:33,559 --> 00:50:34,819
actually this is really simple method

1192
00:50:34,819 --> 00:50:36,469
and we can actually even do better than

1193
00:50:36,469 --> 00:50:38,269
what I'm showing you here so the next

1194
00:50:38,269 --> 00:50:40,130
step is just to actually run this thing

1195
00:50:40,130 --> 00:50:41,479
I showed you earlier which is a more

1196
00:50:41,479 --> 00:50:43,190
fine-grain perfectly grating descent

1197
00:50:43,190 --> 00:50:44,599
method so rather than taking sort of one

1198
00:50:44,599 --> 00:50:46,609
big step maybe the first addition point

1199
00:50:46,609 --> 00:50:47,900
and the first direction pointed to that

1200
00:50:47,900 --> 00:50:50,059
corner but the top the top right corner

1201
00:50:50,059 --> 00:50:51,979
but a top left corner here is actually

1202
00:50:51,979 --> 00:50:54,049
has higher loss so you can imagine if he

1203
00:50:54,049 --> 00:50:55,400
takes sort of small steps in the

1204
00:50:55,400 --> 00:50:57,229
gradient you're gonna end up at a better

1205
00:50:57,229 --> 00:50:59,029
point over all right so you thought that

1206
00:50:59,029 --> 00:51:00,619
this sort of seems intuitive right and

1207
00:51:00,619 --> 00:51:01,999
then you once you get to that corner you

1208
00:51:01,999 --> 00:51:03,469
sort of start trying to escape but just

1209
00:51:03,469 --> 00:51:05,420
keep getting predicted back onto the

1210
00:51:05,420 --> 00:51:07,459
ball there and this is the productive

1211
00:51:07,459 --> 00:51:08,539
gradient descent method or this

1212
00:51:08,539 --> 00:51:11,390
invariance are the method it's much

1213
00:51:11,390 --> 00:51:13,489
slower than FG SM because you are taking

1214
00:51:13,489 --> 00:51:14,540
sort of multiple stuff

1215
00:51:14,540 --> 00:51:16,580
here but it typically finds a better

1216
00:51:16,580 --> 00:51:19,970
Optima for this problem one thing which

1217
00:51:19,970 --> 00:51:21,290
I'm gonna only mentioned bleep which is

1218
00:51:21,290 --> 00:51:22,820
actually really important here in

1219
00:51:22,820 --> 00:51:25,160
practice if you do this is the following

1220
00:51:25,160 --> 00:51:28,610
so if you run PhD to run an attack don't

1221
00:51:28,610 --> 00:51:31,880
just run normal PGD ron was actually

1222
00:51:31,880 --> 00:51:33,650
called projected steepest descent

1223
00:51:33,650 --> 00:51:35,270
actually projected unprinted that

1224
00:51:35,270 --> 00:51:38,270
normalize that steepest descent so the

1225
00:51:38,270 --> 00:51:41,000
idea here is that at the actual examples

1226
00:51:41,000 --> 00:51:43,490
the gradient of the loss function is

1227
00:51:43,490 --> 00:51:45,650
often very small so you have to take a

1228
00:51:45,650 --> 00:51:48,410
really big step size to get anywhere but

1229
00:51:48,410 --> 00:51:50,060
then once you get out of that really

1230
00:51:50,060 --> 00:51:51,830
small region you start sort of taking

1231
00:51:51,830 --> 00:51:53,180
really big steps then you hit the corner

1232
00:51:53,180 --> 00:51:54,770
you kind of reduce to like the fast

1233
00:51:54,770 --> 00:51:57,590
gradient sign method anyway and so what

1234
00:51:57,590 --> 00:51:59,470
we can actually do is instead of taking

1235
00:51:59,470 --> 00:52:01,430
gradient steps we actually could just

1236
00:52:01,430 --> 00:52:02,750
normalize our gradients in the following

1237
00:52:02,750 --> 00:52:05,150
way instead of sort of taking a step

1238
00:52:05,150 --> 00:52:06,470
inertia of the gradient we actually take

1239
00:52:06,470 --> 00:52:08,540
a step in the direction that maximizes

1240
00:52:08,540 --> 00:52:09,560
some inner product of the gradient

1241
00:52:09,560 --> 00:52:11,750
that's the term V here except it to some

1242
00:52:11,750 --> 00:52:13,940
norm constraint on V okay and what this

1243
00:52:13,940 --> 00:52:15,980
does is this basically searches in some

1244
00:52:15,980 --> 00:52:18,590
box that's showing here with a dotted

1245
00:52:18,590 --> 00:52:21,080
line what's the point in that box that

1246
00:52:21,080 --> 00:52:22,670
maximizes the inner product of the

1247
00:52:22,670 --> 00:52:24,710
gradient and for example for the L

1248
00:52:24,710 --> 00:52:26,360
infinity box that will always be one of

1249
00:52:26,360 --> 00:52:27,770
the corners it's actually very similar

1250
00:52:27,770 --> 00:52:32,480
derivation as we had for F GSM what this

1251
00:52:32,480 --> 00:52:34,880
means is if you want to up lent a

1252
00:52:34,880 --> 00:52:36,920
particular instant method you really and

1253
00:52:36,920 --> 00:52:38,330
that's all I'll say we do have some more

1254
00:52:38,330 --> 00:52:40,700
examples and the toriel do this new

1255
00:52:40,700 --> 00:52:42,470
steepest descent don't do unnormalized

1256
00:52:42,470 --> 00:52:43,940
sorry it's normalized steepest descent

1257
00:52:43,940 --> 00:52:45,650
don't do normal to degrading descent

1258
00:52:45,650 --> 00:52:48,470
because the loss the the the the values

1259
00:52:48,470 --> 00:52:51,530
of lost here are very different sorry of

1260
00:52:51,530 --> 00:52:52,670
the gradients are very different over

1261
00:52:52,670 --> 00:52:55,550
this over this region typically alright

1262
00:52:55,550 --> 00:52:57,680
so let's see what this looks like now we

1263
00:52:57,680 --> 00:52:58,880
sort of gone through it in some in some

1264
00:52:58,880 --> 00:53:02,960
detail so we can do things like apply f

1265
00:53:02,960 --> 00:53:05,300
gu7 to that we also do FP PGD doesn't

1266
00:53:05,300 --> 00:53:07,010
look that different actually it's both

1267
00:53:07,010 --> 00:53:08,450
look that's both fool the classifier

1268
00:53:08,450 --> 00:53:09,620
into thinking the seven is actually a

1269
00:53:09,620 --> 00:53:11,960
three but if you look at the actual test

1270
00:53:11,960 --> 00:53:13,160
error there is quite a big difference

1271
00:53:13,160 --> 00:53:16,400
here so f gsm you know gets these errors

1272
00:53:16,400 --> 00:53:18,650
so you know fools 41 percent of the

1273
00:53:18,650 --> 00:53:20,510
examples on on this end this classifier

1274
00:53:20,510 --> 00:53:22,310
where as p DD and proves that quite a

1275
00:53:22,310 --> 00:53:25,730
bit so so there actually is quite a bit

1276
00:53:25,730 --> 00:53:26,810
of difference in practice on this and

1277
00:53:26,810 --> 00:53:27,550
this will come

1278
00:53:27,550 --> 00:53:29,020
back to us and we talk about training

1279
00:53:29,020 --> 00:53:32,980
robust classifiers okay so two more

1280
00:53:32,980 --> 00:53:35,860
quick notes here the first one is that

1281
00:53:35,860 --> 00:53:37,450
what I've described so far as actual

1282
00:53:37,450 --> 00:53:39,250
what's called an untargeted attack and

1283
00:53:39,250 --> 00:53:41,950
that we are just trying to maximize the

1284
00:53:41,950 --> 00:53:44,350
loss no matter where that puts us we can

1285
00:53:44,350 --> 00:53:47,110
also have to try to both maximize the

1286
00:53:47,110 --> 00:53:50,260
loss of the true class and minimize the

1287
00:53:50,260 --> 00:53:52,990
loss of some target class to try to fool

1288
00:53:52,990 --> 00:53:54,640
the classifier into thinking it's after

1289
00:53:54,640 --> 00:53:56,590
that target class I'm some very nice

1290
00:53:56,590 --> 00:53:58,510
thing happens here which is actually

1291
00:53:58,510 --> 00:54:00,340
gonna be the basis for the more complex

1292
00:54:00,340 --> 00:54:02,500
relaxations is if your loss function is

1293
00:54:02,500 --> 00:54:04,470
something like the cross-entropy loss

1294
00:54:04,470 --> 00:54:07,810
then the difference of two of these loss

1295
00:54:07,810 --> 00:54:09,670
functions the first term the

1296
00:54:09,670 --> 00:54:11,680
normalization term actually cancels and

1297
00:54:11,680 --> 00:54:14,320
so the actual target attack is the same

1298
00:54:14,320 --> 00:54:17,440
as just maximizing the difference in the

1299
00:54:17,440 --> 00:54:19,360
class logits that's the output of the

1300
00:54:19,360 --> 00:54:22,060
linear layer of the classifier over the

1301
00:54:22,060 --> 00:54:23,920
two classes that you have alright so

1302
00:54:23,920 --> 00:54:25,540
this last formulation here is actually

1303
00:54:25,540 --> 00:54:27,220
equivalent to doing that and this is

1304
00:54:27,220 --> 00:54:28,650
sort of an important an important

1305
00:54:28,650 --> 00:54:32,290
simplification we can have okay so let's

1306
00:54:32,290 --> 00:54:34,210
see what this looks like we can you know

1307
00:54:34,210 --> 00:54:36,280
take a step we can if we maximize take

1308
00:54:36,280 --> 00:54:39,400
this seven here we maximize the zero

1309
00:54:39,400 --> 00:54:42,250
class the the probability or really the

1310
00:54:42,250 --> 00:54:43,960
logit the linear term of the zero class

1311
00:54:43,960 --> 00:54:46,150
- linear from the seventh class and we

1312
00:54:46,150 --> 00:54:48,970
get the zero if we do the same thing for

1313
00:54:48,970 --> 00:54:50,830
a different loss function maximizing the

1314
00:54:50,830 --> 00:54:53,500
the linear term on the two class and

1315
00:54:53,500 --> 00:54:55,600
minimizing on the seven class we get the

1316
00:54:55,600 --> 00:54:58,390
prediction - now one sort of note here

1317
00:54:58,390 --> 00:55:00,550
is that we might succeed in fooling the

1318
00:55:00,550 --> 00:55:02,230
classifier even if we don't actually

1319
00:55:02,230 --> 00:55:04,300
achieve the class we want because the

1320
00:55:04,300 --> 00:55:06,130
best way to minimize say the to label

1321
00:55:06,130 --> 00:55:09,190
might be to actually minimize the the

1322
00:55:09,190 --> 00:55:12,640
one label even more right so we we might

1323
00:55:12,640 --> 00:55:14,800
not succeed at changing the class to our

1324
00:55:14,800 --> 00:55:17,110
target but we will succeed ideally and

1325
00:55:17,110 --> 00:55:20,590
creating an a personal example and the

1326
00:55:20,590 --> 00:55:22,120
last thing I want to say about these

1327
00:55:22,120 --> 00:55:24,580
sort of local search of taxa so we can

1328
00:55:24,580 --> 00:55:26,050
do them in other norms too so I showed

1329
00:55:26,050 --> 00:55:27,370
everything in terms of the L infinity

1330
00:55:27,370 --> 00:55:29,200
norm but everything we've done here

1331
00:55:29,200 --> 00:55:30,700
right the the steepest descent

1332
00:55:30,700 --> 00:55:32,200
everything normalize deepest descent

1333
00:55:32,200 --> 00:55:33,700
ever like that can be derived just as

1334
00:55:33,700 --> 00:55:35,230
easily for other norms for

1335
00:55:35,230 --> 00:55:37,089
for the Ellen for the l2 norm if the l1

1336
00:55:37,089 --> 00:55:38,890
norm sort of feel zero norm it's a

1337
00:55:38,890 --> 00:55:41,589
little trickier and and it all works so

1338
00:55:41,589 --> 00:55:44,650
for example l2 attacks tend to have more

1339
00:55:44,650 --> 00:55:46,810
mass in certain regions and less sort of

1340
00:55:46,810 --> 00:55:48,130
spread out but the nature of the norm

1341
00:55:48,130 --> 00:55:51,640
itself okay now the last thing I want to

1342
00:55:51,640 --> 00:55:53,349
say since I'm talking about attacks here

1343
00:55:53,349 --> 00:55:55,570
is that you might be asking what about

1344
00:55:55,570 --> 00:55:57,310
all these things I've heard about of you

1345
00:55:57,310 --> 00:55:58,420
know these different attacks people who

1346
00:55:58,420 --> 00:55:59,680
propose because been a lot of papers in

1347
00:55:59,680 --> 00:56:01,960
it and you know with apologies people

1348
00:56:01,960 --> 00:56:02,829
that propose a lot of these things

1349
00:56:02,829 --> 00:56:04,119
there's actually some in the audience I

1350
00:56:04,119 --> 00:56:05,320
know here so we'll see how they how they

1351
00:56:05,320 --> 00:56:08,410
feel about me afterwards but I think and

1352
00:56:08,410 --> 00:56:09,430
these papers are of course really

1353
00:56:09,430 --> 00:56:10,570
important because they were the first to

1354
00:56:10,570 --> 00:56:12,670
study these attacks in some detail but I

1355
00:56:12,670 --> 00:56:13,990
think we're actually at a point in the

1356
00:56:13,990 --> 00:56:16,510
field where it's more valuable to

1357
00:56:16,510 --> 00:56:19,450
describe the attacks in terms of the

1358
00:56:19,450 --> 00:56:21,369
perturbation bound you're allowing and

1359
00:56:21,369 --> 00:56:23,410
the optimization procedure using to

1360
00:56:23,410 --> 00:56:24,820
optimize that right this is sort of an

1361
00:56:24,820 --> 00:56:26,230
opinion now but but I think this is a

1362
00:56:26,230 --> 00:56:28,599
more clear description than relying just

1363
00:56:28,599 --> 00:56:31,359
on the name or the publication they

1364
00:56:31,359 --> 00:56:32,680
proposed this because it isn't always

1365
00:56:32,680 --> 00:56:34,450
clear then what's actually being done

1366
00:56:34,450 --> 00:56:42,400
here okay moving on now this has been

1367
00:56:42,400 --> 00:56:44,079
the most common way of solving this

1368
00:56:44,079 --> 00:56:46,329
inner maximization problem but the thing

1369
00:56:46,329 --> 00:56:47,319
I want to highlight here is there are

1370
00:56:47,319 --> 00:56:48,420
other ways as well

1371
00:56:48,420 --> 00:56:51,550
so in particular we actually can solve

1372
00:56:51,550 --> 00:56:53,050
this exactly we're still gonna solve the

1373
00:56:53,050 --> 00:56:55,660
targeted attack version exactly using

1374
00:56:55,660 --> 00:56:59,170
combinatorial optimization and so to do

1375
00:56:59,170 --> 00:57:00,460
this I want to actually dive into a

1376
00:57:00,460 --> 00:57:02,349
little bit of detail here and and throw

1377
00:57:02,349 --> 00:57:03,400
up a little a little bit more

1378
00:57:03,400 --> 00:57:07,030
mathematical notation to form the

1379
00:57:07,030 --> 00:57:09,640
adversary attack as an optimization

1380
00:57:09,640 --> 00:57:12,339
problem alright so you can write our

1381
00:57:12,339 --> 00:57:13,270
network but I'm actually gonna be

1382
00:57:13,270 --> 00:57:14,650
explicit now and say we have say a

1383
00:57:14,650 --> 00:57:17,530
multi-layer r lu network so our input of

1384
00:57:17,530 --> 00:57:19,089
the first layer z1 is just the in the

1385
00:57:19,089 --> 00:57:21,790
input X and after that each layer is the

1386
00:57:21,790 --> 00:57:23,470
rel u of some linear function of the

1387
00:57:23,470 --> 00:57:27,510
previous layer and our class logits are

1388
00:57:27,510 --> 00:57:31,390
a linear function of the the previous

1389
00:57:31,390 --> 00:57:33,760
layer the second and last layer alright

1390
00:57:33,760 --> 00:57:35,859
so we don't apply our Lu or even a soft

1391
00:57:35,859 --> 00:57:37,750
max to the to the last layer it's just

1392
00:57:37,750 --> 00:57:42,099
the logits it's pre softmax okay so now

1393
00:57:42,099 --> 00:57:46,210
we can actually write a targeted attack

1394
00:57:46,210 --> 00:57:48,760
as an optimization problem

1395
00:57:48,760 --> 00:57:50,380
one to solve but the pointing part this

1396
00:57:50,380 --> 00:57:52,480
is an optimization formulation of how we

1397
00:57:52,480 --> 00:57:55,480
think about attacking classifiers all

1398
00:57:55,480 --> 00:57:57,370
right and the idea here is we're going

1399
00:57:57,370 --> 00:58:01,540
to minimize the unit basis on the true

1400
00:58:01,540 --> 00:58:05,380
class minus the unit basis on the target

1401
00:58:05,380 --> 00:58:08,020
class the inner product of that thing

1402
00:58:08,020 --> 00:58:10,210
and the last layer which are the class

1403
00:58:10,210 --> 00:58:11,770
logits so again all we're really doing

1404
00:58:11,770 --> 00:58:14,710
here is we're minimizing the the class

1405
00:58:14,710 --> 00:58:17,230
logit of the true class and maximizing

1406
00:58:17,230 --> 00:58:19,000
the class logit of the target class

1407
00:58:19,000 --> 00:58:20,620
alright and this is just notation for

1408
00:58:20,620 --> 00:58:21,460
that so it's important to kind of keep

1409
00:58:21,460 --> 00:58:22,660
in mind I'm gonna switch this

1410
00:58:22,660 --> 00:58:23,800
connotation so it's important to kind of

1411
00:58:23,800 --> 00:58:26,980
keep that in mind subject to now our

1412
00:58:26,980 --> 00:58:28,180
relative constraints right we have to

1413
00:58:28,180 --> 00:58:29,830
have you know our next layer be the

1414
00:58:29,830 --> 00:58:31,990
value of the previous layer of a of a

1415
00:58:31,990 --> 00:58:33,160
linear function the previous layer and

1416
00:58:33,160 --> 00:58:38,410
some norm bound constraint okay so the

1417
00:58:38,410 --> 00:58:39,910
problem here is that this is not a

1418
00:58:39,910 --> 00:58:41,590
problem that can be solved by any sort

1419
00:58:41,590 --> 00:58:43,210
of normal optimization propped amaizing

1420
00:58:43,210 --> 00:58:45,340
right it has a nonlinear equality

1421
00:58:45,340 --> 00:58:47,200
constraint in it and of course you know

1422
00:58:47,200 --> 00:58:49,660
this makes it a very hard problem but

1423
00:58:49,660 --> 00:58:51,640
the key point here is that although it's

1424
00:58:51,640 --> 00:58:53,440
a little hard to solve and this is one

1425
00:58:53,440 --> 00:58:55,930
thing we go into a lot of detail with in

1426
00:58:55,930 --> 00:58:59,080
the notes is that we actually can write

1427
00:58:59,080 --> 00:59:00,820
this problem equivalently in a form that

1428
00:59:00,820 --> 00:59:02,590
at least can be solved by standard

1429
00:59:02,590 --> 00:59:04,800
common toriel solvers so things like

1430
00:59:04,800 --> 00:59:07,660
integer programming solvers which has

1431
00:59:07,660 --> 00:59:09,460
been done by some but by us as well some

1432
00:59:09,460 --> 00:59:11,500
other people in the past as well as

1433
00:59:11,500 --> 00:59:14,020
things like SMT solvers there's a lot of

1434
00:59:14,020 --> 00:59:16,480
work on using SMT methods to solve these

1435
00:59:16,480 --> 00:59:19,330
things now in practice off-the-shelf

1436
00:59:19,330 --> 00:59:21,160
solvers do things like see flex or OB if

1437
00:59:21,160 --> 00:59:23,140
you've heard of those they scale maybe

1438
00:59:23,140 --> 00:59:24,550
till maybe I've had a hundred hidden

1439
00:59:24,550 --> 00:59:25,540
units we're not we're talking about

1440
00:59:25,540 --> 00:59:28,000
really really small networks here just

1441
00:59:28,000 --> 00:59:30,220
to be clear but you can get a reasonably

1442
00:59:30,220 --> 00:59:31,750
performing like em in this network and

1443
00:59:31,750 --> 00:59:33,370
actually verify it formally which is

1444
00:59:33,370 --> 00:59:35,290
kind of cool you can she can check for

1445
00:59:35,290 --> 00:59:36,730
again an example you know what's the

1446
00:59:36,730 --> 00:59:40,030
worst you could do on any given task one

1447
00:59:40,030 --> 00:59:41,980
of the key challenges of this actually

1448
00:59:41,980 --> 00:59:43,750
ends up being at just computing

1449
00:59:43,750 --> 00:59:46,240
element-wise bounds on the activations

1450
00:59:46,240 --> 00:59:47,410
so you need to be able to basically

1451
00:59:47,410 --> 00:59:50,170
compute lower and upper bounds on the

1452
00:59:50,170 --> 00:59:52,150
linear terms before you go into the rel

1453
00:59:52,150 --> 00:59:54,460
you this is actually gonna be a thing

1454
00:59:54,460 --> 00:59:55,900
that comes up later also we talk about

1455
00:59:55,900 --> 00:59:57,550
convex methods so I just want to mention

1456
00:59:57,550 --> 00:59:59,830
this here briefly it's actually quite

1457
00:59:59,830 --> 01:00:01,570
easy to come up with simple bounds

1458
01:00:01,570 --> 01:00:03,570
on activations so for example if I have

1459
01:00:03,570 --> 01:00:07,900
some some activation Z where I have an

1460
01:00:07,900 --> 01:00:09,820
upper and lower bound on bat and I want

1461
01:00:09,820 --> 01:00:11,800
to upper and lower bound a linear

1462
01:00:11,800 --> 01:00:13,480
function of that I can just take the

1463
01:00:13,480 --> 01:00:15,940
positive and negative parts of my of my

1464
01:00:15,940 --> 01:00:18,850
W matrix and do it this way but the

1465
01:00:18,850 --> 01:00:20,530
actual thing here was what was the

1466
01:00:20,530 --> 01:00:21,940
figures a little better to show this

1467
01:00:21,940 --> 01:00:23,290
what's really happening here is we're

1468
01:00:23,290 --> 01:00:25,540
propagating intervals to the network so

1469
01:00:25,540 --> 01:00:27,100
we're you know if you just run the point

1470
01:00:27,100 --> 01:00:28,710
to the network it produces some output

1471
01:00:28,710 --> 01:00:31,210
but if you actually think about a region

1472
01:00:31,210 --> 01:00:33,220
around that point that region will

1473
01:00:33,220 --> 01:00:34,750
similarly propagate through the network

1474
01:00:34,750 --> 01:00:36,640
and really typically grow in size this

1475
01:00:36,640 --> 01:00:37,960
goes to the network that's all we're

1476
01:00:37,960 --> 01:00:39,160
doing here we're forming these interval

1477
01:00:39,160 --> 01:00:41,890
bounds to kind of show what sorts of

1478
01:00:41,890 --> 01:00:43,270
regions you can achieve with our

1479
01:00:43,270 --> 01:00:46,150
classifier what's going to return when

1480
01:00:46,150 --> 01:00:49,000
we talk about convex relaxations all

1481
01:00:49,000 --> 01:00:50,200
right so the last thing I want to say

1482
01:00:50,200 --> 01:00:53,950
here is just talk about the fact that we

1483
01:00:53,950 --> 01:00:56,860
can use this technique to certify

1484
01:00:56,860 --> 01:00:59,590
robustness of course of a classifier and

1485
01:00:59,590 --> 01:01:01,210
actually we don't even need to solve the

1486
01:01:01,210 --> 01:01:02,260
problem we don't even actually need to

1487
01:01:02,260 --> 01:01:05,110
find the or know about we need to solve

1488
01:01:05,110 --> 01:01:06,490
it we only if you need to even know what

1489
01:01:06,490 --> 01:01:08,440
the solution is all we need to know to

1490
01:01:08,440 --> 01:01:09,760
know whether we can certified example or

1491
01:01:09,760 --> 01:01:11,530
not is the objective value of that

1492
01:01:11,530 --> 01:01:14,590
solution so let me sort of just sort of

1493
01:01:14,590 --> 01:01:17,490
go to that what I mean by that so

1494
01:01:17,490 --> 01:01:19,900
remember our objective that we're trying

1495
01:01:19,900 --> 01:01:21,250
to optimize in the optimization

1496
01:01:21,250 --> 01:01:23,380
formulation of these attacks is the

1497
01:01:23,380 --> 01:01:24,040
following

1498
01:01:24,040 --> 01:01:27,370
we're trying to minimize the class logit

1499
01:01:27,370 --> 01:01:29,920
of the true class and maximize the class

1500
01:01:29,920 --> 01:01:32,260
logit of the other class some target

1501
01:01:32,260 --> 01:01:35,740
class so if i solve this problem and the

1502
01:01:35,740 --> 01:01:38,200
result is negative that means the target

1503
01:01:38,200 --> 01:01:41,260
class's activation write probability is

1504
01:01:41,260 --> 01:01:44,620
higher now than the true class and so

1505
01:01:44,620 --> 01:01:46,000
this means we have an adversarial

1506
01:01:46,000 --> 01:01:49,180
example on the other hand if I solve

1507
01:01:49,180 --> 01:01:51,820
this same thing and I found my objective

1508
01:01:51,820 --> 01:01:54,790
value is positive this means that there

1509
01:01:54,790 --> 01:01:57,760
is no adversarial example at all right

1510
01:01:57,760 --> 01:02:01,420
because the class probability of the

1511
01:02:01,420 --> 01:02:03,790
true class is still higher than the

1512
01:02:03,790 --> 01:02:06,310
probability of the target class so

1513
01:02:06,310 --> 01:02:08,500
there's no method can't come up with any

1514
01:02:08,500 --> 01:02:10,330
they will change the label at this

1515
01:02:10,330 --> 01:02:13,150
example so just to illustrate here if he

1516
01:02:13,150 --> 01:02:15,100
has a seven here say we say we solve

1517
01:02:15,100 --> 01:02:15,400
some

1518
01:02:15,400 --> 01:02:16,930
problem where we're trying to have a

1519
01:02:16,930 --> 01:02:19,029
targeted attack on the class zero so

1520
01:02:19,029 --> 01:02:21,910
we're maximize minimizing the logit on

1521
01:02:21,910 --> 01:02:24,309
unit seven and maximizing it on unit

1522
01:02:24,309 --> 01:02:26,650
zero and this is negative that means we

1523
01:02:26,650 --> 01:02:29,140
have an adversarial example it actually

1524
01:02:29,140 --> 01:02:30,520
doesn't still necessarily change the

1525
01:02:30,520 --> 01:02:32,170
class to be a zero but it's an

1526
01:02:32,170 --> 01:02:34,480
adversarial example whereas if we do

1527
01:02:34,480 --> 01:02:36,010
something like this and we find that

1528
01:02:36,010 --> 01:02:39,190
we're not trying to minimize the or

1529
01:02:39,190 --> 01:02:42,910
maximize the logit on class one then we

1530
01:02:42,910 --> 01:02:44,079
and this is positive then we actually

1531
01:02:44,079 --> 01:02:45,730
know there's no adverse real example at

1532
01:02:45,730 --> 01:02:47,710
all that can make this classifier

1533
01:02:47,710 --> 01:02:49,779
predict class one okay so this is the

1534
01:02:49,779 --> 01:02:51,430
sort of fundamental ideas and we

1535
01:02:51,430 --> 01:02:52,569
actually have some against some code you

1536
01:02:52,569 --> 01:02:53,500
can play with where you can after you

1537
01:02:53,500 --> 01:02:54,789
can use a linear and integer programming

1538
01:02:54,789 --> 01:02:57,250
solver to formally verify at least some

1539
01:02:57,250 --> 01:02:58,839
small networks but still image net are

1540
01:02:58,839 --> 01:03:00,789
so hypnotic still M this size networks

1541
01:03:00,789 --> 01:03:04,029
not image that size network size okay so

1542
01:03:04,029 --> 01:03:07,779
lastly before I move on to the to the

1543
01:03:07,779 --> 01:03:10,559
next section here I want to talk about

1544
01:03:10,559 --> 01:03:12,880
convex relaxation so this is the third

1545
01:03:12,880 --> 01:03:14,650
point so talked about sort of local

1546
01:03:14,650 --> 01:03:16,270
solutions those are sort of lower bounds

1547
01:03:16,270 --> 01:03:18,160
than subjective we can also talk about

1548
01:03:18,160 --> 01:03:20,349
exact solutions and finally we can talk

1549
01:03:20,349 --> 01:03:22,569
about convex relaxations which provide

1550
01:03:22,569 --> 01:03:24,549
upper bounds on the solution to this

1551
01:03:24,549 --> 01:03:27,400
problem okay so how do these comment

1552
01:03:27,400 --> 01:03:29,650
relaxations work there are actually

1553
01:03:29,650 --> 01:03:32,589
several ways you can do this one of them

1554
01:03:32,589 --> 01:03:35,319
is based essentially on relaxing the

1555
01:03:35,319 --> 01:03:38,440
integer programming formulation of the

1556
01:03:38,440 --> 01:03:41,230
verification procedure so the idea here

1557
01:03:41,230 --> 01:03:43,420
is that the integer program formation

1558
01:03:43,420 --> 01:03:44,859
had these hard constraints that

1559
01:03:44,859 --> 01:03:46,900
basically the post activation had to be

1560
01:03:46,900 --> 01:03:49,329
the rel u of this previous layers linear

1561
01:03:49,329 --> 01:03:51,549
function right within some bounded range

1562
01:03:51,549 --> 01:03:52,809
actually because we actually had an

1563
01:03:52,809 --> 01:03:54,430
upper and lower bound that these could

1564
01:03:54,430 --> 01:03:57,670
attain now what we can do instead is

1565
01:03:57,670 --> 01:04:01,270
replace that non convex set with its

1566
01:04:01,270 --> 01:04:03,700
convex relaxation which means that the

1567
01:04:03,700 --> 01:04:06,180
the next layer and the previous layer

1568
01:04:06,180 --> 01:04:08,799
they don't lie on the rel you anymore

1569
01:04:08,799 --> 01:04:10,900
they now lie within some relaxed linear

1570
01:04:10,900 --> 01:04:13,450
set and the cool thing about this is

1571
01:04:13,450 --> 01:04:15,730
that if you relax the integer program

1572
01:04:15,730 --> 01:04:16,960
this way it's actually a very sort of

1573
01:04:16,960 --> 01:04:18,460
simple straightforward relaxation of the

1574
01:04:18,460 --> 01:04:20,140
of the linear program of the integer

1575
01:04:20,140 --> 01:04:24,849
program you get a linear program now and

1576
01:04:24,849 --> 01:04:27,299
linear programs they're actually are

1577
01:04:27,299 --> 01:04:29,080
efficient polynomial time

1578
01:04:29,080 --> 01:04:31,630
solving them so unlike commentary op

1579
01:04:31,630 --> 01:04:32,740
station we actually expect that we

1580
01:04:32,740 --> 01:04:36,870
sometimes can really solve these things

1581
01:04:37,260 --> 01:04:41,590
okay so a a quick sort of note on this

1582
01:04:41,590 --> 01:04:43,240
another way of sort of viewing this is

1583
01:04:43,240 --> 01:04:46,090
that if you take the norm ball and sort

1584
01:04:46,090 --> 01:04:47,740
of feed it to the network that first

1585
01:04:47,740 --> 01:04:49,300
figure there shows kind of some complex

1586
01:04:49,300 --> 01:04:51,850
shape and what we're doing effectively

1587
01:04:51,850 --> 01:04:53,320
is we're relaxing that shape or thinking

1588
01:04:53,320 --> 01:04:55,060
about an outer bound on that shape that

1589
01:04:55,060 --> 01:04:56,890
gives us more freedom in choosing

1590
01:04:56,890 --> 01:04:59,560
effectively or adverse sail examples now

1591
01:04:59,560 --> 01:05:02,140
what this means is that the objective of

1592
01:05:02,140 --> 01:05:04,570
a targeted attack for the linear program

1593
01:05:04,570 --> 01:05:06,100
is gonna be actually a lower bound on

1594
01:05:06,100 --> 01:05:08,410
the objective of the true thing and this

1595
01:05:08,410 --> 01:05:09,760
will come in handy in a second when I

1596
01:05:09,760 --> 01:05:10,630
talk about when I talk about

1597
01:05:10,630 --> 01:05:13,840
certifications so they actually can

1598
01:05:13,840 --> 01:05:15,730
still be used to sort of prove sometimes

1599
01:05:15,730 --> 01:05:19,150
that no ever sail example exists of

1600
01:05:19,150 --> 01:05:20,740
course solving the LP is kind of slow

1601
01:05:20,740 --> 01:05:23,410
still and so there fortunately there do

1602
01:05:23,410 --> 01:05:24,880
exist fast methods basically um things

1603
01:05:24,880 --> 01:05:26,380
like convex duality for first

1604
01:05:26,380 --> 01:05:29,140
approximately solving that LP but would

1605
01:05:29,140 --> 01:05:30,190
actually want to highlight even more

1606
01:05:30,190 --> 01:05:31,840
than that is even an even faster method

1607
01:05:31,840 --> 01:05:33,370
which some recent work isn't showing

1608
01:05:33,370 --> 01:05:35,140
this is competitive with these more

1609
01:05:35,140 --> 01:05:38,140
complex relaxations and that is just

1610
01:05:38,140 --> 01:05:39,910
interval based bounds so I showed you

1611
01:05:39,910 --> 01:05:41,650
before sort of how we propagate interval

1612
01:05:41,650 --> 01:05:44,560
bounds really these I alluded to how we

1613
01:05:44,560 --> 01:05:45,850
propagate interval balance to the

1614
01:05:45,850 --> 01:05:50,470
network it turns out if you if you sort

1615
01:05:50,470 --> 01:05:53,200
of take those interval bounds we now

1616
01:05:53,200 --> 01:05:55,030
have a known region that the last layer

1617
01:05:55,030 --> 01:05:56,920
of the network can lie in and if we

1618
01:05:56,920 --> 01:05:58,540
throw away everything else from our

1619
01:05:58,540 --> 01:06:00,250
network we still can actually get a

1620
01:06:00,250 --> 01:06:02,620
bound on how much I can sort of decrease

1621
01:06:02,620 --> 01:06:05,590
say my my targeted attack right there

1622
01:06:05,590 --> 01:06:06,850
just the analytical solution to this and

1623
01:06:06,850 --> 01:06:09,520
you can solve it kind of exactly here it

1624
01:06:09,520 --> 01:06:11,050
is if you if you really care but the

1625
01:06:11,050 --> 01:06:14,590
real point here is that I can also use

1626
01:06:14,590 --> 01:06:16,120
just interval balance to provide an even

1627
01:06:16,120 --> 01:06:20,140
looser relaxation of the problem

1628
01:06:20,140 --> 01:06:24,400
okay so I've sort of said a lot here in

1629
01:06:24,400 --> 01:06:26,020
the last two sections and I kind of want

1630
01:06:26,020 --> 01:06:27,850
to pop back up now for a second and

1631
01:06:27,850 --> 01:06:30,490
highlight what the real value of all

1632
01:06:30,490 --> 01:06:35,290
this is okay so if if some of that was a

1633
01:06:35,290 --> 01:06:37,330
little bit into the weeds for you don't

1634
01:06:37,330 --> 01:06:39,340
worry about it we're gonna sort of pop

1635
01:06:39,340 --> 01:06:40,470
back up a little bit right

1636
01:06:40,470 --> 01:06:41,970
and then when we talk about training

1637
01:06:41,970 --> 01:06:43,740
will actually give some sort of examples

1638
01:06:43,740 --> 01:06:47,130
of how these things really work but

1639
01:06:47,130 --> 01:06:48,660
here's the high-level thing I want to

1640
01:06:48,660 --> 01:06:51,030
give to sort of want to show you so say

1641
01:06:51,030 --> 01:06:53,640
we have our trusty digit 7 here this is

1642
01:06:53,640 --> 01:06:55,500
the first example of the M this test set

1643
01:06:55,500 --> 01:06:56,760
by the way this is why it keeps

1644
01:06:56,760 --> 01:06:59,960
appearing here again and again and

1645
01:06:59,960 --> 01:07:02,960
suppose I solve my verification problem

1646
01:07:02,960 --> 01:07:07,470
using the binary integer previously the

1647
01:07:07,470 --> 01:07:09,420
integer programming formulation of my

1648
01:07:09,420 --> 01:07:12,480
exact verification procedure okay and if

1649
01:07:12,480 --> 01:07:14,660
that result gives me a negative solution

1650
01:07:14,660 --> 01:07:16,860
then I know that there exists an

1651
01:07:16,860 --> 01:07:19,830
adversarial example right in this case

1652
01:07:19,830 --> 01:07:22,050
trying to make the thing class zero

1653
01:07:22,050 --> 01:07:25,100
because I'm minimizing the class zero

1654
01:07:25,100 --> 01:07:26,820
maximizing the class zero logit

1655
01:07:26,820 --> 01:07:30,450
minimizing the true class logic if on

1656
01:07:30,450 --> 01:07:34,080
the other hand I do the same thing with

1657
01:07:34,080 --> 01:07:37,740
the convex constraints I I will also get

1658
01:07:37,740 --> 01:07:39,150
a negative number because that one will

1659
01:07:39,150 --> 01:07:40,680
always be actually always be lower than

1660
01:07:40,680 --> 01:07:44,760
the first one but importantly the fact

1661
01:07:44,760 --> 01:07:46,980
that I have a negative term here doesn't

1662
01:07:46,980 --> 01:07:48,960
actually tell me anything I don't know

1663
01:07:48,960 --> 01:07:50,340
that an average tale example exists or

1664
01:07:50,340 --> 01:07:52,740
not because I've relaxed the set of

1665
01:07:52,740 --> 01:07:54,630
allowable sort of activations I can

1666
01:07:54,630 --> 01:07:56,280
achieve and so the fact that I can

1667
01:07:56,280 --> 01:07:57,900
achieve kind of an adversarial example

1668
01:07:57,900 --> 01:07:59,700
in this relaxed set doesn't actually

1669
01:07:59,700 --> 01:08:02,700
tell me anything on the other hand and

1670
01:08:02,700 --> 01:08:05,460
so so for finding adversarial examples

1671
01:08:05,460 --> 01:08:07,710
these convex realizations are actually

1672
01:08:07,710 --> 01:08:09,510
quite not very useful

1673
01:08:09,510 --> 01:08:11,010
they don't really provide much in that

1674
01:08:11,010 --> 01:08:13,110
regard fortunately though we sort of

1675
01:08:13,110 --> 01:08:14,430
don't have that problem we already know

1676
01:08:14,430 --> 01:08:17,430
how to construct adverse examples pretty

1677
01:08:17,430 --> 01:08:18,479
well just using gradient based

1678
01:08:18,479 --> 01:08:20,100
techniques well really don't know how to

1679
01:08:20,100 --> 01:08:22,740
do well with these grading basics is

1680
01:08:22,740 --> 01:08:24,510
verify that no such example can exist

1681
01:08:24,510 --> 01:08:26,870
and the nice thing about these convex

1682
01:08:26,870 --> 01:08:30,689
relaxations is that if we solve the same

1683
01:08:30,689 --> 01:08:32,160
problem say you're trying to make the

1684
01:08:32,160 --> 01:08:33,859
class label look like a class label one

1685
01:08:33,859 --> 01:08:36,540
and get a positive number so in other

1686
01:08:36,540 --> 01:08:39,120
words what that means is the class

1687
01:08:39,120 --> 01:08:41,790
logits the class probability of the true

1688
01:08:41,790 --> 01:08:45,450
class under this relaxed attack is still

1689
01:08:45,450 --> 01:08:47,970
higher than the class probability of the

1690
01:08:47,970 --> 01:08:51,359
alternative class the target class what

1691
01:08:51,359 --> 01:08:53,880
I now know is that there's actually no

1692
01:08:53,880 --> 01:08:56,670
adversarial example for this class so

1693
01:08:56,670 --> 01:08:58,979
I've actually managed to verify that no

1694
01:08:58,979 --> 01:09:02,310
adversarial example exists and do so in

1695
01:09:02,310 --> 01:09:06,500
a way that only involves convex methods

1696
01:09:06,500 --> 01:09:08,640
so that's very that's very nice because

1697
01:09:08,640 --> 01:09:10,770
these things are solvable and we

1698
01:09:10,770 --> 01:09:13,080
actually sometimes cannot just get

1699
01:09:13,080 --> 01:09:16,319
attacks but we might be able to actually

1700
01:09:16,319 --> 01:09:25,439
get proofs that no attack exists okay so

1701
01:09:25,439 --> 01:09:27,990
that actually covers now the three main

1702
01:09:27,990 --> 01:09:30,479
techniques we have for dealing with that

1703
01:09:30,479 --> 01:09:34,830
inner maximization problem and now I'm

1704
01:09:34,830 --> 01:09:41,460
gonna pop back out and start talking

1705
01:09:41,460 --> 01:09:44,310
about training models not just testing

1706
01:09:44,310 --> 01:09:47,100
them so let's take back to the big

1707
01:09:47,100 --> 01:09:51,029
picture here what part one was all about

1708
01:09:51,029 --> 01:09:54,810
really was creating adversarial examples

1709
01:09:54,810 --> 01:09:58,790
right so how do I solve that inner

1710
01:09:58,790 --> 01:10:03,270
maximization problem what part two is

1711
01:10:03,270 --> 01:10:06,330
gonna be about is how do I train a model

1712
01:10:06,330 --> 01:10:09,090
that somehow is robust to these

1713
01:10:09,090 --> 01:10:12,360
adversarial attacks and it turns out

1714
01:10:12,360 --> 01:10:13,650
that actually the strategies I had

1715
01:10:13,650 --> 01:10:15,780
before will exactly lead to methods for

1716
01:10:15,780 --> 01:10:19,590
training so we're now moving from just

1717
01:10:19,590 --> 01:10:21,660
the atomization inner minimization

1718
01:10:21,660 --> 01:10:24,720
maximization to the outer minimization

1719
01:10:24,720 --> 01:10:27,750
and if we think about the techniques we

1720
01:10:27,750 --> 01:10:33,990
use first first search here using our

1721
01:10:33,990 --> 01:10:36,060
local bounds naturally lead to a

1722
01:10:36,060 --> 01:10:37,890
procedure that we call adversarial

1723
01:10:37,890 --> 01:10:40,050
training so this is also sort of

1724
01:10:40,050 --> 01:10:42,240
advocated by ie and Goodfellow and and

1725
01:10:42,240 --> 01:10:44,880
others and I'll describe what that means

1726
01:10:44,880 --> 01:10:48,680
in a second we can't really use

1727
01:10:48,680 --> 01:10:51,150
combinatorial optimization to train

1728
01:10:51,150 --> 01:10:53,490
models it's just too slow it takes on

1729
01:10:53,490 --> 01:10:56,250
the order of minutes or I mean or for

1730
01:10:56,250 --> 01:10:57,420
big enough models you know until the

1731
01:10:57,420 --> 01:10:59,310
heat death of the universe to verify

1732
01:10:59,310 --> 01:11:01,380
them so we're not gonna build a train

1733
01:11:01,380 --> 01:11:03,659
you know models ever and well maybe

1734
01:11:03,659 --> 01:11:05,489
really small ones but not larger ones

1735
01:11:05,489 --> 01:11:09,630
using combinatorial optimization we can

1736
01:11:09,630 --> 01:11:12,150
however train models based upon our

1737
01:11:12,150 --> 01:11:14,760
convex relaxations and these will lead

1738
01:11:14,760 --> 01:11:18,270
to provably robust models models where

1739
01:11:18,270 --> 01:11:20,850
we can say not only do they seem to

1740
01:11:20,850 --> 01:11:22,679
perform well but it actually can

1741
01:11:22,679 --> 01:11:25,920
guarantee you that no attack no test

1742
01:11:25,920 --> 01:11:28,080
time attack is gonna fool this

1743
01:11:28,080 --> 01:11:30,000
classifier at least within a certain

1744
01:11:30,000 --> 01:11:35,159
norm ball okay so let me start first of

1745
01:11:35,159 --> 01:11:38,780
all talking about adversarial training

1746
01:11:38,780 --> 01:11:42,080
the idea here is that how do we go about

1747
01:11:42,080 --> 01:11:44,159
optimizing this objective how do we go

1748
01:11:44,159 --> 01:11:47,760
about training a model that optimizes

1749
01:11:47,760 --> 01:11:50,310
them that minimizes over our model this

1750
01:11:50,310 --> 01:11:54,230
sum of robust adversarial robust losses

1751
01:11:54,230 --> 01:11:56,580
now we would like to be able to optimize

1752
01:11:56,580 --> 01:11:58,860
it with gradient descents the way we

1753
01:11:58,860 --> 01:12:00,900
optimize everything in deep learning but

1754
01:12:00,900 --> 01:12:03,449
how do I take the gradient of that term

1755
01:12:03,449 --> 01:12:07,139
when it involves the maximum well sort

1756
01:12:07,139 --> 01:12:08,790
of very nicely it turns out there's a

1757
01:12:08,790 --> 01:12:10,800
really nice answer this and it's called

1758
01:12:10,800 --> 01:12:14,610
Danskin theorem so the idea here is very

1759
01:12:14,610 --> 01:12:16,350
simple and intuitive but it's actually a

1760
01:12:16,350 --> 01:12:19,050
very subtle point is that if I want to

1761
01:12:19,050 --> 01:12:21,989
take a gradient of this max term that's

1762
01:12:21,989 --> 01:12:23,219
the inner term that I'm trying to

1763
01:12:23,219 --> 01:12:27,690
optimize here what I can do is I can

1764
01:12:27,690 --> 01:12:31,830
just find the optimum in that over that

1765
01:12:31,830 --> 01:12:33,810
set and take the gradient at that

1766
01:12:33,810 --> 01:12:36,449
optimum point this sounds obvious and I

1767
01:12:36,449 --> 01:12:37,560
thought this was obvious but actually

1768
01:12:37,560 --> 01:12:39,480
for general optimizations is a really

1769
01:12:39,480 --> 01:12:40,949
subtle result that takes many many pages

1770
01:12:40,949 --> 01:12:43,350
to prove in the convex case is not quite

1771
01:12:43,350 --> 01:12:44,550
so bad but this is actually a pretty

1772
01:12:44,550 --> 01:12:46,800
subtle result but it's really really

1773
01:12:46,800 --> 01:12:49,409
nice because it means that in order to

1774
01:12:49,409 --> 01:12:52,949
to solve to solve our optimization

1775
01:12:52,949 --> 01:12:54,449
problem with great in the sense we don't

1776
01:12:54,449 --> 01:12:56,330
need some sort of fancy new procedure

1777
01:12:56,330 --> 01:13:00,320
all we really need to do is be able to

1778
01:13:00,320 --> 01:13:05,060
find adversarial examples and then

1779
01:13:05,060 --> 01:13:09,659
optimize from those positions now or

1780
01:13:09,659 --> 01:13:11,730
optimize our parameters at that at those

1781
01:13:11,730 --> 01:13:13,980
points now it turns out that to do this

1782
01:13:13,980 --> 01:13:14,670
I should

1783
01:13:14,670 --> 01:13:16,469
that has sort of a big asterisk here

1784
01:13:16,469 --> 01:13:17,940
because this only applies when we

1785
01:13:17,940 --> 01:13:19,949
perform the inner message in exactly

1786
01:13:19,949 --> 01:13:21,510
which brings us back here to the

1787
01:13:21,510 --> 01:13:22,679
combinatorial states that we don't want

1788
01:13:22,679 --> 01:13:24,780
to deal with but now you know of course

1789
01:13:24,780 --> 01:13:26,070
we have deep learning to the rescue but

1790
01:13:26,070 --> 01:13:28,020
let's just do this anyway

1791
01:13:28,020 --> 01:13:30,540
solving our inner optimism problem sort

1792
01:13:30,540 --> 01:13:32,880
of empirically and not caring about the

1793
01:13:32,880 --> 01:13:34,560
fact that you know we're not really in

1794
01:13:34,560 --> 01:13:36,000
the regime of truth theory here in

1795
01:13:36,000 --> 01:13:39,000
Danskin theorem and this leads to

1796
01:13:39,000 --> 01:13:41,130
exactly kind of the standard adversarial

1797
01:13:41,130 --> 01:13:43,560
training procedure advocated for example

1798
01:13:43,560 --> 01:13:45,360
by in good fell in one of the in one of

1799
01:13:45,360 --> 01:13:47,520
the original adversarial example or one

1800
01:13:47,520 --> 01:13:49,920
of the many papers leaks that excited

1801
01:13:49,920 --> 01:13:52,770
people about adversarial examples so dia

1802
01:13:52,770 --> 01:13:54,690
here is very simple and all it really

1803
01:13:54,690 --> 01:13:56,910
involves is that instead of training at

1804
01:13:56,910 --> 01:13:59,880
the actual data points you train at the

1805
01:13:59,880 --> 01:14:01,679
worst case perturbations of those data

1806
01:14:01,679 --> 01:14:03,540
points right so I select the mini-batch

1807
01:14:03,540 --> 01:14:05,100
for each of the implementing me about

1808
01:14:05,100 --> 01:14:06,719
mini-batch I find the worst case

1809
01:14:06,719 --> 01:14:08,850
perturbation and then I update the

1810
01:14:08,850 --> 01:14:11,489
parameters based upon the gradients at

1811
01:14:11,489 --> 01:14:13,380
those worst case perturbations that's

1812
01:14:13,380 --> 01:14:14,690
actually all I do in this whole process

1813
01:14:14,690 --> 01:14:17,969
and I should say it's also sort of

1814
01:14:17,969 --> 01:14:20,040
comments to sometimes mix robust

1815
01:14:20,040 --> 01:14:22,530
and non robust updates so you might take

1816
01:14:22,530 --> 01:14:23,489
some gradient set to the original

1817
01:14:23,489 --> 01:14:25,440
examples some of the robust exam at the

1818
01:14:25,440 --> 01:14:27,360
sort of worth kiss examples but really

1819
01:14:27,360 --> 01:14:29,310
you can actually just take the worst

1820
01:14:29,310 --> 01:14:32,310
case ones - and this still works ok ok

1821
01:14:32,310 --> 01:14:35,040
so let's say this works now so remember

1822
01:14:35,040 --> 01:14:38,670
we had our old Convenant which we

1823
01:14:38,670 --> 01:14:40,350
transferred with normal training and it

1824
01:14:40,350 --> 01:14:42,060
did really well on standard error here

1825
01:14:42,060 --> 01:14:44,250
but it actually did a lot worse on you

1826
01:14:44,250 --> 01:14:45,660
know if we attack it with a finite the

1827
01:14:45,660 --> 01:14:47,580
fast gradient sign method or with PGD

1828
01:14:47,580 --> 01:14:49,350
press a grading descent it does much

1829
01:14:49,350 --> 01:14:55,440
worse so the nice thing is if I now

1830
01:14:55,440 --> 01:14:59,460
train my same exact architecture using

1831
01:14:59,460 --> 01:15:02,160
this method on the right here we

1832
01:15:02,160 --> 01:15:03,810
actually get a much more robust Network

1833
01:15:03,810 --> 01:15:06,870
right so now if I train it on this

1834
01:15:06,870 --> 01:15:09,510
attack it's only suffers about 2.8

1835
01:15:09,510 --> 01:15:14,370
percent error on this now this seems

1836
01:15:14,370 --> 01:15:17,340
good all right and actually I think Alec

1837
01:15:17,340 --> 01:15:20,010
sessions a little bit and I think he may

1838
01:15:20,010 --> 01:15:23,640
touch on that some more later but we

1839
01:15:23,640 --> 01:15:24,660
have to be really careful about

1840
01:15:24,660 --> 01:15:27,730
declaring success right now right

1841
01:15:27,730 --> 01:15:30,700
cuz what I'm doing in my evaluation here

1842
01:15:30,700 --> 01:15:33,310
is I'm evaluating the method on the

1843
01:15:33,310 --> 01:15:35,290
exact same attack that I trained it on

1844
01:15:35,290 --> 01:15:38,140
and we know deep learning is really good

1845
01:15:38,140 --> 01:15:40,330
when you give it something it's really

1846
01:15:40,330 --> 01:15:42,880
really good at doing that all right so

1847
01:15:42,880 --> 01:15:44,590
so it's really good at automatic exact

1848
01:15:44,590 --> 01:15:45,940
we tell it to optimize and this is

1849
01:15:45,940 --> 01:15:47,739
actually been sort of the the challenge

1850
01:15:47,739 --> 01:15:49,150
for a lot of adversarial examples in the

1851
01:15:49,150 --> 01:15:51,970
past is that is that we have to be sort

1852
01:15:51,970 --> 01:15:52,540
of careful

1853
01:15:52,540 --> 01:15:57,970
so however here's what I will say to the

1854
01:15:57,970 --> 01:16:00,760
best of our empirical knowledge models

1855
01:16:00,760 --> 01:16:02,739
trained in this fashion with a strong

1856
01:16:02,739 --> 01:16:04,810
PGD so using basically predicted

1857
01:16:04,810 --> 01:16:07,870
steepest descent so normalized type of

1858
01:16:07,870 --> 01:16:10,390
PGD with additionally some random

1859
01:16:10,390 --> 01:16:13,420
restarts in the PD process seem to be

1860
01:16:13,420 --> 01:16:16,060
robust to any empirical attack we can

1861
01:16:16,060 --> 01:16:19,090
throw at them so if I run the same thing

1862
01:16:19,090 --> 01:16:20,680
maybe say with more iterations of PGD

1863
01:16:20,680 --> 01:16:23,470
with more randomization on this kind of

1864
01:16:23,470 --> 01:16:23,860
stuff

1865
01:16:23,860 --> 01:16:25,600
I actually don't increase my loss that

1866
01:16:25,600 --> 01:16:28,840
much and in fact one of what Alex's

1867
01:16:28,840 --> 01:16:30,790
group they posted a model they trains

1868
01:16:30,790 --> 01:16:32,320
not not this one but I feel more complex

1869
01:16:32,320 --> 01:16:34,239
model they trained in the same fashion

1870
01:16:34,239 --> 01:16:36,850
and people really haven't been able to

1871
01:16:36,850 --> 01:16:40,090
attack it that much and the crux of the

1872
01:16:40,090 --> 01:16:43,120
issue here is that we are trying in this

1873
01:16:43,120 --> 01:16:46,390
PDD based training to really do as good

1874
01:16:46,390 --> 01:16:48,670
a job as we can of solving that

1875
01:16:48,670 --> 01:16:50,470
internment that inner maximization

1876
01:16:50,470 --> 01:16:53,800
problem and so when we do that we

1877
01:16:53,800 --> 01:16:55,450
actually tend to get classifiers that

1878
01:16:55,450 --> 01:16:57,340
appear to be pretty robust to anything

1879
01:16:57,340 --> 01:17:01,030
we can throw at them at least that you

1880
01:17:01,030 --> 01:17:04,720
know they the the error that PGD gets is

1881
01:17:04,720 --> 01:17:07,120
about the same error that any other

1882
01:17:07,120 --> 01:17:08,800
attack we can really formulate gets with

1883
01:17:08,800 --> 01:17:10,360
with some you know maybe some minor

1884
01:17:10,360 --> 01:17:13,960
improvements at the at the margins so it

1885
01:17:13,960 --> 01:17:15,160
looks good but we should really be

1886
01:17:15,160 --> 01:17:16,720
careful we're clarity to success and so

1887
01:17:16,720 --> 01:17:18,670
actually we have this model again in the

1888
01:17:18,670 --> 01:17:20,260
notes and you can try other attacks but

1889
01:17:20,260 --> 01:17:21,910
I actually think for this you know small

1890
01:17:21,910 --> 01:17:22,900
data set this is probably actually

1891
01:17:22,900 --> 01:17:27,370
pretty robust model again for pretty

1892
01:17:27,370 --> 01:17:28,690
small epsilon this is that perturbation

1893
01:17:28,690 --> 01:17:32,620
of L infinity probations of size 0.1 but

1894
01:17:32,620 --> 01:17:33,460
one thing I want to actually emphasize

1895
01:17:33,460 --> 01:17:35,890
is that what we shouldn't do or what's

1896
01:17:35,890 --> 01:17:37,660
not particularly informative is to

1897
01:17:37,660 --> 01:17:39,840
evaluate against different

1898
01:17:39,840 --> 01:17:41,760
of attacks like taking an elephant a

1899
01:17:41,760 --> 01:17:43,530
trained model and evaluated against l2

1900
01:17:43,530 --> 01:17:45,360
or l1 attacks because of course the

1901
01:17:45,360 --> 01:17:46,980
model is trained on one attack it's not

1902
01:17:46,980 --> 01:17:48,270
going to generalize we'd also know that

1903
01:17:48,270 --> 01:17:51,060
right and if you want to have some we if

1904
01:17:51,060 --> 01:17:52,469
we want to pretend against l1 or l2

1905
01:17:52,469 --> 01:17:54,750
attacks we should just train against l1

1906
01:17:54,750 --> 01:17:59,239
or l2 / Peet whipped with l1 or l2 PGD

1907
01:17:59,239 --> 01:18:01,500
if you want to have some notion of

1908
01:18:01,500 --> 01:18:03,659
generalizing to say new attack modes

1909
01:18:03,659 --> 01:18:05,520
right new sort of general perturbation

1910
01:18:05,520 --> 01:18:09,150
sets we're not quite there in in in the

1911
01:18:09,150 --> 01:18:10,380
field yet you would need to sort of

1912
01:18:10,380 --> 01:18:12,449
define some notion of some set of

1913
01:18:12,449 --> 01:18:14,130
allowable perturbation regions and

1914
01:18:14,130 --> 01:18:15,780
sample from this and then try to

1915
01:18:15,780 --> 01:18:17,250
generalize the new ones and we're not

1916
01:18:17,250 --> 01:18:17,760
there yet

1917
01:18:17,760 --> 01:18:19,080
so what's the one to add a final note

1918
01:18:19,080 --> 01:18:20,580
there this is really not informative to

1919
01:18:20,580 --> 01:18:21,840
evaluate these models against other

1920
01:18:21,840 --> 01:18:23,820
threat models if you want a model that

1921
01:18:23,820 --> 01:18:26,280
will defend well against l2 or l1 attack

1922
01:18:26,280 --> 01:18:28,110
this trainer model against l1 and l2

1923
01:18:28,110 --> 01:18:31,739
attacks for the same PGD method now

1924
01:18:31,739 --> 01:18:33,420
another kind of final question is what

1925
01:18:33,420 --> 01:18:34,889
makes these models robust why are they

1926
01:18:34,889 --> 01:18:36,389
robust and one way to look at this is

1927
01:18:36,389 --> 01:18:37,710
actually by looking at the cost surfaces

1928
01:18:37,710 --> 01:18:39,239
of these models so I'm showing here is

1929
01:18:39,239 --> 01:18:41,699
actually the the lost surface predicted

1930
01:18:41,699 --> 01:18:43,260
along two dimensions here including the

1931
01:18:43,260 --> 01:18:45,179
the adversarial dimension and if we take

1932
01:18:45,179 --> 01:18:47,130
our standard training that loss really

1933
01:18:47,130 --> 01:18:49,650
is quite steep so so very close to the

1934
01:18:49,650 --> 01:18:51,989
point this is all within an epsilon ball

1935
01:18:51,989 --> 01:18:55,679
of zero point zero point one for M miss

1936
01:18:55,679 --> 01:18:57,300
there's there's a lot of directions a

1937
01:18:57,300 --> 01:19:00,840
barely really sharp increase whereas the

1938
01:19:00,840 --> 01:19:02,909
lost surface of the robust model and the

1939
01:19:02,909 --> 01:19:04,199
important point here is actually the

1940
01:19:04,199 --> 01:19:05,699
scale on the right and you can see that

1941
01:19:05,699 --> 01:19:07,920
but it's basically the bumpiest here is

1942
01:19:07,920 --> 01:19:09,300
just sort of numerical things that

1943
01:19:09,300 --> 01:19:10,530
really what's what's happening here is

1944
01:19:10,530 --> 01:19:11,969
that this is essentially a flat surface

1945
01:19:11,969 --> 01:19:15,179
so it seems like when we train these

1946
01:19:15,179 --> 01:19:18,210
things with PGD we genuinely get lost

1947
01:19:18,210 --> 01:19:20,340
surfaces that are quite smooth and flat

1948
01:19:20,340 --> 01:19:22,560
they don't seem to have these big peaks

1949
01:19:22,560 --> 01:19:24,330
and that of course means that they're

1950
01:19:24,330 --> 01:19:25,770
you know they're much they're not going

1951
01:19:25,770 --> 01:19:27,210
to be susceptible to these adversarial

1952
01:19:27,210 --> 01:19:29,639
attacks now of course we can't verify

1953
01:19:29,639 --> 01:19:31,530
that along every dimension right that's

1954
01:19:31,530 --> 01:19:32,969
the whole point is that you know we

1955
01:19:32,969 --> 01:19:35,280
can't check every possible corner of

1956
01:19:35,280 --> 01:19:37,590
this this to the end corners of the of

1957
01:19:37,590 --> 01:19:39,690
the norm ball but we can get some sense

1958
01:19:39,690 --> 01:19:40,860
by looking at these kind of figures and

1959
01:19:40,860 --> 01:19:45,599
looking at the loss in this way ok so

1960
01:19:45,599 --> 01:19:47,610
this is the sort of in some sense the

1961
01:19:47,610 --> 01:19:51,889
the best way we know how to train our

1962
01:19:51,889 --> 01:19:53,490
empirically

1963
01:19:53,490 --> 01:19:56,550
train models against attacks like this

1964
01:19:56,550 --> 01:19:58,950
and while you know the jury is of course

1965
01:19:58,950 --> 01:20:00,630
still out whenever you have a method

1966
01:20:00,630 --> 01:20:02,730
that's only empirical they seem to hold

1967
01:20:02,730 --> 01:20:05,540
up really well against possible attacks

1968
01:20:05,540 --> 01:20:08,580
so the natural question is you know we

1969
01:20:08,580 --> 01:20:11,180
have these seemingly robust models and

1970
01:20:11,180 --> 01:20:13,890
in the last section I also describe to

1971
01:20:13,890 --> 01:20:17,040
you some tractable convex ways of

1972
01:20:17,040 --> 01:20:19,800
verifying whether they're classifiers

1973
01:20:19,800 --> 01:20:22,680
robust or not let's see if we can take

1974
01:20:22,680 --> 01:20:25,410
these robust models and actually verify

1975
01:20:25,410 --> 01:20:31,560
them so let's use the convex balance in

1976
01:20:31,560 --> 01:20:32,580
this case we're gonna use those

1977
01:20:32,580 --> 01:20:34,080
interval-based bounces the simple one

1978
01:20:34,080 --> 01:20:36,870
ascribe at the end to see what sort of

1979
01:20:36,870 --> 01:20:39,750
level of adversarial performance we can

1980
01:20:39,750 --> 01:20:45,300
guarantee for the robust model okay so

1981
01:20:45,300 --> 01:20:45,960
here we go

1982
01:20:45,960 --> 01:20:48,600
here's the big reveal here so clean

1983
01:20:48,600 --> 01:20:50,430
error for both the confident and the

1984
01:20:50,430 --> 01:20:51,810
robust training comm that is about the

1985
01:20:51,810 --> 01:20:53,160
same

1986
01:20:53,160 --> 01:20:55,500
of course F GSM attacks do much better

1987
01:20:55,500 --> 01:20:58,050
on the robust are on the normal or are

1988
01:20:58,050 --> 01:21:00,630
able to fool the the normal model much

1989
01:21:00,630 --> 01:21:03,810
more than the robust model PGD also

1990
01:21:03,810 --> 01:21:07,170
fools the normal model even more and the

1991
01:21:07,170 --> 01:21:08,580
question of course is what's this robust

1992
01:21:08,580 --> 01:21:09,990
bound gonna tell us you know maybe it

1993
01:21:09,990 --> 01:21:11,010
probably can't tell us anything about

1994
01:21:11,010 --> 01:21:13,620
there about the the normal comp net but

1995
01:21:13,620 --> 01:21:14,460
maybe you can tell us something about

1996
01:21:14,460 --> 01:21:17,760
our robust model be trained and

1997
01:21:17,760 --> 01:21:20,130
unfortunately the actual robust balance

1998
01:21:20,130 --> 01:21:21,780
you get if you run the convex procedure

1999
01:21:21,780 --> 01:21:23,820
for both these things is completely

2000
01:21:23,820 --> 01:21:26,220
vacuous and it's not just almost vacuous

2001
01:21:26,220 --> 01:21:27,960
it's like really totally vacuous here

2002
01:21:27,960 --> 01:21:29,430
like I think you have to get epsilon

2003
01:21:29,430 --> 01:21:31,560
down so this is an epsilon of 0.1 I

2004
01:21:31,560 --> 01:21:33,060
think to get a reasonable yet to go to

2005
01:21:33,060 --> 01:21:35,670
Epsilon point zero zero zero one or a

2006
01:21:35,670 --> 01:21:37,020
zero zero zero two or something like

2007
01:21:37,020 --> 01:21:39,480
that it's you're not you're not anywhere

2008
01:21:39,480 --> 01:21:42,210
close with these kind of things so

2009
01:21:42,210 --> 01:21:43,710
what's happening here why you know we

2010
01:21:43,710 --> 01:21:46,050
had this we had this great thing but but

2011
01:21:46,050 --> 01:21:48,480
that we can prove about models but but

2012
01:21:48,480 --> 01:21:50,340
it doesn't give us anything useful of

2013
01:21:50,340 --> 01:21:52,620
course if we could solve the IP about

2014
01:21:52,620 --> 01:21:53,790
these things and we might have more

2015
01:21:53,790 --> 01:21:55,200
stuff but actually this model is too big

2016
01:21:55,200 --> 01:21:57,240
to solve the integer program for that

2017
01:21:57,240 --> 01:21:58,920
that would take I don't know about the

2018
01:21:58,920 --> 01:21:59,940
heat death of the universe we would take

2019
01:21:59,940 --> 01:22:01,230
a really long time at least in my

2020
01:22:01,230 --> 01:22:05,760
implementation to solve this okay so

2021
01:22:05,760 --> 01:22:08,160
so what's going on here and the key

2022
01:22:08,160 --> 01:22:11,660
insight here is that models can be

2023
01:22:11,660 --> 01:22:14,280
bottles that can be broke convex Li

2024
01:22:14,280 --> 01:22:17,730
verified are a small subset of what

2025
01:22:17,730 --> 01:22:20,100
seemed to be the actually robust models

2026
01:22:20,100 --> 01:22:22,110
right and so if you think about sort of

2027
01:22:22,110 --> 01:22:23,760
a Venn diagram here there's all models

2028
01:22:23,760 --> 01:22:25,140
including non robust ones there's some

2029
01:22:25,140 --> 01:22:27,060
space of robust models in that and then

2030
01:22:27,060 --> 01:22:28,710
within that there's some space of convex

2031
01:22:28,710 --> 01:22:31,710
lis verifiable models right maybe we

2032
01:22:31,710 --> 01:22:32,670
haven't actually proven whether that's

2033
01:22:32,670 --> 01:22:34,560
empty or not that could could be empty

2034
01:22:34,560 --> 01:22:35,940
but stay tuned for that

2035
01:22:35,940 --> 01:22:38,520
and it turns out that these convex

2036
01:22:38,520 --> 01:22:40,830
balance sort of the the tractable

2037
01:22:40,830 --> 01:22:44,160
provable balance that we have are very

2038
01:22:44,160 --> 01:22:47,270
loose unless the model is built

2039
01:22:47,270 --> 01:22:50,130
specifically with them in mind all right

2040
01:22:50,130 --> 01:22:52,080
so it's great that we have these bounds

2041
01:22:52,080 --> 01:22:54,090
but we're actually get them say anything

2042
01:22:54,090 --> 01:22:55,830
non vacuous about classifiers

2043
01:22:55,830 --> 01:22:59,040
we actually need to train models upon

2044
01:22:59,040 --> 01:23:02,130
them so this is in fact bringing me

2045
01:23:02,130 --> 01:23:03,510
towards the end of what I'm going to say

2046
01:23:03,510 --> 01:23:06,120
here because it turns out this is

2047
01:23:06,120 --> 01:23:09,540
possible so the convex bounds I describe

2048
01:23:09,540 --> 01:23:10,980
select the interval balance those those

2049
01:23:10,980 --> 01:23:13,170
notions are propagating a box through

2050
01:23:13,170 --> 01:23:16,500
the network this just like a normal

2051
01:23:16,500 --> 01:23:19,020
network is actually also a convex

2052
01:23:19,020 --> 01:23:21,930
function of the network parameters so it

2053
01:23:21,930 --> 01:23:23,730
you know it's a function of the weights

2054
01:23:23,730 --> 01:23:24,989
and the bias these and all its kind of

2055
01:23:24,989 --> 01:23:27,300
stuff and I don't you don't don't don't

2056
01:23:27,300 --> 01:23:28,410
I won't go back to that but it's

2057
01:23:28,410 --> 01:23:29,970
basically involves you know Max's of

2058
01:23:29,970 --> 01:23:31,380
these things and zeros and the positive

2059
01:23:31,380 --> 01:23:34,050
negative parts etc but it's still just a

2060
01:23:34,050 --> 01:23:35,970
differentiable function of the network

2061
01:23:35,970 --> 01:23:37,950
parameters and what that means is I can

2062
01:23:37,950 --> 01:23:40,739
that my final bound that I have my final

2063
01:23:40,739 --> 01:23:42,660
convex bound I have on these things is

2064
01:23:42,660 --> 01:23:45,720
also a differentiable function of the

2065
01:23:45,720 --> 01:23:48,030
model parameters so what can you guys

2066
01:23:48,030 --> 01:23:51,180
these to actually minimize not my actual

2067
01:23:51,180 --> 01:23:53,460
robust loss that's very hard that's sort

2068
01:23:53,460 --> 01:23:55,230
of a comments real hard thing but I can

2069
01:23:55,230 --> 01:23:58,080
use to minimize an upper bound a strict

2070
01:23:58,080 --> 01:24:01,140
provable upper bound on the convex our

2071
01:24:01,140 --> 01:24:04,880
sorry on the robust loss of a classifier

2072
01:24:04,880 --> 01:24:07,380
of course the the actual outer opposite

2073
01:24:07,380 --> 01:24:08,880
reviews still not so long convex but at

2074
01:24:08,880 --> 01:24:11,970
least we have you known people bound and

2075
01:24:11,970 --> 01:24:13,530
actually some reasonable work kind of

2076
01:24:13,530 --> 01:24:16,440
surprisingly shows that interval battle

2077
01:24:16,440 --> 01:24:18,150
interval bound propagation actually can

2078
01:24:18,150 --> 01:24:19,260
sometimes work better

2079
01:24:19,260 --> 01:24:21,840
for this then more complex approaches

2080
01:24:21,840 --> 01:24:22,920
even though I think the jury is still

2081
01:24:22,920 --> 01:24:24,840
out on exactly what level of complexity

2082
01:24:24,840 --> 01:24:27,390
you want for your training versus for

2083
01:24:27,390 --> 01:24:31,260
your for your verification okay so let's

2084
01:24:31,260 --> 01:24:33,060
see the final the final thing here I'm

2085
01:24:33,060 --> 01:24:34,830
going to show before we will pop much

2086
01:24:34,830 --> 01:24:36,930
further out again and and talk at a

2087
01:24:36,930 --> 01:24:38,970
higher level and there should be time

2088
01:24:38,970 --> 01:24:40,890
for questions at the end so if I I think

2089
01:24:40,890 --> 01:24:45,030
we'll finish before before 10:30 okay so

2090
01:24:45,030 --> 01:24:47,160
here's our different settings here is

2091
01:24:47,160 --> 01:24:50,820
again showing test error the the attacks

2092
01:24:50,820 --> 01:24:52,110
of the different things as well as the

2093
01:24:52,110 --> 01:24:53,910
provable bounds are confident you know

2094
01:24:53,910 --> 01:24:56,580
gets bad in all regards except normal

2095
01:24:56,580 --> 01:24:59,700
clean error our robust confident we

2096
01:24:59,700 --> 01:25:02,610
train with PGD works well and all the

2097
01:25:02,610 --> 01:25:04,110
empirical attacks but the bound itself

2098
01:25:04,110 --> 01:25:06,960
is complete vacuous but finally if I

2099
01:25:06,960 --> 01:25:09,180
actually train the network to minimize

2100
01:25:09,180 --> 01:25:12,780
this bound we get the following so this

2101
01:25:12,780 --> 01:25:14,640
is just a quick one that we have again

2102
01:25:14,640 --> 01:25:16,080
in the notes you can do much better than

2103
01:25:16,080 --> 01:25:17,460
this you can get this down at least on

2104
01:25:17,460 --> 01:25:18,840
the end this quite low but I don't want

2105
01:25:18,840 --> 01:25:19,860
actually make it too low because that

2106
01:25:19,860 --> 01:25:20,940
will give a false impression you can do

2107
01:25:20,940 --> 01:25:22,620
that any dataset and other ones it's

2108
01:25:22,620 --> 01:25:24,090
much harder but leaves on em this you

2109
01:25:24,090 --> 01:25:26,010
can get these down pretty low and so

2110
01:25:26,010 --> 01:25:28,200
what we have here it's not gonna win the

2111
01:25:28,200 --> 01:25:30,690
awards by any means what we have is we

2112
01:25:30,690 --> 01:25:35,190
have an amnesty we're on the test set I

2113
01:25:35,190 --> 01:25:38,100
know that no matter what attack I have

2114
01:25:38,100 --> 01:25:39,390
no matter how many more papers are

2115
01:25:39,390 --> 01:25:40,830
written about attacks no matter anything

2116
01:25:40,830 --> 01:25:44,280
no one's gonna have higher error higher

2117
01:25:44,280 --> 01:25:47,180
robust you know adversarial error with

2118
01:25:47,180 --> 01:25:49,380
Ellen a lot of qualifications here right

2119
01:25:49,380 --> 01:25:51,630
with L infinity bad perturbations under

2120
01:25:51,630 --> 01:25:54,360
epsilon of 0.1 no one's gonna ever

2121
01:25:54,360 --> 01:25:57,510
reveal a higher error than then 9.7 on

2122
01:25:57,510 --> 01:25:59,970
this classifier so that's that's that's

2123
01:25:59,970 --> 01:26:01,470
nice it's something we couldn't do I

2124
01:26:01,470 --> 01:26:03,570
didn't know how to do even a little bit

2125
01:26:03,570 --> 01:26:05,940
more than a year ago and these

2126
01:26:05,940 --> 01:26:07,980
techniques have now really been sort of

2127
01:26:07,980 --> 01:26:11,100
been been growing a lot within the

2128
01:26:11,100 --> 01:26:12,600
machine learning literature and so we're

2129
01:26:12,600 --> 01:26:13,860
coming with a lot more techniques for

2130
01:26:13,860 --> 01:26:15,510
verifying them things as well as trained

2131
01:26:15,510 --> 01:26:17,040
as little as well as training these

2132
01:26:17,040 --> 01:26:21,000
verified models okay so one last note

2133
01:26:21,000 --> 01:26:23,210
for me before I throw it back to to Alex

2134
01:26:23,210 --> 01:26:26,910
and that is these results seem good

2135
01:26:26,910 --> 01:26:30,510
right it seems promising here I had you

2136
01:26:30,510 --> 01:26:32,250
know a bad normal model I trained a

2137
01:26:32,250 --> 01:26:32,850
robust mom

2138
01:26:32,850 --> 01:26:36,510
everything looked good I could even

2139
01:26:36,510 --> 01:26:37,890
train a provably robust model to get

2140
01:26:37,890 --> 01:26:39,180
maybe a little say worse accuracy but

2141
01:26:39,180 --> 01:26:40,980
Prius a pretty good accuracy and things

2142
01:26:40,980 --> 01:26:42,240
seemed really good

2143
01:26:42,240 --> 01:26:44,760
however one thing I should emphasize is

2144
01:26:44,760 --> 01:26:47,280
that these sort of promising results at

2145
01:26:47,280 --> 01:26:49,560
least currently are to a large extent

2146
01:26:49,560 --> 01:26:51,750
actually more a function of M inist than

2147
01:26:51,750 --> 01:26:54,600
a function of our ability to scale these

2148
01:26:54,600 --> 01:26:56,520
things to really big problems all right

2149
01:26:56,520 --> 01:26:58,770
so so M this is is also actually quite

2150
01:26:58,770 --> 01:27:01,440
easy against to prevent to secure us

2151
01:27:01,440 --> 01:27:02,640
attacks because it's kind of a binary

2152
01:27:02,640 --> 01:27:04,440
problem right if there's there's pixels

2153
01:27:04,440 --> 01:27:06,240
that are on or off and you know adding a

2154
01:27:06,240 --> 01:27:07,350
little bit noise here and there doesn't

2155
01:27:07,350 --> 01:27:09,840
really make things that much harder

2156
01:27:09,840 --> 01:27:13,730
intuitively but for example like see far

2157
01:27:13,730 --> 01:27:16,860
the best bounds that we know of the best

2158
01:27:16,860 --> 01:27:19,680
empirical balance that we know of which

2159
01:27:19,680 --> 01:27:21,090
you know see if we resolve to like three

2160
01:27:21,090 --> 01:27:22,500
percent error currently maybe even lower

2161
01:27:22,500 --> 01:27:26,160
now see about 10 this is I'll see var 10

2162
01:27:26,160 --> 01:27:28,590
the best empirical bounds we know of say

2163
01:27:28,590 --> 01:27:31,980
that the error is we can't find a TAC

2164
01:27:31,980 --> 01:27:33,900
that gives what that you're pretty small

2165
01:27:33,900 --> 01:27:35,730
perturbation about smaller than the one

2166
01:27:35,730 --> 01:27:38,850
we showed for M this there we have a

2167
01:27:38,850 --> 01:27:43,410
guaranteed bound of 50 we can achieve 53

2168
01:27:43,410 --> 01:27:45,420
percent error with the best methods that

2169
01:27:45,420 --> 01:27:47,130
we know how to do and the provable

2170
01:27:47,130 --> 01:27:48,330
balance does we can do is say there's

2171
01:27:48,330 --> 01:27:50,400
about us you know we know we can form a

2172
01:27:50,400 --> 01:27:51,810
model that has less than 70 percent

2173
01:27:51,810 --> 01:27:54,690
error on C far worse again the state of

2174
01:27:54,690 --> 01:27:57,890
the art now is around 3% so for larger

2175
01:27:57,890 --> 01:28:01,100
problems scaling these things up and

2176
01:28:01,100 --> 01:28:03,330
finding the architectures the both the

2177
01:28:03,330 --> 01:28:05,850
architectures the bounds and the

2178
01:28:05,850 --> 01:28:07,830
optimization procedures they will make

2179
01:28:07,830 --> 01:28:10,380
these things work is still very much an

2180
01:28:10,380 --> 01:28:13,530
open question and it's the topic of

2181
01:28:13,530 --> 01:28:16,110
current and ongoing research both in the

2182
01:28:16,110 --> 01:28:18,060
empirical attack side and defense side

2183
01:28:18,060 --> 01:28:20,910
and in the provable attack in the fence

2184
01:28:20,910 --> 01:28:23,490
side so next up Alex doesn't talk a

2185
01:28:23,490 --> 01:28:24,630
little bit more about this as well as a

2186
01:28:24,630 --> 01:28:27,510
higher level of issues that arise from

2187
01:28:27,510 --> 01:28:31,100
these adversarial examples

2188
01:28:32,060 --> 01:28:39,999
[Applause]

2189
01:28:47,930 --> 01:28:50,780
okay so welcome back thanks it was

2190
01:28:50,780 --> 01:28:54,890
pretty nice even I learned something so

2191
01:28:54,890 --> 01:28:57,650
yeah so so far is eco like this like

2192
01:28:57,650 --> 01:28:59,900
provided this excellent view into kind

2193
01:28:59,900 --> 01:29:02,510
of you know the not and nuts and bolts

2194
01:29:02,510 --> 01:29:05,630
of like how do you diagnose robustness

2195
01:29:05,630 --> 01:29:07,280
how do you ensure robustness and where

2196
01:29:07,280 --> 01:29:10,520
we are but somehow I think we both feel

2197
01:29:10,520 --> 01:29:12,380
is very important here is not to miss

2198
01:29:12,380 --> 01:29:13,910
the big picture here that essentially

2199
01:29:13,910 --> 01:29:16,130
against security and ensuring the

2200
01:29:16,130 --> 01:29:17,780
security is very important but there

2201
01:29:17,780 --> 01:29:20,150
actually is something even kind of a bit

2202
01:29:20,150 --> 01:29:20,900
more high-level

2203
01:29:20,900 --> 01:29:22,370
that is going on here so sort of like

2204
01:29:22,370 --> 01:29:24,050
that the other side robustness might be

2205
01:29:24,050 --> 01:29:26,000
it's an interesting notion by itself

2206
01:29:26,000 --> 01:29:27,950
even if you take it outside of the

2207
01:29:27,950 --> 01:29:30,680
context of security okay so just to

2208
01:29:30,680 --> 01:29:33,140
remind so this is like in some ways this

2209
01:29:33,140 --> 01:29:34,880
conceptual picture that emerges here is

2210
01:29:34,880 --> 01:29:37,970
that kind of we can try to not only

2211
01:29:37,970 --> 01:29:39,860
focus on the applications of ml that

2212
01:29:39,860 --> 01:29:41,870
require security but actually like once

2213
01:29:41,870 --> 01:29:43,550
we know that there are applications

2214
01:29:43,550 --> 01:29:44,660
where we care about this kind of

2215
01:29:44,660 --> 01:29:47,030
security aspects of ml we can take a

2216
01:29:47,030 --> 01:29:48,980
step back and look at machine learning

2217
01:29:48,980 --> 01:29:51,050
and see how does machine learning look

2218
01:29:51,050 --> 01:29:53,240
like to this lens of robotics okay so

2219
01:29:53,240 --> 01:29:55,970
specifically you know if I on one hand

2220
01:29:55,970 --> 01:29:58,040
take this standard machine learning that

2221
01:29:58,040 --> 01:29:59,480
we are doing now which corresponds to

2222
01:29:59,480 --> 01:30:01,100
standard generalization you know how

2223
01:30:01,100 --> 01:30:03,200
does differ from this kind of robust

2224
01:30:03,200 --> 01:30:04,880
machine learning that am I trying to

2225
01:30:04,880 --> 01:30:06,500
build here so she's like on the very

2226
01:30:06,500 --> 01:30:08,270
mathematical level all that we are

2227
01:30:08,270 --> 01:30:10,640
trying to compare is this two fields one

2228
01:30:10,640 --> 01:30:13,340
that tries to maximize like 12 12 to

2229
01:30:13,340 --> 01:30:16,640
achieve the first goal and the other one

2230
01:30:16,640 --> 01:30:17,900
that tries to achieve the second goal

2231
01:30:17,900 --> 01:30:19,670
and one thing that I really want to

2232
01:30:19,670 --> 01:30:21,080
emphasize here the work should be really

2233
01:30:21,080 --> 01:30:23,000
clear from what zico's dog that really

2234
01:30:23,000 --> 01:30:25,910
even though we sort of discovered you

2235
01:30:25,910 --> 01:30:28,910
know discovered in quotes the other cell

2236
01:30:28,910 --> 01:30:30,110
examples in the context of deep learning

2237
01:30:30,110 --> 01:30:32,570
it's really not only a deep learning

2238
01:30:32,570 --> 01:30:34,970
problem by far yes in deep learning it's

2239
01:30:34,970 --> 01:30:36,950
easier to find through examples but

2240
01:30:36,950 --> 01:30:39,050
every type of classifiers that we know

2241
01:30:39,050 --> 01:30:41,390
of has them why does it have them

2242
01:30:41,390 --> 01:30:43,250
because again it was trying to achieve

2243
01:30:43,250 --> 01:30:44,810
some organization it never cared about

2244
01:30:44,810 --> 01:30:46,700
robust generalization so you know if we

2245
01:30:46,700 --> 01:30:48,680
never design for it there is no reason

2246
01:30:48,680 --> 01:30:49,610
for us to expect

2247
01:30:49,610 --> 01:30:51,950
magically things happen okay so that's

2248
01:30:51,950 --> 01:30:53,870
just some point in to keep in mind but

2249
01:30:53,870 --> 01:30:55,010
yeah but this is the question now can

2250
01:30:55,010 --> 01:30:56,390
saying you know we have this two

2251
01:30:56,390 --> 01:30:56,929
different

2252
01:30:56,929 --> 01:30:59,119
notion of machine learning or succeeding

2253
01:30:59,119 --> 01:31:00,949
in machine learning what does change

2254
01:31:00,949 --> 01:31:03,050
when we move from one to the other

2255
01:31:03,050 --> 01:31:04,510
okay so I would just give you some

2256
01:31:04,510 --> 01:31:07,820
vinaigrette so the first one is just

2257
01:31:07,820 --> 01:31:09,920
about like overfitting of robust

2258
01:31:09,920 --> 01:31:12,469
developer okay so we all know the story

2259
01:31:12,469 --> 01:31:13,849
especially for deep learning and

2260
01:31:13,849 --> 01:31:15,679
internal classic optimization of

2261
01:31:15,679 --> 01:31:17,959
overfitting so you know over here you

2262
01:31:17,959 --> 01:31:20,719
can train some classifier and yes like

2263
01:31:20,719 --> 01:31:22,939
this is the training accuracy so after

2264
01:31:22,939 --> 01:31:25,039
pushing it for long enough we get 100%

2265
01:31:25,039 --> 01:31:27,469
training accuracy and of course you know

2266
01:31:27,469 --> 01:31:29,179
so that's what we do in our tools but of

2267
01:31:29,179 --> 01:31:31,579
course the question is okay how you know

2268
01:31:31,579 --> 01:31:33,949
what like what will happen if you

2269
01:31:33,949 --> 01:31:36,260
actually test the solution on the you

2270
01:31:36,260 --> 01:31:39,139
know on the unseen test set and this is

2271
01:31:39,139 --> 01:31:41,090
the usual performance view observe and

2272
01:31:41,090 --> 01:31:42,559
what on what is stuff amazing especially

2273
01:31:42,559 --> 01:31:45,110
for the pet works that actually the gap

2274
01:31:45,110 --> 01:31:47,929
between what we can ensure by training

2275
01:31:47,929 --> 01:31:50,239
and what we actually also get on unseen

2276
01:31:50,239 --> 01:31:52,489
samples legitimization we got tends to

2277
01:31:52,489 --> 01:31:55,010
be really small okay so that's great in

2278
01:31:55,010 --> 01:31:56,719
the standard machine learning but how

2279
01:31:56,719 --> 01:31:58,909
does it look like in robust set okay so

2280
01:31:58,909 --> 01:32:01,159
in robots I think we also well it's it's

2281
01:32:01,159 --> 01:32:02,479
much much harder as will be Zuko

2282
01:32:02,479 --> 01:32:04,670
explained to actually get to this like

2283
01:32:04,670 --> 01:32:06,769
100 percent robust training error

2284
01:32:06,769 --> 01:32:08,840
sometimes even impossible because of of

2285
01:32:08,840 --> 01:32:11,090
data being not of perfectly separable

2286
01:32:11,090 --> 01:32:13,639
but here we can get there and now the

2287
01:32:13,639 --> 01:32:15,439
basic particular this is keep true that

2288
01:32:15,439 --> 01:32:17,179
we get for C far so that was exactly

2289
01:32:17,179 --> 01:32:20,539
what Zico mention at the end this is if

2290
01:32:20,539 --> 01:32:23,630
our so we can train a classifier that

2291
01:32:23,630 --> 01:32:25,579
gets hundred percent robust training

2292
01:32:25,579 --> 01:32:27,590
accuracy so there exists a classifier

2293
01:32:27,590 --> 01:32:30,409
that is robust however if we look what

2294
01:32:30,409 --> 01:32:33,229
happens on unseen examples that's the

2295
01:32:33,229 --> 01:32:35,059
performance we observe so we have this

2296
01:32:35,059 --> 01:32:37,789
really huge generalization gun okay so

2297
01:32:37,789 --> 01:32:39,619
essentially they think we get is that we

2298
01:32:39,619 --> 01:32:41,689
really get under 50 percent robust

2299
01:32:41,689 --> 01:32:42,979
performance even though the training

2300
01:32:42,979 --> 01:32:46,099
performance is 100 or close to it okay

2301
01:32:46,099 --> 01:32:49,969
so well what can we do the obvious idea

2302
01:32:49,969 --> 01:32:51,349
that we know from some person learning

2303
01:32:51,349 --> 01:32:53,689
is just to use regularization and maybe

2304
01:32:53,689 --> 01:32:56,510
this idea does work so far we tried and

2305
01:32:56,510 --> 01:32:58,670
unfortunately even if they're not it

2306
01:32:58,670 --> 01:33:01,309
seemed that it might work it doesn't so

2307
01:33:01,309 --> 01:33:03,110
this is actually quite puzzling for us

2308
01:33:03,110 --> 01:33:04,939
especially because like we try to push

2309
01:33:04,939 --> 01:33:07,699
the C far as a you know in our lab like

2310
01:33:07,699 --> 01:33:08,910
quite a lot and

2311
01:33:08,910 --> 01:33:10,920
to wonder okay maybe there is some

2312
01:33:10,920 --> 01:33:13,260
inherent reason why getting better

2313
01:33:13,260 --> 01:33:15,840
accuracy and avoiding avoiding

2314
01:33:15,840 --> 01:33:18,480
overfitting is a problem here and indeed

2315
01:33:18,480 --> 01:33:20,520
there is something there is some

2316
01:33:20,520 --> 01:33:22,770
phenomenon that we discovered and you

2317
01:33:22,770 --> 01:33:23,940
know the fellowman that we discover is

2318
01:33:23,940 --> 01:33:26,430
it actually turns out that you know if

2319
01:33:26,430 --> 01:33:28,710
we think about aiming for adversely

2320
01:33:28,710 --> 01:33:30,600
dramatisation as opposed to

2321
01:33:30,600 --> 01:33:32,610
standardization then actually it's not

2322
01:33:32,610 --> 01:33:34,410
only a matter of the optimization

2323
01:33:34,410 --> 01:33:35,700
problem corresponding to training become

2324
01:33:35,700 --> 01:33:38,280
ring more difficult it's also the kind

2325
01:33:38,280 --> 01:33:40,530
of sample complexity of this problem the

2326
01:33:40,530 --> 01:33:43,500
you know becomes worse so in other terms

2327
01:33:43,500 --> 01:33:45,540
essentially with attempts to read more

2328
01:33:45,540 --> 01:33:48,420
data to even be able to get a robust

2329
01:33:48,420 --> 01:33:49,950
classifier so a classifier that

2330
01:33:49,950 --> 01:33:52,380
terrorized in a robust way so here is

2331
01:33:52,380 --> 01:33:53,970
just the kind of theorem that you can

2332
01:33:53,970 --> 01:33:55,380
actually prove you can essentially prove

2333
01:33:55,380 --> 01:33:56,790
that there exists you know in the

2334
01:33:56,790 --> 01:33:58,340
dimension some even very simple

2335
01:33:58,340 --> 01:34:00,360
distributions extremism of distributions

2336
01:34:00,360 --> 01:34:03,030
that already exhibit this effect of you

2337
01:34:03,030 --> 01:34:05,100
know I I just have one sample

2338
01:34:05,100 --> 01:34:05,970
distribution

2339
01:34:05,970 --> 01:34:08,070
I am already able to get very good

2340
01:34:08,070 --> 01:34:10,620
classifier that performs extremely well

2341
01:34:10,620 --> 01:34:12,240
on average sense in an average sense

2342
01:34:12,240 --> 01:34:15,170
however in order to have any hope of

2343
01:34:15,170 --> 01:34:18,180
building a robust classifier I need to

2344
01:34:18,180 --> 01:34:20,820
see many many many more samples okay and

2345
01:34:20,820 --> 01:34:22,050
the key here you think that this is like

2346
01:34:22,050 --> 01:34:24,120
information theoretic bound so it

2347
01:34:24,120 --> 01:34:25,350
doesn't matter what you do you can do

2348
01:34:25,350 --> 01:34:26,970
data documentation whatever essentially

2349
01:34:26,970 --> 01:34:29,370
you will be stuck until you have seen

2350
01:34:29,370 --> 01:34:31,050
enough samples and you will be not able

2351
01:34:31,050 --> 01:34:33,450
to get any meaningful robust classifier

2352
01:34:33,450 --> 01:34:35,670
okay so say this rules are very simple

2353
01:34:35,670 --> 01:34:39,000
so one of them is just like a simple

2354
01:34:39,000 --> 01:34:40,980
like know hypercube perform like you

2355
01:34:40,980 --> 01:34:43,530
know perturbed perturbed distribution

2356
01:34:43,530 --> 01:34:45,360
it's essentially kind of models what's

2357
01:34:45,360 --> 01:34:46,830
happening called M needs were linear

2358
01:34:46,830 --> 01:34:49,680
classifiers for robust like for a robust

2359
01:34:49,680 --> 01:34:50,940
case they should require a lot of data

2360
01:34:50,940 --> 01:34:54,060
but you know but essentially if you are

2361
01:34:54,060 --> 01:34:56,280
able to use a new linear classifier as

2362
01:34:56,280 --> 01:34:58,110
something called stress holding if you

2363
01:34:58,110 --> 01:34:59,790
think about how an this works you can

2364
01:34:59,790 --> 01:35:01,440
you know you can very easily get robust

2365
01:35:01,440 --> 01:35:03,270
classifier but then there are also are

2366
01:35:03,270 --> 01:35:04,710
distribution so the first exhibition was

2367
01:35:04,710 --> 01:35:07,110
hard for linear classifiers but easy for

2368
01:35:07,110 --> 01:35:08,730
nonlinear classifiers but you can also

2369
01:35:08,730 --> 01:35:11,130
connect you know distributions that are

2370
01:35:11,130 --> 01:35:13,290
hard for any classifier so sorry no

2371
01:35:13,290 --> 01:35:15,210
matter what has fair use unless you have

2372
01:35:15,210 --> 01:35:16,860
seen enough samples you will not be able

2373
01:35:16,860 --> 01:35:18,630
to get robust as effective classifier

2374
01:35:18,630 --> 01:35:21,489
even though you know average case

2375
01:35:21,489 --> 01:35:22,599
the standard translation is very easy

2376
01:35:22,599 --> 01:35:24,880
even for one sample okay so that's all I

2377
01:35:24,880 --> 01:35:26,679
will say about this there is actually a

2378
01:35:26,679 --> 01:35:28,389
spotlight and posted about this in

2379
01:35:28,389 --> 01:35:31,360
Tuesday but here so that's one effect in

2380
01:35:31,360 --> 01:35:33,340
which kind of the sample complexity of

2381
01:35:33,340 --> 01:35:35,800
the problem changes often quite

2382
01:35:35,800 --> 01:35:38,800
dramatically but there is also another

2383
01:35:38,800 --> 01:35:40,690
issue I wanted to highlight it's just of

2384
01:35:40,690 --> 01:35:44,409
the question of you know how kind of how

2385
01:35:44,409 --> 01:35:47,710
does being robust affect our standard

2386
01:35:47,710 --> 01:35:50,739
realization okay so somehow my maybe

2387
01:35:50,739 --> 01:35:52,570
like where I'm coming from so one may

2388
01:35:52,570 --> 01:35:54,880
believe coming into working with this

2389
01:35:54,880 --> 01:35:56,949
was that somehow once you really do all

2390
01:35:56,949 --> 01:35:59,199
this work and put all this work to get a

2391
01:35:59,199 --> 01:36:02,079
robust classifier then it should be kind

2392
01:36:02,079 --> 01:36:04,840
of superior to a standard standard

2393
01:36:04,840 --> 01:36:07,239
classifier in all aspects okay so in

2394
01:36:07,239 --> 01:36:08,889
particular you know we know that one of

2395
01:36:08,889 --> 01:36:10,690
the effective techniques instead like to

2396
01:36:10,690 --> 01:36:12,849
get a better standardization is de

2397
01:36:12,849 --> 01:36:14,289
documentation is essentially like adding

2398
01:36:14,289 --> 01:36:15,369
some transformation to your training

2399
01:36:15,369 --> 01:36:17,380
data during training to essentially kind

2400
01:36:17,380 --> 01:36:20,289
of you try to like impose a stronger

2401
01:36:20,289 --> 01:36:22,480
prior and indeed works very very well

2402
01:36:22,480 --> 01:36:24,460
and in some ways if you look at adverse

2403
01:36:24,460 --> 01:36:26,679
trader training which is the current

2404
01:36:26,679 --> 01:36:28,030
leading technique to get robustness

2405
01:36:28,030 --> 01:36:29,710
you can view it as an ultimate version

2406
01:36:29,710 --> 01:36:31,030
of the documentation because essentially

2407
01:36:31,030 --> 01:36:32,230
what you are doing you are always

2408
01:36:32,230 --> 01:36:33,849
training on the advertiser perturbation

2409
01:36:33,849 --> 01:36:35,769
of the training data so in some ways you

2410
01:36:35,769 --> 01:36:37,179
are training on the most confusing

2411
01:36:37,179 --> 01:36:40,239
version of the training set okay so

2412
01:36:40,239 --> 01:36:42,010
clearly you would expect that you know

2413
01:36:42,010 --> 01:36:44,199
essentially this is only a good thing

2414
01:36:44,199 --> 01:36:46,780
and in particular that being robust can

2415
01:36:46,780 --> 01:36:49,239
only make your standardization even

2416
01:36:49,239 --> 01:36:50,650
better because you are using this kind

2417
01:36:50,650 --> 01:36:52,809
of implicitly this you know ultimate way

2418
01:36:52,809 --> 01:36:55,780
of doing data alimentation and well

2419
01:36:55,780 --> 01:36:58,090
let's look at the data so yeah over here

2420
01:36:58,090 --> 01:36:59,650
we have just the you know the standard

2421
01:36:59,650 --> 01:37:01,269
test accuracy of a standard model

2422
01:37:01,269 --> 01:37:03,639
trained in the standard way so this is

2423
01:37:03,639 --> 01:37:06,309
just a reference point however if we

2424
01:37:06,309 --> 01:37:08,530
take a look at what happens if I just

2425
01:37:08,530 --> 01:37:10,840
look at the standard accuracy of a

2426
01:37:10,840 --> 01:37:13,360
robust model then we see that there

2427
01:37:13,360 --> 01:37:16,449
actually is a gun it's not a big gap but

2428
01:37:16,449 --> 01:37:18,070
it is a gap it's actually a consistent

2429
01:37:18,070 --> 01:37:21,099
gap you know and yeah we tried it in a

2430
01:37:21,099 --> 01:37:23,829
different setup a normally by the way

2431
01:37:23,829 --> 01:37:27,039
there is a regime like a very like at

2432
01:37:27,039 --> 01:37:28,690
the very beginning data argumentation

2433
01:37:28,690 --> 01:37:30,820
like sorry robust training sometimes

2434
01:37:30,820 --> 01:37:32,800
does held a little bit but in the long

2435
01:37:32,800 --> 01:37:33,330
term

2436
01:37:33,330 --> 01:37:35,250
at the regime were actually we end up

2437
01:37:35,250 --> 01:37:37,620
training it's always consistently lower

2438
01:37:37,620 --> 01:37:40,320
okay so what's going on so where is this

2439
01:37:40,320 --> 01:37:42,810
consistent gap coming from why you know

2440
01:37:42,810 --> 01:37:44,490
why we seem to be inferior in this way

2441
01:37:44,490 --> 01:37:46,470
and in a sense as you know as many

2442
01:37:46,470 --> 01:37:48,450
things in the retrospect it's clear

2443
01:37:48,450 --> 01:37:50,730
what's going on so first of all there is

2444
01:37:50,730 --> 01:37:52,440
this phenomenon that kind of you know if

2445
01:37:52,440 --> 01:37:54,270
I'm trying to train a classifier I

2446
01:37:54,270 --> 01:37:56,610
somehow have to choose if I want to

2447
01:37:56,610 --> 01:37:58,740
maximize this standardization

2448
01:37:58,740 --> 01:38:01,050
performance or do I want the classifier

2449
01:38:01,050 --> 01:38:02,490
be robust and there is actually an

2450
01:38:02,490 --> 01:38:04,050
inherent trade-off between this and

2451
01:38:04,050 --> 01:38:06,000
again the trade-off actually in the end

2452
01:38:06,000 --> 01:38:08,460
is kind of pretty clear what is the key

2453
01:38:08,460 --> 01:38:10,620
idea here is sort of if you actually if

2454
01:38:10,620 --> 01:38:12,240
you look back at standard training and

2455
01:38:12,240 --> 01:38:13,830
just when your goal is just on a

2456
01:38:13,830 --> 01:38:16,770
transition then to you essentially any

2457
01:38:16,770 --> 01:38:19,530
feature that has any correlation with

2458
01:38:19,530 --> 01:38:21,690
the correct label is useful to you okay

2459
01:38:21,690 --> 01:38:23,790
so you can always milk it a bit to get

2460
01:38:23,790 --> 01:38:26,040
even better standard performance however

2461
01:38:26,040 --> 01:38:28,200
in the robot setting you know you can't

2462
01:38:28,200 --> 01:38:30,270
just if there is some feature that is

2463
01:38:30,270 --> 01:38:31,800
maybe very weakly correlated with the

2464
01:38:31,800 --> 01:38:34,320
day with the label but kind of is easy

2465
01:38:34,320 --> 01:38:36,420
for the classifier to perturb well then

2466
01:38:36,420 --> 01:38:38,070
you have to actually make a call if you

2467
01:38:38,070 --> 01:38:39,360
actually want to take advantage of this

2468
01:38:39,360 --> 01:38:42,420
and improve your accuracy or not okay

2469
01:38:42,420 --> 01:38:44,250
and this is the basic kind of you know

2470
01:38:44,250 --> 01:38:46,680
trade-off at play here that kind of

2471
01:38:46,680 --> 01:38:48,600
manifests itself and you can imagine

2472
01:38:48,600 --> 01:38:50,070
imagine this just like a simple way to

2473
01:38:50,070 --> 01:38:51,690
think about it that yeah I have a data

2474
01:38:51,690 --> 01:38:53,610
set in which like the first coordinate

2475
01:38:53,610 --> 01:38:55,890
is strongly correlated with the correct

2476
01:38:55,890 --> 01:38:58,440
label but it's not perfect and then I

2477
01:38:58,440 --> 01:39:00,260
have a bunch of very really independent

2478
01:39:00,260 --> 01:39:03,300
like you know correlated labels oh sorry

2479
01:39:03,300 --> 01:39:05,430
features that you know that are at the

2480
01:39:05,430 --> 01:39:07,530
rest of my the that are in the rest of

2481
01:39:07,530 --> 01:39:09,900
my dataset and somehow what will happen

2482
01:39:09,900 --> 01:39:12,000
is that this weekly corrected features

2483
01:39:12,000 --> 01:39:13,980
of all of them if I allocate them they

2484
01:39:13,980 --> 01:39:16,260
aggregate to a very very meaningful and

2485
01:39:16,260 --> 01:39:18,690
predictive they know meta feature but

2486
01:39:18,690 --> 01:39:20,070
the problem is that in standard training

2487
01:39:20,070 --> 01:39:21,180
you know what you will do you will just

2488
01:39:21,180 --> 01:39:23,220
use this meta feature to get essentially

2489
01:39:23,220 --> 01:39:25,830
perfect classification however in the

2490
01:39:25,830 --> 01:39:28,170
robot setting you are not even Allegra

2491
01:39:28,170 --> 01:39:31,140
lee able to kind of to you know to take

2492
01:39:31,140 --> 01:39:32,100
advantage of all of these features

2493
01:39:32,100 --> 01:39:33,570
because even though they are very

2494
01:39:33,570 --> 01:39:36,060
accurate on average there are very easy

2495
01:39:36,060 --> 01:39:37,620
to be manipulated by the adversary

2496
01:39:37,620 --> 01:39:39,750
because the correlation is only like

2497
01:39:39,750 --> 01:39:41,100
each one of them is only weak

2498
01:39:41,100 --> 01:39:42,750
like that with the with the correct

2499
01:39:42,750 --> 01:39:44,790
labor okay and this is essentially the

2500
01:39:44,790 --> 01:39:46,800
this is essentially the you know the

2501
01:39:46,800 --> 01:39:51,030
explanation okay so okay so let's just

2502
01:39:51,030 --> 01:39:52,590
try to summarize a little bit you know

2503
01:39:52,590 --> 01:39:53,970
where we are where we think about this

2504
01:39:53,970 --> 01:39:56,340
difference between robots machine

2505
01:39:56,340 --> 01:39:57,930
learning and it's not machine learning

2506
01:39:57,930 --> 01:40:00,150
so it's very clear to everyone and

2507
01:40:00,150 --> 01:40:01,650
that's what is expected that there

2508
01:40:01,650 --> 01:40:04,890
actually is a price to be robust okay so

2509
01:40:04,890 --> 01:40:06,960
in particular as we already said as you

2510
01:40:06,960 --> 01:40:08,520
go said first of all you know

2511
01:40:08,520 --> 01:40:10,830
optimization doing that we need to like

2512
01:40:10,830 --> 01:40:11,970
the initial problems we need to solve

2513
01:40:11,970 --> 01:40:14,190
during training is way more difficult

2514
01:40:14,190 --> 01:40:16,290
and also like the one other thing that

2515
01:40:16,290 --> 01:40:18,240
we discovered is that the models have to

2516
01:40:18,240 --> 01:40:20,850
be larger so there is this is clearly an

2517
01:40:20,850 --> 01:40:23,700
undeniably a bad thing you know we also

2518
01:40:23,700 --> 01:40:25,020
just mentioned that actually you might

2519
01:40:25,020 --> 01:40:26,310
also need more data so it's not only

2520
01:40:26,310 --> 01:40:28,500
about training time or like our size of

2521
01:40:28,500 --> 01:40:28,980
the model

2522
01:40:28,980 --> 01:40:31,020
also you might need more data to train

2523
01:40:31,020 --> 01:40:34,020
and and also as we said is that actually

2524
01:40:34,020 --> 01:40:36,750
you might also need to kind of give up a

2525
01:40:36,750 --> 01:40:39,030
bit on standard accuracy okay so this is

2526
01:40:39,030 --> 01:40:40,230
something again so what we observe and

2527
01:40:40,230 --> 01:40:41,820
also there is a more recent work by uber

2528
01:40:41,820 --> 01:40:43,740
get all that kind of tries to make this

2529
01:40:43,740 --> 01:40:45,930
point even more broadly so you know so

2530
01:40:45,930 --> 01:40:47,730
there are all this costs and you know

2531
01:40:47,730 --> 01:40:49,620
that's not to be expected you know we it

2532
01:40:49,620 --> 01:40:51,360
seems we are trying to get something

2533
01:40:51,360 --> 01:40:54,030
stronger so we have to pay for that but

2534
01:40:54,030 --> 01:40:55,080
it turns out that there are also

2535
01:40:55,080 --> 01:40:56,970
somewhat unexpected benefits like

2536
01:40:56,970 --> 01:40:59,190
essentially like beyond just being

2537
01:40:59,190 --> 01:41:01,980
robust it turns out that robust models

2538
01:41:01,980 --> 01:41:03,990
kind of have additional features that

2539
01:41:03,990 --> 01:41:06,270
might be durable okay so one of the

2540
01:41:06,270 --> 01:41:07,740
things I wanted to highlight is in some

2541
01:41:07,740 --> 01:41:09,690
ways you know the model the robust

2542
01:41:09,690 --> 01:41:12,420
models tend to be more semantically

2543
01:41:12,420 --> 01:41:14,850
meaningful okay so what does it mean I

2544
01:41:14,850 --> 01:41:15,990
will just give you an example because

2545
01:41:15,990 --> 01:41:17,340
honestly we still don't really

2546
01:41:17,340 --> 01:41:19,500
understand what's going on there so you

2547
01:41:19,500 --> 01:41:21,870
know one kind of so so what is the going

2548
01:41:21,870 --> 01:41:23,730
back to yes like it's going back to this

2549
01:41:23,730 --> 01:41:25,410
fact that all of you are probably aware

2550
01:41:25,410 --> 01:41:28,320
of is that you know as amazing as no

2551
01:41:28,320 --> 01:41:30,630
technology like deep learning is you

2552
01:41:30,630 --> 01:41:32,520
know it's sort of very black box and

2553
01:41:32,520 --> 01:41:34,200
that's a big problem in since like you

2554
01:41:34,200 --> 01:41:36,150
know when I have a classifier if I plug

2555
01:41:36,150 --> 01:41:37,650
in the input I will get an output and

2556
01:41:37,650 --> 01:41:39,630
this output will be mostly correct but

2557
01:41:39,630 --> 01:41:42,600
it's very hard for us to make sense why

2558
01:41:42,600 --> 01:41:45,360
this output was made on this input okay

2559
01:41:45,360 --> 01:41:46,740
and that's in kind of there is a whole

2560
01:41:46,740 --> 01:41:48,450
field of machine learning currently

2561
01:41:48,450 --> 01:41:51,150
trying to you know kind of help us in

2562
01:41:51,150 --> 01:41:51,640
the

2563
01:41:51,640 --> 01:41:53,680
this problem in particular one of the

2564
01:41:53,680 --> 01:41:56,410
like most basic ways of trying to make

2565
01:41:56,410 --> 01:41:58,240
sense of what let's say deep learning

2566
01:41:58,240 --> 01:42:00,220
classifier like whether why it made the

2567
01:42:00,220 --> 01:42:02,050
particular decision on a given input is

2568
01:42:02,050 --> 01:42:04,240
something called saliency maps so

2569
01:42:04,240 --> 01:42:05,680
essentially what you do is that whenever

2570
01:42:05,680 --> 01:42:08,530
I see an input what I look at I look at

2571
01:42:08,530 --> 01:42:11,050
each pixel and I see what is there how

2572
01:42:11,050 --> 01:42:14,950
sensitive the prediction is on like to

2573
01:42:14,950 --> 01:42:17,320
changing just this pixel around okay and

2574
01:42:17,320 --> 01:42:18,850
then what I do is I just essentially

2575
01:42:18,850 --> 01:42:21,070
construct this heat map that the color

2576
01:42:21,070 --> 01:42:23,530
of a pixel is like more intense the more

2577
01:42:23,530 --> 01:42:26,110
influential it is and you know that's a

2578
01:42:26,110 --> 01:42:29,020
great idea and now however let me do it

2579
01:42:29,020 --> 01:42:31,540
just to a standard model like right away

2580
01:42:31,540 --> 01:42:33,880
you will get in like pictures like this

2581
01:42:33,880 --> 01:42:36,460
which are so much infirmity but also

2582
01:42:36,460 --> 01:42:38,860
show that kind of this model is not

2583
01:42:38,860 --> 01:42:40,960
exactly doing maybe what you are kind of

2584
01:42:40,960 --> 01:42:42,100
you know thinking like that you would

2585
01:42:42,100 --> 01:42:44,290
expect in particular prediction seems to

2586
01:42:44,290 --> 01:42:45,670
be also influenced by things that are

2587
01:42:45,670 --> 01:42:47,290
just artifacts and not something that

2588
01:42:47,290 --> 01:42:49,480
will be actually you know cut like you

2589
01:42:49,480 --> 01:42:51,700
know that you actually tie to the

2590
01:42:51,700 --> 01:42:53,440
correct prediction and of course know

2591
01:42:53,440 --> 01:42:55,600
there is a whole industry of like very

2592
01:42:55,600 --> 01:42:57,310
interesting tools to make the saliency

2593
01:42:57,310 --> 01:42:59,140
maps even better like take them from

2594
01:42:59,140 --> 01:43:00,670
this kind of serene see maps to make

2595
01:43:00,670 --> 01:43:03,610
them much more you know nice but what we

2596
01:43:03,610 --> 01:43:06,100
discovered is that if you just do the

2597
01:43:06,100 --> 01:43:09,100
same experiment just to robust the

2598
01:43:09,100 --> 01:43:11,260
robust model alone okay so you just like

2599
01:43:11,260 --> 01:43:13,510
impose L infinity perturbation so you

2600
01:43:13,510 --> 01:43:15,580
just say that the image specification

2601
01:43:15,580 --> 01:43:17,860
should not move if I just you know

2602
01:43:17,860 --> 01:43:19,990
change each pixel by just tiny amount

2603
01:43:19,990 --> 01:43:22,600
then suddenly you are getting this kind

2604
01:43:22,600 --> 01:43:24,100
of saliency maps right away like with

2605
01:43:24,100 --> 01:43:25,870
like three lines of code you just get

2606
01:43:25,870 --> 01:43:28,780
this okay and this clearly looks much

2607
01:43:28,780 --> 01:43:31,210
more much more closely to what we as

2608
01:43:31,210 --> 01:43:33,970
humans would expect to happen and this

2609
01:43:33,970 --> 01:43:36,040
actually gets even more fun so in some

2610
01:43:36,040 --> 01:43:38,770
way what robustness does it aligns kind

2611
01:43:38,770 --> 01:43:41,680
of the lost landscape of of the model

2612
01:43:41,680 --> 01:43:44,350
like again more like Alliance it more

2613
01:43:44,350 --> 01:43:46,540
with our semantic kind of understanding

2614
01:43:46,540 --> 01:43:48,520
of the problem so let's do another

2615
01:43:48,520 --> 01:43:51,430
experiment so what I have here is that I

2616
01:43:51,430 --> 01:43:53,470
have here a picture of an ape which

2617
01:43:53,470 --> 01:43:55,870
classifies as a primate in imagenet and

2618
01:43:55,870 --> 01:43:57,910
what I will do now is I will essentially

2619
01:43:57,910 --> 01:44:00,430
find the like I will use I will just

2620
01:44:00,430 --> 01:44:03,560
like follow the gradient to find the

2621
01:44:03,560 --> 01:44:06,020
kind of the closest the closest image

2622
01:44:06,020 --> 01:44:08,660
such that it will make the model

2623
01:44:08,660 --> 01:44:10,760
classify this image as a bird okay so I

2624
01:44:10,760 --> 01:44:12,800
would just morph the this picture into

2625
01:44:12,800 --> 01:44:15,620
the closest image that actually leads to

2626
01:44:15,620 --> 01:44:16,610
classification as a bird

2627
01:44:16,610 --> 01:44:22,460
so let me do that done so yeah the model

2628
01:44:22,460 --> 01:44:23,600
is not robust so we should not be

2629
01:44:23,600 --> 01:44:25,040
surprised there is actually like if you

2630
01:44:25,040 --> 01:44:26,270
look very very closely there is a

2631
01:44:26,270 --> 01:44:28,010
difference but it's very tiny like

2632
01:44:28,010 --> 01:44:29,930
robots the mole is not robust so clearly

2633
01:44:29,930 --> 01:44:31,700
we can manipulate it the way we want it

2634
01:44:31,700 --> 01:44:33,620
in a very like with the noise it's

2635
01:44:33,620 --> 01:44:36,260
completely imperceptible okay but let's

2636
01:44:36,260 --> 01:44:38,630
try to do exactly the same experiment

2637
01:44:38,630 --> 01:44:41,660
but to a robust model okay what will

2638
01:44:41,660 --> 01:44:47,330
happen let's just do this so this is

2639
01:44:47,330 --> 01:44:50,030
what you get so for your reference this

2640
01:44:50,030 --> 01:44:52,130
is not a bird if you look very closely

2641
01:44:52,130 --> 01:44:54,260
you will realize that it's not a bird

2642
01:44:54,260 --> 01:44:57,020
still kind of I think no one will blame

2643
01:44:57,020 --> 01:45:00,260
the model for taking this image for for

2644
01:45:00,260 --> 01:45:02,780
being a being a bird and again all we

2645
01:45:02,780 --> 01:45:04,550
did here you just train a standard model

2646
01:45:04,550 --> 01:45:06,530
and just follow the gradient towards the

2647
01:45:06,530 --> 01:45:09,200
kind of over the laws to make the

2648
01:45:09,200 --> 01:45:10,850
prediction be a bird and we get such a

2649
01:45:10,850 --> 01:45:13,070
very interesting and very semantically

2650
01:45:13,070 --> 01:45:15,380
meaningful pictures you know and of

2651
01:45:15,380 --> 01:45:17,000
course you know being where I am I'm

2652
01:45:17,000 --> 01:45:20,270
sure you clearly can draw the analogy to

2653
01:45:20,270 --> 01:45:22,220
something you have seen because you know

2654
01:45:22,220 --> 01:45:24,020
you have things like that so do you know

2655
01:45:24,020 --> 01:45:25,220
one of the there are generative

2656
01:45:25,220 --> 01:45:26,690
adversarial networks that essentially

2657
01:45:26,690 --> 01:45:28,550
makes a very cool technology but in

2658
01:45:28,550 --> 01:45:30,470
particular what it does it allows you to

2659
01:45:30,470 --> 01:45:31,520
come up with this very interesting

2660
01:45:31,520 --> 01:45:33,350
Clayton embeddings and then you can just

2661
01:45:33,350 --> 01:45:35,000
navigate these little embeddings in kind

2662
01:45:35,000 --> 01:45:37,040
of the pictures you get or something

2663
01:45:37,040 --> 01:45:39,530
like that so in some way this is like

2664
01:45:39,530 --> 01:45:41,390
really cool that we can get similar

2665
01:45:41,390 --> 01:45:44,720
effects with robustness in some way it

2666
01:45:44,720 --> 01:45:46,430
should not be surprising because you

2667
01:45:46,430 --> 01:45:47,720
know in both cases like when we do

2668
01:45:47,720 --> 01:45:49,220
address our training we are solving some

2669
01:45:49,220 --> 01:45:51,260
problem and we are training glands we

2670
01:45:51,260 --> 01:45:53,120
are also showing some problem so you can

2671
01:45:53,120 --> 01:45:54,770
view robustus as some kind of you know

2672
01:45:54,770 --> 01:45:57,170
maybe restricted like the unlike like

2673
01:45:57,170 --> 01:45:58,790
the previous like some restricted ganna

2674
01:45:58,790 --> 01:46:00,680
embedding so there is still something to

2675
01:46:00,680 --> 01:46:03,170
be understood here but clearly we see

2676
01:46:03,170 --> 01:46:04,970
the decade they had nothing to do with

2677
01:46:04,970 --> 01:46:08,090
security nothing to do with safety still

2678
01:46:08,090 --> 01:46:10,270
we got some desirable kind of

2679
01:46:10,270 --> 01:46:12,760
you know desirable property of the

2680
01:46:12,760 --> 01:46:14,890
system just out of making it robust

2681
01:46:14,890 --> 01:46:18,780
okay so let me conclude let me actually

2682
01:46:18,780 --> 01:46:22,930
wrap up and yeah so first of all you

2683
01:46:22,930 --> 01:46:24,610
know let's try to you know so I hope you

2684
01:46:24,610 --> 01:46:26,830
know both Zeke and I gave you like a

2685
01:46:26,830 --> 01:46:28,960
little bit of a view of kind of where we

2686
01:46:28,960 --> 01:46:30,460
are in the vessel robustness and kind of

2687
01:46:30,460 --> 01:46:32,350
how to also get started in particular

2688
01:46:32,350 --> 01:46:34,810
you know the as sick already said the

2689
01:46:34,810 --> 01:46:36,850
you know the website has you know above

2690
01:46:36,850 --> 01:46:39,400
the kind of more expanded version of

2691
01:46:39,400 --> 01:46:41,140
what we talked about it's still working

2692
01:46:41,140 --> 01:46:42,700
progress but it will be there and also

2693
01:46:42,700 --> 01:46:44,890
has a code so you can essentially start

2694
01:46:44,890 --> 01:46:46,239
like playing on these things and see

2695
01:46:46,239 --> 01:46:50,110
where they get and Zico said this is

2696
01:46:50,110 --> 01:46:53,050
very very very much open field like we

2697
01:46:53,050 --> 01:46:55,600
really just scratch the surface so just

2698
01:46:55,600 --> 01:46:57,250
to give you some general direction that

2699
01:46:57,250 --> 01:46:59,530
I think are kind of worth pursuing so

2700
01:46:59,530 --> 01:47:01,840
you know one you know one frontier is

2701
01:47:01,840 --> 01:47:04,000
algorithms essentially you know one

2702
01:47:04,000 --> 01:47:06,100
thing that is very like hampering us is

2703
01:47:06,100 --> 01:47:08,350
just an ability to scale up all the

2704
01:47:08,350 --> 01:47:10,270
technique okay so specially like having

2705
01:47:10,270 --> 01:47:11,590
to do robust training all necessary

2706
01:47:11,590 --> 01:47:14,140
training or doing the verification is

2707
01:47:14,140 --> 01:47:16,060
essentially kind of it's essentially

2708
01:47:16,060 --> 01:47:17,950
like a real difficulty and you really

2709
01:47:17,950 --> 01:47:20,260
have to think about how like what has to

2710
01:47:20,260 --> 01:47:22,239
change to be able to play with larger

2711
01:47:22,239 --> 01:47:24,010
models like mes is not really the right

2712
01:47:24,010 --> 01:47:26,350
you know test case to do anything on

2713
01:47:26,350 --> 01:47:28,540
this also yeah we have to think about

2714
01:47:28,540 --> 01:47:30,969
like how can we make our models smaller

2715
01:47:30,969 --> 01:47:32,890
and maybe more compact we're still being

2716
01:47:32,890 --> 01:47:35,110
robust and actually more importantly how

2717
01:47:35,110 --> 01:47:36,940
to change architecture to kind of maybe

2718
01:47:36,940 --> 01:47:39,400
induce more robustness just in the prior

2719
01:47:39,400 --> 01:47:41,800
okay so maybe we should just I think how

2720
01:47:41,800 --> 01:47:44,320
exactly we even like to put our networks

2721
01:47:44,320 --> 01:47:46,239
together and kind of get some robustness

2722
01:47:46,239 --> 01:47:48,670
just out of this so that's one frontier

2723
01:47:48,670 --> 01:47:50,800
the other one is the fury okay so in a

2724
01:47:50,800 --> 01:47:52,180
sense I think what I always find

2725
01:47:52,180 --> 01:47:54,610
fascinating in a like standard ml was

2726
01:47:54,610 --> 01:47:56,469
that the theory was actually very

2727
01:47:56,469 --> 01:47:58,840
informative of what we happen in

2728
01:47:58,840 --> 01:48:00,250
practice okay so we had this kind of

2729
01:48:00,250 --> 01:48:01,420
theoretical boundary of coursework

2730
01:48:01,420 --> 01:48:04,030
theoretical but still the usually the

2731
01:48:04,030 --> 01:48:06,219
guidelines they gave was quite crisp and

2732
01:48:06,219 --> 01:48:08,560
quite relevant to happens when in

2733
01:48:08,560 --> 01:48:10,719
practice you know once the deputy deep

2734
01:48:10,719 --> 01:48:12,580
learning came in we sort of lost it a

2735
01:48:12,580 --> 01:48:14,290
little bit we are slowly recovering it

2736
01:48:14,290 --> 01:48:15,880
and you know the same is true about

2737
01:48:15,880 --> 01:48:17,260
robustness like I think that robustness

2738
01:48:17,260 --> 01:48:19,900
we still don't really have very good

2739
01:48:19,900 --> 01:48:21,580
data underpinning to even guide our

2740
01:48:21,580 --> 01:48:22,409
intuition

2741
01:48:22,409 --> 01:48:24,929
so in particular we don't really have

2742
01:48:24,929 --> 01:48:27,539
like strong or sufficiently meaningful

2743
01:48:27,539 --> 01:48:30,119
non vacuous like a advert vessel robots

2744
01:48:30,119 --> 01:48:32,190
a generation pounds we know that the

2745
01:48:32,190 --> 01:48:35,039
surgeon organization is different to

2746
01:48:35,039 --> 01:48:36,719
summarization but we kind of don't

2747
01:48:36,719 --> 01:48:38,400
really understand it from the physical

2748
01:48:38,400 --> 01:48:41,309
point of view also as I said maybe like

2749
01:48:41,309 --> 01:48:42,989
we need some theory that will guide the

2750
01:48:42,989 --> 01:48:44,699
choice of the requisition techniques

2751
01:48:44,699 --> 01:48:46,170
because the regression techniques for

2752
01:48:46,170 --> 01:48:48,209
robust classification that we have now

2753
01:48:48,209 --> 01:48:50,690
they do not seem to be effective and

2754
01:48:50,690 --> 01:48:52,709
finally and this is actually I can't

2755
01:48:52,709 --> 01:48:55,110
stress it enough you know there is this

2756
01:48:55,110 --> 01:48:57,329
one more domain is that's the sort of

2757
01:48:57,329 --> 01:48:59,190
the data okay so you know machine

2758
01:48:59,190 --> 01:49:02,340
learning is a field about data and you

2759
01:49:02,340 --> 01:49:03,570
know in order to make sure that our

2760
01:49:03,570 --> 01:49:05,280
tools are really meaningful and they

2761
01:49:05,280 --> 01:49:06,389
actually I brought the applicable

2762
01:49:06,389 --> 01:49:08,369
applicable we really have to work with

2763
01:49:08,369 --> 01:49:10,469
the very set of data sets this is not

2764
01:49:10,469 --> 01:49:12,329
really happening currently in the

2765
01:49:12,329 --> 01:49:15,420
robustness again I'm guilty of this too

2766
01:49:15,420 --> 01:49:18,179
but kind of there is a reason is we are

2767
01:49:18,179 --> 01:49:19,920
so stuck for him for at this moment that

2768
01:49:19,920 --> 01:49:22,199
even we cannot figure out this very few

2769
01:49:22,199 --> 01:49:23,249
data says that we are working with

2770
01:49:23,249 --> 01:49:25,079
currently but you know that this should

2771
01:49:25,079 --> 01:49:27,269
not stop us from trying to explore kind

2772
01:49:27,269 --> 01:49:29,639
of how robustus manifests in many

2773
01:49:29,639 --> 01:49:31,650
different scenarios and also like the

2774
01:49:31,650 --> 01:49:32,909
other extremely important point in

2775
01:49:32,909 --> 01:49:34,709
session like you know we have to think

2776
01:49:34,709 --> 01:49:37,320
very hard about you know what are even

2777
01:49:37,320 --> 01:49:39,479
the right set of perturbations that we

2778
01:49:39,479 --> 01:49:40,619
should because very bracket when they

2779
01:49:40,619 --> 01:49:43,320
say my model should be robust what

2780
01:49:43,320 --> 01:49:44,789
should be like what is the exact that

2781
01:49:44,789 --> 01:49:46,739
kind of class of transformation that

2782
01:49:46,739 --> 01:49:49,170
will be robust to as we said the lp

2783
01:49:49,170 --> 01:49:50,550
transformations we consider they are

2784
01:49:50,550 --> 01:49:52,709
clearly necessary but they are far by

2785
01:49:52,709 --> 01:49:54,510
far not sufficient yes because like

2786
01:49:54,510 --> 01:49:56,130
rotations are not captured by LP

2787
01:49:56,130 --> 01:49:57,090
perturbations

2788
01:49:57,090 --> 01:49:58,380
you know so you have to throw in

2789
01:49:58,380 --> 01:49:59,909
notations you know then you can think of

2790
01:49:59,909 --> 01:50:01,590
local deformation like there is a lot of

2791
01:50:01,590 --> 01:50:03,150
stuff to think about and I think we

2792
01:50:03,150 --> 01:50:04,860
should think about and again this is

2793
01:50:04,860 --> 01:50:06,239
just about vision but there are many

2794
01:50:06,239 --> 01:50:07,949
other contexts but this may be important

2795
01:50:07,949 --> 01:50:09,150
and different perturbations have to be

2796
01:50:09,150 --> 01:50:11,969
considered so so this is like kind of

2797
01:50:11,969 --> 01:50:13,619
this three main goal algorithm and

2798
01:50:13,619 --> 01:50:15,510
Furion data I would like this is where I

2799
01:50:15,510 --> 01:50:17,369
would paint is the current like the you

2800
01:50:17,369 --> 01:50:19,349
know the the key directions that we

2801
01:50:19,349 --> 01:50:21,630
should push you in this field but also

2802
01:50:21,630 --> 01:50:22,979
like this is the point that already you

2803
01:50:22,979 --> 01:50:24,360
know is Iike mention but I really want

2804
01:50:24,360 --> 01:50:26,340
to embrace is that somehow you know

2805
01:50:26,340 --> 01:50:28,019
there is something different about

2806
01:50:28,019 --> 01:50:29,880
robust ml that goes a

2807
01:50:29,880 --> 01:50:32,159
that goes beyond just like it's worship

2808
01:50:32,159 --> 01:50:34,409
different definition somehow this max in

2809
01:50:34,409 --> 01:50:36,510
this definition should really really

2810
01:50:36,510 --> 01:50:38,760
make us also think differently about the

2811
01:50:38,760 --> 01:50:40,199
way we approach the problems like we

2812
01:50:40,199 --> 01:50:42,570
should go from the kind of average and

2813
01:50:42,570 --> 01:50:44,310
best-case scenario evaluations to the

2814
01:50:44,310 --> 01:50:46,260
kind of worst-case mindsets okay so we

2815
01:50:46,260 --> 01:50:47,909
really should be in particular you know

2816
01:50:47,909 --> 01:50:49,860
when we evaluate the quality of our

2817
01:50:49,860 --> 01:50:51,810
model we really should always kind of

2818
01:50:51,810 --> 01:50:53,909
think back it's always a game so yes I

2819
01:50:53,909 --> 01:50:56,159
want my mother succeed but in order to

2820
01:50:56,159 --> 01:50:58,080
make sure that I succeed I have to put

2821
01:50:58,080 --> 01:50:59,850
my heart of someone wants to break it

2822
01:50:59,850 --> 01:51:01,650
someone who kind of knows exactly how it

2823
01:51:01,650 --> 01:51:02,010
works

2824
01:51:02,010 --> 01:51:03,870
knows all the potential weak spots and

2825
01:51:03,870 --> 01:51:06,330
just tries hard to exploit it so this is

2826
01:51:06,330 --> 01:51:07,620
again something that happens naturally

2827
01:51:07,620 --> 01:51:09,989
in you know insecurities because that's

2828
01:51:09,989 --> 01:51:11,820
the field they work in it didn't happen

2829
01:51:11,820 --> 01:51:13,170
in machine learning you know because

2830
01:51:13,170 --> 01:51:14,790
there we didn't have to worry about a

2831
01:51:14,790 --> 01:51:16,800
facade yet now we have to and now we

2832
01:51:16,800 --> 01:51:18,510
really have to learn from security

2833
01:51:18,510 --> 01:51:20,760
community you know how to do it you know

2834
01:51:20,760 --> 01:51:23,010
I guess Nicholas Kelly is one of the

2835
01:51:23,010 --> 01:51:24,630
great you know propagators of this kind

2836
01:51:24,630 --> 01:51:26,550
of idea so if you think your model is

2837
01:51:26,550 --> 01:51:28,679
great talk to him and you know then we

2838
01:51:28,679 --> 01:51:30,300
can even talk after right so it's

2839
01:51:30,300 --> 01:51:31,980
essentially he's great at that there are

2840
01:51:31,980 --> 01:51:33,360
some useful resources here there's a

2841
01:51:33,360 --> 01:51:35,340
clever hands which kind of gives you a

2842
01:51:35,340 --> 01:51:36,989
library of attack you should treat it as

2843
01:51:36,989 --> 01:51:39,210
a point of start not the end of your

2844
01:51:39,210 --> 01:51:40,980
evaluation this is just like this we'll

2845
01:51:40,980 --> 01:51:43,199
just see you're thinking of how could I

2846
01:51:43,199 --> 01:51:44,730
go about breaking the models there's

2847
01:51:44,730 --> 01:51:46,949
also a little sense robust ml that tries

2848
01:51:46,949 --> 01:51:49,050
kind of to keep track of like s people

2849
01:51:49,050 --> 01:51:50,969
proposed models you know which photos

2850
01:51:50,969 --> 01:51:53,130
actually were already well well proven

2851
01:51:53,130 --> 01:51:54,449
to be robust or it is there is some

2852
01:51:54,449 --> 01:51:56,070
evidence that are robust and which which

2853
01:51:56,070 --> 01:51:57,900
models were already circumvented so you

2854
01:51:57,900 --> 01:51:59,159
essentially can navigate the field

2855
01:51:59,159 --> 01:52:00,719
better but in general like it's really

2856
01:52:00,719 --> 01:52:02,040
about the mindset I think we really have

2857
01:52:02,040 --> 01:52:03,929
to be much more skeptical and much more

2858
01:52:03,929 --> 01:52:05,719
careful in this field that we usually

2859
01:52:05,719 --> 01:52:08,699
because essentially failure here is kind

2860
01:52:08,699 --> 01:52:10,020
of quite catastrophic and we just can't

2861
01:52:10,020 --> 01:52:12,300
afford that okay so this is about

2862
01:52:12,300 --> 01:52:14,580
Avatara Buster's but again more broadly

2863
01:52:14,580 --> 01:52:17,520
I really want to kind of confirm that

2864
01:52:17,520 --> 01:52:20,010
this is like in some ways I think

2865
01:52:20,010 --> 01:52:21,750
machine learning clearly succeed as a

2866
01:52:21,750 --> 01:52:24,210
succeed as a proof of concept but now

2867
01:52:24,210 --> 01:52:26,880
it's time to go the next lab to do to

2868
01:52:26,880 --> 01:52:28,590
the next mile students is like make it

2869
01:52:28,590 --> 01:52:30,989
truly reliable and kind of something

2870
01:52:30,989 --> 01:52:33,090
that you can be kind of confident that

2871
01:52:33,090 --> 01:52:34,409
you can deploy in the real world life

2872
01:52:34,409 --> 01:52:36,139
and essentially it will work there and

2873
01:52:36,139 --> 01:52:38,850
in particular I really really believe

2874
01:52:38,850 --> 01:52:41,429
that thinking about robustness is not

2875
01:52:41,429 --> 01:52:42,230
only

2876
01:52:42,230 --> 01:52:43,880
and the bring us ml that is better

2877
01:52:43,880 --> 01:52:45,950
suited for like securities in safety

2878
01:52:45,950 --> 01:52:48,020
aspect I think it also will be better in

2879
01:52:48,020 --> 01:52:50,480
other regards as well I just mentioned

2880
01:52:50,480 --> 01:52:52,610
this kind of interpretability angle I

2881
01:52:52,610 --> 01:52:54,080
think actually like in the context of

2882
01:52:54,080 --> 01:52:55,550
fairness and other things I think it

2883
01:52:55,550 --> 01:52:57,710
also be a very useful tool again I don't

2884
01:52:57,710 --> 01:53:00,380
know yet exactly how it will be useful I

2885
01:53:00,380 --> 01:53:01,910
think this is like the exciting question

2886
01:53:01,910 --> 01:53:04,040
but I am pretty sure that it will be

2887
01:53:04,040 --> 01:53:07,580
okay so this is all I have to say I just

2888
01:53:07,580 --> 01:53:09,680
want to again just emphasize you know if

2889
01:53:09,680 --> 01:53:11,570
you got interested in this field there

2890
01:53:11,570 --> 01:53:13,880
is you know plenty of you know plenty of

2891
01:53:13,880 --> 01:53:15,380
stuff to do there is just like you know

2892
01:53:15,380 --> 01:53:16,850
the kind of the cross-section of things

2893
01:53:16,850 --> 01:53:19,010
to do is just overwhelming to be honest

2894
01:53:19,010 --> 01:53:21,110
so yeah if you want to get started we

2895
01:53:21,110 --> 01:53:23,330
have the notes and code over here you

2896
01:53:23,330 --> 01:53:24,470
know that you can just start playing

2897
01:53:24,470 --> 01:53:26,330
quiff we will be adding to it over and

2898
01:53:26,330 --> 01:53:29,000
over also like some of these issues that

2899
01:53:29,000 --> 01:53:31,100
we discover like there are blog posts on

2900
01:53:31,100 --> 01:53:33,230
my on my group website that kind of go a

2901
01:53:33,230 --> 01:53:34,940
little bit in depth they may also help

2902
01:53:34,940 --> 01:53:37,790
you to get started and yeah overall you

2903
01:53:37,790 --> 01:53:38,900
know I just want to say you know it's

2904
01:53:38,900 --> 01:53:40,910
really fun field I think it's really a

2905
01:53:40,910 --> 01:53:43,010
critical field that we know we need an

2906
01:53:43,010 --> 01:53:45,560
influx ideas and work so I hope you know

2907
01:53:45,560 --> 01:53:47,570
some of you will decide to join in I

2908
01:53:47,570 --> 01:53:48,500
think it will be fun

2909
01:53:48,500 --> 01:53:49,850
Thanks

2910
01:53:49,850 --> 01:53:59,549
[Applause]

2911
01:54:02,870 --> 01:54:06,840
can you hear me thank you we have time

2912
01:54:06,840 --> 01:54:09,330
for some questions if you're leaving now

2913
01:54:09,330 --> 01:54:11,970
then please leave quietly and if you

2914
01:54:11,970 --> 01:54:15,030
have questions to ask there is maybe

2915
01:54:15,030 --> 01:54:17,700
just one microphone right here if you

2916
01:54:17,700 --> 01:54:19,650
could come and join the line go ahead

2917
01:54:19,650 --> 01:54:20,430
please

2918
01:54:20,430 --> 01:54:22,860
so to summarize what you've done so far

2919
01:54:22,860 --> 01:54:25,980
you find the adversary I put a max

2920
01:54:25,980 --> 01:54:28,080
around the loss and then you they learn

2921
01:54:28,080 --> 01:54:30,060
to put them in around the max to combat

2922
01:54:30,060 --> 01:54:33,060
that so let me ask the logical question

2923
01:54:33,060 --> 01:54:35,700
which is what if I put a max around your

2924
01:54:35,700 --> 01:54:37,530
mint and then you put another minute on

2925
01:54:37,530 --> 01:54:45,450
my max and so on so I think watch the

2926
01:54:45,450 --> 01:54:47,580
more steps more moves in the game if you

2927
01:54:47,580 --> 01:54:50,100
will in some ways I think there is

2928
01:54:50,100 --> 01:54:52,590
really just like one mean one minion one

2929
01:54:52,590 --> 01:54:53,760
max because there's just like two

2930
01:54:53,760 --> 01:54:55,680
players adversary and us but you are

2931
01:54:55,680 --> 01:54:57,150
right that in some way this will always

2932
01:54:57,150 --> 01:54:59,340
be a cat-and-mouse game essentially like

2933
01:54:59,340 --> 01:55:02,220
whenever I kind of get an idea how to

2934
01:55:02,220 --> 01:55:04,050
kind of toward any of the perturb like

2935
01:55:04,050 --> 01:55:06,150
any of the attacks in one model there's

2936
01:55:06,150 --> 01:55:08,040
always a new kind of you know an

2937
01:55:08,040 --> 01:55:10,110
extension of the model that kind of all

2938
01:55:10,110 --> 01:55:12,120
now we are not secure again and this way

2939
01:55:12,120 --> 01:55:14,130
we'll never have ultimate robustus but

2940
01:55:14,130 --> 01:55:15,810
this is what we also have in the

2941
01:55:15,810 --> 01:55:17,130
classical setting it's like we never

2942
01:55:17,130 --> 01:55:19,320
have perfect security we just only try

2943
01:55:19,320 --> 01:55:20,760
to reason about where we can get

2944
01:55:20,760 --> 01:55:22,830
security at this moment what we are

2945
01:55:22,830 --> 01:55:24,510
missing and kind of at least you know

2946
01:55:24,510 --> 01:55:26,520
yeah it will never be perfect but we

2947
01:55:26,520 --> 01:55:27,990
just need to do better than we do now

2948
01:55:27,990 --> 01:55:30,060
and hopefully in time we will get there

2949
01:55:30,060 --> 01:55:31,860
but yes there is no kind of you know

2950
01:55:31,860 --> 01:55:33,990
solve it all solution administer now

2951
01:55:33,990 --> 01:55:35,940
it's everything is secure period and

2952
01:55:35,940 --> 01:55:37,080
there is nothing else to be done that

2953
01:55:37,080 --> 01:55:38,580
smoothly always an open question but

2954
01:55:38,580 --> 01:55:40,680
currently is actually quite quite bad so

2955
01:55:40,680 --> 01:55:42,000
one point I made there's I think the

2956
01:55:42,000 --> 01:55:44,130
next regime for this by the way is the

2957
01:55:44,130 --> 01:55:46,860
max over big Delta and that would be

2958
01:55:46,860 --> 01:55:48,900
generalization over different classes of

2959
01:55:48,900 --> 01:55:50,850
attacks which i think is the next

2960
01:55:50,850 --> 01:55:52,050
natural extension of this which would be

2961
01:55:52,050 --> 01:55:53,010
really interesting to consider which

2962
01:55:53,010 --> 01:55:54,690
really hasn't been considered very much

2963
01:55:54,690 --> 01:55:58,249
and for example being robust to multiple

2964
01:55:58,249 --> 01:56:01,489
you know distance measures yeah value

2965
01:56:01,489 --> 01:56:03,260
can do to the intersection or the Union

2966
01:56:03,260 --> 01:56:04,369
even you can do that one

2967
01:56:04,369 --> 01:56:06,559
but to actually generalize and have an

2968
01:56:06,559 --> 01:56:08,090
adversary that can adapt their

2969
01:56:08,090 --> 01:56:10,909
perturbation region is a really open

2970
01:56:10,909 --> 01:56:12,229
question that has not been addressed as

2971
01:56:12,229 --> 01:56:19,579
far as I'm aware yeah great now it seems

2972
01:56:19,579 --> 01:56:25,030
to me that in the interpersonal tweeny

2973
01:56:25,030 --> 01:56:28,340
it is assumed that there is no intrinsic

2974
01:56:28,340 --> 01:56:31,880
error in the computation of the of the

2975
01:56:31,880 --> 01:56:35,329
tweeny but people have been trying hard

2976
01:56:35,329 --> 01:56:39,739
to apply a big computing like reducing

2977
01:56:39,739 --> 01:56:42,889
the precision so that when you compute

2978
01:56:42,889 --> 01:56:45,369
their hope is some quantization error

2979
01:56:45,369 --> 01:56:48,920
could you comment on how this robust

2980
01:56:48,920 --> 01:56:50,989
machine learning would work with a

2981
01:56:50,989 --> 01:56:54,650
prospect computing like in the end it's

2982
01:56:54,650 --> 01:56:56,869
like in some ways these two things are

2983
01:56:56,869 --> 01:56:58,880
orthogonal because as you as Eko

2984
01:56:58,880 --> 01:57:01,130
explained in the end when we want to get

2985
01:57:01,130 --> 01:57:02,659
robust models we just have to solve

2986
01:57:02,659 --> 01:57:05,090
certain optimization problem and yeah so

2987
01:57:05,090 --> 01:57:06,530
whatever we understand about solving

2988
01:57:06,530 --> 01:57:08,150
optimization problems in the reduced

2989
01:57:08,150 --> 01:57:09,949
precision thing if we transfer over so I

2990
01:57:09,949 --> 01:57:11,929
don't think it's an important issue and

2991
01:57:11,929 --> 01:57:13,880
we have to think about it but I think we

2992
01:57:13,880 --> 01:57:15,860
stuff have a very clean interface in

2993
01:57:15,860 --> 01:57:17,630
which to understand in what will be the

2994
01:57:17,630 --> 01:57:19,190
impact of limited computation of the

2995
01:57:19,190 --> 01:57:20,869
scissors like how well can we solve this

2996
01:57:20,869 --> 01:57:21,860
optimization problem of limited

2997
01:57:21,860 --> 01:57:23,179
precision if you can do it well

2998
01:57:23,179 --> 01:57:24,860
everything will work if you don't do it

2999
01:57:24,860 --> 01:57:28,090
well it will not

3000
01:57:30,440 --> 01:57:33,660
hi so I think you talked about one

3001
01:57:33,660 --> 01:57:36,840
approach being to find the exact worst

3002
01:57:36,840 --> 01:57:40,980
case adversarial example and you talked

3003
01:57:40,980 --> 01:57:42,920
about using kind of off-the-shelf

3004
01:57:42,920 --> 01:57:46,080
binary optimizer sexy place but I mean I

3005
01:57:46,080 --> 01:57:49,280
was wondering like the zero one binary

3006
01:57:49,280 --> 01:57:50,760
programming problems are actually

3007
01:57:50,760 --> 01:57:51,750
np-hard

3008
01:57:51,750 --> 01:57:53,910
and when you look at what optimizes like

3009
01:57:53,910 --> 01:57:55,470
SEAPLEX do they're actually finding

3010
01:57:55,470 --> 01:57:57,000
approximate solutions like they're

3011
01:57:57,000 --> 01:57:58,650
basically the branch and bound they take

3012
01:57:58,650 --> 01:58:00,990
like a linear relaxation and find an

3013
01:58:00,990 --> 01:58:04,250
approximate solution and then you know

3014
01:58:04,250 --> 01:58:11,190
find another relaxation so you know in

3015
01:58:11,190 --> 01:58:13,110
terms of finding provably hard bounds

3016
01:58:13,110 --> 01:58:14,490
have you actually been able to do this

3017
01:58:14,490 --> 01:58:18,420
successfully based on provably optimal

3018
01:58:18,420 --> 01:58:20,400
solutions to the combinatorial problem

3019
01:58:20,400 --> 01:58:22,860
horror right so so the examples I gave

3020
01:58:22,860 --> 01:58:24,960
there were for the networks and the

3021
01:58:24,960 --> 01:58:26,790
examples in the notebook those are all

3022
01:58:26,790 --> 01:58:28,380
solving branch and bound to optimality

3023
01:58:28,380 --> 01:58:30,360
so you get you you get the full you

3024
01:58:30,360 --> 01:58:32,430
expand the tree entirely to the point

3025
01:58:32,430 --> 01:58:33,630
where you actually are guaranteed off

3026
01:58:33,630 --> 01:58:34,980
melody your upper and lower bounds match

3027
01:58:34,980 --> 01:58:37,740
now of course you can also use stop

3028
01:58:37,740 --> 01:58:40,050
those solvers early in some sense to get

3029
01:58:40,050 --> 01:58:42,600
upper and lower bounds which are not not

3030
01:58:42,600 --> 01:58:46,260
do not meet and actually the combination

3031
01:58:46,260 --> 01:58:48,090
I describe there are exactly some are

3032
01:58:48,090 --> 01:58:51,260
like the first few relaxations that

3033
01:58:51,260 --> 01:58:54,330
method like C flex or Grove it would

3034
01:58:54,330 --> 01:58:56,910
solve so so and I should say there's

3035
01:58:56,910 --> 01:58:58,860
also work and I think there's been more

3036
01:58:58,860 --> 01:59:00,360
some work from from the University of

3037
01:59:00,360 --> 01:59:02,180
Oxford and folks of deep mind on

3038
01:59:02,180 --> 01:59:04,230
adapting like better branch and bound

3039
01:59:04,230 --> 01:59:07,020
search methods for this problem as I

3040
01:59:07,020 --> 01:59:08,370
think is my extra might even be a paper

3041
01:59:08,370 --> 01:59:12,150
here at nips on that so yes it's a lot

3042
01:59:12,150 --> 01:59:13,860
of work in this there's also a lot of

3043
01:59:13,860 --> 01:59:15,480
work on the SMT solver community about

3044
01:59:15,480 --> 01:59:17,520
specialized solvers for these things and

3045
01:59:17,520 --> 01:59:19,680
again compute upper and lower bounds but

3046
01:59:19,680 --> 01:59:22,320
to be clear the the solutions that we

3047
01:59:22,320 --> 01:59:24,780
have in our tutorial are all the cases

3048
01:59:24,780 --> 01:59:26,250
where you solve the branch amount to

3049
01:59:26,250 --> 01:59:27,330
completion where the upper and lower

3050
01:59:27,330 --> 01:59:32,070
bounds match okay and so how many

3051
01:59:32,070 --> 01:59:33,260
dimensions have been able to

3052
01:59:33,260 --> 01:59:35,510
just so they're so for those ones that

3053
01:59:35,510 --> 01:59:38,090
works exactly there's about 80 hidden

3054
01:59:38,090 --> 01:59:39,530
units and those which is the number of

3055
01:59:39,530 --> 01:59:46,730
binary variables what are your thoughts

3056
01:59:46,730 --> 01:59:50,059
on ideas like input free adversarial

3057
01:59:50,059 --> 01:59:52,789
examples so what do you like meaning

3058
01:59:52,789 --> 01:59:54,710
that you don't like what does the input

3059
01:59:54,710 --> 01:59:56,360
free mean exactly like you don't look at

3060
01:59:56,360 --> 01:59:57,920
the input data when you're constructing

3061
01:59:57,920 --> 02:00:01,039
your adversary oh I see oh yes so this

3062
02:00:01,039 --> 02:00:02,269
is a very interesting question actually

3063
02:00:02,269 --> 02:00:04,760
not in some ways yes so not much I think

3064
02:00:04,760 --> 02:00:06,199
is extremely interesting because in some

3065
02:00:06,199 --> 02:00:08,650
ways you would like to kind of make your

3066
02:00:08,650 --> 02:00:11,059
perturbation to be as kind of oblivious

3067
02:00:11,059 --> 02:00:12,739
to where they will be applied as as

3068
02:00:12,739 --> 02:00:15,650
possible so I don't think so then I like

3069
02:00:15,650 --> 02:00:17,000
is they work on if I said perturbation

3070
02:00:17,000 --> 02:00:18,230
which is not exactly what you are saying

3071
02:00:18,230 --> 02:00:20,300
what you are talking about but like it's

3072
02:00:20,300 --> 02:00:21,500
close its to look at the data but you

3073
02:00:21,500 --> 02:00:22,369
would like to come up with one

3074
02:00:22,369 --> 02:00:24,409
perturbation that works for most of the

3075
02:00:24,409 --> 02:00:27,199
images so honestly like input three

3076
02:00:27,199 --> 02:00:31,820
perturbations I think ok so ideally if

3077
02:00:31,820 --> 02:00:34,039
we do well unless there is some actual

3078
02:00:34,039 --> 02:00:36,139
hidden bias in our machine learning

3079
02:00:36,139 --> 02:00:38,150
models that we kind of is unwelcome they

3080
02:00:38,150 --> 02:00:39,139
will be kind of semantically meaningful

3081
02:00:39,139 --> 02:00:41,000
like there should be just something that

3082
02:00:41,000 --> 02:00:43,610
also should you know confuse humans so

3083
02:00:43,610 --> 02:00:45,199
it will be very interesting to study if

3084
02:00:45,199 --> 02:00:46,940
that's the case or not so I think that's

3085
02:00:46,940 --> 02:00:48,829
interesting you know again we are still

3086
02:00:48,829 --> 02:00:49,849
starting I think it's a very interesting

3087
02:00:49,849 --> 02:00:51,860
topic it just I don't really know much

3088
02:00:51,860 --> 02:00:54,199
about yes so so in the case of universal

3089
02:00:54,199 --> 02:00:55,760
perturbations in particular which are

3090
02:00:55,760 --> 02:00:57,619
sort of getting at what you want there's

3091
02:00:57,619 --> 02:00:58,670
actually very nice optimization

3092
02:00:58,670 --> 02:00:59,900
formulation of that too because you can

3093
02:00:59,900 --> 02:01:01,519
just think about the max of a delta

3094
02:01:01,519 --> 02:01:03,320
being outside the sum now or outside

3095
02:01:03,320 --> 02:01:04,849
expectation so it's still a min max

3096
02:01:04,849 --> 02:01:07,250
problem but the max is outside the

3097
02:01:07,250 --> 02:01:08,780
expectation is it up inside of it so

3098
02:01:08,780 --> 02:01:11,869
it's independent of X or it has to be

3099
02:01:11,869 --> 02:01:13,340
universal over all X so that's one of

3100
02:01:13,340 --> 02:01:14,780
the universal perturbation formulation

3101
02:01:14,780 --> 02:01:17,119
which has been studied what's been less

3102
02:01:17,119 --> 02:01:20,510
studied though is models that are robust

3103
02:01:20,510 --> 02:01:22,309
averse to universal perturbations which

3104
02:01:22,309 --> 02:01:23,869
should be an easier problem to handle

3105
02:01:23,869 --> 02:01:25,670
but there hasn't been much work on like

3106
02:01:25,670 --> 02:01:27,559
robust optimization against Universal

3107
02:01:27,559 --> 02:01:28,760
perturbation so yeah that's really a

3108
02:01:28,760 --> 02:01:33,619
really cool topic thank you oh thank you

3109
02:01:33,619 --> 02:01:35,809
for your excellent tutorial the website

3110
02:01:35,809 --> 02:01:37,130
isn't especially a nice touch

3111
02:01:37,130 --> 02:01:40,280
I was wondering so you claim robustness

3112
02:01:40,280 --> 02:01:43,119
at a particular bound

3113
02:01:43,119 --> 02:01:45,010
a particular epsilon but presumably if

3114
02:01:45,010 --> 02:01:47,139
you were to increase the epsilon you

3115
02:01:47,139 --> 02:01:49,209
would then increase there right and so

3116
02:01:49,209 --> 02:01:51,459
on and so forth how has there been any

3117
02:01:51,459 --> 02:01:52,719
research done for example on the

3118
02:01:52,719 --> 02:01:54,669
detection of external attacks such that

3119
02:01:54,669 --> 02:01:56,409
even if you were to increase the epsilon

3120
02:01:56,409 --> 02:01:57,699
further and further you still realize

3121
02:01:57,699 --> 02:01:59,739
that your input is now adversarial and

3122
02:01:59,739 --> 02:02:01,179
if you just like rejected or something

3123
02:02:01,179 --> 02:02:05,559
or do something sensible yeah so we do

3124
02:02:05,559 --> 02:02:07,239
have some work that's just sort of in

3125
02:02:07,239 --> 02:02:08,679
progress but I can talk about it I'm not

3126
02:02:08,679 --> 02:02:09,760
sure if we can do with it

3127
02:02:09,760 --> 02:02:11,079
I've just sort of simple like binary

3128
02:02:11,079 --> 02:02:13,360
search on an example dependent epsilon

3129
02:02:13,360 --> 02:02:14,530
boundaries that you can increase or

3130
02:02:14,530 --> 02:02:17,379
decrease to sort of get each make each

3131
02:02:17,379 --> 02:02:20,379
example as secure as sort of as secure

3132
02:02:20,379 --> 02:02:23,199
as it can be in some sense not just by

3133
02:02:23,199 --> 02:02:24,369
getting what you want but you have more

3134
02:02:24,369 --> 02:02:26,169
to say you know it was an interesting

3135
02:02:26,169 --> 02:02:28,389
question like I'm always a little bit

3136
02:02:28,389 --> 02:02:30,550
skeptical of detecting Anderson examples

3137
02:02:30,550 --> 02:02:33,249
because it's very unclear like if you

3138
02:02:33,249 --> 02:02:34,989
make a data-driven if you make it like

3139
02:02:34,989 --> 02:02:36,879
machine learning algorithms supposed to

3140
02:02:36,879 --> 02:02:38,530
the text stuff then what you can do can

3141
02:02:38,530 --> 02:02:40,570
construct a visual examples against this

3142
02:02:40,570 --> 02:02:42,249
detector so this is like the most

3143
02:02:42,249 --> 02:02:44,499
promising way to do it is definitely not

3144
02:02:44,499 --> 02:02:46,629
I don't think it works doesn't mean it

3145
02:02:46,629 --> 02:02:47,709
can't be done but I think the

3146
02:02:47,709 --> 02:02:49,030
interesting meta question over here is

3147
02:02:49,030 --> 02:02:50,800
exactly like how much our metals over

3148
02:02:50,800 --> 02:02:53,409
fit to the choice of Epsilon okay so we

3149
02:02:53,409 --> 02:02:54,519
definitely did this kind of studies

3150
02:02:54,519 --> 02:02:56,619
where we kind of know to care model that

3151
02:02:56,619 --> 02:02:58,510
we know that is you know that is like

3152
02:02:58,510 --> 02:03:01,929
designed to be robust to 0.1 Epsilon and

3153
02:03:01,929 --> 02:03:03,579
like how does it fire when you go beyond

3154
02:03:03,579 --> 02:03:05,649
it and of course it gets broken it's

3155
02:03:05,649 --> 02:03:07,749
depending on the data on the data set it

3156
02:03:07,749 --> 02:03:09,249
sometimes it's really like it's over

3157
02:03:09,249 --> 02:03:10,809
feats and sometimes just like relatively

3158
02:03:10,809 --> 02:03:12,760
gracefully goes back and sometimes you

3159
02:03:12,760 --> 02:03:14,289
actually wants to kind of a pack it in a

3160
02:03:14,289 --> 02:03:16,149
stronger manner to just get as you know

3161
02:03:16,149 --> 02:03:18,070
as security against smaller Epsilon

3162
02:03:18,070 --> 02:03:19,570
so there I think there's we don't unsend

3163
02:03:19,570 --> 02:03:20,889
it well there's clear stuff interesting

3164
02:03:20,889 --> 02:03:23,289
happening but yet we don't okay thank

3165
02:03:23,289 --> 02:03:26,769
you I thanks again for the talk to his

3166
02:03:26,769 --> 02:03:27,969
excellent

3167
02:03:27,969 --> 02:03:30,459
so only one or two more questions after

3168
02:03:30,459 --> 02:03:32,349
my question actually is actually about

3169
02:03:32,349 --> 02:03:34,780
the nature of the threat models that the

3170
02:03:34,780 --> 02:03:36,099
attackers and the inputs they'll choose

3171
02:03:36,099 --> 02:03:36,640
and that

3172
02:03:36,640 --> 02:03:37,840
I think in most threat models you're

3173
02:03:37,840 --> 02:03:39,220
gonna find the attacker is free to

3174
02:03:39,220 --> 02:03:41,050
choose inputs to perturb that are

3175
02:03:41,050 --> 02:03:43,180
outside the distribution of the task in

3176
02:03:43,180 --> 02:03:45,130
hand you gave a really good example of

3177
02:03:45,130 --> 02:03:47,470
carlini's work where he used music to

3178
02:03:47,470 --> 02:03:49,750
attack speech to text yeah and I think

3179
02:03:49,750 --> 02:03:51,280
most of the fab models I've looked at

3180
02:03:51,280 --> 02:03:52,390
this is perfectly available to the

3181
02:03:52,390 --> 02:03:54,340
attacker and I'm curious if you see any

3182
02:03:54,340 --> 02:03:56,320
path forward for adversarial training

3183
02:03:56,320 --> 02:03:58,330
and verification when you have to

3184
02:03:58,330 --> 02:04:00,070
actually deal with whole family of

3185
02:04:00,070 --> 02:04:01,630
distributions that lie outside your task

3186
02:04:01,630 --> 02:04:04,630
yeah yes so I can definitely see it I

3187
02:04:04,630 --> 02:04:08,620
don't know how to do it yeah I agree

3188
02:04:08,620 --> 02:04:11,440
yeah it's an actual problem if you if

3189
02:04:11,440 --> 02:04:13,360
you say when a so what prevents noise

3190
02:04:13,360 --> 02:04:15,250
from being classified or something yeah

3191
02:04:15,250 --> 02:04:16,270
well because it could be any noise

3192
02:04:16,270 --> 02:04:18,040
anywhere right that's a great adversity

3193
02:04:18,040 --> 02:04:20,440
or graffiti we don't know yet yes so so

3194
02:04:20,440 --> 02:04:21,910
I expect that this would be like a big

3195
02:04:21,910 --> 02:04:23,560
problem like I have some thoughts how to

3196
02:04:23,560 --> 02:04:25,780
go like I think essentially part of the

3197
02:04:25,780 --> 02:04:26,800
problem might be that we are asking for

3198
02:04:26,800 --> 02:04:28,750
too much I think we should not be able

3199
02:04:28,750 --> 02:04:31,600
to robusta classified noise as well but

3200
02:04:31,600 --> 02:04:33,550
yet currently if you start with complete

3201
02:04:33,550 --> 02:04:35,290
noise you will you know you will destroy

3202
02:04:35,290 --> 02:04:36,400
this models there is no question about

3203
02:04:36,400 --> 02:04:38,530
that okay yeah to your point about

3204
02:04:38,530 --> 02:04:40,120
asking too much I think this is where

3205
02:04:40,120 --> 02:04:41,500
the engagement is security community

3206
02:04:41,500 --> 02:04:42,730
which is where I actually come from has

3207
02:04:42,730 --> 02:04:44,710
to really start in earnest exactly as it

3208
02:04:44,710 --> 02:04:46,630
would be the threat models not the ml

3209
02:04:46,630 --> 02:04:48,310
but define what's too much so to speak

3210
02:04:48,310 --> 02:04:50,350
yeah I'm happy to chop that's exactly

3211
02:04:50,350 --> 02:04:50,830
what I think

3212
02:04:50,830 --> 02:04:53,460
thank you

3213
02:04:58,809 --> 02:05:04,280
please okay so most of the tutorial

3214
02:05:04,280 --> 02:05:09,800
about image object detection and I just

3215
02:05:09,800 --> 02:05:12,260
wonder if there's any results on natural

3216
02:05:12,260 --> 02:05:16,210
language processing like question answer

3217
02:05:16,210 --> 02:05:18,710
yes there is I I know personally and

3218
02:05:18,710 --> 02:05:20,960
Daniel that would have some each and

3219
02:05:20,960 --> 02:05:21,920
apparently have some work on that

3220
02:05:21,920 --> 02:05:24,019
Nicolas Carlini right sitting by by you

3221
02:05:24,019 --> 02:05:27,050
has some work on on speech as mention in

3222
02:05:27,050 --> 02:05:29,749
the previous one so yes language is

3223
02:05:29,749 --> 02:05:30,979
different in particular because it's

3224
02:05:30,979 --> 02:05:32,479
much more discrete than images so you

3225
02:05:32,479 --> 02:05:34,039
change a whole word you don't just put

3226
02:05:34,039 --> 02:05:37,610
her word a little bit but it's a it's a

3227
02:05:37,610 --> 02:05:40,429
there is work on that and it's not as

3228
02:05:40,429 --> 02:05:41,510
well developed but there's only is some

3229
02:05:41,510 --> 02:05:42,829
work on that feels like it the latest

3230
02:05:42,829 --> 02:05:43,280
you know

3231
02:05:43,280 --> 02:05:45,110
EEM NLP there were several papers on

3232
02:05:45,110 --> 02:05:47,389
this yes so in the central NLP is that

3233
02:05:47,389 --> 02:05:49,550
like we still are not there yet with

3234
02:05:49,550 --> 02:05:51,199
average case performance so it's no it's

3235
02:05:51,199 --> 02:05:52,400
obviously doesn't work when you do

3236
02:05:52,400 --> 02:05:53,869
adversarial stuff but there is work

3237
02:05:53,869 --> 02:05:55,789
again pearson yang I think is a one of

3238
02:05:55,789 --> 02:05:57,739
the lead person of that so yeah but yeah

3239
02:05:57,739 --> 02:05:59,179
it should apply to any domain in just a

3240
02:05:59,179 --> 02:06:04,219
matter of time at the end of the

3241
02:06:04,219 --> 02:06:04,909
tutorial

3242
02:06:04,909 --> 02:06:07,190
let's thanks Nico and Alex again thank

3243
02:06:07,190 --> 02:06:08,790
you very much for a great tutorial

3244
02:06:08,790 --> 02:06:12,489
[Applause]

