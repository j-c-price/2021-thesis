1
00:00:03,220 --> 00:00:07,140
[Music]

2
00:00:05,260 --> 00:00:07,140
 

3
00:00:05,270 --> 00:00:09,059
 yeah I think next a lot for the

4
00:00:07,130 --> 00:00:09,059
 

5
00:00:07,140 --> 00:00:10,710
 invitation I apologize in advance I've

6
00:00:09,049 --> 00:00:10,710
 

7
00:00:09,059 --> 00:00:12,059
 had this sort of lingered lingering

8
00:00:10,700 --> 00:00:12,059
 

9
00:00:10,710 --> 00:00:15,269
 respiratory thing so I might have

10
00:00:12,049 --> 00:00:15,269
 

11
00:00:12,059 --> 00:00:18,119
 coughing fits throughout so to entertain

12
00:00:15,259 --> 00:00:18,119
 

13
00:00:15,269 --> 00:00:20,369
 you so so please come up at as it goes

14
00:00:18,109 --> 00:00:20,369
 

15
00:00:18,119 --> 00:00:22,170
 and ask questions that will kind of

16
00:00:20,359 --> 00:00:22,170
 

17
00:00:20,369 --> 00:00:24,060
 improve thanks for everything everyone I

18
00:00:22,160 --> 00:00:24,060
 

19
00:00:22,170 --> 00:00:26,430
 think okay so I'm gonna be talking about

20
00:00:24,050 --> 00:00:26,430
 

21
00:00:24,060 --> 00:00:27,810
 scalable Bayesian inference so I'll

22
00:00:26,420 --> 00:00:27,810
 

23
00:00:26,430 --> 00:00:30,150
 start out with the motivation and

24
00:00:27,800 --> 00:00:30,150
 

25
00:00:27,810 --> 00:00:32,880
 background and then I'm gonna have like

26
00:00:30,140 --> 00:00:32,880
 

27
00:00:30,150 --> 00:00:36,510
 two sub parts the the first part is on

28
00:00:32,870 --> 00:00:36,510
 

29
00:00:32,880 --> 00:00:39,210
 big n or big sample size and so you know

30
00:00:36,500 --> 00:00:39,210
 

31
00:00:36,510 --> 00:00:41,010
 you can very roughly think about two

32
00:00:39,200 --> 00:00:41,010
 

33
00:00:39,210 --> 00:00:42,899
 different types of scalability the one

34
00:00:41,000 --> 00:00:42,899
 

35
00:00:41,010 --> 00:00:44,910
 would be where you have a data set

36
00:00:42,889 --> 00:00:44,910
 

37
00:00:42,899 --> 00:00:48,360
 that's really an enormous sample size

38
00:00:44,900 --> 00:00:48,360
 

39
00:00:44,910 --> 00:00:50,340
 but the you know the the complexity per

40
00:00:48,350 --> 00:00:50,340
 

41
00:00:48,360 --> 00:00:51,840
 subject is not that high and so you

42
00:00:50,330 --> 00:00:51,840
 

43
00:00:50,340 --> 00:00:55,020
 might have you might be trying to do a

44
00:00:51,830 --> 00:00:55,020
 

45
00:00:51,840 --> 00:00:57,780
 logistic regression or something like

46
00:00:55,010 --> 00:00:57,780
 

47
00:00:55,020 --> 00:01:00,090
 that and and you might have millions of

48
00:00:57,770 --> 00:01:00,090
 

49
00:00:57,780 --> 00:01:01,770
 subjects or hundreds of millions of

50
00:01:00,080 --> 00:01:01,770
 

51
00:01:00,090 --> 00:01:03,210
 subjects and you want to do something

52
00:01:01,760 --> 00:01:03,210
 

53
00:01:01,770 --> 00:01:05,549
 like logistic regression that's not that

54
00:01:03,200 --> 00:01:05,549
 

55
00:01:03,210 --> 00:01:08,369
 complicated okay and so that'll be the

56
00:01:05,539 --> 00:01:08,369
 

57
00:01:05,549 --> 00:01:10,350
 kind of first chunk of the talk and then

58
00:01:08,359 --> 00:01:10,350
 

59
00:01:08,369 --> 00:01:12,960
 probably more after the break I'll focus

60
00:01:10,340 --> 00:01:12,960
 

61
00:01:10,350 --> 00:01:16,740
 more on high dimensional data which

62
00:01:12,950 --> 00:01:16,740
 

63
00:01:12,960 --> 00:01:20,180
 means that you have a lot of dimension

64
00:01:16,730 --> 00:01:20,180
 

65
00:01:16,740 --> 00:01:22,740
 per or complexity per replicator

66
00:01:20,170 --> 00:01:22,740
 

67
00:01:20,180 --> 00:01:26,610
 subjective it's a biomedical study that

68
00:01:22,730 --> 00:01:26,610
 

69
00:01:22,740 --> 00:01:28,500
 could also be a big model and so if you

70
00:01:26,600 --> 00:01:28,500
 

71
00:01:26,610 --> 00:01:30,840
 had like a deep deep neural network or

72
00:01:28,490 --> 00:01:30,840
 

73
00:01:28,500 --> 00:01:32,970
 some nonparametric Bayesian model with

74
00:01:30,830 --> 00:01:32,970
 

75
00:01:30,840 --> 00:01:35,070
 with many many many many parameters that

76
00:01:32,960 --> 00:01:35,070
 

77
00:01:32,970 --> 00:01:36,659
 would essentially be a kind of big P

78
00:01:35,060 --> 00:01:36,659
 

79
00:01:35,070 --> 00:01:39,299
 problem and so we can roughly think

80
00:01:36,649 --> 00:01:39,299
 

81
00:01:36,659 --> 00:01:41,659
 about big n and big P and big and I

82
00:01:39,289 --> 00:01:41,659
 

83
00:01:39,299 --> 00:01:44,520
 would say would be an area that's much

84
00:01:41,649 --> 00:01:44,520
 

85
00:01:41,659 --> 00:01:46,320
 more well addressed by the existing

86
00:01:44,510 --> 00:01:46,320
 

87
00:01:44,520 --> 00:01:49,320
 literature and big P is more of an

88
00:01:46,310 --> 00:01:49,320
 

89
00:01:46,320 --> 00:01:51,210
 emerging area for which there's some

90
00:01:49,310 --> 00:01:51,210
 

91
00:01:49,320 --> 00:01:53,159
 methods in specific particular cases but

92
00:01:51,200 --> 00:01:53,159
 

93
00:01:51,210 --> 00:01:54,509
 not it's sort of a lack of overarching

94
00:01:53,149 --> 00:01:54,509
 

95
00:01:53,159 --> 00:01:58,680
 methods that can deal with broad

96
00:01:54,499 --> 00:01:58,680
 

97
00:01:54,509 --> 00:02:01,590
 problems okay so um so one thing I'd

98
00:01:58,670 --> 00:02:01,590
 

99
00:01:58,680 --> 00:02:03,630
 like to do partly in this tutorial is to

100
00:02:01,580 --> 00:02:03,630
 

101
00:02:01,590 --> 00:02:05,189
 motivate more of you to work on Bayesian

102
00:02:03,620 --> 00:02:05,189
 

103
00:02:03,630 --> 00:02:08,220
 methods because I think that there is

104
00:02:05,179 --> 00:02:08,220
 

105
00:02:05,189 --> 00:02:10,080
 sort of vast open problems and the huge

106
00:02:08,210 --> 00:02:10,080
 

107
00:02:08,220 --> 00:02:13,780
 potential for new research that can have

108
00:02:10,070 --> 00:02:13,780
 

109
00:02:10,080 --> 00:02:17,970
 a really big impact in broad variety of

110
00:02:13,770 --> 00:02:17,970
 

111
00:02:13,780 --> 00:02:21,130
 areas in in this industry science policy

112
00:02:17,960 --> 00:02:21,130
 

113
00:02:17,970 --> 00:02:23,770
 etc okay and so obviously there's a

114
00:02:21,120 --> 00:02:23,770
 

115
00:02:21,130 --> 00:02:26,170
 really a huge literature already focused

116
00:02:23,760 --> 00:02:26,170
 

117
00:02:23,770 --> 00:02:28,060
 on big data but but I'd say most of the

118
00:02:26,160 --> 00:02:28,060
 

119
00:02:26,170 --> 00:02:30,280
 focus has obviously been on optimization

120
00:02:28,050 --> 00:02:30,280
 

121
00:02:28,060 --> 00:02:32,319
 methods and so as a result there's

122
00:02:30,270 --> 00:02:32,319
 

123
00:02:30,280 --> 00:02:34,239
 there's potentially I would say orders

124
00:02:32,309 --> 00:02:34,239
 

125
00:02:32,319 --> 00:02:37,930
 of magnitude more people working on

126
00:02:34,229 --> 00:02:37,930
 

127
00:02:34,239 --> 00:02:40,450
 trying to do optimization in really

128
00:02:37,920 --> 00:02:40,450
 

129
00:02:37,930 --> 00:02:42,730
 large date complicated data context and

130
00:02:40,440 --> 00:02:42,730
 

131
00:02:40,450 --> 00:02:45,069
 there are people working on Bayesian

132
00:02:42,720 --> 00:02:45,069
 

133
00:02:42,730 --> 00:02:47,620
 methods that do appropriate uncertainty

134
00:02:45,059 --> 00:02:47,620
 

135
00:02:45,069 --> 00:02:48,760
 quantification and and I think that you

136
00:02:47,610 --> 00:02:48,760
 

137
00:02:47,620 --> 00:02:50,680
 know that's really the core of

138
00:02:48,750 --> 00:02:50,680
 

139
00:02:48,760 --> 00:02:52,480
 statistics and really important in a lot

140
00:02:50,670 --> 00:02:52,480
 

141
00:02:50,680 --> 00:02:55,600
 of machine learning applications is we

142
00:02:52,470 --> 00:02:55,600
 

143
00:02:52,480 --> 00:02:57,970
 need to think more about uncertainty and

144
00:02:55,590 --> 00:02:57,970
 

145
00:02:55,600 --> 00:03:00,090
 doing valid inferences in the presence

146
00:02:57,960 --> 00:03:00,090
 

147
00:02:57,970 --> 00:03:02,739
 of uncertainty and so if we have some

148
00:03:00,080 --> 00:03:02,739
 

149
00:03:00,090 --> 00:03:04,690
 algorithm that produces what I'd call a

150
00:03:02,729 --> 00:03:04,690
 

151
00:03:02,739 --> 00:03:07,360
 point estimate it might be a point

152
00:03:04,680 --> 00:03:07,360
 

153
00:03:04,690 --> 00:03:09,400
 estimate for a prediction for an

154
00:03:07,350 --> 00:03:09,400
 

155
00:03:07,360 --> 00:03:11,080
 input-output relationship I think that

156
00:03:09,390 --> 00:03:11,080
 

157
00:03:09,400 --> 00:03:12,970
 that really kind of falls short in a lot

158
00:03:11,070 --> 00:03:12,970
 

159
00:03:11,080 --> 00:03:15,370
 of applications and we can't be just

160
00:03:12,960 --> 00:03:15,370
 

161
00:03:12,970 --> 00:03:17,080
 doing that we need to be actually

162
00:03:15,360 --> 00:03:17,080
 

163
00:03:15,370 --> 00:03:19,810
 characterizing uncertainty in our

164
00:03:17,070 --> 00:03:19,810
 

165
00:03:17,080 --> 00:03:21,250
 predictions and importantly not just in

166
00:03:19,800 --> 00:03:21,250
 

167
00:03:19,810 --> 00:03:23,260
 a black box for prediction but in

168
00:03:21,240 --> 00:03:23,260
 

169
00:03:21,250 --> 00:03:25,600
 something that's really interpretable so

170
00:03:23,250 --> 00:03:25,600
 

171
00:03:23,260 --> 00:03:27,430
 that's really key I think in science and

172
00:03:25,590 --> 00:03:27,430
 

173
00:03:25,600 --> 00:03:29,140
 in other application areas and so we

174
00:03:27,420 --> 00:03:29,140
 

175
00:03:27,430 --> 00:03:31,900
 want to go beyond optimization methods

176
00:03:29,130 --> 00:03:31,900
 

177
00:03:29,140 --> 00:03:34,780
 so again so most of the literature I

178
00:03:31,890 --> 00:03:34,780
 

179
00:03:31,900 --> 00:03:36,519
 would say on scalability is on rapidly

180
00:03:34,770 --> 00:03:36,519
 

181
00:03:34,780 --> 00:03:37,930
 obtaining what I might call a point

182
00:03:36,509 --> 00:03:37,930
 

183
00:03:36,519 --> 00:03:39,610
 estimate it might be a point estimate

184
00:03:37,920 --> 00:03:39,610
 

185
00:03:37,930 --> 00:03:42,459
 for parameters or a point estimate for

186
00:03:39,600 --> 00:03:42,459
 

187
00:03:39,610 --> 00:03:44,470
 an input-output predictive model okay

188
00:03:42,449 --> 00:03:44,470
 

189
00:03:42,459 --> 00:03:46,660
 and we'd like to be able to do that even

190
00:03:44,460 --> 00:03:46,660
 

191
00:03:44,470 --> 00:03:49,720
 when the sample size n and the overall

192
00:03:46,650 --> 00:03:49,720
 

193
00:03:46,660 --> 00:03:52,690
 size of the data is immense I would also

194
00:03:49,710 --> 00:03:52,690
 

195
00:03:49,720 --> 00:03:55,359
 advocate for people working less in a

196
00:03:52,680 --> 00:03:55,359
 

197
00:03:52,690 --> 00:03:56,920
 bandwagon II way I think is I would say

198
00:03:55,349 --> 00:03:56,920
 

199
00:03:55,359 --> 00:03:58,530
 that there's been a huge focus and

200
00:03:56,910 --> 00:03:58,530
 

201
00:03:56,920 --> 00:04:01,269
 statistics and machine learning on

202
00:03:58,520 --> 00:04:01,269
 

203
00:03:58,530 --> 00:04:04,209
 people working on very similar things on

204
00:04:01,259 --> 00:04:04,209
 

205
00:04:01,269 --> 00:04:06,880
 for example linear regression certain

206
00:04:04,199 --> 00:04:06,880
 

207
00:04:04,209 --> 00:04:09,430
 types of imaging problems etc but

208
00:04:06,870 --> 00:04:09,430
 

209
00:04:06,880 --> 00:04:11,560
 there's actually huge open problems that

210
00:04:09,420 --> 00:04:11,560
 

211
00:04:09,430 --> 00:04:13,239
 remain essentially untouched where we

212
00:04:11,550 --> 00:04:13,239
 

213
00:04:11,560 --> 00:04:14,829
 have almost no methods that are

214
00:04:13,229 --> 00:04:14,829
 

215
00:04:13,239 --> 00:04:16,900
 reasonable at all for these types of

216
00:04:14,819 --> 00:04:16,900
 

217
00:04:14,829 --> 00:04:19,030
 problems and so many of those problems

218
00:04:16,890 --> 00:04:19,030
 

219
00:04:16,900 --> 00:04:21,250
 fall into the into the sciences for

220
00:04:19,020 --> 00:04:21,250
 

221
00:04:19,030 --> 00:04:24,060
 example we might want to do something

222
00:04:21,240 --> 00:04:24,060
 

223
00:04:21,250 --> 00:04:26,650
 like precision medicine so we have

224
00:04:24,050 --> 00:04:26,650
 

225
00:04:24,060 --> 00:04:27,310
 millions of observations observed on

226
00:04:26,640 --> 00:04:27,310
 

227
00:04:26,650 --> 00:04:29,650
 each subject

228
00:04:27,300 --> 00:04:29,650
 

229
00:04:27,310 --> 00:04:31,540
 for different biomarkers they might be

230
00:04:29,640 --> 00:04:31,540
 

231
00:04:29,650 --> 00:04:33,940
 genomic they might be neuroscience

232
00:04:31,530 --> 00:04:33,940
 

233
00:04:31,540 --> 00:04:36,210
 biomarkers and we'd like to use that to

234
00:04:33,930 --> 00:04:36,210
 

235
00:04:33,940 --> 00:04:38,740
 predict patient health and to target

236
00:04:36,200 --> 00:04:38,740
 

237
00:04:36,210 --> 00:04:40,150
 treatment for different diseases okay

238
00:04:38,730 --> 00:04:40,150
 

239
00:04:38,740 --> 00:04:41,560
 we'd like to do something like that and

240
00:04:40,140 --> 00:04:41,560
 

241
00:04:40,150 --> 00:04:44,380
 we have no really no idea how to do it

242
00:04:41,550 --> 00:04:44,380
 

243
00:04:41,560 --> 00:04:46,990
 currently okay so I would again advocate

244
00:04:44,370 --> 00:04:46,990
 

245
00:04:44,380 --> 00:04:48,550
 for people working on on those types of

246
00:04:46,980 --> 00:04:48,550
 

247
00:04:46,990 --> 00:04:50,139
 problems on different types of problems

248
00:04:48,540 --> 00:04:50,139
 

249
00:04:48,550 --> 00:04:51,810
 try to avoid working on the same

250
00:04:50,129 --> 00:04:51,810
 

251
00:04:50,139 --> 00:04:54,550
 problems everyone else is working on

252
00:04:51,800 --> 00:04:54,550
 

253
00:04:51,810 --> 00:04:56,470
 okay so my focus is in general on

254
00:04:54,540 --> 00:04:56,470
 

255
00:04:54,550 --> 00:04:58,780
 probability models this is obviously

256
00:04:56,460 --> 00:04:58,780
 

257
00:04:56,470 --> 00:05:00,550
 obviously a tutorial on scalable

258
00:04:58,770 --> 00:05:00,550
 

259
00:04:58,780 --> 00:05:03,970
 Bayesian inference per se but I would

260
00:05:00,540 --> 00:05:03,970
 

261
00:05:00,550 --> 00:05:05,889
 say it's often useful to take the

262
00:05:03,960 --> 00:05:05,889
 

263
00:05:03,970 --> 00:05:07,900
 Bayesian paradigm which is really nice

264
00:05:05,879 --> 00:05:07,900
 

265
00:05:05,889 --> 00:05:09,729
 in terms of using probability to

266
00:05:07,890 --> 00:05:09,729
 

267
00:05:07,900 --> 00:05:12,400
 characterize learning from complicated

268
00:05:09,719 --> 00:05:12,400
 

269
00:05:09,729 --> 00:05:14,950
 data and and take a little step away

270
00:05:12,390 --> 00:05:14,950
 

271
00:05:12,400 --> 00:05:17,110
 sometimes and so the main thing is that

272
00:05:14,940 --> 00:05:17,110
 

273
00:05:14,950 --> 00:05:18,639
 we have a really nice approach for

274
00:05:17,100 --> 00:05:18,639
 

275
00:05:17,110 --> 00:05:21,430
 learning from different types of data

276
00:05:18,629 --> 00:05:21,430
 

277
00:05:18,639 --> 00:05:23,200
 and we're characterizing uncertainty in

278
00:05:21,420 --> 00:05:23,200
 

279
00:05:21,430 --> 00:05:26,530
 that learning and in our inferences and

280
00:05:23,190 --> 00:05:26,530
 

281
00:05:23,200 --> 00:05:28,180
 predictions in a valid way and and

282
00:05:26,520 --> 00:05:28,180
 

283
00:05:26,530 --> 00:05:30,070
 that's really useful if we're using a

284
00:05:28,170 --> 00:05:30,070
 

285
00:05:28,180 --> 00:05:32,229
 probability model and Bayesian

286
00:05:30,060 --> 00:05:32,229
 

287
00:05:30,070 --> 00:05:32,620
 statistics is one type of formulation of

288
00:05:32,219 --> 00:05:32,620
 

289
00:05:32,229 --> 00:05:34,900
 that

290
00:05:32,610 --> 00:05:34,900
 

291
00:05:32,620 --> 00:05:36,370
 okay so we'd like to have some sort of

292
00:05:34,890 --> 00:05:36,370
 

293
00:05:34,900 --> 00:05:39,490
 general probabilistic inference

294
00:05:36,360 --> 00:05:39,490
 

295
00:05:36,370 --> 00:05:40,900
 algorithm from from complex data we'd

296
00:05:39,480 --> 00:05:40,900
 

297
00:05:39,490 --> 00:05:43,720
 like to be able to handle arbitrarily

298
00:05:40,890 --> 00:05:43,720
 

299
00:05:40,900 --> 00:05:45,310
 complex probability models and so you

300
00:05:43,710 --> 00:05:45,310
 

301
00:05:43,720 --> 00:05:47,620
 know often nowadays data are really

302
00:05:45,300 --> 00:05:47,620
 

303
00:05:45,310 --> 00:05:49,330
 increasingly complicated and we need to

304
00:05:47,610 --> 00:05:49,330
 

305
00:05:47,620 --> 00:05:49,690
 be able to model the complexity of the

306
00:05:49,320 --> 00:05:49,690
 

307
00:05:49,330 --> 00:05:51,490
 data

308
00:05:49,680 --> 00:05:51,490
 

309
00:05:49,690 --> 00:05:53,650
 I've got mentioning this precision

310
00:05:51,480 --> 00:05:53,650
 

311
00:05:51,490 --> 00:05:55,470
 medicine application you know you could

312
00:05:53,640 --> 00:05:55,470
 

313
00:05:53,650 --> 00:05:58,150
 think of data from patients coming into

314
00:05:55,460 --> 00:05:58,150
 

315
00:05:55,470 --> 00:06:00,880
 healthcare clinic and there's a lot of

316
00:05:58,140 --> 00:06:00,880
 

317
00:05:58,150 --> 00:06:04,389
 censoring observations collected over

318
00:06:00,870 --> 00:06:04,389
 

319
00:06:00,880 --> 00:06:06,310
 time selection bias which is something

320
00:06:04,379 --> 00:06:06,310
 

321
00:06:04,389 --> 00:06:08,620
 which is incredibly important to take

322
00:06:06,300 --> 00:06:08,620
 

323
00:06:06,310 --> 00:06:10,870
 into account we can have missing data

324
00:06:08,610 --> 00:06:10,870
 

325
00:06:08,620 --> 00:06:12,070
 that's informatively missing and if we

326
00:06:10,860 --> 00:06:12,070
 

327
00:06:10,870 --> 00:06:13,960
 don't characterize this sort of

328
00:06:12,060 --> 00:06:13,960
 

329
00:06:12,070 --> 00:06:15,789
 complexity if we just kind of throw some

330
00:06:13,950 --> 00:06:15,789
 

331
00:06:13,960 --> 00:06:18,370
 sort of neural network or some algorithm

332
00:06:15,779 --> 00:06:18,370
 

333
00:06:15,789 --> 00:06:20,680
 at it putting all this data together

334
00:06:18,360 --> 00:06:20,680
 

335
00:06:18,370 --> 00:06:22,770
 I'm not taking to account the selection

336
00:06:20,670 --> 00:06:22,770
 

337
00:06:20,680 --> 00:06:24,580
 that occurs of the data that we observe

338
00:06:22,760 --> 00:06:24,580
 

339
00:06:22,770 --> 00:06:26,590
 relative to the data in the general

340
00:06:24,570 --> 00:06:26,590
 

341
00:06:24,580 --> 00:06:28,539
 population then we're going to get kind

342
00:06:26,580 --> 00:06:28,539
 

343
00:06:26,590 --> 00:06:31,030
 of bogus garbage basically out of the

344
00:06:28,529 --> 00:06:31,030
 

345
00:06:28,539 --> 00:06:32,919
 out of the results okay so we'd like to

346
00:06:31,020 --> 00:06:32,919
 

347
00:06:31,030 --> 00:06:35,789
 the algorithms to be scalable to huge

348
00:06:32,909 --> 00:06:35,789
 

349
00:06:32,919 --> 00:06:38,680
 data potentially using many computers

350
00:06:35,779 --> 00:06:38,680
 

351
00:06:35,789 --> 00:06:40,720
 okay and an accurate uncertainty

352
00:06:38,670 --> 00:06:40,720
 

353
00:06:38,680 --> 00:06:41,260
 quantification is a critical issue I

354
00:06:40,710 --> 00:06:41,260
 

355
00:06:40,720 --> 00:06:43,120
 would say

356
00:06:41,250 --> 00:06:43,120
 

357
00:06:41,260 --> 00:06:44,650
 you know a lot of this scalable Bayes

358
00:06:43,110 --> 00:06:44,650
 

359
00:06:43,120 --> 00:06:46,270
 inference and I'll touch on this has

360
00:06:44,640 --> 00:06:46,270
 

361
00:06:44,650 --> 00:06:48,610
 been focused on algorithms such as

362
00:06:46,260 --> 00:06:48,610
 

363
00:06:46,270 --> 00:06:50,800
 variational Bayes where you might

364
00:06:48,600 --> 00:06:50,800
 

365
00:06:48,610 --> 00:06:53,290
 actually not characterize uncertainty

366
00:06:50,790 --> 00:06:53,290
 

367
00:06:50,800 --> 00:06:55,420
 very well at all and so you might have

368
00:06:53,280 --> 00:06:55,420
 

369
00:06:53,290 --> 00:06:57,730
 an estimate of a posterior distribution

370
00:06:55,410 --> 00:06:57,730
 

371
00:06:55,420 --> 00:07:01,210
 that is much much much more concentrated

372
00:06:57,720 --> 00:07:01,210
 

373
00:06:57,730 --> 00:07:02,710
 than is realistic given the actual

374
00:07:01,200 --> 00:07:02,710
 

375
00:07:01,210 --> 00:07:04,060
 uncertainty you having a problem and so

376
00:07:02,700 --> 00:07:04,060
 

377
00:07:02,710 --> 00:07:07,270
 we'd like to be able to actually

378
00:07:04,050 --> 00:07:07,270
 

379
00:07:04,060 --> 00:07:10,270
 accurately characterize uncertainty also

380
00:07:07,260 --> 00:07:10,270
 

381
00:07:07,270 --> 00:07:12,610
 an issue that I'm going to touch on and

382
00:07:10,260 --> 00:07:12,610
 

383
00:07:10,270 --> 00:07:14,620
 in a chunk of this talk is on robustness

384
00:07:12,600 --> 00:07:14,620
 

385
00:07:12,610 --> 00:07:17,020
 of inferences and so one one thing

386
00:07:14,610 --> 00:07:17,020
 

387
00:07:14,620 --> 00:07:18,430
 that's really interesting is that okay

388
00:07:17,010 --> 00:07:18,430
 

389
00:07:17,020 --> 00:07:20,500
 we take a Bayesian approach well a

390
00:07:18,420 --> 00:07:20,500
 

391
00:07:18,430 --> 00:07:22,720
 Bayesian approach is one example of a

392
00:07:20,490 --> 00:07:22,720
 

393
00:07:20,500 --> 00:07:24,670
 model based approach to inference and by

394
00:07:22,710 --> 00:07:24,670
 

395
00:07:22,720 --> 00:07:26,740
 model I mean that we need to specify a

396
00:07:24,660 --> 00:07:26,740
 

397
00:07:24,670 --> 00:07:29,020
 likelihood function the generative

398
00:07:26,730 --> 00:07:29,020
 

399
00:07:26,740 --> 00:07:30,400
 likelihood function for the data it

400
00:07:29,010 --> 00:07:30,400
 

401
00:07:29,020 --> 00:07:32,860
 could be that that likelihood function

402
00:07:30,390 --> 00:07:32,860
 

403
00:07:30,400 --> 00:07:35,650
 is very complicated and intricate to

404
00:07:32,850 --> 00:07:35,650
 

405
00:07:32,860 --> 00:07:37,330
 allow for a lot of flexibility but then

406
00:07:35,640 --> 00:07:37,330
 

407
00:07:35,650 --> 00:07:39,130
 what we're worried about is this sort of

408
00:07:37,320 --> 00:07:39,130
 

409
00:07:37,330 --> 00:07:41,770
 robustness of inferences and so that

410
00:07:39,120 --> 00:07:41,770
 

411
00:07:39,130 --> 00:07:44,230
 that can be a particularly large problem

412
00:07:41,760 --> 00:07:44,230
 

413
00:07:41,770 --> 00:07:45,910
 as the sample size gets really big and

414
00:07:44,220 --> 00:07:45,910
 

415
00:07:44,230 --> 00:07:47,740
 so it's it's kind of well-known and

416
00:07:45,900 --> 00:07:47,740
 

417
00:07:45,910 --> 00:07:49,930
 statistics for example let's say I have

418
00:07:47,730 --> 00:07:49,930
 

419
00:07:47,740 --> 00:07:51,760
 a likelihood or something based model

420
00:07:49,920 --> 00:07:51,760
 

421
00:07:49,930 --> 00:07:53,710
 and I might want to be doing some type

422
00:07:51,750 --> 00:07:53,710
 

423
00:07:51,760 --> 00:07:56,500
 of hypothesis testing or variable

424
00:07:53,700 --> 00:07:56,500
 

425
00:07:53,710 --> 00:07:58,240
 selection or or some other type of types

426
00:07:56,490 --> 00:07:58,240
 

427
00:07:56,500 --> 00:08:00,670
 of inferences on the model structure

428
00:07:58,230 --> 00:08:00,670
 

429
00:07:58,240 --> 00:08:02,170
 well what often happens is well if this

430
00:08:00,660 --> 00:08:02,170
 

431
00:08:00,670 --> 00:08:04,030
 sample size gets bigger and bigger and

432
00:08:02,160 --> 00:08:04,030
 

433
00:08:02,170 --> 00:08:05,770
 bigger and bigger then everything

434
00:08:04,020 --> 00:08:05,770
 

435
00:08:04,030 --> 00:08:07,750
 becomes significant and so we have

436
00:08:05,760 --> 00:08:07,750
 

437
00:08:05,770 --> 00:08:11,020
 millions of observations all your

438
00:08:07,740 --> 00:08:11,020
 

439
00:08:07,750 --> 00:08:12,700
 p-value ZAR significant your model gets

440
00:08:11,010 --> 00:08:12,700
 

441
00:08:11,020 --> 00:08:15,580
 bigger and bigger and bigger kind of

442
00:08:12,690 --> 00:08:15,580
 

443
00:08:12,700 --> 00:08:17,230
 without bound and we become very non

444
00:08:15,570 --> 00:08:17,230
 

445
00:08:15,580 --> 00:08:19,990
 robust in the sense and so we might have

446
00:08:17,220 --> 00:08:19,990
 

447
00:08:17,230 --> 00:08:22,660
 a model that's really good that could be

448
00:08:19,980 --> 00:08:22,660
 

449
00:08:19,990 --> 00:08:24,490
 very nice at interpreting the data but

450
00:08:22,650 --> 00:08:24,490
 

451
00:08:22,660 --> 00:08:26,140
 then that the actual data you observe

452
00:08:24,480 --> 00:08:26,140
 

453
00:08:24,490 --> 00:08:27,610
 might not be coming exactly from that

454
00:08:26,130 --> 00:08:27,610
 

455
00:08:26,140 --> 00:08:28,570
 model because all models are wrong and

456
00:08:27,600 --> 00:08:28,570
 

457
00:08:27,610 --> 00:08:30,880
 might be coming from a slight

458
00:08:28,560 --> 00:08:30,880
 

459
00:08:28,570 --> 00:08:33,130
 perturbation of that model and that that

460
00:08:30,870 --> 00:08:33,130
 

461
00:08:30,880 --> 00:08:35,530
 that becomes a big issue when the sample

462
00:08:33,120 --> 00:08:35,530
 

463
00:08:33,130 --> 00:08:37,900
 size is really large and so we end up

464
00:08:35,520 --> 00:08:37,900
 

465
00:08:35,530 --> 00:08:40,510
 having sort of less robust inferences as

466
00:08:37,890 --> 00:08:40,510
 

467
00:08:37,900 --> 00:08:42,220
 ample size becomes really really big and

468
00:08:40,500 --> 00:08:42,220
 

469
00:08:40,510 --> 00:08:44,590
 if we just care about some sort of

470
00:08:42,210 --> 00:08:44,590
 

471
00:08:42,220 --> 00:08:46,540
 predictive input/output blackbox which

472
00:08:44,580 --> 00:08:46,540
 

473
00:08:44,590 --> 00:08:49,240
 is a lot of machine learning then we

474
00:08:46,530 --> 00:08:49,240
 

475
00:08:46,540 --> 00:08:50,590
 might not care so much but if we want to

476
00:08:49,230 --> 00:08:50,590
 

477
00:08:49,240 --> 00:08:52,420
 actually do kind of scientific

478
00:08:50,580 --> 00:08:52,420
 

479
00:08:50,590 --> 00:08:54,670
 inferences based on our model based

480
00:08:52,410 --> 00:08:54,670
 

481
00:08:52,420 --> 00:08:55,120
 inferences that then it's a very big

482
00:08:54,660 --> 00:08:55,120
 

483
00:08:54,670 --> 00:08:58,449
 issue

484
00:08:55,110 --> 00:08:58,449
 

485
00:08:55,120 --> 00:09:01,120
 and I'll discuss that okay so that's the

486
00:08:58,439 --> 00:09:01,120
 

487
00:08:58,449 --> 00:09:03,220
 general focus obviously we're gonna be

488
00:09:01,110 --> 00:09:03,220
 

489
00:09:01,120 --> 00:09:04,689
 focusing on Bayesian approaches I would

490
00:09:03,210 --> 00:09:04,689
 

491
00:09:03,220 --> 00:09:06,879
 say Bayesian methods offer an attractive

492
00:09:04,679 --> 00:09:06,879
 

493
00:09:04,689 --> 00:09:09,819
 general approach for for modeling

494
00:09:06,869 --> 00:09:09,819
 

495
00:09:06,879 --> 00:09:11,589
 complicated data and so to just just

496
00:09:09,809 --> 00:09:11,589
 

497
00:09:09,819 --> 00:09:13,600
 sort of set the stage here many of you

498
00:09:11,579 --> 00:09:13,600
 

499
00:09:11,589 --> 00:09:15,189
 of course will be familiar with basic

500
00:09:13,590 --> 00:09:15,189
 

501
00:09:13,600 --> 00:09:17,439
 Bayesian inference but probably not all

502
00:09:15,179 --> 00:09:17,439
 

503
00:09:15,189 --> 00:09:19,870
 of you and so here we're gonna say well

504
00:09:17,429 --> 00:09:19,870
 

505
00:09:17,439 --> 00:09:21,069
 we have some sort of likelihood model

506
00:09:19,860 --> 00:09:21,069
 

507
00:09:19,870 --> 00:09:22,689
 and so this is some generative

508
00:09:21,059 --> 00:09:22,689
 

509
00:09:21,069 --> 00:09:24,459
 probability model that could generate

510
00:09:22,679 --> 00:09:24,459
 

511
00:09:22,689 --> 00:09:26,439
 the dataview observe and I'm gonna say

512
00:09:24,449 --> 00:09:26,439
 

513
00:09:24,459 --> 00:09:30,009
 that's why and I'm gonna put a

514
00:09:26,429 --> 00:09:30,009
 

515
00:09:26,439 --> 00:09:32,649
 superscript in on that why to index the

516
00:09:29,999 --> 00:09:32,649
 

517
00:09:30,009 --> 00:09:34,180
 the sample size okay and so um I might

518
00:09:32,639 --> 00:09:34,180
 

519
00:09:32,649 --> 00:09:36,459
 be dropping some of that that fine

520
00:09:34,170 --> 00:09:36,459
 

521
00:09:34,180 --> 00:09:38,079
 notation as we go but let's just say the

522
00:09:36,449 --> 00:09:38,079
 

523
00:09:36,459 --> 00:09:40,480
 like this is the likelihood of some

524
00:09:38,069 --> 00:09:40,480
 

525
00:09:38,079 --> 00:09:42,370
 giant collection of data Y superscript n

526
00:09:40,470 --> 00:09:42,370
 

527
00:09:40,480 --> 00:09:44,740
 and it's parameterised by some

528
00:09:42,360 --> 00:09:44,740
 

529
00:09:42,370 --> 00:09:47,649
 parameters theta okay so this might be

530
00:09:44,730 --> 00:09:47,649
 

531
00:09:44,740 --> 00:09:49,749
 some mixture model it could be some sort

532
00:09:47,639 --> 00:09:49,749
 

533
00:09:47,649 --> 00:09:51,360
 of generative neural network it could be

534
00:09:49,739 --> 00:09:51,360
 

535
00:09:49,749 --> 00:09:53,529
 a Gaussian process it could be anything

536
00:09:51,350 --> 00:09:53,529
 

537
00:09:51,360 --> 00:09:55,660
 and so that's our data here's our

538
00:09:53,519 --> 00:09:55,660
 

539
00:09:53,529 --> 00:09:56,980
 parameters and so if we take a Bayesian

540
00:09:55,650 --> 00:09:56,980
 

541
00:09:55,660 --> 00:09:59,170
 approach then we need to put down a

542
00:09:56,970 --> 00:09:59,170
 

543
00:09:56,980 --> 00:10:00,670
 prior probability distribution on these

544
00:09:59,160 --> 00:10:00,670
 

545
00:09:59,170 --> 00:10:02,949
 parameters and I'll call that PI of

546
00:10:00,660 --> 00:10:02,949
 

547
00:10:00,670 --> 00:10:05,439
 theta okay and so we have a prior PI of

548
00:10:02,939 --> 00:10:05,439
 

549
00:10:02,949 --> 00:10:07,569
 theta that sort of encodes information

550
00:10:05,429 --> 00:10:07,569
 

551
00:10:05,439 --> 00:10:10,059
 and in the parameters prior to observing

552
00:10:07,559 --> 00:10:10,059
 

553
00:10:07,569 --> 00:10:12,160
 the current the current data and now the

554
00:10:10,049 --> 00:10:12,160
 

555
00:10:10,059 --> 00:10:14,139
 likelihood is encoding information in

556
00:10:12,150 --> 00:10:14,139
 

557
00:10:12,160 --> 00:10:15,550
 the data about those parameters and then

558
00:10:14,129 --> 00:10:15,550
 

559
00:10:14,139 --> 00:10:19,059
 we kind of put it into our Bayes rule

560
00:10:15,540 --> 00:10:19,059
 

561
00:10:15,550 --> 00:10:21,279
 okay so here's the prior here's the

562
00:10:19,049 --> 00:10:21,279
 

563
00:10:19,059 --> 00:10:23,050
 likelihood and now we we need to turn

564
00:10:21,269 --> 00:10:23,050
 

565
00:10:21,279 --> 00:10:25,120
 this into a probability distribution and

566
00:10:23,040 --> 00:10:25,120
 

567
00:10:23,050 --> 00:10:26,620
 so we normalize it so we divide by the

568
00:10:25,110 --> 00:10:26,620
 

569
00:10:25,120 --> 00:10:27,879
 integral of the prior times the

570
00:10:26,610 --> 00:10:27,879
 

571
00:10:26,620 --> 00:10:30,279
 likelihood over the parameter space

572
00:10:27,869 --> 00:10:30,279
 

573
00:10:27,879 --> 00:10:32,829
 that's often known as the marginal

574
00:10:30,269 --> 00:10:32,829
 

575
00:10:30,279 --> 00:10:34,480
 likelihood or the evidence okay and then

576
00:10:32,819 --> 00:10:34,480
 

577
00:10:32,829 --> 00:10:36,279
 we get this posterior probability

578
00:10:34,470 --> 00:10:36,279
 

579
00:10:34,480 --> 00:10:38,410
 distribution so the Bayesian updating

580
00:10:36,269 --> 00:10:38,410
 

581
00:10:36,279 --> 00:10:39,939
 occurs when we update the information in

582
00:10:38,400 --> 00:10:39,939
 

583
00:10:38,410 --> 00:10:42,399
 the prior which might be a relatively

584
00:10:39,929 --> 00:10:42,399
 

585
00:10:39,939 --> 00:10:44,290
 flat probability distribution with the

586
00:10:42,389 --> 00:10:44,290
 

587
00:10:42,399 --> 00:10:46,660
 information in the likelihood and then

588
00:10:44,280 --> 00:10:46,660
 

589
00:10:44,290 --> 00:10:48,249
 we're doing a type of learning okay and

590
00:10:46,650 --> 00:10:48,249
 

591
00:10:46,660 --> 00:10:50,410
 that's one really nice thing in that we

592
00:10:48,239 --> 00:10:50,410
 

593
00:10:48,249 --> 00:10:52,269
 can we can keep doing that over and over

594
00:10:50,400 --> 00:10:52,269
 

595
00:10:50,410 --> 00:10:54,100
 again as we get more more in different

596
00:10:52,259 --> 00:10:54,100
 

597
00:10:52,269 --> 00:10:55,749
 types and disparate sources of

598
00:10:54,090 --> 00:10:55,749
 

599
00:10:54,100 --> 00:10:58,089
 information we can kind of keep plugging

600
00:10:55,739 --> 00:10:58,089
 

601
00:10:55,749 --> 00:11:00,300
 in and doing this Bayesian updating to

602
00:10:58,079 --> 00:11:00,300
 

603
00:10:58,089 --> 00:11:02,350
 learn ever more about this kind of

604
00:11:00,290 --> 00:11:02,350
 

605
00:11:00,300 --> 00:11:06,519
 posterior distribution there's more data

606
00:11:02,340 --> 00:11:06,519
 

607
00:11:02,350 --> 00:11:07,779
 become available okay the posterior is

608
00:11:06,509 --> 00:11:07,779
 

609
00:11:06,519 --> 00:11:08,740
 really nice and characterizing

610
00:11:07,769 --> 00:11:08,740
 

611
00:11:07,779 --> 00:11:12,850
 uncertainty in the

612
00:11:08,730 --> 00:11:12,850
 

613
00:11:08,740 --> 00:11:14,709
 rameters and and also I would say in any

614
00:11:12,840 --> 00:11:14,709
 

615
00:11:12,850 --> 00:11:16,930
 functional of interest and because we

616
00:11:14,699 --> 00:11:16,930
 

617
00:11:14,709 --> 00:11:18,760
 might not be interested directly in

618
00:11:16,920 --> 00:11:18,760
 

619
00:11:16,930 --> 00:11:20,050
 these parameters theta these are just

620
00:11:18,750 --> 00:11:20,050
 

621
00:11:18,760 --> 00:11:21,790
 parameters that are convenient to

622
00:11:20,040 --> 00:11:21,790
 

623
00:11:20,050 --> 00:11:24,670
 specify our likelihood function you know

624
00:11:21,780 --> 00:11:24,670
 

625
00:11:21,790 --> 00:11:26,380
 and so we might have a model like a some

626
00:11:24,660 --> 00:11:26,380
 

627
00:11:24,670 --> 00:11:28,149
 sort of generative deep neural network

628
00:11:26,370 --> 00:11:28,149
 

629
00:11:26,380 --> 00:11:30,160
 with a lot of uninterpretable parameters

630
00:11:28,139 --> 00:11:30,160
 

631
00:11:28,149 --> 00:11:33,160
 but we might be interested actually in

632
00:11:30,150 --> 00:11:33,160
 

633
00:11:30,160 --> 00:11:35,560
 some sort of function of the parameters

634
00:11:33,150 --> 00:11:35,560
 

635
00:11:33,160 --> 00:11:38,560
 that might be for example the mean

636
00:11:35,550 --> 00:11:38,560
 

637
00:11:35,560 --> 00:11:40,390
 output or some sort of percentile of an

638
00:11:38,550 --> 00:11:40,390
 

639
00:11:38,560 --> 00:11:42,339
 output given an input something like

640
00:11:40,380 --> 00:11:42,339
 

641
00:11:40,390 --> 00:11:44,589
 that okay and so that might be our

642
00:11:42,329 --> 00:11:44,589
 

643
00:11:42,339 --> 00:11:45,940
 functional of interest parameters might

644
00:11:44,579 --> 00:11:45,940
 

645
00:11:44,589 --> 00:11:48,160
 not be interpreted but the functional

646
00:11:45,930 --> 00:11:48,160
 

647
00:11:45,940 --> 00:11:49,570
 might be interpreted okay and we might

648
00:11:48,150 --> 00:11:49,570
 

649
00:11:48,160 --> 00:11:51,339
 also be interested in predictive

650
00:11:49,560 --> 00:11:51,339
 

651
00:11:49,570 --> 00:11:53,440
 distributions even marginalizing out the

652
00:11:51,329 --> 00:11:53,440
 

653
00:11:51,339 --> 00:11:54,970
 parameters as a sort of nuisance we

654
00:11:53,430 --> 00:11:54,970
 

655
00:11:53,440 --> 00:11:56,920
 would like to get the predictive

656
00:11:54,960 --> 00:11:56,920
 

657
00:11:54,970 --> 00:11:59,740
 distribution for a new observation given

658
00:11:56,910 --> 00:11:59,740
 

659
00:11:56,920 --> 00:12:01,480
 features on that new observation using

660
00:11:59,730 --> 00:12:01,480
 

661
00:11:59,740 --> 00:12:03,010
 all the information we have so far okay

662
00:12:01,470 --> 00:12:03,010
 

663
00:12:01,480 --> 00:12:05,890
 and we can do all that sort of thing

664
00:12:03,000 --> 00:12:05,890
 

665
00:12:03,010 --> 00:12:08,770
 using this kind of Bayesian calculus in

666
00:12:05,880 --> 00:12:08,770
 

667
00:12:05,890 --> 00:12:13,180
 some sense okay so um so what's the

668
00:12:08,760 --> 00:12:13,180
 

669
00:12:08,770 --> 00:12:14,980
 problem well I'm you know if you look at

670
00:12:13,170 --> 00:12:14,980
 

671
00:12:13,180 --> 00:12:17,079
 this equation often the theta is

672
00:12:14,970 --> 00:12:17,079
 

673
00:12:14,980 --> 00:12:18,670
 moderate to high dimensional and if you

674
00:12:17,069 --> 00:12:18,670
 

675
00:12:17,079 --> 00:12:20,620
 have an interesting model not some

676
00:12:18,660 --> 00:12:20,620
 

677
00:12:18,670 --> 00:12:22,450
 really simple thing then then this

678
00:12:20,610 --> 00:12:22,450
 

679
00:12:20,620 --> 00:12:25,440
 integral down here is gonna be some

680
00:12:22,440 --> 00:12:25,440
 

681
00:12:22,450 --> 00:12:27,339
 nasty beast and we're not very good at

682
00:12:25,430 --> 00:12:27,339
 

683
00:12:25,440 --> 00:12:28,690
 approximating high dimensional integrals

684
00:12:27,329 --> 00:12:28,690
 

685
00:12:27,339 --> 00:12:31,060
 there's been a lot of literature on that

686
00:12:28,680 --> 00:12:31,060
 

687
00:12:28,690 --> 00:12:33,399
 many many different methods are proposed

688
00:12:31,050 --> 00:12:33,399
 

689
00:12:31,060 --> 00:12:35,589
 to kind of approximate this so-called

690
00:12:33,389 --> 00:12:35,589
 

691
00:12:33,399 --> 00:12:37,870
 marginal likelihood in the denominator

692
00:12:35,579 --> 00:12:37,870
 

693
00:12:35,589 --> 00:12:40,589
 but many of these methods aren't very

694
00:12:37,860 --> 00:12:40,589
 

695
00:12:37,870 --> 00:12:42,000
 good okay and so we run into problems

696
00:12:40,579 --> 00:12:42,000
 

697
00:12:40,589 --> 00:12:44,020
 computationally and kind of

698
00:12:41,990 --> 00:12:44,020
 

699
00:12:42,000 --> 00:12:46,000
 approximating this integral and hence

700
00:12:44,010 --> 00:12:46,000
 

701
00:12:44,020 --> 00:12:47,529
 approximating the posterior because

702
00:12:45,990 --> 00:12:47,529
 

703
00:12:46,000 --> 00:12:49,450
 often we have the prior and the

704
00:12:47,519 --> 00:12:49,450
 

705
00:12:47,529 --> 00:12:51,550
 likelihood sometimes we don't and so

706
00:12:49,440 --> 00:12:51,550
 

707
00:12:49,450 --> 00:12:52,600
 called doubly intractable problems but

708
00:12:51,540 --> 00:12:52,600
 

709
00:12:51,550 --> 00:12:53,950
 often we have the prior in the

710
00:12:52,590 --> 00:12:53,950
 

711
00:12:52,600 --> 00:12:57,310
 likelihood but we can't we can't

712
00:12:53,940 --> 00:12:57,310
 

713
00:12:53,950 --> 00:12:59,200
 evaluate this this integral okay okay so

714
00:12:57,300 --> 00:12:59,200
 

715
00:12:57,310 --> 00:13:01,180
 then the question is well in interesting

716
00:12:59,190 --> 00:13:01,180
 

717
00:12:59,200 --> 00:13:04,240
 models the posterior is not available

718
00:13:01,170 --> 00:13:04,240
 

719
00:13:01,180 --> 00:13:08,200
 analytically what the heck do we do okay

720
00:13:04,230 --> 00:13:08,200
 

721
00:13:04,240 --> 00:13:09,880
 well one thing we could do well if we

722
00:13:08,190 --> 00:13:09,880
 

723
00:13:08,200 --> 00:13:11,079
 had a simple model that simple models

724
00:13:09,870 --> 00:13:11,079
 

725
00:13:09,880 --> 00:13:12,910
 they're often called conjugate

726
00:13:11,069 --> 00:13:12,910
 

727
00:13:11,079 --> 00:13:14,680
 well--what's conjugates so if the model

728
00:13:12,900 --> 00:13:14,680
 

729
00:13:12,910 --> 00:13:16,240
 is conjugate then we can write

730
00:13:14,670 --> 00:13:16,240
 

731
00:13:14,680 --> 00:13:17,529
 everything down in a really simple form

732
00:13:16,230 --> 00:13:17,529
 

733
00:13:16,240 --> 00:13:20,500
 so that means that the prior

734
00:13:17,519 --> 00:13:20,500
 

735
00:13:17,529 --> 00:13:22,570
 distribution PI of theta and the like

736
00:13:20,490 --> 00:13:22,570
 

737
00:13:20,500 --> 00:13:24,880
 the the posterior distribution

738
00:13:22,560 --> 00:13:24,880
 

739
00:13:22,570 --> 00:13:26,500
 pi of theta given why are in the same

740
00:13:24,870 --> 00:13:26,500
 

741
00:13:24,880 --> 00:13:28,960
 form and so maybe we start out with a

742
00:13:26,490 --> 00:13:28,960
 

743
00:13:26,500 --> 00:13:31,060
 Gaussian prior and we update with a

744
00:13:28,950 --> 00:13:31,060
 

745
00:13:28,960 --> 00:13:33,370
 linear model Gaussian linear model

746
00:13:31,050 --> 00:13:33,370
 

747
00:13:31,060 --> 00:13:36,490
 likelihood and now the posterior is

748
00:13:33,360 --> 00:13:36,490
 

749
00:13:33,370 --> 00:13:37,660
 still Gaussian okay or it's still a t or

750
00:13:36,480 --> 00:13:37,660
 

751
00:13:36,490 --> 00:13:39,670
 normal inverse gamma distribution

752
00:13:37,650 --> 00:13:39,670
 

753
00:13:37,660 --> 00:13:42,250
 there's some sort of exponential family

754
00:13:39,660 --> 00:13:42,250
 

755
00:13:39,670 --> 00:13:45,040
 form okay and and so if we have that

756
00:13:42,240 --> 00:13:45,040
 

757
00:13:42,250 --> 00:13:46,510
 type of conjugacy then life is good we

758
00:13:45,030 --> 00:13:46,510
 

759
00:13:45,040 --> 00:13:48,370
 can write down the posterior

760
00:13:46,500 --> 00:13:48,370
 

761
00:13:46,510 --> 00:13:51,040
 analytically and we can do a lot of

762
00:13:48,360 --> 00:13:51,040
 

763
00:13:48,370 --> 00:13:53,080
 things okay and so that that's that's

764
00:13:51,030 --> 00:13:53,080
 

765
00:13:51,040 --> 00:13:55,080
 often true even and somewhat interesting

766
00:13:53,070 --> 00:13:55,080
 

767
00:13:53,080 --> 00:13:57,730
 models if you write down a linear model

768
00:13:55,070 --> 00:13:57,730
 

769
00:13:55,080 --> 00:13:59,740
 where you can have a lot of flexibility

770
00:13:57,720 --> 00:13:59,740
 

771
00:13:57,730 --> 00:14:03,220
 through some type of basis expansion you

772
00:13:59,730 --> 00:14:03,220
 

773
00:13:59,740 --> 00:14:05,380
 can still get conjugacy okay

774
00:14:03,210 --> 00:14:05,380
 

775
00:14:03,220 --> 00:14:06,820
 so in a more complicated settings what

776
00:14:05,370 --> 00:14:06,820
 

777
00:14:05,380 --> 00:14:08,770
 we could do is we could potentially

778
00:14:06,810 --> 00:14:08,770
 

779
00:14:06,820 --> 00:14:11,350
 approximate the posterior using some

780
00:14:08,760 --> 00:14:11,350
 

781
00:14:08,770 --> 00:14:15,580
 tractable class of distributions okay

782
00:14:11,340 --> 00:14:15,580
 

783
00:14:11,350 --> 00:14:18,520
 and one one one popular class which it's

784
00:14:15,570 --> 00:14:18,520
 

785
00:14:15,580 --> 00:14:20,440
 interesting to me I think that you know

786
00:14:18,510 --> 00:14:20,440
 

787
00:14:18,520 --> 00:14:22,870
 machine learning often people will focus

788
00:14:20,430 --> 00:14:22,870
 

789
00:14:20,440 --> 00:14:25,030
 on variational Bayes approximations but

790
00:14:22,860 --> 00:14:25,030
 

791
00:14:22,870 --> 00:14:26,500
 there are actually these kind of

792
00:14:25,020 --> 00:14:26,500
 

793
00:14:25,030 --> 00:14:28,360
 classical approximations that might

794
00:14:26,490 --> 00:14:28,360
 

795
00:14:26,500 --> 00:14:29,740
 might even often do better than

796
00:14:28,350 --> 00:14:29,740
 

797
00:14:28,360 --> 00:14:31,750
 variational Bayes approximations

798
00:14:29,730 --> 00:14:31,750
 

799
00:14:29,740 --> 00:14:33,520
 particularly in terms of uncertainty

800
00:14:31,740 --> 00:14:33,520
 

801
00:14:31,750 --> 00:14:36,070
 quantification and one is this just sort

802
00:14:33,510 --> 00:14:36,070
 

803
00:14:33,520 --> 00:14:40,570
 of good old large sample Gaussian

804
00:14:36,060 --> 00:14:40,570
 

805
00:14:36,070 --> 00:14:42,040
 approximation okay what happens is if

806
00:14:40,560 --> 00:14:42,040
 

807
00:14:40,570 --> 00:14:44,050
 you have under some regularity

808
00:14:42,030 --> 00:14:44,050
 

809
00:14:42,040 --> 00:14:46,630
 conditions that I'll sketch sketch real

810
00:14:44,040 --> 00:14:46,630
 

811
00:14:44,050 --> 00:14:48,910
 quickly if you have a big enough sample

812
00:14:46,620 --> 00:14:48,910
 

813
00:14:46,630 --> 00:14:50,710
 size you know and so that's what we're

814
00:14:48,900 --> 00:14:50,710
 

815
00:14:48,910 --> 00:14:53,200
 worried about we have a huge sample size

816
00:14:50,700 --> 00:14:53,200
 

817
00:14:50,710 --> 00:14:55,630
 how do we do computation well often if

818
00:14:53,190 --> 00:14:55,630
 

819
00:14:53,200 --> 00:14:57,100
 the models not that complicated then the

820
00:14:55,620 --> 00:14:57,100
 

821
00:14:55,630 --> 00:14:59,020
 posterior is gonna be approximately

822
00:14:57,090 --> 00:14:59,020
 

823
00:14:57,100 --> 00:15:00,970
 Gaussian anyway by via what's called the

824
00:14:59,010 --> 00:15:00,970
 

825
00:14:59,020 --> 00:15:03,070
 Bayesian central limit theorem often

826
00:15:00,960 --> 00:15:03,070
 

827
00:15:00,970 --> 00:15:05,890
 known as Bernstein von Mises theorem or

828
00:15:03,060 --> 00:15:05,890
 

829
00:15:03,070 --> 00:15:08,340
 BVM theorems are well as the sample size

830
00:15:05,880 --> 00:15:08,340
 

831
00:15:05,890 --> 00:15:11,710
 get big we get in approximately a

832
00:15:08,330 --> 00:15:11,710
 

833
00:15:08,340 --> 00:15:13,420
 Gaussian elimination and we can easily

834
00:15:11,700 --> 00:15:13,420
 

835
00:15:11,710 --> 00:15:16,720
 calculate the mean and the covariance of

836
00:15:13,410 --> 00:15:16,720
 

837
00:15:13,420 --> 00:15:18,640
 the Gaussian okay and so this is relying

838
00:15:16,710 --> 00:15:18,640
 

839
00:15:16,720 --> 00:15:20,560
 on the sample size n being large

840
00:15:18,630 --> 00:15:20,560
 

841
00:15:18,640 --> 00:15:22,930
 relative to the number of parameters P

842
00:15:20,550 --> 00:15:22,930
 

843
00:15:20,560 --> 00:15:24,760
 um the likely it has to be smooth and

844
00:15:22,920 --> 00:15:24,760
 

845
00:15:22,930 --> 00:15:26,620
 differentiable particularly around some

846
00:15:24,750 --> 00:15:26,620
 

847
00:15:24,760 --> 00:15:28,570
 sort of true value theta not in the

848
00:15:26,610 --> 00:15:28,570
 

849
00:15:26,620 --> 00:15:30,160
 interior of the parameter space and that

850
00:15:28,560 --> 00:15:30,160
 

851
00:15:28,570 --> 00:15:32,320
 really doesn't need to be the true value

852
00:15:30,150 --> 00:15:32,320
 

853
00:15:30,160 --> 00:15:34,150
 it needs to be essentially it's going to

854
00:15:32,310 --> 00:15:34,150
 

855
00:15:32,320 --> 00:15:36,170
 be concentrating around a theta naught

856
00:15:34,140 --> 00:15:36,170
 

857
00:15:34,150 --> 00:15:38,810
 which is the best value

858
00:15:36,160 --> 00:15:38,810
 

859
00:15:36,170 --> 00:15:40,340
 of theta in the parametric class under

860
00:15:38,800 --> 00:15:40,340
 

861
00:15:38,810 --> 00:15:41,510
 consideration and so if we had some

862
00:15:40,330 --> 00:15:41,510
 

863
00:15:40,340 --> 00:15:44,240
 likelihood function that we're

864
00:15:41,500 --> 00:15:44,240
 

865
00:15:41,510 --> 00:15:45,830
 parameterizing L of Y given theta it

866
00:15:44,230 --> 00:15:45,830
 

867
00:15:44,240 --> 00:15:47,630
 might be that the truth is outside of

868
00:15:45,820 --> 00:15:47,630
 

869
00:15:45,830 --> 00:15:50,690
 that class but there's some minimal KL

870
00:15:47,620 --> 00:15:50,690
 

871
00:15:47,630 --> 00:15:52,400
 point outside of the class I mean to the

872
00:15:50,680 --> 00:15:52,400
 

873
00:15:50,690 --> 00:15:55,310
 true data generate model and then that

874
00:15:52,390 --> 00:15:55,310
 

875
00:15:52,400 --> 00:15:56,870
 theta not within the class that's at the

876
00:15:55,300 --> 00:15:56,870
 

877
00:15:55,310 --> 00:15:58,580
 minimal Cal point that's where the

878
00:15:56,860 --> 00:15:58,580
 

879
00:15:56,870 --> 00:16:00,740
 posterior is going to concentrate okay

880
00:15:58,570 --> 00:16:00,740
 

881
00:15:58,580 --> 00:16:02,180
 and it's gonna be eventually Gaussian

882
00:16:00,730 --> 00:16:02,180
 

883
00:16:00,740 --> 00:16:03,310
 under some of these types of conditions

884
00:16:02,170 --> 00:16:03,310
 

885
00:16:02,180 --> 00:16:06,110
 okay

886
00:16:03,300 --> 00:16:06,110
 

887
00:16:03,310 --> 00:16:08,060
 I'd say like I would I would say in most

888
00:16:06,100 --> 00:16:08,060
 

889
00:16:06,110 --> 00:16:10,370
 of the literature that I've looked at on

890
00:16:08,050 --> 00:16:10,370
 

891
00:16:08,060 --> 00:16:12,320
 scalable Bayesian inference it's

892
00:16:10,360 --> 00:16:12,320
 

893
00:16:10,370 --> 00:16:14,030
 probably on all of these conditions

894
00:16:12,310 --> 00:16:14,030
 

895
00:16:12,320 --> 00:16:16,550
 essentially hold and we might have a

896
00:16:14,020 --> 00:16:16,550
 

897
00:16:14,030 --> 00:16:18,590
 large sample Gaussian where we did some

898
00:16:16,540 --> 00:16:18,590
 

899
00:16:16,550 --> 00:16:20,600
 sort of you know divide and conquer

900
00:16:18,580 --> 00:16:20,600
 

901
00:16:18,590 --> 00:16:22,850
 matrix tricks to calculate the mean and

902
00:16:20,590 --> 00:16:22,850
 

903
00:16:20,600 --> 00:16:25,540
 covariance might actually be competitive

904
00:16:22,840 --> 00:16:25,540
 

905
00:16:22,850 --> 00:16:28,100
 or better than many of the methods okay

906
00:16:25,530 --> 00:16:28,100
 

907
00:16:25,540 --> 00:16:31,280
 another class of approximations which is

908
00:16:28,090 --> 00:16:31,280
 

909
00:16:28,100 --> 00:16:33,620
 really closely related to the to the

910
00:16:31,270 --> 00:16:33,620
 

911
00:16:31,280 --> 00:16:35,450
 large sample Gaussian approximation is

912
00:16:33,610 --> 00:16:35,450
 

913
00:16:33,620 --> 00:16:38,090
 the Laplace approximation and so we

914
00:16:35,440 --> 00:16:38,090
 

915
00:16:35,450 --> 00:16:39,950
 could take this posterior back here and

916
00:16:38,080 --> 00:16:39,950
 

917
00:16:38,090 --> 00:16:42,440
 then we'd say oh well here's a hard

918
00:16:39,940 --> 00:16:42,440
 

919
00:16:39,950 --> 00:16:44,360
 integral that we can't calculate really

920
00:16:42,430 --> 00:16:44,360
 

921
00:16:42,440 --> 00:16:46,190
 easily and so we could use Laplace's

922
00:16:44,350 --> 00:16:46,190
 

923
00:16:44,360 --> 00:16:48,260
 method to approximate that integral and

924
00:16:46,180 --> 00:16:48,260
 

925
00:16:46,190 --> 00:16:50,330
 we end up with essentially a large

926
00:16:48,250 --> 00:16:50,330
 

927
00:16:48,260 --> 00:16:52,100
 sample Gaussian type approximation to

928
00:16:50,320 --> 00:16:52,100
 

929
00:16:50,330 --> 00:16:54,200
 the posterior and that's called the

930
00:16:52,090 --> 00:16:54,200
 

931
00:16:52,100 --> 00:16:55,700
 Laplace of proximation and there's a

932
00:16:54,190 --> 00:16:55,700
 

933
00:16:54,200 --> 00:16:58,310
 really big literature on laplace

934
00:16:55,690 --> 00:16:58,310
 

935
00:16:55,700 --> 00:17:00,050
 approximations often they do remarkably

936
00:16:58,300 --> 00:17:00,050
 

937
00:16:58,310 --> 00:17:01,610
 well they do a good job it's sort of

938
00:17:00,040 --> 00:17:01,610
 

939
00:17:00,050 --> 00:17:03,380
 characterizing the first and second

940
00:17:01,600 --> 00:17:03,380
 

941
00:17:01,610 --> 00:17:05,660
 moment of the posterior whereas many

942
00:17:03,370 --> 00:17:05,660
 

943
00:17:03,380 --> 00:17:08,030
 variational methods might butcher the

944
00:17:05,650 --> 00:17:08,030
 

945
00:17:05,660 --> 00:17:09,500
 the second moment you know they're not

946
00:17:08,020 --> 00:17:09,500
 

947
00:17:08,030 --> 00:17:11,449
 going to be do a good job at all if the

948
00:17:09,490 --> 00:17:11,449
 

949
00:17:09,500 --> 00:17:14,240
 posterior is like multimodal or skewed

950
00:17:11,439 --> 00:17:14,240
 

951
00:17:11,449 --> 00:17:15,890
 or weird but you know large samples if

952
00:17:14,230 --> 00:17:15,890
 

953
00:17:14,240 --> 00:17:17,870
 we have a you know not too complicated

954
00:17:15,880 --> 00:17:17,870
 

955
00:17:15,890 --> 00:17:19,640
 model then then this kind of good old

956
00:17:17,860 --> 00:17:19,640
 

957
00:17:17,870 --> 00:17:22,040
 Laplace approximation which has been

958
00:17:19,630 --> 00:17:22,040
 

959
00:17:19,640 --> 00:17:23,810
 around for a really long time can do can

960
00:17:22,030 --> 00:17:23,810
 

961
00:17:22,040 --> 00:17:25,730
 do remarkably well and I would encourage

962
00:17:23,800 --> 00:17:25,730
 

963
00:17:23,810 --> 00:17:27,709
 people working in scalable Bayes

964
00:17:25,720 --> 00:17:27,709
 

965
00:17:25,730 --> 00:17:29,660
 inference to compare to the Laplace

966
00:17:27,699 --> 00:17:29,660
 

967
00:17:27,709 --> 00:17:31,610
 approximation which often they don't and

968
00:17:29,650 --> 00:17:31,610
 

969
00:17:29,660 --> 00:17:33,260
 probably I could define a scalable

970
00:17:31,600 --> 00:17:33,260
 

971
00:17:31,610 --> 00:17:35,140
 Laplace approximation that beats your

972
00:17:33,250 --> 00:17:35,140
 

973
00:17:33,260 --> 00:17:38,390
 variational proximation

974
00:17:35,130 --> 00:17:38,390
 

975
00:17:35,140 --> 00:17:40,910
 okay so that's the kind of really good

976
00:17:38,380 --> 00:17:40,910
 

977
00:17:38,390 --> 00:17:42,710
 classic kind of old-school Bayesian

978
00:17:40,900 --> 00:17:42,710
 

979
00:17:40,910 --> 00:17:44,630
 approximations and large sample sizes

980
00:17:42,700 --> 00:17:44,630
 

981
00:17:42,710 --> 00:17:47,790
 okay and we could certainly define

982
00:17:44,620 --> 00:17:47,790
 

983
00:17:44,630 --> 00:17:50,440
 scalable versions of them

984
00:17:47,780 --> 00:17:50,440
 

985
00:17:47,790 --> 00:17:53,040
 so what could we do instead well it's an

986
00:17:50,430 --> 00:17:53,040
 

987
00:17:50,440 --> 00:17:55,360
 alternative to these kind of really old

988
00:17:53,030 --> 00:17:55,360
 

989
00:17:53,040 --> 00:17:57,460
 approximations we could we could think

990
00:17:55,350 --> 00:17:57,460
 

991
00:17:55,360 --> 00:17:59,890
 of defining some type of approximating

992
00:17:57,450 --> 00:17:59,890
 

993
00:17:57,460 --> 00:18:01,990
 class q of theta so our posterior

994
00:17:59,880 --> 00:18:01,990
 

995
00:17:59,890 --> 00:18:05,620
 distribution apply is PI of theta given

996
00:18:01,980 --> 00:18:05,620
 

997
00:18:01,990 --> 00:18:08,380
 Y and we we that's intractable and so

998
00:18:05,610 --> 00:18:08,380
 

999
00:18:05,620 --> 00:18:09,760
 we'd like to get close close to it so

1000
00:18:08,370 --> 00:18:09,760
 

1001
00:18:08,380 --> 00:18:12,550
 let's say well let's approximate it with

1002
00:18:09,750 --> 00:18:12,550
 

1003
00:18:09,760 --> 00:18:15,460
 some some some you know tractable class

1004
00:18:12,540 --> 00:18:15,460
 

1005
00:18:12,550 --> 00:18:17,500
 q of theta and then q of theta might be

1006
00:18:15,450 --> 00:18:17,500
 

1007
00:18:15,460 --> 00:18:19,210
 something like a product of exponential

1008
00:18:17,490 --> 00:18:19,210
 

1009
00:18:17,500 --> 00:18:22,210
 family distributions parameterize by

1010
00:18:19,200 --> 00:18:22,210
 

1011
00:18:19,210 --> 00:18:23,710
 some you know working parameters i which

1012
00:18:22,200 --> 00:18:23,710
 

1013
00:18:22,210 --> 00:18:25,030
 is just an algorithmic parameter

1014
00:18:23,700 --> 00:18:25,030
 

1015
00:18:23,710 --> 00:18:27,610
 controlling the accuracy of the

1016
00:18:25,020 --> 00:18:27,610
 

1017
00:18:25,030 --> 00:18:29,320
 approximation and then we could think of

1018
00:18:27,600 --> 00:18:29,320
 

1019
00:18:27,610 --> 00:18:32,620
 to define some type of discrepancy

1020
00:18:29,310 --> 00:18:32,620
 

1021
00:18:29,320 --> 00:18:35,410
 between Q and and pi the the target

1022
00:18:32,610 --> 00:18:35,410
 

1023
00:18:32,620 --> 00:18:36,790
 posterior distribution and and if we can

1024
00:18:35,400 --> 00:18:36,790
 

1025
00:18:35,410 --> 00:18:39,760
 then define some type of optimization

1026
00:18:36,780 --> 00:18:39,760
 

1027
00:18:36,790 --> 00:18:42,070
 problem to minimize this discrepancy the

1028
00:18:39,750 --> 00:18:42,070
 

1029
00:18:39,760 --> 00:18:44,530
 resulting q hat might give us a decent

1030
00:18:42,060 --> 00:18:44,530
 

1031
00:18:42,070 --> 00:18:47,200
 approximation and in variational Bayes

1032
00:18:44,520 --> 00:18:47,200
 

1033
00:18:44,530 --> 00:18:49,630
 is one if one flavor of this but but

1034
00:18:47,190 --> 00:18:49,630
 

1035
00:18:47,200 --> 00:18:50,800
 also expectation propagation and related

1036
00:18:49,620 --> 00:18:50,800
 

1037
00:18:49,630 --> 00:18:52,260
 methods there's been a really

1038
00:18:50,790 --> 00:18:52,260
 

1039
00:18:50,800 --> 00:18:55,750
 interesting recent literature

1040
00:18:52,250 --> 00:18:55,750
 

1041
00:18:52,260 --> 00:18:58,330
 generalizing variational Bayes to to use

1042
00:18:55,740 --> 00:18:58,330
 

1043
00:18:55,750 --> 00:19:00,640
 Bregman divergences of alpha divergences

1044
00:18:58,320 --> 00:19:00,640
 

1045
00:18:58,330 --> 00:19:01,870
 instead of coal-black lobular and I'm

1046
00:19:00,630 --> 00:19:01,870
 

1047
00:19:00,640 --> 00:19:03,610
 not going to talk about that stuff very

1048
00:19:01,860 --> 00:19:03,610
 

1049
00:19:01,870 --> 00:19:06,520
 much today but it's a quite interesting

1050
00:19:03,600 --> 00:19:06,520
 

1051
00:19:03,610 --> 00:19:08,770
 area in terms of variational days I

1052
00:19:06,510 --> 00:19:08,770
 

1053
00:19:06,520 --> 00:19:12,510
 would I would encourage you to look at

1054
00:19:08,760 --> 00:19:12,510
 

1055
00:19:08,770 --> 00:19:14,730
 tamra Broderick's really nice ICML 2018

1056
00:19:12,500 --> 00:19:14,730
 

1057
00:19:12,510 --> 00:19:16,990
 tutorial where she gives us sort of

1058
00:19:14,720 --> 00:19:16,990
 

1059
00:19:14,730 --> 00:19:19,780
 state-of-the-art overview on on

1060
00:19:16,980 --> 00:19:19,780
 

1061
00:19:16,990 --> 00:19:21,160
 variational methods okay and so the

1062
00:19:19,770 --> 00:19:21,160
 

1063
00:19:19,780 --> 00:19:22,990
 variational methods are based on

1064
00:19:21,150 --> 00:19:22,990
 

1065
00:19:21,160 --> 00:19:24,610
 maximizing a lower bound discarding an

1066
00:19:22,980 --> 00:19:24,610
 

1067
00:19:22,990 --> 00:19:27,250
 intractable term in the coal-black libor

1068
00:19:24,600 --> 00:19:27,250
 

1069
00:19:24,610 --> 00:19:29,020
 divergence the reason that I'm not going

1070
00:19:27,240 --> 00:19:29,020
 

1071
00:19:27,250 --> 00:19:31,480
 to talk about variational Bayes methods

1072
00:19:29,010 --> 00:19:31,480
 

1073
00:19:29,020 --> 00:19:34,090
 today is that to me the whole the whole

1074
00:19:31,470 --> 00:19:34,090
 

1075
00:19:31,480 --> 00:19:35,980
 beauty and and you know reason that I'm

1076
00:19:34,080 --> 00:19:35,980
 

1077
00:19:34,090 --> 00:19:37,590
 interested in this Bayesian formulation

1078
00:19:35,970 --> 00:19:37,590
 

1079
00:19:35,980 --> 00:19:40,120
 is I'd like to do a really good job

1080
00:19:37,580 --> 00:19:40,120
 

1081
00:19:37,590 --> 00:19:41,890
 characterizing uncertainty okay

1082
00:19:40,110 --> 00:19:41,890
 

1083
00:19:40,120 --> 00:19:43,450
 and if I have a variational Bayes

1084
00:19:41,880 --> 00:19:43,450
 

1085
00:19:41,890 --> 00:19:45,940
 approximation I've thrown out an

1086
00:19:43,440 --> 00:19:45,940
 

1087
00:19:43,450 --> 00:19:47,830
 intractable term and then I do all this

1088
00:19:45,930 --> 00:19:47,830
 

1089
00:19:45,940 --> 00:19:50,200
 then then I don't know what I have I

1090
00:19:47,820 --> 00:19:50,200
 

1091
00:19:47,830 --> 00:19:52,870
 have some sort of approximation but it's

1092
00:19:50,190 --> 00:19:52,870
 

1093
00:19:50,200 --> 00:19:55,540
 to me I'm I'm you know a mathematical

1094
00:19:52,860 --> 00:19:55,540
 

1095
00:19:52,870 --> 00:19:57,760
 statistician partly and I'd like to know

1096
00:19:55,530 --> 00:19:57,760
 

1097
00:19:55,540 --> 00:19:59,470
 if I if I'm calling an approximation I

1098
00:19:57,750 --> 00:19:59,470
 

1099
00:19:57,760 --> 00:20:01,130
 have to have some sort of guarantees

1100
00:19:59,460 --> 00:20:01,130
 

1101
00:19:59,470 --> 00:20:02,570
 that is actually approximating

1102
00:20:01,120 --> 00:20:02,570
 

1103
00:20:01,130 --> 00:20:05,330
 otherwise it's like just something else

1104
00:20:02,560 --> 00:20:05,330
 

1105
00:20:02,570 --> 00:20:07,790
 so I have something else which is a Q of

1106
00:20:05,320 --> 00:20:07,790
 

1107
00:20:05,330 --> 00:20:09,170
 theta which is in no sense really an

1108
00:20:07,780 --> 00:20:09,170
 

1109
00:20:07,790 --> 00:20:11,210
 approximation to a posterior

1110
00:20:09,160 --> 00:20:11,210
 

1111
00:20:09,170 --> 00:20:12,770
 distribution and is really no sense

1112
00:20:11,200 --> 00:20:12,770
 

1113
00:20:11,210 --> 00:20:14,720
 Bayesian it might be useful from a

1114
00:20:12,760 --> 00:20:14,720
 

1115
00:20:12,770 --> 00:20:16,670
 machine learning perspective but in most

1116
00:20:14,710 --> 00:20:16,670
 

1117
00:20:14,720 --> 00:20:17,930
 settings I've actually no clue at all

1118
00:20:16,660 --> 00:20:17,930
 

1119
00:20:16,670 --> 00:20:20,840
 how well it's doing

1120
00:20:17,920 --> 00:20:20,840
 

1121
00:20:17,930 --> 00:20:22,430
 I could I could see but 99% of the time

1122
00:20:20,830 --> 00:20:22,430
 

1123
00:20:20,840 --> 00:20:23,690
 when people use it they don't even look

1124
00:20:22,420 --> 00:20:23,690
 

1125
00:20:22,430 --> 00:20:25,670
 at how well it's doing in terms of

1126
00:20:23,680 --> 00:20:25,670
 

1127
00:20:23,690 --> 00:20:26,870
 uncertainty quantification so I become

1128
00:20:25,660 --> 00:20:26,870
 

1129
00:20:25,670 --> 00:20:28,730
 quite skeptical of these kind of

1130
00:20:26,860 --> 00:20:28,730
 

1131
00:20:26,870 --> 00:20:30,440
 variational Bayes bay based AI methods

1132
00:20:28,720 --> 00:20:30,440
 

1133
00:20:28,730 --> 00:20:35,150
 for those reasons because we don't know

1134
00:20:30,430 --> 00:20:35,150
 

1135
00:20:30,440 --> 00:20:37,370
 that they're accurate at all okay there

1136
00:20:35,140 --> 00:20:37,370
 

1137
00:20:35,150 --> 00:20:40,370
 is some not really nice literature Mike

1138
00:20:37,360 --> 00:20:40,370
 

1139
00:20:37,370 --> 00:20:42,350
 Jordan and Tamra Broderick and Giordano

1140
00:20:40,360 --> 00:20:42,350
 

1141
00:20:40,370 --> 00:20:44,960
 have this nice approach I really like

1142
00:20:42,340 --> 00:20:44,960
 

1143
00:20:42,350 --> 00:20:46,490
 which kind of fixes up the uncertainty

1144
00:20:44,950 --> 00:20:46,490
 

1145
00:20:44,960 --> 00:20:48,830
 to a kind of using kind of linear

1146
00:20:46,480 --> 00:20:48,830
 

1147
00:20:46,490 --> 00:20:50,630
 approximation locally so you can think

1148
00:20:48,820 --> 00:20:50,630
 

1149
00:20:48,830 --> 00:20:53,060
 of it it's almost like taking

1150
00:20:50,620 --> 00:20:53,060
 

1151
00:20:50,630 --> 00:20:54,770
 variational Bayes and then taking the

1152
00:20:53,050 --> 00:20:54,770
 

1153
00:20:53,060 --> 00:20:56,510
 Laplace approximation and losing a

1154
00:20:54,760 --> 00:20:56,510
 

1155
00:20:54,770 --> 00:20:58,550
 Laplace approximation to kind of fix up

1156
00:20:56,500 --> 00:20:58,550
 

1157
00:20:56,510 --> 00:21:00,410
 variational base it's not exactly that

1158
00:20:58,540 --> 00:21:00,410
 

1159
00:20:58,550 --> 00:21:03,590
 but that's the kind of flavor of what

1160
00:21:00,400 --> 00:21:03,590
 

1161
00:21:00,410 --> 00:21:04,820
 what you can do so these types of fix

1162
00:21:03,580 --> 00:21:04,820
 

1163
00:21:03,590 --> 00:21:08,120
 ups can improve the variance

1164
00:21:04,810 --> 00:21:08,120
 

1165
00:21:04,820 --> 00:21:10,490
 characterization in a local mode but you

1166
00:21:08,110 --> 00:21:10,490
 

1167
00:21:08,120 --> 00:21:13,190
 don't know other than that I'd also like

1168
00:21:10,480 --> 00:21:13,190
 

1169
00:21:10,490 --> 00:21:14,960
 to highlight a recent article maybe I'm

1170
00:21:13,180 --> 00:21:14,960
 

1171
00:21:13,190 --> 00:21:17,360
 biased towards this article since it's

1172
00:21:14,950 --> 00:21:17,360
 

1173
00:21:14,960 --> 00:21:18,920
 from three of my former students deputy

1174
00:21:17,350 --> 00:21:18,920
 

1175
00:21:17,360 --> 00:21:21,170
 patty on our blonde Bhattacharyya and

1176
00:21:18,910 --> 00:21:21,170
 

1177
00:21:18,920 --> 00:21:23,840
 yin yang have this really nice article

1178
00:21:21,160 --> 00:21:23,840
 

1179
00:21:21,170 --> 00:21:26,240
 showing really strong theoretical

1180
00:21:23,830 --> 00:21:26,240
 

1181
00:21:23,840 --> 00:21:28,010
 guarantees on statistical optimality of

1182
00:21:26,230 --> 00:21:28,010
 

1183
00:21:26,240 --> 00:21:30,860
 variational Bayes but they're doing it

1184
00:21:28,000 --> 00:21:30,860
 

1185
00:21:28,010 --> 00:21:33,110
 from a point estimation perspective and

1186
00:21:30,850 --> 00:21:33,110
 

1187
00:21:30,860 --> 00:21:35,060
 so they can show that a variational

1188
00:21:33,100 --> 00:21:35,060
 

1189
00:21:33,110 --> 00:21:37,240
 posterior will actually concentrate in a

1190
00:21:35,050 --> 00:21:37,240
 

1191
00:21:35,060 --> 00:21:40,370
 very optimal way giving you a wonderful

1192
00:21:37,230 --> 00:21:40,370
 

1193
00:21:37,240 --> 00:21:43,130
 point estimate but not showing that the

1194
00:21:40,360 --> 00:21:43,130
 

1195
00:21:40,370 --> 00:21:44,600
 variance is right actually they find

1196
00:21:43,120 --> 00:21:44,600
 

1197
00:21:43,130 --> 00:21:47,420
 that the variance is often very wrong

1198
00:21:44,590 --> 00:21:47,420
 

1199
00:21:44,600 --> 00:21:50,330
 okay okay so there's no theory on

1200
00:21:47,410 --> 00:21:50,330
 

1201
00:21:47,420 --> 00:21:52,340
 accuracy of UQ and so for this reason

1202
00:21:50,320 --> 00:21:52,340
 

1203
00:21:50,330 --> 00:21:54,800
 I'm gonna spend most of the talk really

1204
00:21:52,330 --> 00:21:54,800
 

1205
00:21:52,340 --> 00:21:56,570
 on on Markov chain Monte Carlo methods

1206
00:21:54,790 --> 00:21:56,570
 

1207
00:21:54,800 --> 00:21:59,750
 and it's been interesting to me that

1208
00:21:56,560 --> 00:21:59,750
 

1209
00:21:56,570 --> 00:22:01,760
 Markov chain Monte Carlo is this sort of

1210
00:21:59,740 --> 00:22:01,760
 

1211
00:21:59,750 --> 00:22:03,920
 that now and kind of old-school method

1212
00:22:01,750 --> 00:22:03,920
 

1213
00:22:01,760 --> 00:22:05,840
 for doing Bayesian computation and I

1214
00:22:03,910 --> 00:22:05,840
 

1215
00:22:03,920 --> 00:22:07,610
 often find I hear a lot of talks where

1216
00:22:05,830 --> 00:22:07,610
 

1217
00:22:05,840 --> 00:22:09,080
 people will just discount it entirely

1218
00:22:07,600 --> 00:22:09,080
 

1219
00:22:07,610 --> 00:22:10,750
 they'll say well I'm simply is not

1220
00:22:09,070 --> 00:22:10,750
 

1221
00:22:09,080 --> 00:22:12,620
 scalable so I'm going to use this

1222
00:22:10,740 --> 00:22:12,620
 

1223
00:22:10,750 --> 00:22:14,270
 variational approximation that I know

1224
00:22:12,610 --> 00:22:14,270
 

1225
00:22:12,620 --> 00:22:14,870
 nothing about but at least I can compute

1226
00:22:14,260 --> 00:22:14,870
 

1227
00:22:14,270 --> 00:22:17,059
 it

1228
00:22:14,860 --> 00:22:17,059
 

1229
00:22:14,870 --> 00:22:19,370
 but actually there's now a really really

1230
00:22:17,049 --> 00:22:19,370
 

1231
00:22:17,059 --> 00:22:21,620
 rich and beautiful literature emerging

1232
00:22:19,360 --> 00:22:21,620
 

1233
00:22:19,370 --> 00:22:23,720
 on how we can scale up Markov chain

1234
00:22:21,610 --> 00:22:23,720
 

1235
00:22:21,620 --> 00:22:26,059
 Monte Carlo and I'd say it's just not

1236
00:22:23,710 --> 00:22:26,059
 

1237
00:22:23,720 --> 00:22:27,380
 the case anymore you you should not be

1238
00:22:26,049 --> 00:22:27,380
 

1239
00:22:26,059 --> 00:22:29,180
 making the statement the Markov chain

1240
00:22:27,370 --> 00:22:29,180
 

1241
00:22:27,380 --> 00:22:31,100
 Monte Carlo is not scalable

1242
00:22:29,170 --> 00:22:31,100
 

1243
00:22:29,180 --> 00:22:33,140
 it's like saying anything else is not

1244
00:22:31,090 --> 00:22:33,140
 

1245
00:22:31,100 --> 00:22:34,610
 scalable oh well the EM algorithm is not

1246
00:22:33,130 --> 00:22:34,610
 

1247
00:22:33,140 --> 00:22:37,130
 scalable well that's because you're not

1248
00:22:34,600 --> 00:22:37,130
 

1249
00:22:34,610 --> 00:22:38,900
 using a scalable version okay so if you

1250
00:22:37,120 --> 00:22:38,900
 

1251
00:22:37,130 --> 00:22:40,430
 just use the naive version of Markov

1252
00:22:38,890 --> 00:22:40,430
 

1253
00:22:38,900 --> 00:22:42,559
 chain Monte Carlo yeah it can be really

1254
00:22:40,420 --> 00:22:42,559
 

1255
00:22:40,430 --> 00:22:45,110
 bloody slow but now we have a lot of

1256
00:22:42,549 --> 00:22:45,110
 

1257
00:22:42,559 --> 00:22:47,120
 tricks for scaling up Markov chain Monte

1258
00:22:45,100 --> 00:22:47,120
 

1259
00:22:45,110 --> 00:22:49,400
 Carlo and there's an emerging beautiful

1260
00:22:47,110 --> 00:22:49,400
 

1261
00:22:47,120 --> 00:22:51,320
 theoretical and impractical literature

1262
00:22:49,390 --> 00:22:51,320
 

1263
00:22:49,400 --> 00:22:54,110
 that that I hope I can inspire some of

1264
00:22:51,310 --> 00:22:54,110
 

1265
00:22:51,320 --> 00:22:54,860
 you to to add to in the in the coming

1266
00:22:54,100 --> 00:22:54,860
 

1267
00:22:54,110 --> 00:22:57,559
 years okay

1268
00:22:54,850 --> 00:22:57,559
 

1269
00:22:54,860 --> 00:22:58,970
 and so um the one reason that Markov

1270
00:22:57,549 --> 00:22:58,970
 

1271
00:22:57,559 --> 00:23:01,460
 chain Monte Carlo is kind of remained

1272
00:22:58,960 --> 00:23:01,460
 

1273
00:22:58,970 --> 00:23:04,280
 popular is that these analytic

1274
00:23:01,450 --> 00:23:04,280
 

1275
00:23:01,460 --> 00:23:05,840
 approximations like variational Bayes we

1276
00:23:04,270 --> 00:23:05,840
 

1277
00:23:04,280 --> 00:23:07,580
 can't really show that they do well and

1278
00:23:05,830 --> 00:23:07,580
 

1279
00:23:05,840 --> 00:23:10,030
 they they might don't not do well after

1280
00:23:07,570 --> 00:23:10,030
 

1281
00:23:07,580 --> 00:23:12,590
 out of narrow outsider narrow settings

1282
00:23:10,020 --> 00:23:12,590
 

1283
00:23:10,030 --> 00:23:14,090
 so Markov chain Monte Carlo and other

1284
00:23:12,580 --> 00:23:14,090
 

1285
00:23:12,590 --> 00:23:17,300
 posterior sampling algorithms provide

1286
00:23:14,080 --> 00:23:17,300
 

1287
00:23:14,090 --> 00:23:19,820
 provide an alternative okay and so I'm

1288
00:23:17,290 --> 00:23:19,820
 

1289
00:23:17,300 --> 00:23:21,800
 so MCMC what the game is and the kind of

1290
00:23:19,810 --> 00:23:21,800
 

1291
00:23:19,820 --> 00:23:25,760
 beauty behind it in some sense is that

1292
00:23:21,790 --> 00:23:25,760
 

1293
00:23:21,800 --> 00:23:28,100
 we can take we can take sampling we have

1294
00:23:25,750 --> 00:23:28,100
 

1295
00:23:25,760 --> 00:23:29,900
 a sampling algorithm and we set up a

1296
00:23:28,090 --> 00:23:29,900
 

1297
00:23:28,100 --> 00:23:31,940
 remarkably simple sampling algorithm

1298
00:23:29,890 --> 00:23:31,940
 

1299
00:23:29,900 --> 00:23:33,920
 where the the the stationary

1300
00:23:31,930 --> 00:23:33,920
 

1301
00:23:31,940 --> 00:23:36,050
 distribution of the samples is the true

1302
00:23:33,910 --> 00:23:36,050
 

1303
00:23:33,920 --> 00:23:37,309
 posterior distribution exactly often

1304
00:23:36,040 --> 00:23:37,309
 

1305
00:23:36,050 --> 00:23:40,040
 though we might put in approximations

1306
00:23:37,299 --> 00:23:40,040
 

1307
00:23:37,309 --> 00:23:42,679
 and so we can we want to the goal is to

1308
00:23:40,030 --> 00:23:42,679
 

1309
00:23:40,040 --> 00:23:44,630
 draw samples not get an analytic

1310
00:23:42,669 --> 00:23:44,630
 

1311
00:23:42,679 --> 00:23:46,280
 approximation but draw samples of PI of

1312
00:23:44,620 --> 00:23:46,280
 

1313
00:23:44,630 --> 00:23:48,740
 theta given Y the posterior distribution

1314
00:23:46,270 --> 00:23:48,740
 

1315
00:23:46,280 --> 00:23:51,290
 and the one reason this is really nice

1316
00:23:48,730 --> 00:23:51,290
 

1317
00:23:48,740 --> 00:23:53,179
 is that let's say we have a complicated

1318
00:23:51,280 --> 00:23:53,179
 

1319
00:23:51,290 --> 00:23:55,490
 analytic approximation of pi of theta

1320
00:23:53,169 --> 00:23:55,490
 

1321
00:23:53,179 --> 00:23:58,130
 given Y well that's really actually not

1322
00:23:55,480 --> 00:23:58,130
 

1323
00:23:55,490 --> 00:24:00,650
 that useful to me because you know theta

1324
00:23:58,120 --> 00:24:00,650
 

1325
00:23:58,130 --> 00:24:02,480
 might be a thousand dimensional I can't

1326
00:24:00,640 --> 00:24:02,480
 

1327
00:24:00,650 --> 00:24:04,640
 visualize it I can't do anything with it

1328
00:24:02,470 --> 00:24:04,640
 

1329
00:24:02,480 --> 00:24:05,900
 and so really I'm not ever usually

1330
00:24:04,630 --> 00:24:05,900
 

1331
00:24:04,640 --> 00:24:08,120
 interested in the full posterior

1332
00:24:05,890 --> 00:24:08,120
 

1333
00:24:05,900 --> 00:24:09,530
 distribution I'm interested in some

1334
00:24:08,110 --> 00:24:09,530
 

1335
00:24:08,120 --> 00:24:11,179
 functionals I want a one a one

1336
00:24:09,520 --> 00:24:11,179
 

1337
00:24:09,530 --> 00:24:12,740
 dimensional functional for example I

1338
00:24:11,169 --> 00:24:12,740
 

1339
00:24:11,179 --> 00:24:16,429
 might want the predictive distribution

1340
00:24:12,730 --> 00:24:16,429
 

1341
00:24:12,740 --> 00:24:17,960
 of Y for a new patient coming in I'm

1342
00:24:16,419 --> 00:24:17,960
 

1343
00:24:16,429 --> 00:24:19,940
 given their features or something like

1344
00:24:17,950 --> 00:24:19,940
 

1345
00:24:17,960 --> 00:24:21,679
 that I'd like that you know where I'd

1346
00:24:19,930 --> 00:24:21,679
 

1347
00:24:19,940 --> 00:24:23,929
 like I'd like to know something about

1348
00:24:21,669 --> 00:24:23,929
 

1349
00:24:21,679 --> 00:24:25,910
 particular functionals of the parameters

1350
00:24:23,919 --> 00:24:25,910
 

1351
00:24:23,929 --> 00:24:27,620
 that are interpretable okay but if I

1352
00:24:25,900 --> 00:24:27,620
 

1353
00:24:25,910 --> 00:24:28,519
 have an analytic approximation I can't

1354
00:24:27,610 --> 00:24:28,519
 

1355
00:24:27,620 --> 00:24:30,799
 get that at all

1356
00:24:28,509 --> 00:24:30,799
 

1357
00:24:28,519 --> 00:24:32,419
 but if I had samples from this then I

1358
00:24:30,789 --> 00:24:32,419
 

1359
00:24:30,799 --> 00:24:34,190
 would have wonderful that I could do a

1360
00:24:32,409 --> 00:24:34,190
 

1361
00:24:32,419 --> 00:24:36,559
 lot I could just draw a bunch of samples

1362
00:24:34,180 --> 00:24:36,559
 

1363
00:24:34,190 --> 00:24:38,719
 I could apply the functional to each

1364
00:24:36,549 --> 00:24:38,719
 

1365
00:24:36,559 --> 00:24:40,219
 sample and then I can calculate any

1366
00:24:38,709 --> 00:24:40,219
 

1367
00:24:38,719 --> 00:24:42,830
 posterior summary I want I can calculate

1368
00:24:40,209 --> 00:24:42,830
 

1369
00:24:40,219 --> 00:24:45,109
 the mean I can calculate an interval I

1370
00:24:42,820 --> 00:24:45,109
 

1371
00:24:42,830 --> 00:24:46,669
 can calculate a kernel smooth density

1372
00:24:45,099 --> 00:24:46,669
 

1373
00:24:45,109 --> 00:24:47,869
 estimate for that for that one

1374
00:24:46,659 --> 00:24:47,869
 

1375
00:24:46,669 --> 00:24:49,489
 dimensional functional that I'm

1376
00:24:47,859 --> 00:24:49,489
 

1377
00:24:47,869 --> 00:24:51,379
 interested in okay and that's usually

1378
00:24:49,479 --> 00:24:51,379
 

1379
00:24:49,489 --> 00:24:53,659
 how people do things in practice and so

1380
00:24:51,369 --> 00:24:53,659
 

1381
00:24:51,379 --> 00:24:55,299
 so samples I don't see that there's a

1382
00:24:53,649 --> 00:24:55,299
 

1383
00:24:53,659 --> 00:24:57,799
 really good substitute for samples

1384
00:24:55,289 --> 00:24:57,799
 

1385
00:24:55,299 --> 00:24:59,929
 samples are not inherently slow or

1386
00:24:57,789 --> 00:24:59,929
 

1387
00:24:57,799 --> 00:25:01,820
 unscalable and they give us an enormous

1388
00:24:59,919 --> 00:25:01,820
 

1389
00:24:59,929 --> 00:25:03,229
 amount of flexibility and type in terms

1390
00:25:01,810 --> 00:25:03,229
 

1391
00:25:01,820 --> 00:25:09,200
 of the types of inferences that we can

1392
00:25:03,219 --> 00:25:09,200
 

1393
00:25:03,229 --> 00:25:11,239
 do okay okay so um one of the beautiful

1394
00:25:09,190 --> 00:25:11,239
 

1395
00:25:09,200 --> 00:25:13,219
 things about Markov chain Monte Carlo is

1396
00:25:11,229 --> 00:25:13,219
 

1397
00:25:11,239 --> 00:25:14,749
 that bypasses the need to approximate

1398
00:25:13,209 --> 00:25:14,749
 

1399
00:25:13,219 --> 00:25:16,820
 the marginal likelihood so that's this

1400
00:25:14,739 --> 00:25:16,820
 

1401
00:25:14,749 --> 00:25:19,309
 dude in the denominator down here he's a

1402
00:25:16,810 --> 00:25:19,309
 

1403
00:25:16,820 --> 00:25:20,929
 horrible creature and so there's all

1404
00:25:19,299 --> 00:25:20,929
 

1405
00:25:19,309 --> 00:25:22,969
 these papers on approximating the

1406
00:25:20,919 --> 00:25:22,969
 

1407
00:25:20,929 --> 00:25:25,639
 marginal likelihood and basically all

1408
00:25:22,959 --> 00:25:25,639
 

1409
00:25:22,969 --> 00:25:27,979
 the algorithms are not very good and and

1410
00:25:25,629 --> 00:25:27,979
 

1411
00:25:25,639 --> 00:25:30,289
 and I'd like to just avoid approximating

1412
00:25:27,969 --> 00:25:30,289
 

1413
00:25:27,979 --> 00:25:32,089
 this entirely and MCMC cleverly does

1414
00:25:30,279 --> 00:25:32,089
 

1415
00:25:30,289 --> 00:25:34,609
 that so we can only we only need to

1416
00:25:32,079 --> 00:25:34,609
 

1417
00:25:32,089 --> 00:25:39,289
 evaluate the the numerator and we the

1418
00:25:34,599 --> 00:25:39,289
 

1419
00:25:34,609 --> 00:25:40,789
 denominator cancels okay yeah and so

1420
00:25:39,279 --> 00:25:40,789
 

1421
00:25:39,289 --> 00:25:42,469
 this is just the point that the samples

1422
00:25:40,779 --> 00:25:42,469
 

1423
00:25:40,789 --> 00:25:48,589
 are often more useful than an analytic

1424
00:25:42,459 --> 00:25:48,589
 

1425
00:25:42,469 --> 00:25:50,570
 form anyway okay okay so MCMC what are

1426
00:25:48,579 --> 00:25:50,570
 

1427
00:25:48,589 --> 00:25:51,829
 we doing so we gonna get MC MC based

1428
00:25:50,560 --> 00:25:51,829
 

1429
00:25:50,570 --> 00:25:54,200
 summaries of the posterior for any

1430
00:25:51,819 --> 00:25:54,200
 

1431
00:25:51,829 --> 00:25:56,149
 functional F of theta as the number of

1432
00:25:54,190 --> 00:25:56,149
 

1433
00:25:54,200 --> 00:25:57,950
 samples increases these summaries become

1434
00:25:56,139 --> 00:25:57,950
 

1435
00:25:56,149 --> 00:25:59,209
 more accurate okay and so they're

1436
00:25:57,940 --> 00:25:59,209
 

1437
00:25:57,950 --> 00:26:03,200
 converging to the true posterior

1438
00:25:59,199 --> 00:26:03,200
 

1439
00:25:59,209 --> 00:26:05,419
 summaries so as sad as MC MC work so MC

1440
00:26:03,190 --> 00:26:05,419
 

1441
00:26:03,200 --> 00:26:07,159
 MC constructs a Markov chain with a

1442
00:26:05,409 --> 00:26:07,159
 

1443
00:26:05,419 --> 00:26:09,919
 stationary distribution that's a true

1444
00:26:07,149 --> 00:26:09,919
 

1445
00:26:07,159 --> 00:26:11,239
 posterior distribution and and to

1446
00:26:09,909 --> 00:26:11,239
 

1447
00:26:09,919 --> 00:26:12,859
 construct the Markov chain we need

1448
00:26:11,229 --> 00:26:12,859
 

1449
00:26:11,239 --> 00:26:14,570
 what's called a transition kernel and

1450
00:26:12,849 --> 00:26:14,570
 

1451
00:26:12,859 --> 00:26:16,729
 that transition kernel needs to follow

1452
00:26:14,560 --> 00:26:16,729
 

1453
00:26:14,570 --> 00:26:19,099
 some types of rules and one of the

1454
00:26:16,719 --> 00:26:19,099
 

1455
00:26:16,729 --> 00:26:20,809
 classes of rules are most of them are

1456
00:26:19,089 --> 00:26:20,809
 

1457
00:26:19,099 --> 00:26:23,419
 within what's called a metropolis

1458
00:26:20,799 --> 00:26:23,419
 

1459
00:26:20,809 --> 00:26:25,039
 Hastings algorithm and the metropolis

1460
00:26:23,409 --> 00:26:25,039
 

1461
00:26:23,419 --> 00:26:27,469
 Hastings algorithm to just give a bit of

1462
00:26:25,029 --> 00:26:27,469
 

1463
00:26:25,039 --> 00:26:29,749
 theory certainly one of the most

1464
00:26:27,459 --> 00:26:29,749
 

1465
00:26:27,469 --> 00:26:31,219
 beautiful algorithms ever devised in my

1466
00:26:29,739 --> 00:26:31,219
 

1467
00:26:29,749 --> 00:26:33,440
 view I'm certainly in the in the last

1468
00:26:31,209 --> 00:26:33,440
 

1469
00:26:31,219 --> 00:26:35,179
 hundred years and and the metropolis

1470
00:26:33,430 --> 00:26:35,179
 

1471
00:26:33,440 --> 00:26:37,879
 Hastings algorithm was originally

1472
00:26:35,169 --> 00:26:37,879
 

1473
00:26:35,179 --> 00:26:39,619
 published in the paper by metropolis a

1474
00:26:37,869 --> 00:26:39,619
 

1475
00:26:37,879 --> 00:26:41,710
 tall and in the 1950s

1476
00:26:39,609 --> 00:26:41,710
 

1477
00:26:39,619 --> 00:26:43,150
 of people working on the on the ball

1478
00:26:41,700 --> 00:26:43,150
 

1479
00:26:41,710 --> 00:26:47,020
 and then there was this beautiful paper

1480
00:26:43,140 --> 00:26:47,020
 

1481
00:26:43,150 --> 00:26:48,790
 by Hastings in 1970 which which kind of

1482
00:26:47,010 --> 00:26:48,790
 

1483
00:26:47,020 --> 00:26:50,530
 took it and made it much broader and

1484
00:26:48,780 --> 00:26:50,530
 

1485
00:26:48,790 --> 00:26:51,730
 that's the kind of give us giving us the

1486
00:26:50,520 --> 00:26:51,730
 

1487
00:26:50,530 --> 00:26:53,650
 kind of modern class a metropolis

1488
00:26:51,720 --> 00:26:53,650
 

1489
00:26:51,730 --> 00:26:54,460
 Hastings algorithms and and they're

1490
00:26:53,640 --> 00:26:54,460
 

1491
00:26:53,650 --> 00:26:56,590
 gonna have they're gonna have a

1492
00:26:54,450 --> 00:26:56,590
 

1493
00:26:54,460 --> 00:26:58,150
 celebration of the metropolis Hastings

1494
00:26:56,580 --> 00:26:58,150
 

1495
00:26:56,590 --> 00:27:01,720
 algorithm which was published in

1496
00:26:58,140 --> 00:27:01,720
 

1497
00:26:58,150 --> 00:27:03,610
 biometrika in in 1970 the 50th year

1498
00:27:01,710 --> 00:27:03,610
 

1499
00:27:01,720 --> 00:27:05,440
 anniversary I'm gonna be writing a kind

1500
00:27:03,600 --> 00:27:05,440
 

1501
00:27:03,610 --> 00:27:07,750
 of special paper I'm highlighting that

1502
00:27:05,430 --> 00:27:07,750
 

1503
00:27:05,440 --> 00:27:09,700
 and biometrika which should appear next

1504
00:27:07,740 --> 00:27:09,700
 

1505
00:27:07,750 --> 00:27:12,190
 year and so what the metropolis Hastings

1506
00:27:09,690 --> 00:27:12,190
 

1507
00:27:09,700 --> 00:27:14,170
 algorithm does so it's quite simple and

1508
00:27:12,180 --> 00:27:14,170
 

1509
00:27:12,190 --> 00:27:17,140
 broad and so what we do is we would say

1510
00:27:14,160 --> 00:27:17,140
 

1511
00:27:14,170 --> 00:27:20,410
 let's draw a candidate so let's say

1512
00:27:17,130 --> 00:27:20,410
 

1513
00:27:17,140 --> 00:27:26,560
 theta star is drawn from some sort of G

1514
00:27:20,400 --> 00:27:26,560
 

1515
00:27:20,410 --> 00:27:29,560
 of theta t minus one and so the t minus

1516
00:27:26,550 --> 00:27:29,560
 

1517
00:27:26,560 --> 00:27:31,570
 one is denoting the the t is indexing

1518
00:27:29,550 --> 00:27:31,570
 

1519
00:27:29,560 --> 00:27:33,730
 the the MCMC iterate and so we're

1520
00:27:31,560 --> 00:27:33,730
 

1521
00:27:31,570 --> 00:27:35,770
 running an iterative algorithm t minus

1522
00:27:33,720 --> 00:27:35,770
 

1523
00:27:33,730 --> 00:27:39,070
 one is the theta t minus one is the last

1524
00:27:35,760 --> 00:27:39,070
 

1525
00:27:35,770 --> 00:27:42,040
 iteration okay and theta T is the sample

1526
00:27:39,060 --> 00:27:42,040
 

1527
00:27:39,070 --> 00:27:44,740
 of step T so we draw a candidate for

1528
00:27:42,030 --> 00:27:44,740
 

1529
00:27:42,040 --> 00:27:46,840
 theta T we call theta star from some

1530
00:27:44,730 --> 00:27:46,840
 

1531
00:27:44,740 --> 00:27:49,390
 candidate generating density G which

1532
00:27:46,830 --> 00:27:49,390
 

1533
00:27:46,840 --> 00:27:51,040
 might depend on the previous value of

1534
00:27:49,380 --> 00:27:51,040
 

1535
00:27:49,390 --> 00:27:51,760
 the parameter and on the on the data

1536
00:27:51,030 --> 00:27:51,760
 

1537
00:27:51,040 --> 00:27:53,410
 okay

1538
00:27:51,750 --> 00:27:53,410
 

1539
00:27:51,760 --> 00:27:55,720
 but it doesn't depend on farther away

1540
00:27:53,400 --> 00:27:55,720
 

1541
00:27:53,410 --> 00:27:57,640
 and that's why it's a Markov chain okay

1542
00:27:55,710 --> 00:27:57,640
 

1543
00:27:55,720 --> 00:27:59,530
 and then we accept the proposal by

1544
00:27:57,630 --> 00:27:59,530
 

1545
00:27:57,640 --> 00:28:00,970
 letting theta T equal theta star with

1546
00:27:59,520 --> 00:28:00,970
 

1547
00:27:59,530 --> 00:28:03,580
 this probability which is just the

1548
00:28:00,960 --> 00:28:03,580
 

1549
00:28:00,970 --> 00:28:06,190
 minimum of one and throwing out this

1550
00:28:03,570 --> 00:28:06,190
 

1551
00:28:03,580 --> 00:28:08,290
 part just a ratio of the prior times the

1552
00:28:06,180 --> 00:28:08,290
 

1553
00:28:06,190 --> 00:28:10,120
 likelihood at the candidate value

1554
00:28:08,280 --> 00:28:10,120
 

1555
00:28:08,290 --> 00:28:12,670
 divided by the prior times the

1556
00:28:10,110 --> 00:28:12,670
 

1557
00:28:10,120 --> 00:28:14,080
 likelihood at the previous value okay

1558
00:28:12,660 --> 00:28:14,080
 

1559
00:28:12,670 --> 00:28:16,690
 and then the G just gives you an

1560
00:28:14,070 --> 00:28:16,690
 

1561
00:28:14,080 --> 00:28:18,340
 adjustment for asymmetry and it's a

1562
00:28:16,680 --> 00:28:18,340
 

1563
00:28:16,690 --> 00:28:20,230
 metropolis algorithm if there's no G

1564
00:28:18,330 --> 00:28:20,230
 

1565
00:28:18,340 --> 00:28:22,720
 because we were limited to using a

1566
00:28:20,220 --> 00:28:22,720
 

1567
00:28:20,230 --> 00:28:24,760
 symmetric G in using metropolis okay

1568
00:28:22,710 --> 00:28:24,760
 

1569
00:28:22,720 --> 00:28:26,440
 so this is really simple if we have some

1570
00:28:24,750 --> 00:28:26,440
 

1571
00:28:24,760 --> 00:28:27,850
 G we can always sort of do this as long

1572
00:28:26,430 --> 00:28:27,850
 

1573
00:28:26,440 --> 00:28:32,260
 as we can evaluate the prior in the

1574
00:28:27,840 --> 00:28:32,260
 

1575
00:28:27,850 --> 00:28:34,480
 likelihood okay so um so what what's the

1576
00:28:32,250 --> 00:28:34,480
 

1577
00:28:32,260 --> 00:28:35,770
 game then while the the the if we wanted

1578
00:28:34,470 --> 00:28:35,770
 

1579
00:28:34,480 --> 00:28:38,050
 to do something efficient and

1580
00:28:35,760 --> 00:28:38,050
 

1581
00:28:35,770 --> 00:28:40,900
 particularly formally scalable to very

1582
00:28:38,040 --> 00:28:40,900
 

1583
00:28:38,050 --> 00:28:42,850
 large problem sizes large an and P then

1584
00:28:40,890 --> 00:28:42,850
 

1585
00:28:40,900 --> 00:28:46,030
 we need to choose the good proposal G

1586
00:28:42,840 --> 00:28:46,030
 

1587
00:28:42,850 --> 00:28:47,500
 okay so G can depend on the previous

1588
00:28:46,020 --> 00:28:47,500
 

1589
00:28:46,030 --> 00:28:49,960
 value of theta and on the data but not

1590
00:28:47,490 --> 00:28:49,960
 

1591
00:28:47,500 --> 00:28:51,940
 on farther back samples except in what's

1592
00:28:49,950 --> 00:28:51,940
 

1593
00:28:49,960 --> 00:28:54,400
 called a adaptive metropolis Hastings

1594
00:28:51,930 --> 00:28:54,400
 

1595
00:28:51,940 --> 00:28:55,540
 and so the reason is the Markov chain is

1596
00:28:54,390 --> 00:28:55,540
 

1597
00:28:54,400 --> 00:28:57,070
 that we're just sampling

1598
00:28:55,530 --> 00:28:57,070
 

1599
00:28:55,540 --> 00:28:59,650
 based on the previous value of the

1600
00:28:57,060 --> 00:28:59,650
 

1601
00:28:57,070 --> 00:29:02,770
 sample but not 20 samples ago so it's

1602
00:28:59,640 --> 00:29:02,770
 

1603
00:28:59,650 --> 00:29:04,900
 like a first-order Markov chain so I'm

1604
00:29:02,760 --> 00:29:04,900
 

1605
00:29:02,770 --> 00:29:06,670
 special cases the Gibbs sample or what

1606
00:29:04,890 --> 00:29:06,670
 

1607
00:29:04,900 --> 00:29:08,590
 the gift sampler does is we if we just

1608
00:29:06,660 --> 00:29:08,590
 

1609
00:29:06,670 --> 00:29:10,630
 say all the parameters are theta it's a

1610
00:29:08,580 --> 00:29:10,630
 

1611
00:29:08,590 --> 00:29:13,060
 big giant vector parameters we have

1612
00:29:10,620 --> 00:29:13,060
 

1613
00:29:10,630 --> 00:29:14,680
 theta 1 to theta P then we can draw

1614
00:29:13,050 --> 00:29:14,680
 

1615
00:29:13,060 --> 00:29:16,090
 subsets of theta from their exact

1616
00:29:14,670 --> 00:29:16,090
 

1617
00:29:14,680 --> 00:29:17,410
 conditional posterior distributions

1618
00:29:16,080 --> 00:29:17,410
 

1619
00:29:16,090 --> 00:29:19,690
 fixing the others we have a Gibbs

1620
00:29:17,400 --> 00:29:19,690
 

1621
00:29:17,410 --> 00:29:21,430
 sampler and so what we might do there is

1622
00:29:19,680 --> 00:29:21,430
 

1623
00:29:19,690 --> 00:29:23,440
 it might be that while theta wine is

1624
00:29:21,420 --> 00:29:23,440
 

1625
00:29:21,430 --> 00:29:24,940
 like if we fix all the other parameters

1626
00:29:23,430 --> 00:29:24,940
 

1627
00:29:23,440 --> 00:29:26,080
 and go through the algebra

1628
00:29:24,930 --> 00:29:26,080
 

1629
00:29:24,940 --> 00:29:29,260
 well that might just be a Gaussian

1630
00:29:26,070 --> 00:29:29,260
 

1631
00:29:26,080 --> 00:29:31,450
 distribution theta 2 fixing all the

1632
00:29:29,250 --> 00:29:31,450
 

1633
00:29:29,260 --> 00:29:34,240
 parameters might be a gamma distribution

1634
00:29:31,440 --> 00:29:34,240
 

1635
00:29:31,450 --> 00:29:35,890
 at Zeta 3 might be a Poisson

1636
00:29:34,230 --> 00:29:35,890
 

1637
00:29:34,240 --> 00:29:38,170
 distribution and we might just iterate

1638
00:29:35,880 --> 00:29:38,170
 

1639
00:29:35,890 --> 00:29:39,640
 through sampling from those conditional

1640
00:29:38,160 --> 00:29:39,640
 

1641
00:29:38,170 --> 00:29:43,360
 distributions that would be a Gibbs

1642
00:29:39,630 --> 00:29:43,360
 

1643
00:29:39,640 --> 00:29:46,810
 sampler okay a random that'll be a

1644
00:29:43,350 --> 00:29:46,810
 

1645
00:29:43,360 --> 00:29:48,400
 special case in which the G cancels with

1646
00:29:46,800 --> 00:29:48,400
 

1647
00:29:46,810 --> 00:29:50,710
 the other part and we always accept

1648
00:29:48,390 --> 00:29:50,710
 

1649
00:29:48,400 --> 00:29:52,660
 another special K important special case

1650
00:29:50,700 --> 00:29:52,660
 

1651
00:29:50,710 --> 00:29:54,370
 is what's called a random walk and so

1652
00:29:52,650 --> 00:29:54,370
 

1653
00:29:52,660 --> 00:29:57,250
 there way we would start out we're

1654
00:29:54,360 --> 00:29:57,250
 

1655
00:29:54,370 --> 00:29:59,500
 sitting there at theta t minus 1 we

1656
00:29:57,240 --> 00:29:59,500
 

1657
00:29:57,250 --> 00:30:00,910
 proposed a proposal around theta t minus

1658
00:29:59,490 --> 00:30:00,910
 

1659
00:29:59,500 --> 00:30:03,340
 1 just to say from a Gaussian

1660
00:30:00,900 --> 00:30:03,340
 

1661
00:30:00,910 --> 00:30:04,720
 distribution and then we accept or

1662
00:30:03,330 --> 00:30:04,720
 

1663
00:30:03,340 --> 00:30:06,490
 reject and that would be a random walk

1664
00:30:04,710 --> 00:30:06,490
 

1665
00:30:04,720 --> 00:30:08,830
 ok and there's a quite interesting

1666
00:30:06,480 --> 00:30:08,830
 

1667
00:30:06,490 --> 00:30:10,600
 literature on how much are these types

1668
00:30:08,820 --> 00:30:10,600
 

1669
00:30:08,830 --> 00:30:14,950
 of random walks scalable to large

1670
00:30:10,590 --> 00:30:14,950
 

1671
00:30:10,600 --> 00:30:16,360
 dimensional problems ok some more fancy

1672
00:30:14,940 --> 00:30:16,360
 

1673
00:30:14,950 --> 00:30:18,220
 things that people have been using in

1674
00:30:16,350 --> 00:30:18,220
 

1675
00:30:16,360 --> 00:30:20,380
 recent years or like Hamiltonian

1676
00:30:18,210 --> 00:30:20,380
 

1677
00:30:18,220 --> 00:30:22,330
 Montecarlo or launch event algorithms

1678
00:30:20,370 --> 00:30:22,330
 

1679
00:30:20,380 --> 00:30:25,450
 what they would do is they would try to

1680
00:30:22,320 --> 00:30:25,450
 

1681
00:30:22,330 --> 00:30:27,700
 try to design a really good G by

1682
00:30:25,440 --> 00:30:27,700
 

1683
00:30:25,450 --> 00:30:31,390
 exploiting gradient information and so

1684
00:30:27,690 --> 00:30:31,390
 

1685
00:30:27,700 --> 00:30:32,800
 that we can get a draw from that's a

1686
00:30:31,380 --> 00:30:32,800
 

1687
00:30:31,390 --> 00:30:40,150
 really good draw but far from the

1688
00:30:32,790 --> 00:30:40,150
 

1689
00:30:32,800 --> 00:30:43,390
 current draw so what we worry about what

1690
00:30:40,140 --> 00:30:43,390
 

1691
00:30:40,150 --> 00:30:46,120
 we worry about is is having a G which

1692
00:30:43,380 --> 00:30:46,120
 

1693
00:30:43,390 --> 00:30:47,710
 gives you a value really close but then

1694
00:30:46,110 --> 00:30:47,710
 

1695
00:30:46,120 --> 00:30:49,600
 it's accepted with high probability but

1696
00:30:47,700 --> 00:30:49,600
 

1697
00:30:47,710 --> 00:30:51,280
 then but then it doesn't move far enough

1698
00:30:49,590 --> 00:30:51,280
 

1699
00:30:49,600 --> 00:30:53,260
 and that can create inefficiency so

1700
00:30:51,270 --> 00:30:53,260
 

1701
00:30:51,280 --> 00:30:56,560
 these HMC or lines event algorithms are

1702
00:30:53,250 --> 00:30:56,560
 

1703
00:30:53,260 --> 00:30:59,440
 designed to overcome that ok so let's

1704
00:30:56,550 --> 00:30:59,440
 

1705
00:30:56,560 --> 00:31:01,960
 get a little bit into the scalability so

1706
00:30:59,430 --> 00:31:01,960
 

1707
00:30:59,440 --> 00:31:06,760
 what about why why is MCMC thought to be

1708
00:31:01,950 --> 00:31:06,760
 

1709
00:31:01,960 --> 00:31:08,410
 slow well you know potentially the time

1710
00:31:06,750 --> 00:31:08,410
 

1711
00:31:06,760 --> 00:31:08,960
 per iteration might increase with the

1712
00:31:08,400 --> 00:31:08,960
 

1713
00:31:08,410 --> 00:31:11,720
 number Prem

1714
00:31:08,950 --> 00:31:11,720
 

1715
00:31:08,960 --> 00:31:13,820
 our unknowns and can also increase with

1716
00:31:11,710 --> 00:31:13,820
 

1717
00:31:11,720 --> 00:31:15,770
 the sample size okay this would also be

1718
00:31:13,810 --> 00:31:15,770
 

1719
00:31:13,820 --> 00:31:18,049
 true for just about every optimization

1720
00:31:15,760 --> 00:31:18,049
 

1721
00:31:15,770 --> 00:31:21,110
 algorithm in the world as well but we

1722
00:31:18,039 --> 00:31:21,110
 

1723
00:31:18,049 --> 00:31:22,880
 can start there okay so um so what

1724
00:31:21,100 --> 00:31:22,880
 

1725
00:31:21,110 --> 00:31:24,860
 happens is that we have to we have to

1726
00:31:22,870 --> 00:31:24,860
 

1727
00:31:22,880 --> 00:31:26,390
 sample and so what we're doing is we

1728
00:31:24,850 --> 00:31:26,390
 

1729
00:31:24,860 --> 00:31:28,730
 have to have the cost of sampling the

1730
00:31:26,380 --> 00:31:28,730
 

1731
00:31:26,390 --> 00:31:31,039
 proposal and also calculating the

1732
00:31:28,720 --> 00:31:31,039
 

1733
00:31:28,730 --> 00:31:32,450
 acceptance probability and so if we you

1734
00:31:31,029 --> 00:31:32,450
 

1735
00:31:31,039 --> 00:31:34,490
 know if we think about just a simple

1736
00:31:32,440 --> 00:31:34,490
 

1737
00:31:32,450 --> 00:31:36,169
 random walk we're drawing from a

1738
00:31:34,480 --> 00:31:36,169
 

1739
00:31:34,490 --> 00:31:38,480
 multivariate Gaussian distribution well

1740
00:31:36,159 --> 00:31:38,480
 

1741
00:31:36,169 --> 00:31:40,580
 that might be not be very expensive but

1742
00:31:38,470 --> 00:31:40,580
 

1743
00:31:38,480 --> 00:31:42,080
 then we have to accept or reject well

1744
00:31:40,570 --> 00:31:42,080
 

1745
00:31:40,580 --> 00:31:43,429
 that's a ratio of the prior times the

1746
00:31:42,070 --> 00:31:43,429
 

1747
00:31:42,080 --> 00:31:45,289
 likelihood so we have to evaluate the

1748
00:31:43,419 --> 00:31:45,289
 

1749
00:31:43,429 --> 00:31:46,880
 likelihood okay well that might be some

1750
00:31:45,279 --> 00:31:46,880
 

1751
00:31:45,289 --> 00:31:49,490
 sort of polynomial in the sample size

1752
00:31:46,870 --> 00:31:49,490
 

1753
00:31:46,880 --> 00:31:50,690
 evaluating the likelihood okay and we

1754
00:31:49,480 --> 00:31:50,690
 

1755
00:31:49,490 --> 00:31:51,980
 have to do that to calculate the

1756
00:31:50,680 --> 00:31:51,980
 

1757
00:31:50,690 --> 00:31:54,289
 acceptance probability and so that's

1758
00:31:51,970 --> 00:31:54,289
 

1759
00:31:51,980 --> 00:31:55,789
 going to be slow potentially or if we

1760
00:31:54,279 --> 00:31:55,789
 

1761
00:31:54,289 --> 00:31:57,529
 are sampling directly using Gibbs

1762
00:31:55,779 --> 00:31:57,529
 

1763
00:31:55,789 --> 00:31:59,870
 sampling we might need to also sample

1764
00:31:57,519 --> 00:31:59,870
 

1765
00:31:57,529 --> 00:32:01,760
 from some sort of big Gaussian with a

1766
00:31:59,860 --> 00:32:01,760
 

1767
00:31:59,870 --> 00:32:03,320
 complicated covariance which is hard to

1768
00:32:01,750 --> 00:32:03,320
 

1769
00:32:01,760 --> 00:32:05,620
 invert and that might be some sort of

1770
00:32:03,310 --> 00:32:05,620
 

1771
00:32:03,320 --> 00:32:08,240
 polynomial in the sample size as well

1772
00:32:05,610 --> 00:32:08,240
 

1773
00:32:05,620 --> 00:32:10,880
 okay so I would say that you know really

1774
00:32:08,230 --> 00:32:10,880
 

1775
00:32:08,240 --> 00:32:14,210
 the time per iteration cost is quite

1776
00:32:10,870 --> 00:32:14,210
 

1777
00:32:10,880 --> 00:32:16,490
 similar to Casa occur in many or most

1778
00:32:14,200 --> 00:32:16,490
 

1779
00:32:14,210 --> 00:32:18,590
 optimization algorithms and we can do do

1780
00:32:16,480 --> 00:32:18,590
 

1781
00:32:16,490 --> 00:32:20,179
 similar things in the MCMC context even

1782
00:32:18,580 --> 00:32:20,179
 

1783
00:32:18,590 --> 00:32:22,039
 though we're not converging to a point

1784
00:32:20,169 --> 00:32:22,039
 

1785
00:32:20,179 --> 00:32:24,760
 estimate but we're sampling and we're

1786
00:32:22,029 --> 00:32:24,760
 

1787
00:32:22,039 --> 00:32:27,500
 converging to a stationary distribution

1788
00:32:24,750 --> 00:32:27,500
 

1789
00:32:24,760 --> 00:32:28,909
 yeah so an example is that the

1790
00:32:27,490 --> 00:32:28,909
 

1791
00:32:27,500 --> 00:32:30,950
 computational bottleneck might be

1792
00:32:28,899 --> 00:32:30,950
 

1793
00:32:28,909 --> 00:32:33,140
 attributable to gradient evaluations and

1794
00:32:30,940 --> 00:32:33,140
 

1795
00:32:30,950 --> 00:32:34,520
 so we need to in running Hamiltonian

1796
00:32:33,130 --> 00:32:34,520
 

1797
00:32:33,140 --> 00:32:35,899
 Monte Carlo we need to evaluate the

1798
00:32:34,510 --> 00:32:35,899
 

1799
00:32:34,520 --> 00:32:38,299
 gradient a bunch of times and well

1800
00:32:35,889 --> 00:32:38,299
 

1801
00:32:35,899 --> 00:32:40,730
 what's the most popular algorithm in

1802
00:32:38,289 --> 00:32:40,730
 

1803
00:32:38,299 --> 00:32:42,529
 recent years for doing optimization one

1804
00:32:40,720 --> 00:32:42,529
 

1805
00:32:40,730 --> 00:32:43,940
 of the most popular tricks is to cast a

1806
00:32:42,519 --> 00:32:43,940
 

1807
00:32:42,529 --> 00:32:46,850
 gradient and so we could do that here as

1808
00:32:43,930 --> 00:32:46,850
 

1809
00:32:43,940 --> 00:32:48,890
 well okay so um so that's the first type

1810
00:32:46,840 --> 00:32:48,890
 

1811
00:32:46,850 --> 00:32:50,539
 of computational bottleneck is the time

1812
00:32:48,880 --> 00:32:50,539
 

1813
00:32:48,890 --> 00:32:52,640
 per iteration okay the time per

1814
00:32:50,529 --> 00:32:52,640
 

1815
00:32:50,539 --> 00:32:54,260
 iteration cost and so that that's very

1816
00:32:52,630 --> 00:32:54,260
 

1817
00:32:52,640 --> 00:32:56,960
 similar to the same thing in

1818
00:32:54,250 --> 00:32:56,960
 

1819
00:32:54,260 --> 00:32:58,940
 optimization algorithms MCMC also has a

1820
00:32:56,950 --> 00:32:58,940
 

1821
00:32:56,960 --> 00:33:00,830
 potential second bottleneck which which

1822
00:32:58,930 --> 00:33:00,830
 

1823
00:32:58,940 --> 00:33:02,960
 might create problems it's also similar

1824
00:33:00,820 --> 00:33:02,960
 

1825
00:33:00,830 --> 00:33:04,250
 to you know things that affect the

1826
00:33:02,950 --> 00:33:04,250
 

1827
00:33:02,960 --> 00:33:06,830
 convergence rate in optimization

1828
00:33:04,240 --> 00:33:06,830
 

1829
00:33:04,250 --> 00:33:08,360
 algorithms because MCMC doesn't produce

1830
00:33:06,820 --> 00:33:08,360
 

1831
00:33:06,830 --> 00:33:12,919
 independent samples from the posterior

1832
00:33:08,350 --> 00:33:12,919
 

1833
00:33:08,360 --> 00:33:14,779
 distribution so the draws are Auto

1834
00:33:12,909 --> 00:33:14,779
 

1835
00:33:12,919 --> 00:33:17,240
 correlated what I mean by that is that

1836
00:33:14,769 --> 00:33:17,240
 

1837
00:33:14,779 --> 00:33:19,370
 the draw at theta T is going to be kind

1838
00:33:17,230 --> 00:33:19,370
 

1839
00:33:17,240 --> 00:33:21,320
 of correlated with theta T minus 1 and

1840
00:33:19,360 --> 00:33:21,320
 

1841
00:33:19,370 --> 00:33:22,880
 that's going to create sort of redundant

1842
00:33:21,310 --> 00:33:22,880
 

1843
00:33:21,320 --> 00:33:24,530
 information and so if

1844
00:33:22,870 --> 00:33:24,530
 

1845
00:33:22,880 --> 00:33:26,000
 we could get independent draws we might

1846
00:33:24,520 --> 00:33:26,000
 

1847
00:33:24,530 --> 00:33:28,040
 be getting much more information than we

1848
00:33:25,990 --> 00:33:28,040
 

1849
00:33:26,000 --> 00:33:30,290
 get out of the MCMC drawers okay and so

1850
00:33:28,030 --> 00:33:30,290
 

1851
00:33:28,040 --> 00:33:33,620
 that makes it so that we need to draw

1852
00:33:30,280 --> 00:33:33,620
 

1853
00:33:30,290 --> 00:33:35,600
 more samples than we would like because

1854
00:33:33,610 --> 00:33:35,600
 

1855
00:33:33,620 --> 00:33:38,660
 of this kind of what autocorrelation or

1856
00:33:35,590 --> 00:33:38,660
 

1857
00:33:35,600 --> 00:33:40,610
 slow mixing problem so so the slow

1858
00:33:38,650 --> 00:33:40,610
 

1859
00:33:38,660 --> 00:33:41,810
 mixing Markov chain problem is what

1860
00:33:40,600 --> 00:33:41,810
 

1861
00:33:40,610 --> 00:33:43,880
 we're really worried about in these

1862
00:33:41,800 --> 00:33:43,880
 

1863
00:33:41,810 --> 00:33:46,340
 these high dimensional cases is

1864
00:33:43,870 --> 00:33:46,340
 

1865
00:33:43,880 --> 00:33:47,960
 increasing autocorrelation as the

1866
00:33:46,330 --> 00:33:47,960
 

1867
00:33:46,340 --> 00:33:53,390
 problem size increases and this can

1868
00:33:47,950 --> 00:33:53,390
 

1869
00:33:47,960 --> 00:33:55,460
 occur both in N and in P okay so

1870
00:33:53,380 --> 00:33:55,460
 

1871
00:33:53,390 --> 00:33:58,040
 well-designed MCMC algorithm with a good

1872
00:33:55,450 --> 00:33:58,040
 

1873
00:33:55,460 --> 00:34:00,800
 proposal should ideally be kind of

1874
00:33:58,030 --> 00:34:00,800
 

1875
00:33:58,040 --> 00:34:02,480
 scalable in the sense of the the mixing

1876
00:34:00,790 --> 00:34:02,480
 

1877
00:34:00,800 --> 00:34:04,760
 rate not getting worse and worse in the

1878
00:34:02,470 --> 00:34:04,760
 

1879
00:34:02,480 --> 00:34:08,300
 problem size you know we don't want the

1880
00:34:04,750 --> 00:34:08,300
 

1881
00:34:04,760 --> 00:34:10,760
 mixing rate to kind of be exploding and

1882
00:34:08,290 --> 00:34:10,760
 

1883
00:34:08,300 --> 00:34:12,500
 the because that what that means is

1884
00:34:10,750 --> 00:34:12,500
 

1885
00:34:10,760 --> 00:34:15,290
 basically the correlation is going up

1886
00:34:12,490 --> 00:34:15,290
 

1887
00:34:12,500 --> 00:34:16,879
 and the samples and so we get we need to

1888
00:34:15,280 --> 00:34:16,879
 

1889
00:34:15,290 --> 00:34:19,700
 run we need to run more and more

1890
00:34:16,869 --> 00:34:19,700
 

1891
00:34:16,879 --> 00:34:22,129
 iterations of the larger and larger the

1892
00:34:19,690 --> 00:34:22,129
 

1893
00:34:19,700 --> 00:34:24,679
 problem okay and then we also have a

1894
00:34:22,119 --> 00:34:24,679
 

1895
00:34:22,129 --> 00:34:27,169
 greater per iteration computational cost

1896
00:34:24,669 --> 00:34:27,169
 

1897
00:34:24,679 --> 00:34:29,480
 and that's where MCMC will grind to a

1898
00:34:27,159 --> 00:34:29,480
 

1899
00:34:27,169 --> 00:34:32,570
 halt okay and we need to be clever in

1900
00:34:29,470 --> 00:34:32,570
 

1901
00:34:29,480 --> 00:34:34,220
 improving it yes otherwise the Monte

1902
00:34:32,560 --> 00:34:34,220
 

1903
00:34:32,570 --> 00:34:36,290
 Carlo Erin post your summaries might be

1904
00:34:34,210 --> 00:34:36,290
 

1905
00:34:34,220 --> 00:34:38,899
 high and what we like to target in terms

1906
00:34:36,280 --> 00:34:38,899
 

1907
00:34:36,290 --> 00:34:40,070
 of scalability is let's say we run the

1908
00:34:38,889 --> 00:34:40,070
 

1909
00:34:38,899 --> 00:34:42,620
 algorithm for some particular

1910
00:34:40,060 --> 00:34:42,620
 

1911
00:34:40,070 --> 00:34:44,870
 computational clock time or something we

1912
00:34:42,610 --> 00:34:44,870
 

1913
00:34:42,620 --> 00:34:47,840
 run it for five minutes on a particular

1914
00:34:44,860 --> 00:34:47,840
 

1915
00:34:44,870 --> 00:34:49,760
 platform then we would like the the

1916
00:34:47,830 --> 00:34:49,760
 

1917
00:34:47,840 --> 00:34:51,770
 error and the Pope Monte Carlo error and

1918
00:34:49,750 --> 00:34:51,770
 

1919
00:34:49,760 --> 00:34:53,510
 postera summaries of interest to be as

1920
00:34:51,760 --> 00:34:53,510
 

1921
00:34:51,770 --> 00:34:55,490
 small as possible okay we'd like to

1922
00:34:53,500 --> 00:34:55,490
 

1923
00:34:53,510 --> 00:34:58,250
 design the algorithm targeted towards

1924
00:34:55,480 --> 00:34:58,250
 

1925
00:34:55,490 --> 00:35:01,730
 small Monte Carlo error in a given in

1926
00:34:58,240 --> 00:35:01,730
 

1927
00:34:58,250 --> 00:35:03,260
 giving computational time okay so often

1928
00:35:01,720 --> 00:35:03,260
 

1929
00:35:01,730 --> 00:35:06,530
 the mixing gets worse as a problem size

1930
00:35:03,250 --> 00:35:06,530
 

1931
00:35:03,260 --> 00:35:08,210
 grows data dimension or sample size so

1932
00:35:06,520 --> 00:35:08,210
 

1933
00:35:06,530 --> 00:35:10,640
 so in some cases we might have a double

1934
00:35:08,200 --> 00:35:10,640
 

1935
00:35:08,210 --> 00:35:12,460
 bottleneck is a worsening mixing and we

1936
00:35:10,630 --> 00:35:12,460
 

1937
00:35:10,640 --> 00:35:15,440
 can think of mixing it's similar to

1938
00:35:12,450 --> 00:35:15,440
 

1939
00:35:12,460 --> 00:35:17,810
 optimization problems with convergence

1940
00:35:15,430 --> 00:35:17,810
 

1941
00:35:15,440 --> 00:35:20,390
 rate and so our problem is too much got

1942
00:35:17,800 --> 00:35:20,390
 

1943
00:35:17,810 --> 00:35:23,420
 a correlation it's sort of related to a

1944
00:35:20,380 --> 00:35:23,420
 

1945
00:35:20,390 --> 00:35:25,450
 problem of slow convergence rate and we

1946
00:35:23,410 --> 00:35:25,450
 

1947
00:35:23,420 --> 00:35:27,890
 have a worsening time for iteration

1948
00:35:25,440 --> 00:35:27,890
 

1949
00:35:25,450 --> 00:35:31,070
 another issue that we run into is that

1950
00:35:27,880 --> 00:35:31,070
 

1951
00:35:27,890 --> 00:35:33,170
 MCMC is inherently a serial algorithm so

1952
00:35:31,060 --> 00:35:33,170
 

1953
00:35:31,070 --> 00:35:35,240
 not a naive implementation might require

1954
00:35:33,160 --> 00:35:35,240
 

1955
00:35:33,170 --> 00:35:36,180
 storing and processing all of the data

1956
00:35:35,230 --> 00:35:36,180
 

1957
00:35:35,240 --> 00:35:38,190
 on one machine

1958
00:35:36,170 --> 00:35:38,190
 

1959
00:35:36,180 --> 00:35:39,869
 and so if we'd like just take all the

1960
00:35:38,180 --> 00:35:39,869
 

1961
00:35:38,190 --> 00:35:42,150
 data on one machine and it's just

1962
00:35:39,859 --> 00:35:42,150
 

1963
00:35:39,869 --> 00:35:43,950
 enormous amounts of crap and memory and

1964
00:35:42,140 --> 00:35:43,950
 

1965
00:35:42,150 --> 00:35:45,450
 then we're trying to do like this kind

1966
00:35:43,940 --> 00:35:45,450
 

1967
00:35:43,950 --> 00:35:47,010
 of iterative algorithm and like one

1968
00:35:45,440 --> 00:35:47,010
 

1969
00:35:45,450 --> 00:35:48,630
 processor then you know everything's

1970
00:35:47,000 --> 00:35:48,630
 

1971
00:35:47,010 --> 00:35:50,779
 gonna be horrible it's that of course

1972
00:35:48,620 --> 00:35:50,779
 

1973
00:35:48,630 --> 00:35:53,309
 and so we need to be smarter than that

1974
00:35:50,769 --> 00:35:53,309
 

1975
00:35:50,779 --> 00:35:54,869
 but it also that it limits the ease at

1976
00:35:53,299 --> 00:35:54,869
 

1977
00:35:53,309 --> 00:35:58,289
 which divine conquer strategies can be

1978
00:35:54,859 --> 00:35:58,289
 

1979
00:35:54,869 --> 00:36:00,930
 applied okay so so these are the reasons

1980
00:35:58,279 --> 00:36:00,930
 

1981
00:35:58,289 --> 00:36:04,260
 that it's commonly dis stated that MCMC

1982
00:36:00,920 --> 00:36:04,260
 

1983
00:36:00,930 --> 00:36:05,460
 is it's just simply not scalable but but

1984
00:36:04,250 --> 00:36:05,460
 

1985
00:36:04,260 --> 00:36:06,809
 i would say that each of the above

1986
00:36:05,450 --> 00:36:06,809
 

1987
00:36:05,460 --> 00:36:09,000
 problems can be addressed there's

1988
00:36:06,799 --> 00:36:09,000
 

1989
00:36:06,809 --> 00:36:10,680
 already methods in the literature and

1990
00:36:08,990 --> 00:36:10,680
 

1991
00:36:09,000 --> 00:36:13,920
 there's gonna be a kind of rich emerging

1992
00:36:10,670 --> 00:36:13,920
 

1993
00:36:10,680 --> 00:36:15,450
 literature in this area i think and i

1994
00:36:13,910 --> 00:36:15,450
 

1995
00:36:13,920 --> 00:36:18,180
 would say that this is particularly

1996
00:36:15,440 --> 00:36:18,180
 

1997
00:36:15,450 --> 00:36:19,859
 promising because there's at least an

1998
00:36:18,170 --> 00:36:19,859
 

1999
00:36:18,180 --> 00:36:22,819
 order of magnitude if not orders of

2000
00:36:19,849 --> 00:36:22,819
 

2001
00:36:19,859 --> 00:36:24,690
 magnitude more people working on

2002
00:36:22,809 --> 00:36:24,690
 

2003
00:36:22,819 --> 00:36:26,640
 optimization and trying to develop

2004
00:36:24,680 --> 00:36:26,640
 

2005
00:36:24,690 --> 00:36:28,799
 scalable optimization algorithms and

2006
00:36:26,630 --> 00:36:28,799
 

2007
00:36:26,640 --> 00:36:30,270
 there are working on scalable sampling

2008
00:36:28,789 --> 00:36:30,270
 

2009
00:36:28,799 --> 00:36:31,529
 algorithms there's really not only a

2010
00:36:30,260 --> 00:36:31,529
 

2011
00:36:30,270 --> 00:36:34,140
 handful of people working on these

2012
00:36:31,519 --> 00:36:34,140
 

2013
00:36:31,529 --> 00:36:35,640
 problems and even given that there's

2014
00:36:34,130 --> 00:36:35,640
 

2015
00:36:34,140 --> 00:36:37,470
 already been some really quite quite

2016
00:36:35,630 --> 00:36:37,470
 

2017
00:36:35,640 --> 00:36:40,980
 nice developments and i think that

2018
00:36:37,460 --> 00:36:40,980
 

2019
00:36:37,470 --> 00:36:43,230
 that's quite promising okay so for an

2020
00:36:40,970 --> 00:36:43,230
 

2021
00:36:40,980 --> 00:36:44,880
 MCMC algorithm to be scalable the monte

2022
00:36:43,220 --> 00:36:44,880
 

2023
00:36:43,230 --> 00:36:46,200
 carlo and the posterior marie's based on

2024
00:36:44,870 --> 00:36:46,200
 

2025
00:36:44,880 --> 00:36:49,289
 running for some time tau should not

2026
00:36:46,190 --> 00:36:49,289
 

2027
00:36:46,200 --> 00:36:50,640
 explode with dimensionality some popular

2028
00:36:49,279 --> 00:36:50,640
 

2029
00:36:49,289 --> 00:36:52,680
 algorithms have been shown to not be

2030
00:36:50,630 --> 00:36:52,680
 

2031
00:36:50,640 --> 00:36:54,359
 scalable why others can be made scalable

2032
00:36:52,670 --> 00:36:54,359
 

2033
00:36:52,680 --> 00:36:56,849
 and that's one interesting thing like we

2034
00:36:54,349 --> 00:36:56,849
 

2035
00:36:54,359 --> 00:36:58,740
 were working recently you know one of

2036
00:36:56,839 --> 00:36:58,740
 

2037
00:36:56,849 --> 00:37:00,630
 the most popular algorithms for bayesian

2038
00:36:58,730 --> 00:37:00,630
 

2039
00:36:58,740 --> 00:37:02,819
 computation and categorical data models

2040
00:37:00,620 --> 00:37:02,819
 

2041
00:37:00,630 --> 00:37:04,920
 is data augmentation where you will

2042
00:37:02,809 --> 00:37:04,920
 

2043
00:37:02,819 --> 00:37:06,779
 throw in a bunch of latent data you'll

2044
00:37:04,910 --> 00:37:06,779
 

2045
00:37:04,920 --> 00:37:08,940
 you'll sample a bunch of latent data you

2046
00:37:06,769 --> 00:37:08,940
 

2047
00:37:06,779 --> 00:37:10,380
 want to run a Gibb sampler instead of

2048
00:37:08,930 --> 00:37:10,380
 

2049
00:37:08,940 --> 00:37:12,930
 having to tune some metropolis Hastings

2050
00:37:10,370 --> 00:37:12,930
 

2051
00:37:10,380 --> 00:37:14,700
 algorithm and so so you'll you'll sample

2052
00:37:12,920 --> 00:37:14,700
 

2053
00:37:12,930 --> 00:37:16,109
 a bunch of latent data okay and then

2054
00:37:14,690 --> 00:37:16,109
 

2055
00:37:14,700 --> 00:37:18,809
 given the latent data then you can

2056
00:37:16,099 --> 00:37:18,809
 

2057
00:37:16,109 --> 00:37:20,400
 sample all the Thetas from close form

2058
00:37:18,799 --> 00:37:20,400
 

2059
00:37:18,809 --> 00:37:22,109
 simple full conditionals that might be

2060
00:37:20,390 --> 00:37:22,109
 

2061
00:37:20,400 --> 00:37:24,630
 normal or something so we impute a bunch

2062
00:37:22,099 --> 00:37:24,630
 

2063
00:37:22,109 --> 00:37:26,460
 of late data from truncated normals and

2064
00:37:24,620 --> 00:37:26,460
 

2065
00:37:24,630 --> 00:37:29,039
 then we sample a bunch of parameters

2066
00:37:26,450 --> 00:37:29,039
 

2067
00:37:26,460 --> 00:37:30,510
 from a Gaussian okay well actually you

2068
00:37:29,029 --> 00:37:30,510
 

2069
00:37:29,039 --> 00:37:32,579
 can show that often that's not very

2070
00:37:30,500 --> 00:37:32,579
 

2071
00:37:30,510 --> 00:37:34,400
 scalable like in a theoretical way and

2072
00:37:32,569 --> 00:37:34,400
 

2073
00:37:32,579 --> 00:37:37,470
 so maybe that's bad and so just by using

2074
00:37:34,390 --> 00:37:37,470
 

2075
00:37:34,400 --> 00:37:39,660
 the simple theory well not so simple

2076
00:37:37,460 --> 00:37:39,660
 

2077
00:37:37,470 --> 00:37:41,250
 theory arguments you can you can see

2078
00:37:39,650 --> 00:37:41,250
 

2079
00:37:39,660 --> 00:37:43,680
 well this class of algorithms is very

2080
00:37:41,240 --> 00:37:43,680
 

2081
00:37:41,250 --> 00:37:45,180
 not scalable in this case therefore we

2082
00:37:43,670 --> 00:37:45,180
 

2083
00:37:43,680 --> 00:37:46,619
 should just avoid them theoretic for

2084
00:37:45,170 --> 00:37:46,619
 

2085
00:37:45,180 --> 00:37:48,420
 theoretical reasons and then we can

2086
00:37:46,609 --> 00:37:48,420
 

2087
00:37:46,619 --> 00:37:49,000
 focus on another class of algorithms and

2088
00:37:48,410 --> 00:37:49,000
 

2089
00:37:48,420 --> 00:37:51,130
 I would

2090
00:37:48,990 --> 00:37:51,130
 

2091
00:37:49,000 --> 00:37:52,960
 say that this is an amazingly

2092
00:37:51,120 --> 00:37:52,960
 

2093
00:37:51,130 --> 00:37:56,290
 interesting area that I would encourage

2094
00:37:52,950 --> 00:37:56,290
 

2095
00:37:52,960 --> 00:37:58,150
 people to work on much more is how can

2096
00:37:56,280 --> 00:37:58,150
 

2097
00:37:56,290 --> 00:38:00,640
 we develop more of that type of theory

2098
00:37:58,140 --> 00:38:00,640
 

2099
00:37:58,150 --> 00:38:03,190
 so we can use some sort of theory lens

2100
00:38:00,630 --> 00:38:03,190
 

2101
00:38:00,640 --> 00:38:04,900
 to target which algorithms that we can

2102
00:38:03,180 --> 00:38:04,900
 

2103
00:38:03,190 --> 00:38:07,060
 use in practice in which practical

2104
00:38:04,890 --> 00:38:07,060
 

2105
00:38:04,900 --> 00:38:10,810
 algorithms can be used in certain types

2106
00:38:07,050 --> 00:38:10,810
 

2107
00:38:07,060 --> 00:38:13,090
 of large problems okay so anyway so um

2108
00:38:10,800 --> 00:38:13,090
 

2109
00:38:10,810 --> 00:38:15,460
 so that's my my very long introduction

2110
00:38:13,080 --> 00:38:15,460
 

2111
00:38:13,090 --> 00:38:17,410
 in some sense but I'm gonna focus on

2112
00:38:15,450 --> 00:38:17,410
 

2113
00:38:15,460 --> 00:38:19,540
 highlighting some of the relevant recent

2114
00:38:17,400 --> 00:38:19,540
 

2115
00:38:17,410 --> 00:38:21,070
 work starting by focusing on big end

2116
00:38:19,530 --> 00:38:21,070
 

2117
00:38:19,540 --> 00:38:22,660
 problems and then transitioning the big

2118
00:38:21,060 --> 00:38:22,660
 

2119
00:38:21,070 --> 00:38:25,060
 P and I'm gonna give a disclaimer and

2120
00:38:22,650 --> 00:38:25,060
 

2121
00:38:22,660 --> 00:38:26,950
 that I'm just you know given the time

2122
00:38:25,050 --> 00:38:26,950
 

2123
00:38:25,060 --> 00:38:28,990
 I'm not providing like an overview of

2124
00:38:26,940 --> 00:38:28,990
 

2125
00:38:26,950 --> 00:38:30,310
 the literature but I'm just focusing on

2126
00:38:28,980 --> 00:38:30,310
 

2127
00:38:28,990 --> 00:38:31,810
 some some of the work we've done and

2128
00:38:30,300 --> 00:38:31,810
 

2129
00:38:30,310 --> 00:38:33,400
 I'll give references and then we've

2130
00:38:31,800 --> 00:38:33,400
 

2131
00:38:31,810 --> 00:38:36,580
 cited the broader literature and our

2132
00:38:33,390 --> 00:38:36,580
 

2133
00:38:33,400 --> 00:38:38,710
 papers okay so now we're on the big end

2134
00:38:36,570 --> 00:38:38,710
 

2135
00:38:36,580 --> 00:38:41,620
 and and I'll give a little bit before we

2136
00:38:38,700 --> 00:38:41,620
 

2137
00:38:38,710 --> 00:38:43,360
 break okay so some solutions to the the

2138
00:38:41,610 --> 00:38:43,360
 

2139
00:38:41,620 --> 00:38:46,030
 big end problem the first is what's

2140
00:38:43,350 --> 00:38:46,030
 

2141
00:38:43,360 --> 00:38:47,620
 called embarrassingly parallel Markov

2142
00:38:46,020 --> 00:38:47,620
 

2143
00:38:46,030 --> 00:38:49,090
 chain Monte Carlo one of the types of

2144
00:38:47,610 --> 00:38:49,090
 

2145
00:38:47,620 --> 00:38:51,430
 solutions that I I'm not going to talk

2146
00:38:49,080 --> 00:38:51,430
 

2147
00:38:49,090 --> 00:38:52,960
 about is is to just take any MCMC

2148
00:38:51,420 --> 00:38:52,960
 

2149
00:38:51,430 --> 00:38:54,820
 algorithm and then say well maybe I can

2150
00:38:52,950 --> 00:38:54,820
 

2151
00:38:52,960 --> 00:38:57,340
 use GPUs or something to break up the

2152
00:38:54,810 --> 00:38:57,340
 

2153
00:38:54,820 --> 00:38:58,780
 computation that I think is somewhat

2154
00:38:57,330 --> 00:38:58,780
 

2155
00:38:57,340 --> 00:39:01,060
 limited in scope because of the

2156
00:38:58,770 --> 00:39:01,060
 

2157
00:38:58,780 --> 00:39:03,730
 communication cost and so if we really

2158
00:39:01,050 --> 00:39:03,730
 

2159
00:39:01,060 --> 00:39:05,530
 want to scale up I'd say well we'd like

2160
00:39:03,720 --> 00:39:05,530
 

2161
00:39:03,730 --> 00:39:07,660
 to be able to have a giant data set

2162
00:39:05,520 --> 00:39:07,660
 

2163
00:39:05,530 --> 00:39:09,430
 stored on different computers not be

2164
00:39:07,650 --> 00:39:09,430
 

2165
00:39:07,660 --> 00:39:12,310
 sending it around not be sticking stuff

2166
00:39:09,420 --> 00:39:12,310
 

2167
00:39:09,430 --> 00:39:13,930
 on one computer and limit communication

2168
00:39:12,300 --> 00:39:13,930
 

2169
00:39:12,310 --> 00:39:15,730
 cost so maybe we can take the data and

2170
00:39:13,920 --> 00:39:15,730
 

2171
00:39:13,930 --> 00:39:18,580
 throw shards of the data on different

2172
00:39:15,720 --> 00:39:18,580
 

2173
00:39:15,730 --> 00:39:20,050
 computers run em same same parallel for

2174
00:39:18,570 --> 00:39:20,050
 

2175
00:39:18,580 --> 00:39:21,550
 different subsets of the data limit

2176
00:39:20,040 --> 00:39:21,550
 

2177
00:39:20,050 --> 00:39:24,040
 communication costs and then combine

2178
00:39:21,540 --> 00:39:24,040
 

2179
00:39:21,550 --> 00:39:25,060
 okay and that's called EPM CMC and

2180
00:39:24,030 --> 00:39:25,060
 

2181
00:39:24,040 --> 00:39:28,480
 there's some really quite nice

2182
00:39:25,050 --> 00:39:28,480
 

2183
00:39:25,060 --> 00:39:31,240
 algorithms in in that class by us and

2184
00:39:28,470 --> 00:39:31,240
 

2185
00:39:28,480 --> 00:39:32,920
 other groups as well another type of

2186
00:39:31,230 --> 00:39:32,920
 

2187
00:39:31,240 --> 00:39:34,840
 class of algorithms is what I might call

2188
00:39:32,910 --> 00:39:34,840
 

2189
00:39:32,920 --> 00:39:36,700
 approximate Markov chain Monte Carlo

2190
00:39:34,830 --> 00:39:36,700
 

2191
00:39:34,840 --> 00:39:38,260
 then we could just look at a Markov

2192
00:39:36,690 --> 00:39:38,260
 

2193
00:39:36,700 --> 00:39:40,270
 chain Monte Carlo algorithm be like well

2194
00:39:38,250 --> 00:39:40,270
 

2195
00:39:38,260 --> 00:39:42,370
 geez that gradient calculation is really

2196
00:39:40,260 --> 00:39:42,370
 

2197
00:39:40,270 --> 00:39:44,770
 bloody expensive let's use stochastic

2198
00:39:42,360 --> 00:39:44,770
 

2199
00:39:42,370 --> 00:39:46,840
 gradient there or over here or let's use

2200
00:39:44,760 --> 00:39:46,840
 

2201
00:39:44,770 --> 00:39:47,740
 some subset of the data or let's use

2202
00:39:46,830 --> 00:39:47,740
 

2203
00:39:46,840 --> 00:39:50,170
 some sort of low-rank approximation

2204
00:39:47,730 --> 00:39:50,170
 

2205
00:39:47,740 --> 00:39:52,920
 linear algebra trick here and then we

2206
00:39:50,160 --> 00:39:52,920
 

2207
00:39:50,170 --> 00:39:54,850
 can free up the bottlenecks by taking a

2208
00:39:52,910 --> 00:39:54,850
 

2209
00:39:52,920 --> 00:39:56,770
 transition kernel that will converge to

2210
00:39:54,840 --> 00:39:56,770
 

2211
00:39:54,850 --> 00:39:59,020
 exactly the right target distribution

2212
00:39:56,760 --> 00:39:59,020
 

2213
00:39:56,770 --> 00:40:00,790
 and replacing it with some type of alum

2214
00:39:59,010 --> 00:40:00,790
 

2215
00:39:59,020 --> 00:40:03,010
 provably accurate approximation now I

2216
00:40:00,780 --> 00:40:03,010
 

2217
00:40:00,790 --> 00:40:05,380
 call that approximate MCM

2218
00:40:03,000 --> 00:40:05,380
 

2219
00:40:03,010 --> 00:40:08,260
 and these two can be used together as

2220
00:40:05,370 --> 00:40:08,260
 

2221
00:40:05,380 --> 00:40:12,280
 well of course something that I haven't

2222
00:40:08,250 --> 00:40:12,280
 

2223
00:40:08,260 --> 00:40:14,410
 been talking about yet but I will focus

2224
00:40:12,270 --> 00:40:14,410
 

2225
00:40:12,280 --> 00:40:16,690
 on a bit is that we'd like to also

2226
00:40:14,400 --> 00:40:16,690
 

2227
00:40:14,410 --> 00:40:18,400
 develop robust methods kind of

2228
00:40:16,680 --> 00:40:18,400
 

2229
00:40:16,690 --> 00:40:20,140
 regardless of the scalability issue we'd

2230
00:40:18,390 --> 00:40:20,140
 

2231
00:40:18,400 --> 00:40:22,390
 like to also have methods that are more

2232
00:40:20,130 --> 00:40:22,390
 

2233
00:40:20,140 --> 00:40:24,400
 inherently robust in large sample sizes

2234
00:40:22,380 --> 00:40:24,400
 

2235
00:40:22,390 --> 00:40:27,430
 and we have a such a method which I will

2236
00:40:24,390 --> 00:40:27,430
 

2237
00:40:24,400 --> 00:40:29,320
 highlight soon I'm something I find

2238
00:40:27,420 --> 00:40:29,320
 

2239
00:40:27,430 --> 00:40:31,540
 really promising as well as to do type

2240
00:40:29,310 --> 00:40:31,540
 

2241
00:40:29,320 --> 00:40:33,760
 of hybrid algorithms and so instead of

2242
00:40:31,530 --> 00:40:33,760
 

2243
00:40:31,540 --> 00:40:35,710
 instead of running MCMC for everything

2244
00:40:33,750 --> 00:40:35,710
 

2245
00:40:33,760 --> 00:40:37,660
 maybe we can use some sort of fast

2246
00:40:35,700 --> 00:40:37,660
 

2247
00:40:35,710 --> 00:40:39,370
 estimate for some of the parameters and

2248
00:40:37,650 --> 00:40:39,370
 

2249
00:40:37,660 --> 00:40:41,620
 then run MCMC for a subset of the

2250
00:40:39,360 --> 00:40:41,620
 

2251
00:40:39,370 --> 00:40:43,570
 parameters we've had a lot of luck with

2252
00:40:41,610 --> 00:40:43,570
 

2253
00:40:41,620 --> 00:40:45,670
 this type of method making it like very

2254
00:40:43,560 --> 00:40:45,670
 

2255
00:40:43,570 --> 00:40:48,520
 very much scalable for but very flexible

2256
00:40:45,660 --> 00:40:48,520
 

2257
00:40:45,670 --> 00:40:50,830
 models you can scale as best as well as

2258
00:40:48,510 --> 00:40:50,830
 

2259
00:40:48,520 --> 00:40:53,440
 like you know state of the art a lasso

2260
00:40:50,820 --> 00:40:53,440
 

2261
00:40:50,830 --> 00:40:55,180
 algorithms for very complicated problems

2262
00:40:53,430 --> 00:40:55,180
 

2263
00:40:53,440 --> 00:40:58,060
 by doing a bit of optimization and then

2264
00:40:55,170 --> 00:40:58,060
 

2265
00:40:55,180 --> 00:41:01,840
 sampling for the rest okay so I'm going

2266
00:40:58,050 --> 00:41:01,840
 

2267
00:40:58,060 --> 00:41:03,520
 to start out on this EP MCMC thread and

2268
00:41:01,830 --> 00:41:03,520
 

2269
00:41:01,840 --> 00:41:06,670
 so the idea is that we have some big

2270
00:41:03,510 --> 00:41:06,670
 

2271
00:41:03,520 --> 00:41:08,650
 data set with really large n we'd like

2272
00:41:06,660 --> 00:41:08,650
 

2273
00:41:06,670 --> 00:41:10,600
 to divide the data into subsets and

2274
00:41:08,640 --> 00:41:10,600
 

2275
00:41:08,650 --> 00:41:13,120
 sometimes I find we're really kind of

2276
00:41:10,590 --> 00:41:13,120
 

2277
00:41:10,600 --> 00:41:14,860
 trying to work seriously on really large

2278
00:41:13,110 --> 00:41:14,860
 

2279
00:41:13,120 --> 00:41:16,960
 data problems for example we have a

2280
00:41:14,850 --> 00:41:16,960
 

2281
00:41:14,860 --> 00:41:18,700
 collaboration with Alibaba where they

2282
00:41:16,950 --> 00:41:18,700
 

2283
00:41:16,960 --> 00:41:21,340
 have really like billions and billions

2284
00:41:18,690 --> 00:41:21,340
 

2285
00:41:18,700 --> 00:41:22,900
 of users in different domains and we'd

2286
00:41:21,330 --> 00:41:22,900
 

2287
00:41:21,340 --> 00:41:24,340
 like to be kind of applying these types

2288
00:41:22,890 --> 00:41:24,340
 

2289
00:41:22,900 --> 00:41:26,650
 of methods and so we're kind of working

2290
00:41:24,330 --> 00:41:26,650
 

2291
00:41:24,340 --> 00:41:29,110
 seriously in these types of domains also

2292
00:41:26,640 --> 00:41:29,110
 

2293
00:41:26,650 --> 00:41:30,550
 with SAS and we find that often you

2294
00:41:29,100 --> 00:41:30,550
 

2295
00:41:29,110 --> 00:41:32,440
 can't really move the data around and

2296
00:41:30,540 --> 00:41:32,440
 

2297
00:41:30,550 --> 00:41:33,940
 cap much control of the subsets and so

2298
00:41:32,430 --> 00:41:33,940
 

2299
00:41:32,440 --> 00:41:35,680
 that's an interesting area is to

2300
00:41:33,930 --> 00:41:35,680
 

2301
00:41:33,940 --> 00:41:37,570
 actually where you just kind of stock

2302
00:41:35,670 --> 00:41:37,570
 

2303
00:41:35,680 --> 00:41:39,310
 with data on some machine from a

2304
00:41:37,560 --> 00:41:39,310
 

2305
00:41:37,570 --> 00:41:41,710
 particular client or some subset of

2306
00:41:39,300 --> 00:41:41,710
 

2307
00:41:39,310 --> 00:41:43,690
 users we can't move things around that

2308
00:41:41,700 --> 00:41:43,690
 

2309
00:41:41,710 --> 00:41:46,060
 much and so we have these data subsets

2310
00:41:43,680 --> 00:41:46,060
 

2311
00:41:43,690 --> 00:41:47,970
 here I'm going to focus on more more

2312
00:41:46,050 --> 00:41:47,970
 

2313
00:41:46,060 --> 00:41:51,070
 intelligently constructed data subsets

2314
00:41:47,960 --> 00:41:51,070
 

2315
00:41:47,970 --> 00:41:53,590
 okay so we we take big data we develop

2316
00:41:51,060 --> 00:41:53,590
 

2317
00:41:51,070 --> 00:41:55,420
 we break it into shards and then based

2318
00:41:53,580 --> 00:41:55,420
 

2319
00:41:53,590 --> 00:41:57,190
 on each shard of data we get a subset

2320
00:41:55,410 --> 00:41:57,190
 

2321
00:41:55,420 --> 00:41:59,380
 posterior then we somehow combine them

2322
00:41:57,180 --> 00:41:59,380
 

2323
00:41:57,190 --> 00:42:02,440
 magically into a beautiful accurate

2324
00:41:59,370 --> 00:42:02,440
 

2325
00:41:59,380 --> 00:42:04,600
 posterior approximation okay so we're

2326
00:42:02,430 --> 00:42:04,600
 

2327
00:42:02,440 --> 00:42:07,570
 gonna be drawing samples from each

2328
00:42:04,590 --> 00:42:07,570
 

2329
00:42:04,600 --> 00:42:09,700
 subset posterior and parallel and then

2330
00:42:07,560 --> 00:42:09,700
 

2331
00:42:07,570 --> 00:42:11,380
 then we get sort of atoms or a bunch of

2332
00:42:09,690 --> 00:42:11,380
 

2333
00:42:09,700 --> 00:42:14,380
 samples and then based on the samples

2334
00:42:11,370 --> 00:42:14,380
 

2335
00:42:11,380 --> 00:42:15,790
 we'd like to combine so how can we do

2336
00:42:14,370 --> 00:42:15,790
 

2337
00:42:14,380 --> 00:42:16,700
 that well here here's this got a toy

2338
00:42:15,780 --> 00:42:16,700
 

2339
00:42:15,790 --> 00:42:18,560
 example that

2340
00:42:16,690 --> 00:42:18,560
 

2341
00:42:16,700 --> 00:42:20,930
 use a lot of logistic regression and so

2342
00:42:18,550 --> 00:42:20,930
 

2343
00:42:18,560 --> 00:42:23,810
 we might say you know what's the label

2344
00:42:20,920 --> 00:42:23,810
 

2345
00:42:20,930 --> 00:42:26,869
 on the ice example given the vector

2346
00:42:23,800 --> 00:42:26,869
 

2347
00:42:23,810 --> 00:42:29,390
 features X I 1 through X IP and some

2348
00:42:26,859 --> 00:42:29,390
 

2349
00:42:26,869 --> 00:42:31,670
 parameters theta okay so that's just a

2350
00:42:29,380 --> 00:42:31,670
 

2351
00:42:29,390 --> 00:42:33,650
 logistic regression and and well here

2352
00:42:31,660 --> 00:42:33,650
 

2353
00:42:31,670 --> 00:42:34,970
 here shows a bunch of subsets posteriors

2354
00:42:33,640 --> 00:42:34,970
 

2355
00:42:33,650 --> 00:42:36,859
 and so we get a bunch of noisy

2356
00:42:34,960 --> 00:42:36,859
 

2357
00:42:34,970 --> 00:42:39,410
 approximations the full data posterior

2358
00:42:36,849 --> 00:42:39,410
 

2359
00:42:36,859 --> 00:42:41,060
 and you know you think usually often

2360
00:42:39,400 --> 00:42:41,060
 

2361
00:42:39,410 --> 00:42:43,339
 divide and conquer algorithms we have a

2362
00:42:41,050 --> 00:42:43,339
 

2363
00:42:41,060 --> 00:42:46,010
 bunch of chunks of the data and we get

2364
00:42:43,329 --> 00:42:46,010
 

2365
00:42:43,339 --> 00:42:47,510
 like some some estimate of theta hat or

2366
00:42:46,000 --> 00:42:47,510
 

2367
00:42:46,010 --> 00:42:49,420
 something on each subset and then we

2368
00:42:47,500 --> 00:42:49,420
 

2369
00:42:47,510 --> 00:42:52,369
 take the median or we combine some how

2370
00:42:49,410 --> 00:42:52,369
 

2371
00:42:49,420 --> 00:42:54,650
 well here the challenge is that we have

2372
00:42:52,359 --> 00:42:54,650
 

2373
00:42:52,369 --> 00:42:56,690
 a probability distribution and so we

2374
00:42:54,640 --> 00:42:56,690
 

2375
00:42:54,650 --> 00:42:59,570
 have a full probability distribution for

2376
00:42:56,680 --> 00:42:59,570
 

2377
00:42:56,690 --> 00:43:00,920
 each subset and you know will this give

2378
00:42:59,560 --> 00:43:00,920
 

2379
00:42:59,570 --> 00:43:03,619
 it only the subset it should be like

2380
00:43:00,910 --> 00:43:03,619
 

2381
00:43:00,920 --> 00:43:04,970
 it's too it's too variable you know

2382
00:43:03,609 --> 00:43:04,970
 

2383
00:43:03,619 --> 00:43:06,290
 relative to the full data posterior is

2384
00:43:04,960 --> 00:43:06,290
 

2385
00:43:04,970 --> 00:43:07,700
 really this really concentrated thing

2386
00:43:06,280 --> 00:43:07,700
 

2387
00:43:06,290 --> 00:43:09,829
 around some value maybe it's even

2388
00:43:07,690 --> 00:43:09,829
 

2389
00:43:07,700 --> 00:43:11,510
 multimodal and then the subset posterior

2390
00:43:09,819 --> 00:43:11,510
 

2391
00:43:09,829 --> 00:43:13,670
 czar like to two variable but they're

2392
00:43:11,500 --> 00:43:13,670
 

2393
00:43:11,510 --> 00:43:15,170
 all probability distributions and so we

2394
00:43:13,660 --> 00:43:15,170
 

2395
00:43:13,670 --> 00:43:18,230
 end up with this really big problem and

2396
00:43:15,160 --> 00:43:18,230
 

2397
00:43:15,170 --> 00:43:20,329
 how do we how do we combine them and so

2398
00:43:18,220 --> 00:43:20,329
 

2399
00:43:18,230 --> 00:43:22,220
 here I just shown some results some

2400
00:43:20,319 --> 00:43:22,220
 

2401
00:43:20,329 --> 00:43:25,040
 where the contour plots were just two

2402
00:43:22,210 --> 00:43:25,040
 

2403
00:43:22,220 --> 00:43:26,960
 the parameters the the based on MCMC

2404
00:43:25,030 --> 00:43:26,960
 

2405
00:43:25,040 --> 00:43:29,660
 which we ran for a week or something is

2406
00:43:26,950 --> 00:43:29,660
 

2407
00:43:26,960 --> 00:43:32,930
 is here in blue we have an approach

2408
00:43:29,650 --> 00:43:32,930
 

2409
00:43:29,660 --> 00:43:35,180
 called wasp which is in red which gives

2410
00:43:32,920 --> 00:43:35,180
 

2411
00:43:32,930 --> 00:43:36,890
 you a very accurate approximation and in

2412
00:43:35,170 --> 00:43:36,890
 

2413
00:43:35,180 --> 00:43:40,520
 green shows the contours for the subset

2414
00:43:36,880 --> 00:43:40,520
 

2415
00:43:36,890 --> 00:43:41,780
 posteriors okay so so basically what

2416
00:43:40,510 --> 00:43:41,780
 

2417
00:43:40,520 --> 00:43:43,880
 we're going to do is use this type of

2418
00:43:41,770 --> 00:43:43,880
 

2419
00:43:41,780 --> 00:43:46,400
 stochastic approximation to the full

2420
00:43:43,870 --> 00:43:46,400
 

2421
00:43:43,880 --> 00:43:48,170
 data posterior distribution based on

2422
00:43:46,390 --> 00:43:48,170
 

2423
00:43:46,400 --> 00:43:51,020
 each subset and we're going to average

2424
00:43:48,160 --> 00:43:51,020
 

2425
00:43:48,170 --> 00:43:52,700
 in some type of way geometrically to

2426
00:43:51,010 --> 00:43:52,700
 

2427
00:43:51,020 --> 00:43:54,470
 reduce noise okay so it's a type of

2428
00:43:52,690 --> 00:43:54,470
 

2429
00:43:52,700 --> 00:43:57,980
 stochastic approximation algorithm that

2430
00:43:54,460 --> 00:43:57,980
 

2431
00:43:54,470 --> 00:43:59,839
 produces this ok so we start out we have

2432
00:43:57,970 --> 00:43:59,839
 

2433
00:43:57,980 --> 00:44:03,140
 the full data posterior distribution PI

2434
00:43:59,829 --> 00:44:03,140
 

2435
00:43:59,839 --> 00:44:05,119
 of theta given Y and this is just you

2436
00:44:03,130 --> 00:44:05,119
 

2437
00:44:03,140 --> 00:44:08,060
 know some notation for that we're gonna

2438
00:44:05,109 --> 00:44:08,060
 

2439
00:44:05,119 --> 00:44:11,630
 divide the full data Y n into K subsets

2440
00:44:08,050 --> 00:44:11,630
 

2441
00:44:08,060 --> 00:44:13,369
 of size M okay break up all the data

2442
00:44:11,620 --> 00:44:13,369
 

2443
00:44:11,630 --> 00:44:16,339
 into shards throw them on different

2444
00:44:13,359 --> 00:44:16,339
 

2445
00:44:13,369 --> 00:44:20,960
 machines or processors and then based on

2446
00:44:16,329 --> 00:44:20,960
 

2447
00:44:16,339 --> 00:44:23,690
 each chunk of the data Y Y subscript J

2448
00:44:20,950 --> 00:44:23,690
 

2449
00:44:20,960 --> 00:44:25,700
 in brackets we define some sort of

2450
00:44:23,680 --> 00:44:25,700
 

2451
00:44:23,690 --> 00:44:27,109
 subset posterior and the goal here is

2452
00:44:25,690 --> 00:44:27,109
 

2453
00:44:25,700 --> 00:44:29,839
 we'd like to take a subset of the data

2454
00:44:27,099 --> 00:44:29,839
 

2455
00:44:27,109 --> 00:44:30,450
 to approximate give us a noisy

2456
00:44:29,829 --> 00:44:30,450
 

2457
00:44:29,839 --> 00:44:32,190
 approximate

2458
00:44:30,440 --> 00:44:32,190
 

2459
00:44:30,450 --> 00:44:33,839
 the full data posterior distribution

2460
00:44:32,180 --> 00:44:33,839
 

2461
00:44:32,190 --> 00:44:35,460
 okay and then we're going to sort of

2462
00:44:33,829 --> 00:44:35,460
 

2463
00:44:33,839 --> 00:44:37,560
 average those to get a really nice

2464
00:44:35,450 --> 00:44:37,560
 

2465
00:44:35,460 --> 00:44:40,109
 approximation of the full data posterior

2466
00:44:37,550 --> 00:44:40,109
 

2467
00:44:37,560 --> 00:44:42,060
 reducing noise and so how we do that is

2468
00:44:40,099 --> 00:44:42,060
 

2469
00:44:40,109 --> 00:44:44,880
 we take the light this is a likelihood

2470
00:44:42,050 --> 00:44:44,880
 

2471
00:44:42,060 --> 00:44:46,740
 for one observation we take that all

2472
00:44:44,870 --> 00:44:46,740
 

2473
00:44:44,880 --> 00:44:48,540
 those observations in that shard and

2474
00:44:46,730 --> 00:44:48,540
 

2475
00:44:46,740 --> 00:44:50,880
 we're gonna raise those likelihood to

2476
00:44:48,530 --> 00:44:50,880
 

2477
00:44:48,540 --> 00:44:53,280
 some power gamma and so it's a type of

2478
00:44:50,870 --> 00:44:53,280
 

2479
00:44:50,880 --> 00:44:56,280
 gamma as this power chosen to minimize

2480
00:44:53,270 --> 00:44:56,280
 

2481
00:44:53,280 --> 00:44:57,930
 approximation error okay but over all

2482
00:44:56,270 --> 00:44:57,930
 

2483
00:44:56,280 --> 00:44:59,550
 the subsets posterior just looks just

2484
00:44:57,920 --> 00:44:59,550
 

2485
00:44:57,930 --> 00:45:02,420
 like a posterior distribution just with

2486
00:44:59,540 --> 00:45:02,420
 

2487
00:44:59,550 --> 00:45:05,190
 this tweak with this power gamma okay

2488
00:45:02,410 --> 00:45:05,190
 

2489
00:45:02,420 --> 00:45:07,980
 otherwise we could just do MCMC within

2490
00:45:05,180 --> 00:45:07,980
 

2491
00:45:05,190 --> 00:45:10,320
 subset note no problem okay so um so

2492
00:45:07,970 --> 00:45:10,320
 

2493
00:45:07,980 --> 00:45:12,109
 then that what we're gonna do is we're

2494
00:45:10,310 --> 00:45:12,109
 

2495
00:45:10,320 --> 00:45:14,430
 gonna use this notion of a Wasserstein

2496
00:45:12,099 --> 00:45:14,430
 

2497
00:45:12,109 --> 00:45:16,140
 barycentre which has become increasingly

2498
00:45:14,420 --> 00:45:16,140
 

2499
00:45:14,430 --> 00:45:17,700
 popular i think a lot of people are

2500
00:45:16,130 --> 00:45:17,700
 

2501
00:45:16,140 --> 00:45:19,170
 interested in the machine learning and

2502
00:45:17,690 --> 00:45:19,170
 

2503
00:45:17,700 --> 00:45:21,390
 statistics and optimal transport

2504
00:45:19,160 --> 00:45:21,390
 

2505
00:45:19,170 --> 00:45:23,130
 problems and so we're kind of defining

2506
00:45:21,380 --> 00:45:23,130
 

2507
00:45:21,390 --> 00:45:24,990
 things as a type of optimal transport

2508
00:45:23,120 --> 00:45:24,990
 

2509
00:45:23,130 --> 00:45:26,940
 problem and so we're getting a bunch of

2510
00:45:24,980 --> 00:45:26,940
 

2511
00:45:24,990 --> 00:45:29,550
 noisy approximations to the full data

2512
00:45:26,930 --> 00:45:29,550
 

2513
00:45:26,940 --> 00:45:30,930
 posterior based on subsets and we would

2514
00:45:29,540 --> 00:45:30,930
 

2515
00:45:29,550 --> 00:45:34,440
 like to kind of take some sort of

2516
00:45:30,920 --> 00:45:34,440
 

2517
00:45:30,930 --> 00:45:35,550
 central approximation or barycenter and

2518
00:45:34,430 --> 00:45:35,550
 

2519
00:45:34,440 --> 00:45:38,160
 we're gonna use that as our

2520
00:45:35,540 --> 00:45:38,160
 

2521
00:45:35,550 --> 00:45:39,690
 approximation okay and so if we can

2522
00:45:38,150 --> 00:45:39,690
 

2523
00:45:38,160 --> 00:45:41,369
 define some sort of distance between

2524
00:45:39,680 --> 00:45:41,369
 

2525
00:45:39,690 --> 00:45:43,800
 probability measures and we'll use a

2526
00:45:41,359 --> 00:45:43,800
 

2527
00:45:41,369 --> 00:45:45,510
 Wasserstein distance then we can kind of

2528
00:45:43,790 --> 00:45:45,510
 

2529
00:45:43,800 --> 00:45:46,800
 solve this optimization problem to get a

2530
00:45:45,500 --> 00:45:46,800
 

2531
00:45:45,510 --> 00:45:49,859
 really nice approximation of the

2532
00:45:46,790 --> 00:45:49,859
 

2533
00:45:46,800 --> 00:45:52,290
 posterior okay and so that's this kind

2534
00:45:49,849 --> 00:45:52,290
 

2535
00:45:49,859 --> 00:45:54,420
 of wasp method or Wasserstein barycenter

2536
00:45:52,280 --> 00:45:54,420
 

2537
00:45:52,290 --> 00:45:57,540
 subset posteriors we define a

2538
00:45:54,410 --> 00:45:57,540
 

2539
00:45:54,420 --> 00:46:00,750
 Wasserstein distance between probability

2540
00:45:57,530 --> 00:46:00,750
 

2541
00:45:57,540 --> 00:46:02,790
 distributions and then we can just solve

2542
00:46:00,740 --> 00:46:02,790
 

2543
00:46:00,750 --> 00:46:05,369
 a type of optimization problem using

2544
00:46:02,780 --> 00:46:05,369
 

2545
00:46:02,790 --> 00:46:07,109
 this Wasserstein very center which is a

2546
00:46:05,359 --> 00:46:07,109
 

2547
00:46:05,369 --> 00:46:10,260
 well known quantity in the optimization

2548
00:46:07,099 --> 00:46:10,260
 

2549
00:46:07,109 --> 00:46:12,060
 literature okay and so what one nice

2550
00:46:10,250 --> 00:46:12,060
 

2551
00:46:10,260 --> 00:46:13,740
 thing is that what we would do is we

2552
00:46:12,050 --> 00:46:13,740
 

2553
00:46:12,060 --> 00:46:16,380
 would take each of these subsets

2554
00:46:13,730 --> 00:46:16,380
 

2555
00:46:13,740 --> 00:46:18,030
 posteriors and we would get samples from

2556
00:46:16,370 --> 00:46:18,030
 

2557
00:46:16,380 --> 00:46:20,609
 them and so in parallel on each machine

2558
00:46:18,020 --> 00:46:20,609
 

2559
00:46:18,030 --> 00:46:23,640
 we have a subset of the pub set of the

2560
00:46:20,599 --> 00:46:23,640
 

2561
00:46:20,609 --> 00:46:25,410
 data we run MCMC that's easy because now

2562
00:46:23,630 --> 00:46:25,410
 

2563
00:46:23,640 --> 00:46:26,940
 the problem size isn't big because we

2564
00:46:25,400 --> 00:46:26,940
 

2565
00:46:25,410 --> 00:46:28,980
 have a small sample size on each machine

2566
00:46:26,930 --> 00:46:28,980
 

2567
00:46:26,940 --> 00:46:32,069
 we just run in parallel using whatever

2568
00:46:28,970 --> 00:46:32,069
 

2569
00:46:28,980 --> 00:46:33,750
 MCMC algorithm we take those samples we

2570
00:46:32,059 --> 00:46:33,750
 

2571
00:46:32,069 --> 00:46:35,069
 plug them into this discrete optimal

2572
00:46:33,740 --> 00:46:35,069
 

2573
00:46:33,750 --> 00:46:37,230
 transport problem to calculate the

2574
00:46:35,059 --> 00:46:37,230
 

2575
00:46:35,069 --> 00:46:38,520
 barycenter it's a sparse linear program

2576
00:46:37,220 --> 00:46:38,520
 

2577
00:46:37,230 --> 00:46:42,000
 that we can calculate it very quickly

2578
00:46:38,510 --> 00:46:42,000
 

2579
00:46:38,520 --> 00:46:43,980
 okay and that's all we do okay it also

2580
00:46:41,990 --> 00:46:43,980
 

2581
00:46:42,000 --> 00:46:44,369
 then gives you an atomic approximation

2582
00:46:43,970 --> 00:46:44,369
 

2583
00:46:43,980 --> 00:46:45,809
 and

2584
00:46:44,359 --> 00:46:45,809
 

2585
00:46:44,369 --> 00:46:47,759
 because it's going to take those samples

2586
00:46:45,799 --> 00:46:47,759
 

2587
00:46:45,809 --> 00:46:49,769
 and reweighed them and then now at the

2588
00:46:47,749 --> 00:46:49,769
 

2589
00:46:47,759 --> 00:46:52,109
 end of the day we're like oh we have

2590
00:46:49,759 --> 00:46:52,109
 

2591
00:46:49,769 --> 00:46:54,630
 waited samples from the full posterior

2592
00:46:52,099 --> 00:46:54,630
 

2593
00:46:52,109 --> 00:46:56,430
 distribution given the whole data and we

2594
00:46:54,620 --> 00:46:56,430
 

2595
00:46:54,630 --> 00:46:58,470
 can use that to calculate any post ear

2596
00:46:56,420 --> 00:46:58,470
 

2597
00:46:56,430 --> 00:47:03,059
 summaries we want for any functional

2598
00:46:58,460 --> 00:47:03,059
 

2599
00:46:58,470 --> 00:47:04,710
 just quite easily okay yeah we were

2600
00:47:03,049 --> 00:47:04,710
 

2601
00:47:03,059 --> 00:47:08,099
 supposed to break but I'll just finish

2602
00:47:04,700 --> 00:47:08,099
 

2603
00:47:04,710 --> 00:47:09,749
 up this this this shard of the talk okay

2604
00:47:08,089 --> 00:47:09,749
 

2605
00:47:08,099 --> 00:47:11,640
 so the minimizing Wasserstein is a

2606
00:47:09,739 --> 00:47:11,640
 

2607
00:47:09,749 --> 00:47:13,619
 solution to a discrete optimal transport

2608
00:47:11,630 --> 00:47:13,619
 

2609
00:47:11,640 --> 00:47:15,960
 problem and we could say oh well like a

2610
00:47:13,609 --> 00:47:15,960
 

2611
00:47:13,619 --> 00:47:19,049
 pair of subsets posteriors we can write

2612
00:47:15,950 --> 00:47:19,049
 

2613
00:47:15,960 --> 00:47:20,700
 as like on this atomic form they're like

2614
00:47:19,039 --> 00:47:20,700
 

2615
00:47:19,049 --> 00:47:23,400
 weights on different atoms the atoms

2616
00:47:20,690 --> 00:47:23,400
 

2617
00:47:20,700 --> 00:47:25,200
 we're taking by sampling from MCMC and

2618
00:47:23,390 --> 00:47:25,200
 

2619
00:47:23,400 --> 00:47:26,970
 we have a couple of probability

2620
00:47:25,190 --> 00:47:26,970
 

2621
00:47:25,200 --> 00:47:28,890
 distributions and then we can define a

2622
00:47:26,960 --> 00:47:28,890
 

2623
00:47:26,970 --> 00:47:31,920
 matrix of squared differences in the

2624
00:47:28,880 --> 00:47:31,920
 

2625
00:47:28,890 --> 00:47:34,529
 atoms we can define a what's called an

2626
00:47:31,910 --> 00:47:34,529
 

2627
00:47:31,920 --> 00:47:36,569
 optimal transportation polytope and we

2628
00:47:34,519 --> 00:47:36,569
 

2629
00:47:34,529 --> 00:47:38,849
 can then solve this type of optimization

2630
00:47:36,559 --> 00:47:38,849
 

2631
00:47:36,569 --> 00:47:40,559
 problem using you know just stuff that's

2632
00:47:38,839 --> 00:47:40,559
 

2633
00:47:38,849 --> 00:47:43,470
 already been developed in the literature

2634
00:47:40,549 --> 00:47:43,470
 

2635
00:47:40,559 --> 00:47:45,119
 for solving these types of optimal

2636
00:47:43,460 --> 00:47:45,119
 

2637
00:47:43,470 --> 00:47:48,509
 transport problems using sparse LP

2638
00:47:45,109 --> 00:47:48,509
 

2639
00:47:45,119 --> 00:47:50,930
 solver solvers okay and we won one nice

2640
00:47:48,499 --> 00:47:50,930
 

2641
00:47:48,509 --> 00:47:53,819
 thing is that then we get this wasp

2642
00:47:50,920 --> 00:47:53,819
 

2643
00:47:50,930 --> 00:47:55,650
 posterior approximation and we can prove

2644
00:47:53,809 --> 00:47:55,650
 

2645
00:47:53,819 --> 00:47:58,440
 lots of things about this approximation

2646
00:47:55,640 --> 00:47:58,440
 

2647
00:47:55,650 --> 00:48:01,109
 that actually you know is an accurate

2648
00:47:58,430 --> 00:48:01,109
 

2649
00:47:58,440 --> 00:48:03,630
 approximation and it converges nicely

2650
00:48:01,099 --> 00:48:03,630
 

2651
00:48:01,109 --> 00:48:06,089
 about some optimal value of the

2652
00:48:03,620 --> 00:48:06,089
 

2653
00:48:03,630 --> 00:48:08,160
 parameters and so I'm going to stop here

2654
00:48:06,079 --> 00:48:08,160
 

2655
00:48:06,089 --> 00:48:10,230
 because it's break time because this is

2656
00:48:08,150 --> 00:48:10,230
 

2657
00:48:08,160 --> 00:48:12,809
 actually a simple vert simpler version

2658
00:48:10,220 --> 00:48:12,809
 

2659
00:48:10,230 --> 00:48:14,220
 that's I think much more useful in

2660
00:48:12,799 --> 00:48:14,220
 

2661
00:48:12,809 --> 00:48:16,410
 practice because we can actually bypass

2662
00:48:14,210 --> 00:48:16,410
 

2663
00:48:14,220 --> 00:48:18,029
 doing all of that optimal transport part

2664
00:48:16,400 --> 00:48:18,029
 

2665
00:48:16,410 --> 00:48:19,650
 which may which makes things a lot

2666
00:48:18,019 --> 00:48:19,650
 

2667
00:48:18,029 --> 00:48:21,779
 easier but I'll talk about that after

2668
00:48:19,640 --> 00:48:21,779
 

2669
00:48:19,650 --> 00:48:27,619
 the break I think we have ten minutes

2670
00:48:21,769 --> 00:48:27,619
 

2671
00:48:21,779 --> 00:48:27,619
 come back at 9:30 okay

2672
00:48:31,370 --> 00:48:31,370
 

2673
00:48:31,380 --> 00:48:36,940
 so this is another variant that's some

2674
00:48:34,290 --> 00:48:36,940
 

2675
00:48:34,300 --> 00:48:39,730
 kind of similar but that we just had

2676
00:48:36,930 --> 00:48:39,730
 

2677
00:48:36,940 --> 00:48:42,790
 published last year in biometrika so the

2678
00:48:39,720 --> 00:48:42,790
 

2679
00:48:39,730 --> 00:48:45,760
 idea here is that usually when we're

2680
00:48:42,780 --> 00:48:45,760
 

2681
00:48:42,790 --> 00:48:47,800
 doing Bayesian inference we're almost

2682
00:48:45,750 --> 00:48:47,800
 

2683
00:48:45,760 --> 00:48:49,390
 always focusing on one-dimensional

2684
00:48:47,790 --> 00:48:49,390
 

2685
00:48:47,800 --> 00:48:50,920
 functionals and so we have this joint

2686
00:48:49,380 --> 00:48:50,920
 

2687
00:48:49,390 --> 00:48:54,340
 posterior distribution which might be

2688
00:48:50,910 --> 00:48:54,340
 

2689
00:48:50,920 --> 00:48:56,170
 four five thousand parameters but we're

2690
00:48:54,330 --> 00:48:56,170
 

2691
00:48:54,340 --> 00:48:58,170
 usually focusing on one-dimensional

2692
00:48:56,160 --> 00:48:58,170
 

2693
00:48:56,170 --> 00:49:00,600
 things when we're reporting the results

2694
00:48:58,160 --> 00:49:00,600
 

2695
00:48:58,170 --> 00:49:03,670
 so the one-dimensional thing might be

2696
00:49:00,590 --> 00:49:03,670
 

2697
00:49:00,600 --> 00:49:06,490
 the probability of a class label or a

2698
00:49:03,660 --> 00:49:06,490
 

2699
00:49:03,670 --> 00:49:08,380
 predictive distribution or you know a

2700
00:49:06,480 --> 00:49:08,380
 

2701
00:49:06,490 --> 00:49:09,490
 parameter of interest or functional the

2702
00:49:08,370 --> 00:49:09,490
 

2703
00:49:08,380 --> 00:49:11,530
 parameters of interest it's almost

2704
00:49:09,480 --> 00:49:11,530
 

2705
00:49:09,490 --> 00:49:14,010
 always a one-dimensional thing that we

2706
00:49:11,520 --> 00:49:14,010
 

2707
00:49:11,530 --> 00:49:16,450
 we're poor it at the end of the day okay

2708
00:49:14,000 --> 00:49:16,450
 

2709
00:49:14,010 --> 00:49:18,120
 so we're reporting point in interval

2710
00:49:16,440 --> 00:49:18,120
 

2711
00:49:16,450 --> 00:49:22,120
 estimates for different 1d functionals

2712
00:49:18,110 --> 00:49:22,120
 

2713
00:49:18,120 --> 00:49:23,680
 okay so keep that in mind so so if we do

2714
00:49:22,110 --> 00:49:23,680
 

2715
00:49:22,120 --> 00:49:26,860
 that and we think about well the the

2716
00:49:23,670 --> 00:49:26,860
 

2717
00:49:23,680 --> 00:49:28,840
 Wasserstein barycentre of the subsets it

2718
00:49:26,850 --> 00:49:28,840
 

2719
00:49:26,860 --> 00:49:31,440
 has an explicit relationship with the

2720
00:49:28,830 --> 00:49:31,440
 

2721
00:49:28,840 --> 00:49:35,170
 subset posteriors in 1d okay in

2722
00:49:31,430 --> 00:49:35,170
 

2723
00:49:31,440 --> 00:49:36,910
 particular the quantiles of our simple

2724
00:49:35,160 --> 00:49:36,910
 

2725
00:49:35,170 --> 00:49:40,090
 averages of quantiles of the subsets

2726
00:49:36,900 --> 00:49:40,090
 

2727
00:49:36,910 --> 00:49:41,920
 posteriors okay and so that this leads

2728
00:49:40,080 --> 00:49:41,920
 

2729
00:49:40,090 --> 00:49:45,160
 to a really trivial algorithm and so the

2730
00:49:41,910 --> 00:49:45,160
 

2731
00:49:41,920 --> 00:49:46,990
 algorithm is super super trivial and so

2732
00:49:45,150 --> 00:49:46,990
 

2733
00:49:45,160 --> 00:49:49,240
 what we do is we have a giant data set

2734
00:49:46,980 --> 00:49:49,240
 

2735
00:49:46,990 --> 00:49:51,940
 we break it up into shards we throw

2736
00:49:49,230 --> 00:49:51,940
 

2737
00:49:49,240 --> 00:49:53,590
 those on different machines we run MCMC

2738
00:49:51,930 --> 00:49:53,590
 

2739
00:49:51,940 --> 00:49:55,930
 in parallel with the likelyhood raised

2740
00:49:53,580 --> 00:49:55,930
 

2741
00:49:53,590 --> 00:49:57,820
 to that power that I mentioned and

2742
00:49:55,920 --> 00:49:57,820
 

2743
00:49:55,930 --> 00:50:00,130
 that's really easy and okay now what do

2744
00:49:57,810 --> 00:50:00,130
 

2745
00:49:57,820 --> 00:50:02,740
 we do so now we're interested in some

2746
00:50:00,120 --> 00:50:02,740
 

2747
00:50:00,130 --> 00:50:06,480
 posterior functional f of theta or some

2748
00:50:02,730 --> 00:50:06,480
 

2749
00:50:02,740 --> 00:50:10,170
 predictive distribution F of Y given X

2750
00:50:06,470 --> 00:50:10,170
 

2751
00:50:06,480 --> 00:50:14,050
 okay well we we keep track of that

2752
00:50:10,160 --> 00:50:14,050
 

2753
00:50:10,170 --> 00:50:16,750
 samples from that on each machine we we

2754
00:50:14,040 --> 00:50:16,750
 

2755
00:50:14,050 --> 00:50:18,610
 calculate a percentile of the posterior

2756
00:50:16,740 --> 00:50:18,610
 

2757
00:50:16,750 --> 00:50:20,530
 distribution on each machine in parallel

2758
00:50:18,600 --> 00:50:20,530
 

2759
00:50:18,610 --> 00:50:22,990
 we feed those back to the central

2760
00:50:20,520 --> 00:50:22,990
 

2761
00:50:20,530 --> 00:50:25,630
 processor and we average them okay and

2762
00:50:22,980 --> 00:50:25,630
 

2763
00:50:22,990 --> 00:50:27,570
 then we can based on that we can get we

2764
00:50:25,620 --> 00:50:27,570
 

2765
00:50:25,630 --> 00:50:31,090
 can get a approvable super accurate

2766
00:50:27,560 --> 00:50:31,090
 

2767
00:50:27,570 --> 00:50:34,510
 approximate 1d posterior approximation

2768
00:50:31,080 --> 00:50:34,510
 

2769
00:50:31,090 --> 00:50:36,400
 for any functional of interest and have

2770
00:50:34,500 --> 00:50:36,400
 

2771
00:50:34,510 --> 00:50:39,250
 very very very strong theoretical

2772
00:50:36,390 --> 00:50:39,250
 

2773
00:50:36,400 --> 00:50:40,750
 guarantees and and good at performance

2774
00:50:39,240 --> 00:50:40,750
 

2775
00:50:39,250 --> 00:50:43,810
 on those and and it's really really easy

2776
00:50:40,740 --> 00:50:43,810
 

2777
00:50:40,750 --> 00:50:44,849
 all we do is just break the data up put

2778
00:50:43,800 --> 00:50:44,849
 

2779
00:50:43,810 --> 00:50:46,829
 it on different machines

2780
00:50:44,839 --> 00:50:46,829
 

2781
00:50:44,849 --> 00:50:48,299
 I'm sensing parallel calculate a

2782
00:50:46,819 --> 00:50:48,299
 

2783
00:50:46,829 --> 00:50:50,849
 percentile or quantile for any

2784
00:50:48,289 --> 00:50:50,849
 

2785
00:50:48,299 --> 00:50:52,440
 functional of interest send those back

2786
00:50:50,839 --> 00:50:52,440
 

2787
00:50:50,849 --> 00:50:55,559
 to the central processor and average

2788
00:50:52,430 --> 00:50:55,559
 

2789
00:50:52,440 --> 00:50:57,690
 them okay we didn't realize this at the

2790
00:50:55,549 --> 00:50:57,690
 

2791
00:50:55,559 --> 00:50:59,970
 time but it's sort of reminiscent of a

2792
00:50:57,680 --> 00:50:59,970
 

2793
00:50:57,690 --> 00:51:01,710
 paper that Mike Jordan and collaborators

2794
00:50:59,960 --> 00:51:01,710
 

2795
00:50:59,970 --> 00:51:03,720
 came up with called bag of little

2796
00:51:01,700 --> 00:51:03,720
 

2797
00:51:01,710 --> 00:51:07,200
 bootstraps so it's sort of almost like

2798
00:51:03,710 --> 00:51:07,200
 

2799
00:51:03,720 --> 00:51:08,940
 an MCMC version of that okay so we have

2800
00:51:07,190 --> 00:51:08,940
 

2801
00:51:07,200 --> 00:51:11,160
 quite nice theory showing accuracy of

2802
00:51:08,930 --> 00:51:11,160
 

2803
00:51:08,940 --> 00:51:13,019
 these approximations you can potentially

2804
00:51:11,150 --> 00:51:13,019
 

2805
00:51:11,160 --> 00:51:14,759
 implement it and Stan which is a sort of

2806
00:51:13,009 --> 00:51:14,759
 

2807
00:51:13,019 --> 00:51:16,950
 probabilistic programming language for

2808
00:51:14,749 --> 00:51:16,950
 

2809
00:51:14,759 --> 00:51:20,160
 Bayesian inference which because they

2810
00:51:16,940 --> 00:51:20,160
 

2811
00:51:16,950 --> 00:51:21,869
 allow powered likelihoods okay I don't

2812
00:51:20,150 --> 00:51:21,869
 

2813
00:51:20,160 --> 00:51:26,249
 want to get too much into the theory

2814
00:51:21,859 --> 00:51:26,249
 

2815
00:51:21,869 --> 00:51:28,079
 today but but did you just say that too

2816
00:51:26,239 --> 00:51:28,079
 

2817
00:51:26,249 --> 00:51:30,029
 you know as the subset sample size

2818
00:51:28,069 --> 00:51:30,029
 

2819
00:51:28,079 --> 00:51:32,009
 increases it doesn't have to it's not

2820
00:51:30,019 --> 00:51:32,009
 

2821
00:51:30,029 --> 00:51:35,579
 really dependent on the subset it's

2822
00:51:31,999 --> 00:51:35,579
 

2823
00:51:32,009 --> 00:51:37,710
 getting large but it as they grow really

2824
00:51:35,569 --> 00:51:37,710
 

2825
00:51:35,579 --> 00:51:38,999
 rapidly we can we can have good good

2826
00:51:37,700 --> 00:51:38,999
 

2827
00:51:37,710 --> 00:51:41,970
 good

2828
00:51:38,989 --> 00:51:41,970
 

2829
00:51:38,999 --> 00:51:43,739
 accurate approximations okay and so the

2830
00:51:41,960 --> 00:51:43,739
 

2831
00:51:41,970 --> 00:51:45,059
 bias variance is quantiles only

2832
00:51:43,729 --> 00:51:45,059
 

2833
00:51:43,739 --> 00:51:47,460
 different higher orders of the total

2834
00:51:45,049 --> 00:51:47,460
 

2835
00:51:45,059 --> 00:51:48,869
 sample size okay

2836
00:51:47,450 --> 00:51:48,869
 

2837
00:51:47,460 --> 00:51:51,299
 so we've implemented this in a really

2838
00:51:48,859 --> 00:51:51,299
 

2839
00:51:48,869 --> 00:51:53,160
 broad variety of data and models I don't

2840
00:51:51,289 --> 00:51:53,160
 

2841
00:51:51,299 --> 00:51:55,469
 have time to show results for this part

2842
00:51:53,150 --> 00:51:55,469
 

2843
00:51:53,160 --> 00:51:57,049
 today but logistic linear random effects

2844
00:51:55,459 --> 00:51:57,049
 

2845
00:51:55,469 --> 00:51:59,339
 models mixtures models matrix

2846
00:51:57,039 --> 00:51:59,339
 

2847
00:51:57,049 --> 00:52:01,670
 factorizations for recommender systems

2848
00:51:59,329 --> 00:52:01,670
 

2849
00:51:59,339 --> 00:52:05,400
 Gaussian process regression

2850
00:52:01,660 --> 00:52:05,400
 

2851
00:52:01,670 --> 00:52:08,670
 nonparametric models etc so we can

2852
00:52:05,390 --> 00:52:08,670
 

2853
00:52:05,400 --> 00:52:10,859
 compare to long runs of MCMC and VB it's

2854
00:52:08,660 --> 00:52:10,859
 

2855
00:52:08,670 --> 00:52:12,630
 much much much faster than MCMC doing

2856
00:52:10,849 --> 00:52:12,630
 

2857
00:52:10,859 --> 00:52:15,359
 this and you get this sort of a win-win

2858
00:52:12,620 --> 00:52:15,359
 

2859
00:52:12,630 --> 00:52:17,400
 because you think well the the time per

2860
00:52:15,349 --> 00:52:17,400
 

2861
00:52:15,359 --> 00:52:19,950
 iteration is too slow as the sample size

2862
00:52:17,390 --> 00:52:19,950
 

2863
00:52:17,400 --> 00:52:21,479
 increases and also the mixing might get

2864
00:52:19,940 --> 00:52:21,479
 

2865
00:52:19,950 --> 00:52:23,400
 worse with the sample size but we're

2866
00:52:21,469 --> 00:52:23,400
 

2867
00:52:21,479 --> 00:52:25,380
 we're breaking up the data and running

2868
00:52:23,390 --> 00:52:25,380
 

2869
00:52:23,400 --> 00:52:27,450
 it in parallel and so we're exploiting

2870
00:52:25,370 --> 00:52:27,450
 

2871
00:52:25,380 --> 00:52:30,089
 parallelization to improve the whole

2872
00:52:27,440 --> 00:52:30,089
 

2873
00:52:27,450 --> 00:52:33,660
 mixing issue and also massively improve

2874
00:52:30,079 --> 00:52:33,660
 

2875
00:52:30,089 --> 00:52:35,039
 the time for iteration okay I found that

2876
00:52:33,650 --> 00:52:35,039
 

2877
00:52:33,660 --> 00:52:36,719
 in many of these problems that there

2878
00:52:35,029 --> 00:52:36,719
 

2879
00:52:35,039 --> 00:52:38,819
 there are actually quite quite good

2880
00:52:36,709 --> 00:52:38,819
 

2881
00:52:36,719 --> 00:52:43,289
 variational Bayes implementations that

2882
00:52:38,809 --> 00:52:43,289
 

2883
00:52:38,819 --> 00:52:45,210
 were competitive okay so I'm you know

2884
00:52:43,279 --> 00:52:45,210
 

2885
00:52:43,289 --> 00:52:46,799
 kind of given the time and the what

2886
00:52:45,200 --> 00:52:46,799
 

2887
00:52:45,210 --> 00:52:48,450
 people were interested in over the break

2888
00:52:46,789 --> 00:52:48,450
 

2889
00:52:46,799 --> 00:52:50,910
 I might just kind of skim through this

2890
00:52:48,440 --> 00:52:50,910
 

2891
00:52:48,450 --> 00:52:52,619
 part but you remember that like in the

2892
00:52:50,900 --> 00:52:52,619
 

2893
00:52:50,910 --> 00:52:56,099
 the big end case there's these different

2894
00:52:52,609 --> 00:52:56,099
 

2895
00:52:52,619 --> 00:52:58,299
 types of methods the one is EPM CMC the

2896
00:52:56,089 --> 00:52:58,299
 

2897
00:52:56,099 --> 00:53:00,429
 the second type of approach is

2898
00:52:58,289 --> 00:53:00,429
 

2899
00:52:58,299 --> 00:53:02,380
 approximate Markov chain Monte Carlo and

2900
00:53:00,419 --> 00:53:02,380
 

2901
00:53:00,429 --> 00:53:03,999
 and and there's a rich literature on

2902
00:53:02,370 --> 00:53:03,999
 

2903
00:53:02,380 --> 00:53:05,769
 this I just kind of listed this one

2904
00:53:03,989 --> 00:53:05,769
 

2905
00:53:03,999 --> 00:53:07,359
 archive paper by a really great student

2906
00:53:05,759 --> 00:53:07,359
 

2907
00:53:05,769 --> 00:53:10,509
 a former student of mine James John drew

2908
00:53:07,349 --> 00:53:10,509
 

2909
00:53:07,359 --> 00:53:13,599
 but the idea here is that we have some

2910
00:53:10,499 --> 00:53:13,599
 

2911
00:53:10,509 --> 00:53:15,339
 MCMC algorithm that it that converges to

2912
00:53:13,589 --> 00:53:15,339
 

2913
00:53:13,599 --> 00:53:18,369
 exactly the right thing but it's too too

2914
00:53:15,329 --> 00:53:18,369
 

2915
00:53:15,339 --> 00:53:21,219
 slow it's too expensive to to evaluate

2916
00:53:18,359 --> 00:53:21,219
 

2917
00:53:18,369 --> 00:53:23,469
 the transition kernel and so we might

2918
00:53:21,209 --> 00:53:23,469
 

2919
00:53:21,219 --> 00:53:24,699
 use instead an approximation for example

2920
00:53:23,459 --> 00:53:24,699
 

2921
00:53:23,469 --> 00:53:26,559
 we might approximate a conditional

2922
00:53:24,689 --> 00:53:26,559
 

2923
00:53:24,699 --> 00:53:28,660
 distribution and give sampling with a

2924
00:53:26,549 --> 00:53:28,660
 

2925
00:53:26,559 --> 00:53:30,479
 gaussian or using a sub sample of the

2926
00:53:28,650 --> 00:53:30,479
 

2927
00:53:28,660 --> 00:53:33,099
 data and so that gives us a type of

2928
00:53:30,469 --> 00:53:33,099
 

2929
00:53:30,479 --> 00:53:35,109
 faster or thing that we can sample from

2930
00:53:33,089 --> 00:53:35,109
 

2931
00:53:33,099 --> 00:53:38,380
 more more quickly and approximate the

2932
00:53:35,099 --> 00:53:38,380
 

2933
00:53:35,109 --> 00:53:40,539
 actual conditional distribution so using

2934
00:53:38,370 --> 00:53:40,539
 

2935
00:53:38,380 --> 00:53:42,099
 this trick we can potentially vastly

2936
00:53:40,529 --> 00:53:42,099
 

2937
00:53:40,539 --> 00:53:44,349
 speed-up MCMC sampling in high

2938
00:53:42,089 --> 00:53:44,349
 

2939
00:53:42,099 --> 00:53:46,150
 dimensional settings one one example is

2940
00:53:44,339 --> 00:53:46,150
 

2941
00:53:44,349 --> 00:53:48,969
 that there's this huge literature on you

2942
00:53:46,140 --> 00:53:48,969
 

2943
00:53:46,150 --> 00:53:50,469
 know Gaussian processes where if I if I

2944
00:53:48,959 --> 00:53:50,469
 

2945
00:53:48,969 --> 00:53:52,119
 don't use an approximation then I have

2946
00:53:50,459 --> 00:53:52,119
 

2947
00:53:50,469 --> 00:53:54,729
 an order n cube matrix inversion

2948
00:53:52,109 --> 00:53:54,729
 

2949
00:53:52,119 --> 00:53:56,650
 bottleneck and so then instead I use one

2950
00:53:54,719 --> 00:53:56,650
 

2951
00:53:54,729 --> 00:53:57,759
 of a hundred different approximations

2952
00:53:56,640 --> 00:53:57,759
 

2953
00:53:56,650 --> 00:53:59,890
 that have been proposed in the

2954
00:53:57,749 --> 00:53:59,890
 

2955
00:53:57,759 --> 00:54:01,660
 literature that that are low rank or

2956
00:53:59,880 --> 00:54:01,660
 

2957
00:53:59,890 --> 00:54:03,880
 nearest neighbor Gaussian process

2958
00:54:01,650 --> 00:54:03,880
 

2959
00:54:01,660 --> 00:54:07,119
 approximations or whatever okay and so

2960
00:54:03,870 --> 00:54:07,119
 

2961
00:54:03,880 --> 00:54:09,729
 that would be a type of a MCMC so the

2962
00:54:07,109 --> 00:54:09,729
 

2963
00:54:07,119 --> 00:54:11,949
 original MCMC sampler converges to the

2964
00:54:09,719 --> 00:54:11,949
 

2965
00:54:09,729 --> 00:54:13,269
 exact posterior distribution but you

2966
00:54:11,939 --> 00:54:13,269
 

2967
00:54:11,949 --> 00:54:14,859
 know it's not necessarily clear what

2968
00:54:13,259 --> 00:54:14,859
 

2969
00:54:13,269 --> 00:54:16,959
 happens when we take that one and we

2970
00:54:14,849 --> 00:54:16,959
 

2971
00:54:14,859 --> 00:54:18,459
 perturb it so we like oh we have this

2972
00:54:16,949 --> 00:54:18,459
 

2973
00:54:16,959 --> 00:54:20,589
 transition kernel now we've like use

2974
00:54:18,449 --> 00:54:20,589
 

2975
00:54:18,459 --> 00:54:22,209
 this approximation well does the thing

2976
00:54:20,579 --> 00:54:22,209
 

2977
00:54:20,589 --> 00:54:26,079
 even converge then what does it converts

2978
00:54:22,199 --> 00:54:26,079
 

2979
00:54:22,209 --> 00:54:28,059
 to excetera so but but this method is

2980
00:54:26,069 --> 00:54:28,059
 

2981
00:54:26,079 --> 00:54:30,039
 used routinely anyway and often there's

2982
00:54:28,049 --> 00:54:30,039
 

2983
00:54:28,059 --> 00:54:32,019
 a rich empirical evidence that the

2984
00:54:30,029 --> 00:54:32,019
 

2985
00:54:30,039 --> 00:54:35,229
 methods do do quite well in terms of

2986
00:54:32,009 --> 00:54:35,229
 

2987
00:54:32,019 --> 00:54:36,819
 uncertainty quantification okay so but

2988
00:54:35,219 --> 00:54:36,819
 

2989
00:54:35,229 --> 00:54:39,549
 we can we can develop some theory in

2990
00:54:36,809 --> 00:54:39,549
 

2991
00:54:36,819 --> 00:54:41,259
 this case so we're gonna define an exact

2992
00:54:39,539 --> 00:54:41,259
 

2993
00:54:39,549 --> 00:54:43,689
 MCMC algorithm which is computationally

2994
00:54:41,249 --> 00:54:43,689
 

2995
00:54:41,259 --> 00:54:45,640
 tractable but has good mixing and we're

2996
00:54:43,679 --> 00:54:45,640
 

2997
00:54:43,689 --> 00:54:46,869
 gonna leverage on that okay and so we're

2998
00:54:45,630 --> 00:54:46,869
 

2999
00:54:45,640 --> 00:54:48,459
 gonna approximate with a more

3000
00:54:46,859 --> 00:54:48,459
 

3001
00:54:46,869 --> 00:54:50,979
 computationally tractable alternative

3002
00:54:48,449 --> 00:54:50,979
 

3003
00:54:48,459 --> 00:54:52,900
 and then we can kind of come up with a

3004
00:54:50,969 --> 00:54:52,900
 

3005
00:54:50,979 --> 00:54:54,880
 type of theory framework that we call

3006
00:54:52,890 --> 00:54:54,880
 

3007
00:54:52,900 --> 00:54:56,650
 cop minimax and so you might have a

3008
00:54:54,870 --> 00:54:56,650
 

3009
00:54:54,880 --> 00:54:59,019
 particular computational budget and then

3010
00:54:56,640 --> 00:54:59,019
 

3011
00:54:56,650 --> 00:55:00,489
 you want to have the optimal algorithm

3012
00:54:59,009 --> 00:55:00,489
 

3013
00:54:59,019 --> 00:55:02,079
 within some class of algorithms to

3014
00:55:00,479 --> 00:55:02,079
 

3015
00:55:00,489 --> 00:55:04,869
 produce the lowest error in a particular

3016
00:55:02,069 --> 00:55:04,869
 

3017
00:55:02,079 --> 00:55:07,089
 computational time and so we can then

3018
00:55:04,859 --> 00:55:07,089
 

3019
00:55:04,869 --> 00:55:09,249
 get get you know tight finite sample

3020
00:55:07,079 --> 00:55:09,249
 

3021
00:55:07,089 --> 00:55:11,559
 bounds on l2 error for a broad class of

3022
00:55:09,239 --> 00:55:11,559
 

3023
00:55:09,249 --> 00:55:12,010
 of approximate Markov chain Monte Carlo

3024
00:55:11,549 --> 00:55:12,010
 

3025
00:55:11,559 --> 00:55:14,290
 algorithm

3026
00:55:12,000 --> 00:55:14,290
 

3027
00:55:12,010 --> 00:55:17,910
 and you can see while which types of

3028
00:55:14,280 --> 00:55:17,910
 

3029
00:55:14,290 --> 00:55:20,140
 algorithms work well often having a

3030
00:55:17,900 --> 00:55:20,140
 

3031
00:55:17,910 --> 00:55:22,420
 non-negligible approximation error works

3032
00:55:20,130 --> 00:55:22,420
 

3033
00:55:20,140 --> 00:55:23,920
 for low computational budgets but as the

3034
00:55:22,410 --> 00:55:23,920
 

3035
00:55:22,420 --> 00:55:25,920
 computational budget increases you want

3036
00:55:23,910 --> 00:55:25,920
 

3037
00:55:23,920 --> 00:55:28,780
 to have a more accurate approximation

3038
00:55:25,910 --> 00:55:28,780
 

3039
00:55:25,920 --> 00:55:30,369
 okay so I'm just gonna skip over this I

3040
00:55:28,770 --> 00:55:30,369
 

3041
00:55:28,780 --> 00:55:32,290
 could you could do it for subsets you

3042
00:55:30,359 --> 00:55:32,290
 

3043
00:55:30,369 --> 00:55:34,630
 can do it for binary outcomes you can do

3044
00:55:32,280 --> 00:55:34,630
 

3045
00:55:32,290 --> 00:55:39,580
 it for mixer models you can do it for

3046
00:55:34,620 --> 00:55:39,580
 

3047
00:55:34,630 --> 00:55:42,820
 GPS etc so let's um I want to skip to

3048
00:55:39,570 --> 00:55:42,820
 

3049
00:55:39,580 --> 00:55:45,730
 the robustness problem okay so so you

3050
00:55:42,810 --> 00:55:45,730
 

3051
00:55:42,820 --> 00:55:47,770
 know this a MCMC EP MCMC these are just

3052
00:55:45,720 --> 00:55:47,770
 

3053
00:55:45,730 --> 00:55:50,320
 these are these are focused on providing

3054
00:55:47,760 --> 00:55:50,320
 

3055
00:55:47,770 --> 00:55:52,660
 algorithms for scaling up MCMC in a

3056
00:55:50,310 --> 00:55:52,660
 

3057
00:55:50,320 --> 00:55:55,330
 formal sense and a practical sense to

3058
00:55:52,650 --> 00:55:55,330
 

3059
00:55:52,660 --> 00:55:57,340
 large data problems but then you know

3060
00:55:55,320 --> 00:55:57,340
 

3061
00:55:55,330 --> 00:55:59,680
 we're trying to approximate the exact

3062
00:55:57,330 --> 00:55:59,680
 

3063
00:55:57,340 --> 00:56:01,420
 posterior distribution and one thing we

3064
00:55:59,670 --> 00:56:01,420
 

3065
00:55:59,680 --> 00:56:03,040
 might worry about is that well the exact

3066
00:56:01,410 --> 00:56:03,040
 

3067
00:56:01,420 --> 00:56:05,350
 posterior distribution might not be very

3068
00:56:03,030 --> 00:56:05,350
 

3069
00:56:03,040 --> 00:56:06,940
 robust in large sample problems and

3070
00:56:05,340 --> 00:56:06,940
 

3071
00:56:05,350 --> 00:56:09,670
 because in standard Bayesian inference

3072
00:56:06,930 --> 00:56:09,670
 

3073
00:56:06,940 --> 00:56:11,320
 as in other model-based inferences it's

3074
00:56:09,660 --> 00:56:11,320
 

3075
00:56:09,670 --> 00:56:14,530
 it's typically assumed that the model is

3076
00:56:11,310 --> 00:56:14,530
 

3077
00:56:11,320 --> 00:56:16,690
 correct okay and so what we worry about

3078
00:56:14,520 --> 00:56:16,690
 

3079
00:56:14,530 --> 00:56:18,280
 is the one thing that having a model is

3080
00:56:16,680 --> 00:56:18,280
 

3081
00:56:16,690 --> 00:56:19,780
 nice and that the parameters might be

3082
00:56:18,270 --> 00:56:19,780
 

3083
00:56:18,280 --> 00:56:21,130
 interpretable we'd like to kind of do

3084
00:56:19,770 --> 00:56:21,130
 

3085
00:56:19,780 --> 00:56:23,980
 inferences on these parameters they

3086
00:56:21,120 --> 00:56:23,980
 

3087
00:56:21,130 --> 00:56:25,780
 might have physical meaning etc but but

3088
00:56:23,970 --> 00:56:25,780
 

3089
00:56:23,980 --> 00:56:27,910
 small violations of the assumption that

3090
00:56:25,770 --> 00:56:27,910
 

3091
00:56:25,780 --> 00:56:29,320
 the model is correct can sometimes have

3092
00:56:27,900 --> 00:56:29,320
 

3093
00:56:27,910 --> 00:56:33,760
 a large impact particularly in large

3094
00:56:29,310 --> 00:56:33,760
 

3095
00:56:29,320 --> 00:56:35,980
 data sets you know it's very famous

3096
00:56:33,750 --> 00:56:35,980
 

3097
00:56:33,760 --> 00:56:37,990
 quote is that all models are wrong and

3098
00:56:35,970 --> 00:56:37,990
 

3099
00:56:35,980 --> 00:56:39,880
 the ability to carefully check modeling

3100
00:56:37,980 --> 00:56:39,880
 

3101
00:56:37,990 --> 00:56:43,210
 assumptions can kind of decrease for big

3102
00:56:39,870 --> 00:56:43,210
 

3103
00:56:39,880 --> 00:56:44,980
 and complicated datasets so what we were

3104
00:56:43,200 --> 00:56:44,980
 

3105
00:56:43,210 --> 00:56:47,290
 interested in doing and this is joint

3106
00:56:44,970 --> 00:56:47,290
 

3107
00:56:44,980 --> 00:56:49,810
 work with a former postdoc a brilliant

3108
00:56:47,280 --> 00:56:49,810
 

3109
00:56:47,290 --> 00:56:51,160
 guy who worked with me Jeff Miller is

3110
00:56:49,800 --> 00:56:51,160
 

3111
00:56:49,810 --> 00:56:53,619
 now at now at Harvard and biostatistics

3112
00:56:51,150 --> 00:56:53,619
 

3113
00:56:51,160 --> 00:56:55,810
 and and we came up with this idea that

3114
00:56:53,609 --> 00:56:55,810
 

3115
00:56:53,619 --> 00:56:57,670
 you could maybe potentially tweak the

3116
00:56:55,800 --> 00:56:57,670
 

3117
00:56:55,810 --> 00:57:00,550
 Bayesian paradigm to be inherently more

3118
00:56:57,660 --> 00:57:00,550
 

3119
00:56:57,670 --> 00:57:03,460
 robust and I'll give an example of the

3120
00:57:00,540 --> 00:57:03,460
 

3121
00:57:00,550 --> 00:57:05,260
 non robustness here so um let's say we

3122
00:57:03,450 --> 00:57:05,260
 

3123
00:57:03,460 --> 00:57:07,000
 wanted to use a mixture of gaussians for

3124
00:57:05,250 --> 00:57:07,000
 

3125
00:57:05,260 --> 00:57:09,369
 clustering and so clustering is like you

3126
00:57:06,990 --> 00:57:09,369
 

3127
00:57:07,000 --> 00:57:11,859
 know unsupervised the big unsupervised

3128
00:57:09,359 --> 00:57:11,859
 

3129
00:57:09,369 --> 00:57:13,810
 learning problem model-based clustering

3130
00:57:11,849 --> 00:57:13,810
 

3131
00:57:11,859 --> 00:57:16,450
 is an incredibly popular type of method

3132
00:57:13,800 --> 00:57:16,450
 

3133
00:57:13,810 --> 00:57:18,190
 and.and you know one type of very

3134
00:57:16,440 --> 00:57:18,190
 

3135
00:57:16,450 --> 00:57:22,720
 popular model-based clustering method is

3136
00:57:18,180 --> 00:57:22,720
 

3137
00:57:18,190 --> 00:57:24,340
 to mix Gaussian okay let's say that the

3138
00:57:22,710 --> 00:57:24,340
 

3139
00:57:22,720 --> 00:57:25,870
 true data distributions like this okay

3140
00:57:24,330 --> 00:57:25,870
 

3141
00:57:24,340 --> 00:57:27,970
 so we have a

3142
00:57:25,860 --> 00:57:27,970
 

3143
00:57:25,870 --> 00:57:30,640
 a mixture of gaussians so we're gonna

3144
00:57:27,960 --> 00:57:30,640
 

3145
00:57:27,970 --> 00:57:32,440
 take a mixture of gaussians and then

3146
00:57:30,630 --> 00:57:32,440
 

3147
00:57:30,640 --> 00:57:34,060
 we're gonna say the true model is not

3148
00:57:32,430 --> 00:57:34,060
 

3149
00:57:32,440 --> 00:57:37,390
 exactly mixture gaussians we're just

3150
00:57:34,050 --> 00:57:37,390
 

3151
00:57:34,060 --> 00:57:38,500
 gonna change the distribution system ixr

3152
00:57:37,380 --> 00:57:38,500
 

3153
00:57:37,390 --> 00:57:40,150
 of gaussians would still be a nice

3154
00:57:38,490 --> 00:57:40,150
 

3155
00:57:38,500 --> 00:57:42,160
 approximation that we would might want

3156
00:57:40,140 --> 00:57:42,160
 

3157
00:57:40,150 --> 00:57:46,570
 to use and then we're gonna see what

3158
00:57:42,150 --> 00:57:46,570
 

3159
00:57:42,160 --> 00:57:48,520
 happens as n increases so the the going

3160
00:57:46,560 --> 00:57:48,520
 

3161
00:57:46,570 --> 00:57:50,830
 down here we go from N equals 200 to N

3162
00:57:48,510 --> 00:57:50,830
 

3163
00:57:48,520 --> 00:57:52,630
 equals 20,000 and now we fit a model

3164
00:57:50,820 --> 00:57:52,630
 

3165
00:57:50,830 --> 00:57:54,160
 that that allows the the number of

3166
00:57:52,620 --> 00:57:54,160
 

3167
00:57:52,630 --> 00:57:57,040
 mixture components or clusters to be

3168
00:57:54,150 --> 00:57:57,040
 

3169
00:57:54,160 --> 00:57:58,990
 unknown what we see is that if we just

3170
00:57:57,030 --> 00:57:58,990
 

3171
00:57:57,040 --> 00:58:00,760
 even slightly perturb the gaussians you

3172
00:57:58,980 --> 00:58:00,760
 

3173
00:57:58,990 --> 00:58:02,500
 can see the red line versus the blue

3174
00:58:00,750 --> 00:58:02,500
 

3175
00:58:00,760 --> 00:58:04,840
 line that we're gonna keep adding

3176
00:58:02,490 --> 00:58:04,840
 

3177
00:58:02,500 --> 00:58:06,430
 mixture components without bound as the

3178
00:58:04,830 --> 00:58:06,430
 

3179
00:58:04,840 --> 00:58:08,080
 sample size goes up so we're sort of

3180
00:58:06,420 --> 00:58:08,080
 

3181
00:58:06,430 --> 00:58:10,270
 letting making the model be more and

3182
00:58:08,070 --> 00:58:10,270
 

3183
00:58:08,080 --> 00:58:11,830
 more complicated even though this

3184
00:58:10,260 --> 00:58:11,830
 

3185
00:58:10,270 --> 00:58:13,600
 mixture of two gaussians provides a

3186
00:58:11,820 --> 00:58:13,600
 

3187
00:58:11,830 --> 00:58:16,060
 really accurate approximation we would

3188
00:58:13,590 --> 00:58:16,060
 

3189
00:58:13,600 --> 00:58:17,770
 like to do that but we instead add more

3190
00:58:16,050 --> 00:58:17,770
 

3191
00:58:16,060 --> 00:58:21,100
 components unnecessarily as the sample

3192
00:58:17,760 --> 00:58:21,100
 

3193
00:58:17,770 --> 00:58:22,840
 size increases yes so the point is that

3194
00:58:21,090 --> 00:58:22,840
 

3195
00:58:21,100 --> 00:58:25,180
 that's the right answer from a patient

3196
00:58:22,830 --> 00:58:25,180
 

3197
00:58:22,840 --> 00:58:27,970
 perspective is that we're allowing the

3198
00:58:25,170 --> 00:58:27,970
 

3199
00:58:25,180 --> 00:58:29,920
 model to be flexible and it can figure

3200
00:58:27,960 --> 00:58:29,920
 

3201
00:58:27,970 --> 00:58:31,690
 out which model to use using beta

3202
00:58:29,910 --> 00:58:31,690
 

3203
00:58:29,920 --> 00:58:33,640
 bayesian model selection and averaging

3204
00:58:31,680 --> 00:58:33,640
 

3205
00:58:31,690 --> 00:58:35,680
 but it's going to then introduce more

3206
00:58:33,630 --> 00:58:35,680
 

3207
00:58:33,640 --> 00:58:38,710
 and more components as the sample size

3208
00:58:35,670 --> 00:58:38,710
 

3209
00:58:35,680 --> 00:58:40,180
 increases to fit the data and then the

3210
00:58:38,700 --> 00:58:40,180
 

3211
00:58:38,710 --> 00:58:42,160
 interpretability the clusters breaks

3212
00:58:40,170 --> 00:58:42,160
 

3213
00:58:40,180 --> 00:58:44,290
 down in large sample sizes and this is a

3214
00:58:42,150 --> 00:58:44,290
 

3215
00:58:42,160 --> 00:58:45,910
 very broad problem it can occur we've

3216
00:58:44,280 --> 00:58:45,910
 

3217
00:58:44,290 --> 00:58:48,820
 found and met many many models

3218
00:58:45,900 --> 00:58:48,820
 

3219
00:58:45,910 --> 00:58:50,590
 particularly when we have a model where

3220
00:58:48,810 --> 00:58:50,590
 

3221
00:58:48,820 --> 00:58:52,060
 the models flexible and so we're doing

3222
00:58:50,580 --> 00:58:52,060
 

3223
00:58:50,590 --> 00:58:54,220
 something like model averaging or

3224
00:58:52,050 --> 00:58:54,220
 

3225
00:58:52,060 --> 00:58:55,720
 there's a set of models and we have some

3226
00:58:54,210 --> 00:58:55,720
 

3227
00:58:54,220 --> 00:58:57,580
 sort of parameter comparing the

3228
00:58:55,710 --> 00:58:57,580
 

3229
00:58:55,720 --> 00:59:01,030
 controlling the complexity of the model

3230
00:58:57,570 --> 00:59:01,030
 

3231
00:58:57,580 --> 00:59:02,680
 in general as we add sample size the

3232
00:59:01,020 --> 00:59:02,680
 

3233
00:59:01,030 --> 00:59:05,190
 complexity will be forced to increase

3234
00:59:02,670 --> 00:59:05,190
 

3235
00:59:02,680 --> 00:59:07,990
 because the model isn't exactly right

3236
00:59:05,180 --> 00:59:07,990
 

3237
00:59:05,190 --> 00:59:11,290
 here's a nice example to flow cytometry

3238
00:59:07,980 --> 00:59:11,290
 

3239
00:59:07,990 --> 00:59:12,940
 clustering so each sample has 3 to 20

3240
00:59:11,280 --> 00:59:12,940
 

3241
00:59:11,290 --> 00:59:15,550
 dimensional measurements on tens of

3242
00:59:12,930 --> 00:59:15,550
 

3243
00:59:12,940 --> 00:59:18,100
 thousands of cells and here's like just

3244
00:59:15,540 --> 00:59:18,100
 

3245
00:59:15,550 --> 00:59:19,810
 kind of two dimensions so shown and so

3246
00:59:18,090 --> 00:59:19,810
 

3247
00:59:18,100 --> 00:59:21,730
 here's like clusters these are actually

3248
00:59:19,800 --> 00:59:21,730
 

3249
00:59:19,810 --> 00:59:26,050
 correspond to concretely two different

3250
00:59:21,720 --> 00:59:26,050
 

3251
00:59:21,730 --> 00:59:27,880
 cell types and this is based on manual

3252
00:59:26,040 --> 00:59:27,880
 

3253
00:59:26,050 --> 00:59:30,940
 gating and so somebody goes in and

3254
00:59:27,870 --> 00:59:30,940
 

3255
00:59:27,880 --> 00:59:32,830
 labels the data and then here if we use

3256
00:59:30,930 --> 00:59:32,830
 

3257
00:59:30,940 --> 00:59:34,570
 a mixture of gaussians it'll break up

3258
00:59:32,820 --> 00:59:34,570
 

3259
00:59:32,830 --> 00:59:36,330
 these like very non Gaussian looking but

3260
00:59:34,560 --> 00:59:36,330
 

3261
00:59:34,570 --> 00:59:38,440
 you know may be approximately Gaussian

3262
00:59:36,320 --> 00:59:38,440
 

3263
00:59:36,330 --> 00:59:39,960
 into different sub clusters and over

3264
00:59:38,430 --> 00:59:39,960
 

3265
00:59:38,440 --> 00:59:42,400
 cluster

3266
00:59:39,950 --> 00:59:42,400
 

3267
00:59:39,960 --> 00:59:44,860
 okay so we'd like to do it automatically

3268
00:59:42,390 --> 00:59:44,860
 

3269
00:59:42,400 --> 00:59:46,840
 but then these are the kind of usual

3270
00:59:44,850 --> 00:59:46,840
 

3271
00:59:44,860 --> 00:59:48,850
 model based clustering methods don't

3272
00:59:46,830 --> 00:59:48,850
 

3273
00:59:46,840 --> 00:59:51,030
 don't work very well and distance based

3274
00:59:48,840 --> 00:59:51,030
 

3275
00:59:48,850 --> 00:59:54,730
 clustering also doesn't work very well

3276
00:59:51,020 --> 00:59:54,730
 

3277
00:59:51,030 --> 00:59:56,080
 okay so what can we do so if the model

3278
00:59:54,720 --> 00:59:56,080
 

3279
00:59:54,730 --> 00:59:58,030
 is wrong why don't we just fix it well

3280
00:59:56,070 --> 00:59:58,030
 

3281
00:59:56,080 --> 00:59:59,560
 what we could do in this case is like

3282
00:59:58,020 --> 00:59:59,560
 

3283
00:59:58,030 --> 01:00:00,820
 well let's instead of using a mixture of

3284
00:59:59,550 --> 01:00:00,820
 

3285
00:59:59,560 --> 01:00:02,470
 gaussians that come up with like a

3286
01:00:00,810 --> 01:00:02,470
 

3287
01:00:00,820 --> 01:00:04,750
 mixture of some strange kernel and

3288
01:00:02,460 --> 01:00:04,750
 

3289
01:00:02,470 --> 01:00:06,250
 there's a literature on that but it's

3290
01:00:04,740 --> 01:00:06,250
 

3291
01:00:04,750 --> 01:00:07,960
 kind of hard to do that you know come up

3292
01:00:06,240 --> 01:00:07,960
 

3293
01:00:06,250 --> 01:00:09,880
 with this weird kind of complicated

3294
01:00:07,950 --> 01:00:09,880
 

3295
01:00:07,960 --> 01:00:12,550
 flexible kernels and then we run into

3296
01:00:09,870 --> 01:00:12,550
 

3297
01:00:09,880 --> 01:00:13,930
 computational problems for you know

3298
01:00:12,540 --> 01:00:13,930
 

3299
01:00:12,550 --> 01:00:15,520
 implementing these models with weird

3300
01:00:13,920 --> 01:00:15,520
 

3301
01:00:13,930 --> 01:00:17,560
 flexible kernels and so we maybe don't

3302
01:00:15,510 --> 01:00:17,560
 

3303
01:00:15,520 --> 01:00:23,380
 want to do that

3304
01:00:17,550 --> 01:00:23,380
 

3305
01:00:17,560 --> 01:00:25,690
 yes also the simple model might be

3306
01:00:23,370 --> 01:00:25,690
 

3307
01:00:23,380 --> 01:00:28,480
 inappropriate more appropriate even if

3308
01:00:25,680 --> 01:00:28,480
 

3309
01:00:25,690 --> 01:00:30,340
 it's slightly wrong you know it might be

3310
01:00:28,470 --> 01:00:30,340
 

3311
01:00:28,480 --> 01:00:31,810
 that that you know the data are always

3312
01:00:30,330 --> 01:00:31,810
 

3313
01:00:30,340 --> 01:00:33,190
 noisy it might even be that the mixture

3314
01:00:31,800 --> 01:00:33,190
 

3315
01:00:31,810 --> 01:00:35,110
 of gaussians is right but we just have

3316
01:00:33,180 --> 01:00:35,110
 

3317
01:00:33,190 --> 01:00:37,570
 some noise in the data contamination in

3318
01:00:35,100 --> 01:00:37,570
 

3319
01:00:35,110 --> 01:00:38,770
 the data somebody's you know typed in

3320
01:00:37,560 --> 01:00:38,770
 

3321
01:00:37,570 --> 01:00:40,300
 some observations that are slightly

3322
01:00:38,760 --> 01:00:40,300
 

3323
01:00:38,770 --> 01:00:42,520
 wrong or there's some machine

3324
01:00:40,290 --> 01:00:42,520
 

3325
01:00:40,300 --> 01:00:44,110
 calibration issue and so we'd like to

3326
01:00:42,510 --> 01:00:44,110
 

3327
01:00:42,520 --> 01:00:47,680
 like allow first light did good degree

3328
01:00:44,100 --> 01:00:47,680
 

3329
01:00:44,110 --> 01:00:49,660
 of can take contamination okay so what

3330
01:00:47,670 --> 01:00:49,660
 

3331
01:00:47,680 --> 01:00:52,480
 we proposed this paper just come in

3332
01:00:49,650 --> 01:00:52,480
 

3333
01:00:49,660 --> 01:00:55,300
 Jassa is as follows and so we call it C

3334
01:00:52,470 --> 01:00:55,300
 

3335
01:00:52,480 --> 01:00:57,000
 Bayes or a course and posterior maybe

3336
01:00:55,290 --> 01:00:57,000
 

3337
01:00:55,300 --> 01:00:59,530
 it's not a very sexy name but see Bayes

3338
01:00:56,990 --> 01:00:59,530
 

3339
01:00:57,000 --> 01:01:01,810
 so here's the kind of setting and so

3340
01:00:59,520 --> 01:01:01,810
 

3341
01:00:59,530 --> 01:01:05,140
 we're gonna assume a model P of theta in

3342
01:01:01,800 --> 01:01:05,140
 

3343
01:01:01,810 --> 01:01:07,150
 a prior PI of theta okay so we're gonna

3344
01:01:05,130 --> 01:01:07,150
 

3345
01:01:05,140 --> 01:01:08,920
 say theta I represents the idealized

3346
01:01:07,140 --> 01:01:08,920
 

3347
01:01:07,150 --> 01:01:14,200
 distribution of the data so that's over

3348
01:01:08,910 --> 01:01:14,200
 

3349
01:01:08,920 --> 01:01:16,150
 here so theta I is the true state of

3350
01:01:14,190 --> 01:01:16,150
 

3351
01:01:14,200 --> 01:01:19,480
 nature about which one's interested in

3352
01:01:16,140 --> 01:01:19,480
 

3353
01:01:16,150 --> 01:01:22,930
 making inferences okay but then you know

3354
01:01:19,470 --> 01:01:22,930
 

3355
01:01:19,480 --> 01:01:25,180
 we're gonna say that the the x1 to xn or

3356
01:01:22,920 --> 01:01:25,180
 

3357
01:01:22,930 --> 01:01:26,590
 let's just say for now or iid you don't

3358
01:01:25,170 --> 01:01:26,590
 

3359
01:01:25,180 --> 01:01:31,630
 have to have iid in any of these methods

3360
01:01:26,580 --> 01:01:31,630
 

3361
01:01:26,590 --> 01:01:34,240
 or unobserved idealized data okay but

3362
01:01:31,620 --> 01:01:34,240
 

3363
01:01:31,630 --> 01:01:35,950
 but the observed data are actually

3364
01:01:34,230 --> 01:01:35,950
 

3365
01:01:34,240 --> 01:01:37,450
 slightly corrupted version and so our

3366
01:01:35,940 --> 01:01:37,450
 

3367
01:01:35,950 --> 01:01:39,010
 you know our observed data are never

3368
01:01:37,440 --> 01:01:39,010
 

3369
01:01:37,450 --> 01:01:40,360
 perfect so we're saying like well

3370
01:01:39,000 --> 01:01:40,360
 

3371
01:01:39,010 --> 01:01:42,790
 there's these idealized data that we

3372
01:01:40,350 --> 01:01:42,790
 

3373
01:01:40,360 --> 01:01:44,410
 could have generated but it that would

3374
01:01:42,780 --> 01:01:44,410
 

3375
01:01:42,790 --> 01:01:46,330
 be under these very perfect conditions

3376
01:01:44,400 --> 01:01:46,330
 

3377
01:01:44,410 --> 01:01:48,040
 in reality we have these like

3378
01:01:46,320 --> 01:01:48,040
 

3379
01:01:46,330 --> 01:01:50,860
 error-prone conditions where we observe

3380
01:01:48,030 --> 01:01:50,860
 

3381
01:01:48,040 --> 01:01:51,720
 data little X 1 xn instead of idealized

3382
01:01:50,850 --> 01:01:51,720
 

3383
01:01:50,860 --> 01:01:55,970
 data at

3384
01:01:51,710 --> 01:01:55,970
 

3385
01:01:51,720 --> 01:01:59,099
 x1 to xn okay but you know we think that

3386
01:01:55,960 --> 01:01:59,099
 

3387
01:01:55,970 --> 01:02:02,340
 that the the statistics um deviance or

3388
01:01:59,089 --> 01:02:02,340
 

3389
01:01:59,099 --> 01:02:06,330
 statistical distribution its are closed

3390
01:02:02,330 --> 01:02:06,330
 

3391
01:02:02,340 --> 01:02:09,300
 for the observed data little x1 xn and

3392
01:02:06,320 --> 01:02:09,300
 

3393
01:02:06,330 --> 01:02:11,520
 the corrupted data okay and so that's

3394
01:02:09,290 --> 01:02:11,520
 

3395
01:02:09,300 --> 01:02:12,690
 the game so we're gonna say if there

3396
01:02:11,510 --> 01:02:12,690
 

3397
01:02:11,520 --> 01:02:14,400
 were no corruption we should use the

3398
01:02:12,680 --> 01:02:14,400
 

3399
01:02:12,690 --> 01:02:17,099
 standard posterior that would be PI of

3400
01:02:14,390 --> 01:02:17,099
 

3401
01:02:14,400 --> 01:02:20,070
 theta given given big ax is equal to

3402
01:02:17,089 --> 01:02:20,070
 

3403
01:02:17,099 --> 01:02:21,270
 little X okay but due to corruption that

3404
01:02:20,060 --> 01:02:21,270
 

3405
01:02:20,070 --> 01:02:23,190
 would be clearly incorrect and that

3406
01:02:21,260 --> 01:02:23,190
 

3407
01:02:21,270 --> 01:02:24,599
 causes the problems with with with you

3408
01:02:23,180 --> 01:02:24,599
 

3409
01:02:23,190 --> 01:02:26,160
 know too many mixture components and the

3410
01:02:24,589 --> 01:02:26,160
 

3411
01:02:24,599 --> 01:02:29,310
 model growing in complexity with sample

3412
01:02:26,150 --> 01:02:29,310
 

3413
01:02:26,160 --> 01:02:31,170
 size so instead what we could do is we

3414
01:02:29,300 --> 01:02:31,170
 

3415
01:02:29,310 --> 01:02:33,690
 could condition on what is known which

3416
01:02:31,160 --> 01:02:33,690
 

3417
01:02:31,170 --> 01:02:37,290
 is just PI of theta given some sort of

3418
01:02:33,680 --> 01:02:37,290
 

3419
01:02:33,690 --> 01:02:38,730
 the distribution of the data the

3420
01:02:37,280 --> 01:02:38,730
 

3421
01:02:37,290 --> 01:02:40,410
 idealized data and the observed data

3422
01:02:38,720 --> 01:02:40,410
 

3423
01:02:38,730 --> 01:02:42,390
 those two things are close they're

3424
01:02:40,400 --> 01:02:42,390
 

3425
01:02:40,410 --> 01:02:44,700
 within R so we can define some sort of

3426
01:02:42,380 --> 01:02:44,700
 

3427
01:02:42,390 --> 01:02:46,800
 discrepancy between the empirical

3428
01:02:44,690 --> 01:02:46,800
 

3429
01:02:44,700 --> 01:02:49,440
 distributions of the of the data

3430
01:02:46,790 --> 01:02:49,440
 

3431
01:02:46,800 --> 01:02:52,770
 observed and the data idealized data

3432
01:02:49,430 --> 01:02:52,770
 

3433
01:02:49,440 --> 01:02:54,150
 generated from the assumed model okay so

3434
01:02:52,760 --> 01:02:54,150
 

3435
01:02:52,770 --> 01:02:55,470
 this seems kind of complicated but it'll

3436
01:02:54,140 --> 01:02:55,470
 

3437
01:02:54,150 --> 01:02:58,740
 it'll actually turn into something quite

3438
01:02:55,460 --> 01:02:58,740
 

3439
01:02:55,470 --> 01:03:00,780
 simple so R might be different difficult

3440
01:02:58,730 --> 01:03:00,780
 

3441
01:02:58,740 --> 01:03:03,030
 to choose and so R is a sort of tuning

3442
01:03:00,770 --> 01:03:03,030
 

3443
01:03:00,780 --> 01:03:05,070
 parameter controlling the kind of model

3444
01:03:03,020 --> 01:03:05,070
 

3445
01:03:03,030 --> 01:03:06,720
 mismatch or model MS specification in

3446
01:03:05,060 --> 01:03:06,720
 

3447
01:03:05,070 --> 01:03:09,359
 some sense and we could put a prior and

3448
01:03:06,710 --> 01:03:09,359
 

3449
01:03:06,720 --> 01:03:10,980
 there are and and then we could have

3450
01:03:09,349 --> 01:03:10,980
 

3451
01:03:09,359 --> 01:03:14,550
 some sort of class of posterior

3452
01:03:10,970 --> 01:03:14,550
 

3453
01:03:10,980 --> 01:03:16,260
 distributions okay so what happens is

3454
01:03:14,540 --> 01:03:16,260
 

3455
01:03:14,550 --> 01:03:17,760
 that that we could you well you know to

3456
01:03:16,250 --> 01:03:17,760
 

3457
01:03:16,260 --> 01:03:21,030
 do this in practice we need to choose

3458
01:03:17,750 --> 01:03:21,030
 

3459
01:03:17,760 --> 01:03:23,430
 the D a discrepancy as well as the R so

3460
01:03:21,020 --> 01:03:23,430
 

3461
01:03:21,030 --> 01:03:25,770
 if we if we do something in particular

3462
01:03:23,420 --> 01:03:25,770
 

3463
01:03:23,430 --> 01:03:27,599
 choosing relative entropy and we go

3464
01:03:25,760 --> 01:03:27,599
 

3465
01:03:25,770 --> 01:03:30,540
 through some mathematical calculations

3466
01:03:27,589 --> 01:03:30,540
 

3467
01:03:27,599 --> 01:03:32,609
 then we can show that the the court the

3468
01:03:30,530 --> 01:03:32,609
 

3469
01:03:30,540 --> 01:03:34,800
 C posterior distribution which is

3470
01:03:32,599 --> 01:03:34,800
 

3471
01:03:32,609 --> 01:03:37,830
 conditioning on on this kind of event it

3472
01:03:34,790 --> 01:03:37,830
 

3473
01:03:34,800 --> 01:03:40,080
 is is it's very close to this guy okay

3474
01:03:37,820 --> 01:03:40,080
 

3475
01:03:37,830 --> 01:03:41,880
 and so this guy is what so this is the

3476
01:03:40,070 --> 01:03:41,880
 

3477
01:03:40,080 --> 01:03:44,010
 the PI of theta that's the prior

3478
01:03:41,870 --> 01:03:44,010
 

3479
01:03:41,880 --> 01:03:45,240
 distribution of theta and now this is

3480
01:03:44,000 --> 01:03:45,240
 

3481
01:03:44,010 --> 01:03:48,480
 what we have in place of the likelihood

3482
01:03:45,230 --> 01:03:48,480
 

3483
01:03:45,240 --> 01:03:50,460
 and like we have you know there would be

3484
01:03:48,470 --> 01:03:50,460
 

3485
01:03:48,480 --> 01:03:51,930
 no no no that wouldn't be there at all

3486
01:03:50,450 --> 01:03:51,930
 

3487
01:03:50,460 --> 01:03:53,640
 if we just had the regular likelihood

3488
01:03:51,920 --> 01:03:53,640
 

3489
01:03:51,930 --> 01:03:55,650
 and if the model was perfectly specified

3490
01:03:53,630 --> 01:03:55,650
 

3491
01:03:53,640 --> 01:03:57,180
 but instead due to the due to the miss

3492
01:03:55,640 --> 01:03:57,180
 

3493
01:03:55,650 --> 01:04:00,180
 specification we've raised to some power

3494
01:03:57,170 --> 01:04:00,180
 

3495
01:03:57,180 --> 01:04:02,880
 Zeta n okay each of the observations and

3496
01:04:00,170 --> 01:04:02,880
 

3497
01:04:00,180 --> 01:04:05,000
 the Zeta n is just alpha over alpha plus

3498
01:04:02,870 --> 01:04:05,000
 

3499
01:04:02,880 --> 01:04:06,830
 ad okay and so this we

3500
01:04:04,990 --> 01:04:06,830
 

3501
01:04:05,000 --> 01:04:09,020
 taking this likelihood we've raised it

3502
01:04:06,820 --> 01:04:09,020
 

3503
01:04:06,830 --> 01:04:10,790
 to a power and raising it to the power

3504
01:04:09,010 --> 01:04:10,790
 

3505
01:04:09,020 --> 01:04:12,800
 is gonna automatically give us more

3506
01:04:10,780 --> 01:04:12,800
 

3507
01:04:10,790 --> 01:04:15,620
 robustness okay if we use this power

3508
01:04:12,790 --> 01:04:15,620
 

3509
01:04:12,800 --> 01:04:18,500
 okay and then we can you know as in the

3510
01:04:15,610 --> 01:04:18,500
 

3511
01:04:15,620 --> 01:04:20,600
 EPM CMC algorithms where we had a power

3512
01:04:18,490 --> 01:04:20,600
 

3513
01:04:18,500 --> 01:04:23,210
 but the power was motivated by getting a

3514
01:04:20,590 --> 01:04:23,210
 

3515
01:04:20,600 --> 01:04:25,610
 good posterior proximation and parallel

3516
01:04:23,200 --> 01:04:25,610
 

3517
01:04:23,210 --> 01:04:28,490
 inference here we instead have a power

3518
01:04:25,600 --> 01:04:28,490
 

3519
01:04:25,610 --> 01:04:32,600
 that's motivated by robustness in large

3520
01:04:28,480 --> 01:04:32,600
 

3521
01:04:28,490 --> 01:04:33,980
 sample sizes okay so the power posterior

3522
01:04:32,590 --> 01:04:33,980
 

3523
01:04:32,600 --> 01:04:35,900
 labels inference using standard

3524
01:04:33,970 --> 01:04:35,900
 

3525
01:04:33,980 --> 01:04:38,300
 techniques we can if we can use

3526
01:04:35,890 --> 01:04:38,300
 

3527
01:04:35,900 --> 01:04:40,310
 conjugate priors or we can also use MCMC

3528
01:04:38,290 --> 01:04:40,310
 

3529
01:04:38,300 --> 01:04:43,190
 just using this some likelihood raised

3530
01:04:40,300 --> 01:04:43,190
 

3531
01:04:40,310 --> 01:04:44,810
 to a particular power okay so i'll

3532
01:04:43,180 --> 01:04:44,810
 

3533
01:04:43,190 --> 01:04:46,460
 illustrate this kind of model miss

3534
01:04:44,800 --> 01:04:46,460
 

3535
01:04:44,810 --> 01:04:48,590
 specification issue with just this

3536
01:04:46,450 --> 01:04:48,590
 

3537
01:04:46,460 --> 01:04:50,990
 really simple coin flipping example so

3538
01:04:48,580 --> 01:04:50,990
 

3539
01:04:48,590 --> 01:04:54,800
 let's say our interest was in testing

3540
01:04:50,980 --> 01:04:54,800
 

3541
01:04:50,990 --> 01:04:56,840
 whether or not the coin was unbiased or

3542
01:04:54,790 --> 01:04:56,840
 

3543
01:04:54,800 --> 01:04:58,970
 not so we we flip the coin a lot of

3544
01:04:56,830 --> 01:04:58,970
 

3545
01:04:56,840 --> 01:05:01,730
 times and our null hypothesis is that

3546
01:04:58,960 --> 01:05:01,730
 

3547
01:04:58,970 --> 01:05:04,970
 theta is 0.5 so we have a 50% chance of

3548
01:05:01,720 --> 01:05:04,970
 

3549
01:05:01,730 --> 01:05:06,680
 a head okay but then we say well the

3550
01:05:04,960 --> 01:05:06,680
 

3551
01:05:04,970 --> 01:05:08,510
 date and practice are a little corrupted

3552
01:05:06,670 --> 01:05:08,510
 

3553
01:05:06,680 --> 01:05:11,840
 and so they might say they behave like

3554
01:05:08,500 --> 01:05:11,840
 

3555
01:05:08,510 --> 01:05:13,520
 Bernoulli 0.51 and said of point 5 so

3556
01:05:11,830 --> 01:05:13,520
 

3557
01:05:11,840 --> 01:05:16,430
 the null hypothesis was exactly true

3558
01:05:13,510 --> 01:05:16,430
 

3559
01:05:13,520 --> 01:05:19,490
 would be 0.5 but it's 0.5 1 and our data

3560
01:05:16,420 --> 01:05:19,490
 

3561
01:05:16,430 --> 01:05:21,200
 we observe so our C posterior is robust

3562
01:05:19,480 --> 01:05:21,200
 

3563
01:05:19,490 --> 01:05:24,170
 to this but the standard posterior is

3564
01:05:21,190 --> 01:05:24,170
 

3565
01:05:21,200 --> 01:05:26,810
 not you know and so the on the on the

3566
01:05:24,160 --> 01:05:26,810
 

3567
01:05:24,170 --> 01:05:29,300
 x-axis is the sample size so the sample

3568
01:05:26,800 --> 01:05:29,300
 

3569
01:05:26,810 --> 01:05:31,010
 size is blowing up and on the y-axis is

3570
01:05:29,290 --> 01:05:31,010
 

3571
01:05:29,300 --> 01:05:33,350
 the posterior probability of the null

3572
01:05:31,000 --> 01:05:33,350
 

3573
01:05:31,010 --> 01:05:34,970
 hypothesis given the data okay

3574
01:05:33,340 --> 01:05:34,970
 

3575
01:05:33,350 --> 01:05:37,520
 and so and technically the null

3576
01:05:34,960 --> 01:05:37,520
 

3577
01:05:34,970 --> 01:05:40,070
 hypothesis isn't true because it's 0.5 1

3578
01:05:37,510 --> 01:05:40,070
 

3579
01:05:37,520 --> 01:05:42,680
 but it's really close to true and so

3580
01:05:40,060 --> 01:05:42,680
 

3581
01:05:40,070 --> 01:05:44,180
 what we get is we get that you know as

3582
01:05:42,670 --> 01:05:44,180
 

3583
01:05:42,680 --> 01:05:46,250
 the sample size increases initially

3584
01:05:44,170 --> 01:05:46,250
 

3585
01:05:44,180 --> 01:05:47,630
 we're getting up the troop the exact

3586
01:05:46,240 --> 01:05:47,630
 

3587
01:05:46,250 --> 01:05:49,760
 posteriors hat looks like it has

3588
01:05:47,620 --> 01:05:49,760
 

3589
01:05:47,630 --> 01:05:51,530
 evidence building up for the null

3590
01:05:49,750 --> 01:05:51,530
 

3591
01:05:49,760 --> 01:05:54,410
 hypothesis and then that but then at

3592
01:05:51,520 --> 01:05:54,410
 

3593
01:05:51,530 --> 01:05:55,940
 some point it drops off okay because

3594
01:05:54,400 --> 01:05:55,940
 

3595
01:05:54,410 --> 01:05:58,640
 then the data are big enough to show

3596
01:05:55,930 --> 01:05:58,640
 

3597
01:05:55,940 --> 01:06:04,610
 that well actually 0.5 1 isn't isn't 0.5

3598
01:05:58,630 --> 01:06:04,610
 

3599
01:05:58,640 --> 01:06:06,890
 ok and we get this kind of problem we

3600
01:06:04,600 --> 01:06:06,890
 

3601
01:06:04,610 --> 01:06:08,600
 also want wanted to see whether we could

3602
01:06:06,880 --> 01:06:08,600
 

3603
01:06:06,890 --> 01:06:10,370
 do the same thing in mixture models to

3604
01:06:08,590 --> 01:06:10,370
 

3605
01:06:08,600 --> 01:06:13,070
 many other models and so let's say we

3606
01:06:10,360 --> 01:06:13,070
 

3607
01:06:10,370 --> 01:06:15,530
 have a model x1 through xn they're iid

3608
01:06:13,060 --> 01:06:15,530
 

3609
01:06:13,070 --> 01:06:17,240
 from some kernel mixture and we do a

3610
01:06:15,520 --> 01:06:17,240
 

3611
01:06:15,530 --> 01:06:18,510
 Bayesian thing and so we put a prior on

3612
01:06:17,230 --> 01:06:18,510
 

3613
01:06:17,240 --> 01:06:20,610
 the different parameters

3614
01:06:18,500 --> 01:06:20,610
 

3615
01:06:18,510 --> 01:06:22,650
 and then we have this see posterior

3616
01:06:20,600 --> 01:06:22,650
 

3617
01:06:20,610 --> 01:06:25,530
 approximation okay and then we can run

3618
01:06:22,640 --> 01:06:25,530
 

3619
01:06:22,650 --> 01:06:27,960
 MCMC so we could run MC MC for a mixture

3620
01:06:25,520 --> 01:06:27,960
 

3621
01:06:25,530 --> 01:06:31,410
 model for for model based clustering and

3622
01:06:27,950 --> 01:06:31,410
 

3623
01:06:27,960 --> 01:06:34,260
 compare our RC posterior or remote more

3624
01:06:31,400 --> 01:06:34,260
 

3625
01:06:31,410 --> 01:06:36,000
 robust posterior to the exact one and we

3626
01:06:34,250 --> 01:06:36,000
 

3627
01:06:34,260 --> 01:06:38,610
 have an M stamps algorithm that we could

3628
01:06:35,990 --> 01:06:38,610
 

3629
01:06:36,000 --> 01:06:40,770
 easily implement it for quite large data

3630
01:06:38,600 --> 01:06:40,770
 

3631
01:06:38,610 --> 01:06:42,560
 sets and then we could further scale up

3632
01:06:40,760 --> 01:06:42,560
 

3633
01:06:40,770 --> 01:06:45,390
 using our previous tricks

3634
01:06:42,550 --> 01:06:45,390
 

3635
01:06:42,560 --> 01:06:47,220
 okay so here's just some results and so

3636
01:06:45,380 --> 01:06:47,220
 

3637
01:06:45,390 --> 01:06:49,590
 here I go back to that perturb mixture

3638
01:06:47,210 --> 01:06:49,590
 

3639
01:06:47,220 --> 01:06:51,510
 of gaussians where the true data

3640
01:06:49,580 --> 01:06:51,510
 

3641
01:06:49,590 --> 01:06:54,660
 generating model we've simulated is very

3642
01:06:51,500 --> 01:06:54,660
 

3643
01:06:51,510 --> 01:06:56,010
 close to a mixture of two gaussians but

3644
01:06:54,650 --> 01:06:56,010
 

3645
01:06:54,660 --> 01:06:58,080
 they're slightly perturbed and then if

3646
01:06:56,000 --> 01:06:58,080
 

3647
01:06:56,010 --> 01:06:59,820
 we use a standard posterior we keep

3648
01:06:58,070 --> 01:06:59,820
 

3649
01:06:58,080 --> 01:07:02,880
 adding Gaussian components as the sample

3650
01:06:59,810 --> 01:07:02,880
 

3651
01:06:59,820 --> 01:07:04,560
 size goes up but if we use the the the C

3652
01:07:02,870 --> 01:07:04,560
 

3653
01:07:02,880 --> 01:07:07,350
 posterior then that doesn't happen at

3654
01:07:04,550 --> 01:07:07,350
 

3655
01:07:04,560 --> 01:07:09,000
 all actually we just we get consistent

3656
01:07:07,340 --> 01:07:09,000
 

3657
01:07:07,350 --> 01:07:10,680
 inferences as the sample size goes up we

3658
01:07:08,990 --> 01:07:10,680
 

3659
01:07:09,000 --> 01:07:15,150
 we're always choosing two components

3660
01:07:10,670 --> 01:07:15,150
 

3661
01:07:10,680 --> 01:07:17,160
 okay I mean that also happens it happens

3662
01:07:15,140 --> 01:07:17,160
 

3663
01:07:15,150 --> 01:07:18,840
 if we use four components and we perturb

3664
01:07:17,150 --> 01:07:18,840
 

3665
01:07:17,160 --> 01:07:22,230
 those as well the same the same kind of

3666
01:07:18,830 --> 01:07:22,230
 

3667
01:07:18,840 --> 01:07:24,330
 deal and we also got beautiful results

3668
01:07:22,220 --> 01:07:24,330
 

3669
01:07:22,230 --> 01:07:27,060
 in this quite important applied flow

3670
01:07:24,320 --> 01:07:27,060
 

3671
01:07:24,330 --> 01:07:28,800
 cytometry clustering data becomes much

3672
01:07:27,050 --> 01:07:28,800
 

3673
01:07:27,060 --> 01:07:30,270
 more robust the clustering to the shape

3674
01:07:28,790 --> 01:07:30,270
 

3675
01:07:28,800 --> 01:07:33,150
 of the kernel miss specification and the

3676
01:07:30,260 --> 01:07:33,150
 

3677
01:07:30,270 --> 01:07:36,150
 shape of the kernel okay and we we

3678
01:07:33,140 --> 01:07:36,150
 

3679
01:07:33,150 --> 01:07:37,830
 actually had a label data set and from

3680
01:07:36,140 --> 01:07:37,830
 

3681
01:07:36,150 --> 01:07:40,320
 people going in manually and doing this

3682
01:07:37,820 --> 01:07:40,320
 

3683
01:07:37,830 --> 01:07:42,240
 manually gate gating to label the data

3684
01:07:40,310 --> 01:07:42,240
 

3685
01:07:40,320 --> 01:07:44,390
 and we had a whole bunch of different

3686
01:07:42,230 --> 01:07:44,390
 

3687
01:07:42,240 --> 01:07:46,860
 label data sets 7 through 12 here and

3688
01:07:44,380 --> 01:07:46,860
 

3689
01:07:44,390 --> 01:07:49,050
 looking at an F measure we did you know

3690
01:07:46,850 --> 01:07:49,050
 

3691
01:07:46,860 --> 01:07:51,630
 dramatically better than using a usual

3692
01:07:49,040 --> 01:07:51,630
 

3693
01:07:49,050 --> 01:08:01,170
 um Bayesian posterior distribution in

3694
01:07:51,620 --> 01:08:01,170
 

3695
01:07:51,630 --> 01:08:02,490
 terms of clustering okay so this C Bayes

3696
01:08:01,160 --> 01:08:02,490
 

3697
01:08:01,170 --> 01:08:04,350
 provides a framework for improving

3698
01:08:02,480 --> 01:08:04,350
 

3699
01:08:02,490 --> 01:08:06,750
 robustness to model miss specification

3700
01:08:04,340 --> 01:08:06,750
 

3701
01:08:04,350 --> 01:08:08,700
 it's particularly useful when when

3702
01:08:06,740 --> 01:08:08,700
 

3703
01:08:06,750 --> 01:08:10,470
 interest is in model-based inferences

3704
01:08:08,690 --> 01:08:10,470
 

3705
01:08:08,700 --> 01:08:12,180
 instead of some black box we want to

3706
01:08:10,460 --> 01:08:12,180
 

3707
01:08:10,470 --> 01:08:15,840
 interpret the parameters for example and

3708
01:08:12,170 --> 01:08:15,840
 

3709
01:08:12,180 --> 01:08:17,970
 the sample size is large it can be

3710
01:08:15,830 --> 01:08:17,970
 

3711
01:08:15,840 --> 01:08:19,440
 implemented quite easily using MCMC and

3712
01:08:17,960 --> 01:08:19,440
 

3713
01:08:17,970 --> 01:08:21,410
 those scalable MCMC tricks we talked

3714
01:08:19,430 --> 01:08:21,410
 

3715
01:08:19,440 --> 01:08:24,359
 about earlier okay

3716
01:08:21,400 --> 01:08:24,359
 

3717
01:08:21,410 --> 01:08:26,970
 okay I'm just gonna skip that okay so um

3718
01:08:24,349 --> 01:08:26,970
 

3719
01:08:24,359 --> 01:08:28,950
 so in the remaining time I'd like to

3720
01:08:26,960 --> 01:08:28,950
 

3721
01:08:26,970 --> 01:08:31,650
 just transition to talking about high

3722
01:08:28,940 --> 01:08:31,650
 

3723
01:08:28,950 --> 01:08:35,049
 dimensional data okay or big P

3724
01:08:31,640 --> 01:08:35,049
 

3725
01:08:31,650 --> 01:08:36,819
 so so we focused on so far on solving

3726
01:08:35,039 --> 01:08:36,819
 

3727
01:08:35,049 --> 01:08:39,309
 computational robustness problems

3728
01:08:36,809 --> 01:08:39,309
 

3729
01:08:36,819 --> 01:08:41,650
 arising in large n and I would say

3730
01:08:39,299 --> 01:08:41,650
 

3731
01:08:39,309 --> 01:08:43,539
 really many ways these problems are

3732
01:08:41,640 --> 01:08:43,539
 

3733
01:08:41,650 --> 01:08:45,339
 quite easier to deal with than then

3734
01:08:43,529 --> 01:08:45,339
 

3735
01:08:43,539 --> 01:08:47,980
 issues with high dimensional complicated

3736
01:08:45,329 --> 01:08:47,980
 

3737
01:08:45,339 --> 01:08:50,380
 data so that that is really I would say

3738
01:08:47,970 --> 01:08:50,380
 

3739
01:08:47,980 --> 01:08:52,389
 the most important problem moving

3740
01:08:50,370 --> 01:08:52,389
 

3741
01:08:50,380 --> 01:08:54,400
 forward I think in statistics and

3742
01:08:52,379 --> 01:08:54,400
 

3743
01:08:52,389 --> 01:08:56,049
 machine learning is how the hell do we

3744
01:08:54,390 --> 01:08:56,049
 

3745
01:08:54,400 --> 01:08:58,150
 deal with all these complicated data

3746
01:08:56,039 --> 01:08:58,150
 

3747
01:08:56,049 --> 01:08:59,679
 where we don't have a giant sample size

3748
01:08:58,140 --> 01:08:59,679
 

3749
01:08:58,150 --> 01:09:00,909
 you know all this stuff with deep

3750
01:08:59,669 --> 01:09:00,909
 

3751
01:08:59,679 --> 01:09:02,769
 learning and everything's been very

3752
01:09:00,899 --> 01:09:02,769
 

3753
01:09:00,909 --> 01:09:04,359
 exciting but usually in those settings

3754
01:09:02,759 --> 01:09:04,359
 

3755
01:09:02,769 --> 01:09:05,949
 we have really structured data and we

3756
01:09:04,349 --> 01:09:05,949
 

3757
01:09:04,359 --> 01:09:08,710
 have a live we have a big sample size we

3758
01:09:05,939 --> 01:09:08,710
 

3759
01:09:05,949 --> 01:09:10,509
 have a lot of labels etc well what if we

3760
01:09:08,700 --> 01:09:10,509
 

3761
01:09:08,710 --> 01:09:12,279
 have you know data that aren't so

3762
01:09:10,499 --> 01:09:12,279
 

3763
01:09:10,509 --> 01:09:14,650
 structured or the structure is not so

3764
01:09:12,269 --> 01:09:14,650
 

3765
01:09:12,279 --> 01:09:16,179
 clear and the sample size is enormous I

3766
01:09:14,640 --> 01:09:16,179
 

3767
01:09:14,650 --> 01:09:17,769
 mean the sample size isn't that big but

3768
01:09:16,169 --> 01:09:17,769
 

3769
01:09:16,179 --> 01:09:19,989
 the dimension of the data is enormous

3770
01:09:17,759 --> 01:09:19,989
 

3771
01:09:17,769 --> 01:09:22,839
 and so for each patient in the study

3772
01:09:19,979 --> 01:09:22,839
 

3773
01:09:19,989 --> 01:09:24,909
 I've measured you know a billion

3774
01:09:22,829 --> 01:09:24,909
 

3775
01:09:22,839 --> 01:09:28,329
 different omics things about them or

3776
01:09:24,899 --> 01:09:28,329
 

3777
01:09:24,909 --> 01:09:29,949
 I've you know taken brain scans and

3778
01:09:28,319 --> 01:09:29,949
 

3779
01:09:28,329 --> 01:09:32,409
 neuroscience and measured like their

3780
01:09:29,939 --> 01:09:32,409
 

3781
01:09:29,949 --> 01:09:34,089
 entire brain connectome you know and we

3782
01:09:32,399 --> 01:09:34,089
 

3783
01:09:32,409 --> 01:09:36,309
 keep getting more and more and more

3784
01:09:34,079 --> 01:09:36,309
 

3785
01:09:34,089 --> 01:09:38,710
 higher resolution and better measuring

3786
01:09:36,299 --> 01:09:38,710
 

3787
01:09:36,309 --> 01:09:40,440
 technologies for massive dimensional

3788
01:09:38,700 --> 01:09:40,440
 

3789
01:09:38,710 --> 01:09:42,670
 data but we really don't have

3790
01:09:40,430 --> 01:09:42,670
 

3791
01:09:40,440 --> 01:09:44,859
 statistical tools for making sense of it

3792
01:09:42,660 --> 01:09:44,859
 

3793
01:09:42,670 --> 01:09:46,809
 and often what happens is this is

3794
01:09:44,849 --> 01:09:46,809
 

3795
01:09:44,859 --> 01:09:49,089
 complete garbage in my mind in some

3796
01:09:46,799 --> 01:09:49,089
 

3797
01:09:46,809 --> 01:09:51,159
 sense is that okay well I have some data

3798
01:09:49,079 --> 01:09:51,159
 

3799
01:09:49,089 --> 01:09:52,900
 set I have some outcome Y and I have

3800
01:09:51,149 --> 01:09:52,900
 

3801
01:09:51,159 --> 01:09:54,639
 some features X and the X is really high

3802
01:09:52,890 --> 01:09:54,639
 

3803
01:09:52,900 --> 01:09:57,369
 dimensional and I don't have that

3804
01:09:54,629 --> 01:09:57,369
 

3805
01:09:54,639 --> 01:09:58,750
 biggest sample size and I'm doing basic

3806
01:09:57,359 --> 01:09:58,750
 

3807
01:09:57,369 --> 01:10:01,869
 science or I'm doing medical research

3808
01:09:58,740 --> 01:10:01,869
 

3809
01:09:58,750 --> 01:10:03,940
 I'm doing neuroscience and I go in and I

3810
01:10:01,859 --> 01:10:03,940
 

3811
01:10:01,869 --> 01:10:06,280
 and I fit some machine learning

3812
01:10:03,930 --> 01:10:06,280
 

3813
01:10:03,940 --> 01:10:08,559
 algorithm random forests or a neural

3814
01:10:06,270 --> 01:10:08,559
 

3815
01:10:06,280 --> 01:10:10,719
 network or I do lasso or something or

3816
01:10:08,549 --> 01:10:10,719
 

3817
01:10:08,559 --> 01:10:13,090
 elastic net and I get a lot of variables

3818
01:10:10,709 --> 01:10:13,090
 

3819
01:10:10,719 --> 01:10:15,340
 that are spit out and the scientists are

3820
01:10:13,080 --> 01:10:15,340
 

3821
01:10:13,090 --> 01:10:17,559
 gonna like interpret those variables you

3822
01:10:15,330 --> 01:10:17,559
 

3823
01:10:15,340 --> 01:10:19,480
 know and then really like that's really

3824
01:10:17,549 --> 01:10:19,480
 

3825
01:10:17,559 --> 01:10:22,179
 unreliable and we're running into like a

3826
01:10:19,470 --> 01:10:22,179
 

3827
01:10:19,480 --> 01:10:25,300
 replicability crisis and science and so

3828
01:10:22,169 --> 01:10:25,300
 

3829
01:10:22,179 --> 01:10:26,980
 we'd like to be able to say in a lot of

3830
01:10:25,290 --> 01:10:26,980
 

3831
01:10:25,300 --> 01:10:29,920
 cases I think that we'd like to be able

3832
01:10:26,970 --> 01:10:29,920
 

3833
01:10:26,980 --> 01:10:31,690
 to say that actually you know scientists

3834
01:10:29,910 --> 01:10:31,690
 

3835
01:10:29,920 --> 01:10:34,360
 we can't do what you'd like us to do

3836
01:10:31,680 --> 01:10:34,360
 

3837
01:10:31,690 --> 01:10:37,260
 you've given us a billion predictors and

3838
01:10:34,350 --> 01:10:37,260
 

3839
01:10:34,360 --> 01:10:40,420
 you've run this study on ten mice or

3840
01:10:37,250 --> 01:10:40,420
 

3841
01:10:37,260 --> 01:10:41,650
 twenty patients we can't actually decide

3842
01:10:40,410 --> 01:10:41,650
 

3843
01:10:40,420 --> 01:10:43,690
 which of those predictors are the

3844
01:10:41,640 --> 01:10:43,690
 

3845
01:10:41,650 --> 01:10:45,370
 important ones that's just not going to

3846
01:10:43,680 --> 01:10:45,370
 

3847
01:10:43,690 --> 01:10:47,230
 work there's an

3848
01:10:45,360 --> 01:10:47,230
 

3849
01:10:45,370 --> 01:10:49,270
 we're on the wrong side of some horrible

3850
01:10:47,220 --> 01:10:49,270
 

3851
01:10:47,230 --> 01:10:51,220
 face transition but a lot of the

3852
01:10:49,260 --> 01:10:51,220
 

3853
01:10:49,270 --> 01:10:52,870
 algorithms in the literature is focusing

3854
01:10:51,210 --> 01:10:52,870
 

3855
01:10:51,220 --> 01:10:54,340
 on giving us positive results you know

3856
01:10:52,860 --> 01:10:54,340
 

3857
01:10:52,870 --> 01:10:56,530
 like I stick it in this optimization

3858
01:10:54,330 --> 01:10:56,530
 

3859
01:10:54,340 --> 01:10:59,290
 algorithm here's my features and it

3860
01:10:56,520 --> 01:10:59,290
 

3861
01:10:56,530 --> 01:11:00,760
 doesn't like flag and say actually all

3862
01:10:59,280 --> 01:11:00,760
 

3863
01:10:59,290 --> 01:11:03,310
 your features are probably almost surely

3864
01:11:00,750 --> 01:11:03,310
 

3865
01:11:00,760 --> 01:11:04,780
 wrong you know like you've got all false

3866
01:11:03,300 --> 01:11:04,780
 

3867
01:11:03,310 --> 01:11:06,430
 pause in the false negatives because the

3868
01:11:04,770 --> 01:11:06,430
 

3869
01:11:04,780 --> 01:11:08,620
 problems too big you have too high

3870
01:11:06,420 --> 01:11:08,620
 

3871
01:11:06,430 --> 01:11:11,290
 correlation the dimension is too high

3872
01:11:08,610 --> 01:11:11,290
 

3873
01:11:08,620 --> 01:11:12,820
 and so I want better ways to deal with

3874
01:11:11,280 --> 01:11:12,820
 

3875
01:11:11,290 --> 01:11:14,320
 uncertainty quantification and give me

3876
01:11:12,810 --> 01:11:14,320
 

3877
01:11:12,820 --> 01:11:17,140
 negative results give me realistic

3878
01:11:14,310 --> 01:11:17,140
 

3879
01:11:14,320 --> 01:11:19,780
 results give me ways to like course and

3880
01:11:17,130 --> 01:11:19,780
 

3881
01:11:17,140 --> 01:11:21,460
 the questions being asked so that we can

3882
01:11:19,770 --> 01:11:21,460
 

3883
01:11:19,780 --> 01:11:23,200
 get something much more reliable and

3884
01:11:21,450 --> 01:11:23,200
 

3885
01:11:21,460 --> 01:11:25,000
 reproducible in these high dimensional

3886
01:11:23,190 --> 01:11:25,000
 

3887
01:11:23,200 --> 01:11:26,380
 settings there's there a way to like

3888
01:11:24,990 --> 01:11:26,380
 

3889
01:11:25,000 --> 01:11:28,270
 learn Labette or low dimensional

3890
01:11:26,370 --> 01:11:28,270
 

3891
01:11:26,380 --> 01:11:29,800
 structures and more reliably okay

3892
01:11:28,260 --> 01:11:29,800
 

3893
01:11:28,270 --> 01:11:32,110
 and so that's some really beyond

3894
01:11:29,790 --> 01:11:32,110
 

3895
01:11:29,800 --> 01:11:34,060
 Bayesian inference but I would just like

3896
01:11:32,100 --> 01:11:34,060
 

3897
01:11:32,110 --> 01:11:35,830
 give a plea for many of you to kind of

3898
01:11:34,050 --> 01:11:35,830
 

3899
01:11:34,060 --> 01:11:39,310
 try to work on these types of problems

3900
01:11:35,820 --> 01:11:39,310
 

3901
01:11:35,830 --> 01:11:42,670
 with me not with me particularly but you

3902
01:11:39,300 --> 01:11:42,670
 

3903
01:11:39,310 --> 01:11:44,950
 do it and then tell me how to do it okay

3904
01:11:42,660 --> 01:11:44,950
 

3905
01:11:42,670 --> 01:11:47,080
 so that's the yeah so these are types of

3906
01:11:44,940 --> 01:11:47,080
 

3907
01:11:44,950 --> 01:11:48,700
 things I work on all the time and so we

3908
01:11:47,070 --> 01:11:48,700
 

3909
01:11:47,080 --> 01:11:51,580
 have very few label data relative to

3910
01:11:48,690 --> 01:11:51,580
 

3911
01:11:48,700 --> 01:11:53,560
 data dimensionality and and it's really

3912
01:11:51,570 --> 01:11:53,560
 

3913
01:11:51,580 --> 01:11:54,790
 important as well that you know even

3914
01:11:53,550 --> 01:11:54,790
 

3915
01:11:53,560 --> 01:11:56,620
 though you're like oh I'm in some

3916
01:11:54,780 --> 01:11:56,620
 

3917
01:11:54,790 --> 01:11:58,870
 medical context I'd like to say diagnose

3918
01:11:56,610 --> 01:11:58,870
 

3919
01:11:56,620 --> 01:12:01,480
 a patient or cell to tell me which which

3920
01:11:58,860 --> 01:12:01,480
 

3921
01:11:58,870 --> 01:12:02,710
 treatment to give the patient but we

3922
01:12:01,470 --> 01:12:02,710
 

3923
01:12:01,480 --> 01:12:04,450
 don't really want a black box for

3924
01:12:02,700 --> 01:12:04,450
 

3925
01:12:02,710 --> 01:12:06,040
 prediction no doctor is gonna use

3926
01:12:04,440 --> 01:12:06,040
 

3927
01:12:04,450 --> 01:12:07,450
 something that just says oh we should do

3928
01:12:06,030 --> 01:12:07,450
 

3929
01:12:06,040 --> 01:12:10,360
 this through this patient they want to

3930
01:12:07,440 --> 01:12:10,360
 

3931
01:12:07,450 --> 01:12:12,310
 know why well how is it working you know

3932
01:12:10,350 --> 01:12:12,310
 

3933
01:12:10,360 --> 01:12:13,780
 what features are important etc we often

3934
01:12:12,300 --> 01:12:13,780
 

3935
01:12:12,310 --> 01:12:15,000
 want to do some version of variable

3936
01:12:13,770 --> 01:12:15,000
 

3937
01:12:13,780 --> 01:12:17,410
 selection

3938
01:12:14,990 --> 01:12:17,410
 

3939
01:12:15,000 --> 01:12:20,350
 okay so Bayes for big P I'd say is a

3940
01:12:17,400 --> 01:12:20,350
 

3941
01:12:17,410 --> 01:12:22,180
 huge topic I'm just gonna provide some

3942
01:12:20,340 --> 01:12:22,180
 

3943
01:12:20,350 --> 01:12:23,770
 vignettes to give a flavor in the

3944
01:12:22,170 --> 01:12:23,770
 

3945
01:12:22,180 --> 01:12:27,910
 remaining kind of 30 minutes before we

3946
01:12:23,760 --> 01:12:27,910
 

3947
01:12:23,770 --> 01:12:29,830
 run out of time okay so um so one of the

3948
01:12:27,900 --> 01:12:29,830
 

3949
01:12:27,910 --> 01:12:32,050
 kind of very popular things in these

3950
01:12:29,820 --> 01:12:32,050
 

3951
01:12:29,830 --> 01:12:33,670
 types of scientific fields that I've

3952
01:12:32,040 --> 01:12:33,670
 

3953
01:12:32,050 --> 01:12:35,140
 just been talking about is to do some

3954
01:12:33,660 --> 01:12:35,140
 

3955
01:12:33,670 --> 01:12:39,160
 type of variable or a feature selection

3956
01:12:35,130 --> 01:12:39,160
 

3957
01:12:35,140 --> 01:12:41,440
 that's all often the focus okay so um

3958
01:12:39,150 --> 01:12:41,440
 

3959
01:12:39,160 --> 01:12:43,690
 you know one example would be I have

3960
01:12:41,430 --> 01:12:43,690
 

3961
01:12:41,440 --> 01:12:46,180
 some phenotype a response variable for a

3962
01:12:43,680 --> 01:12:46,180
 

3963
01:12:43,690 --> 01:12:47,830
 patient Y some health response I work a

3964
01:12:46,170 --> 01:12:47,830
 

3965
01:12:46,180 --> 01:12:50,440
 lot on cancer genomics so this might be

3966
01:12:47,820 --> 01:12:50,440
 

3967
01:12:47,830 --> 01:12:53,260
 your subtype of cancer and then I have a

3968
01:12:50,430 --> 01:12:53,260
 

3969
01:12:50,440 --> 01:12:55,180
 lot of genetic variants XJ associated

3970
01:12:53,250 --> 01:12:55,180
 

3971
01:12:53,260 --> 01:12:56,770
 with that response okay but there might

3972
01:12:55,170 --> 01:12:56,770
 

3973
01:12:55,180 --> 01:12:58,810
 be you know now I have all of these like

3974
01:12:56,760 --> 01:12:58,810
 

3975
01:12:56,770 --> 01:12:59,110
 a whole genome sequencing and all this

3976
01:12:58,800 --> 01:12:59,110
 

3977
01:12:58,810 --> 01:13:00,370
 stuff

3978
01:12:59,100 --> 01:13:00,370
 

3979
01:12:59,110 --> 01:13:03,370
 and so there might be a lot of these

3980
01:13:00,360 --> 01:13:03,370
 

3981
01:13:00,370 --> 01:13:05,020
 guys the sample size is going to be

3982
01:13:03,360 --> 01:13:05,020
 

3983
01:13:03,370 --> 01:13:07,210
 modest the number of genetic variants is

3984
01:13:05,010 --> 01:13:07,210
 

3985
01:13:05,020 --> 01:13:09,820
 huge so we have this large piece small

3986
01:13:07,200 --> 01:13:09,820
 

3987
01:13:07,210 --> 01:13:12,160
 end problem so what do we do so what

3988
01:13:09,810 --> 01:13:12,160
 

3989
01:13:09,820 --> 01:13:13,930
 mostly what people do you know beyond

3990
01:13:12,150 --> 01:13:13,930
 

3991
01:13:12,160 --> 01:13:15,880
 this kind of Bayes vs. frequentist but

3992
01:13:13,920 --> 01:13:15,880
 

3993
01:13:13,930 --> 01:13:17,740
 mostly what people do are two main

3994
01:13:15,870 --> 01:13:17,740
 

3995
01:13:15,880 --> 01:13:19,060
 approaches the most popular I would say

3996
01:13:17,730 --> 01:13:19,060
 

3997
01:13:17,740 --> 01:13:21,280
 is is what's called independent

3998
01:13:19,050 --> 01:13:21,280
 

3999
01:13:19,060 --> 01:13:23,800
 screening and so we might like look at

4000
01:13:21,270 --> 01:13:23,800
 

4001
01:13:21,280 --> 01:13:26,710
 some sort of statistical test for

4002
01:13:23,790 --> 01:13:26,710
 

4003
01:13:23,800 --> 01:13:29,020
 association between yxj we're gonna do

4004
01:13:26,700 --> 01:13:29,020
 

4005
01:13:26,710 --> 01:13:30,790
 that separately for each J I'm gonna get

4006
01:13:29,010 --> 01:13:30,790
 

4007
01:13:29,020 --> 01:13:33,010
 a p-value and get a billion people use

4008
01:13:30,780 --> 01:13:33,010
 

4009
01:13:30,790 --> 01:13:34,900
 and then threshold them or something the

4010
01:13:33,000 --> 01:13:34,900
 

4011
01:13:33,010 --> 01:13:36,640
 other type of approach people use often

4012
01:13:34,890 --> 01:13:36,640
 

4013
01:13:34,900 --> 01:13:39,340
 is some sort of penalize a scaleable

4014
01:13:36,630 --> 01:13:39,340
 

4015
01:13:36,640 --> 01:13:42,240
 penalized estimation or shrinkage lasso

4016
01:13:39,330 --> 01:13:42,240
 

4017
01:13:39,340 --> 01:13:45,040
 elastic net etc being popular examples

4018
01:13:42,230 --> 01:13:45,040
 

4019
01:13:42,240 --> 01:13:46,270
 okay so um so we test for association

4020
01:13:45,030 --> 01:13:46,270
 

4021
01:13:45,040 --> 01:13:48,640
 between two variables at the time a

4022
01:13:46,260 --> 01:13:48,640
 

4023
01:13:46,270 --> 01:13:51,130
 phenotype in a snip we repeat this for

4024
01:13:48,630 --> 01:13:51,130
 

4025
01:13:48,640 --> 01:13:53,560
 all possible pairs we get a third number

4026
01:13:51,120 --> 01:13:53,560
 

4027
01:13:51,130 --> 01:13:54,670
 of p-values and then we choose some sort

4028
01:13:53,550 --> 01:13:54,670
 

4029
01:13:53,560 --> 01:13:56,500
 of p-value threshold

4030
01:13:54,660 --> 01:13:56,500
 

4031
01:13:54,670 --> 01:13:58,780
 controlling the what's called the false

4032
01:13:56,490 --> 01:13:58,780
 

4033
01:13:56,500 --> 01:14:00,310
 discovery rate I'm using for example

4034
01:13:58,770 --> 01:14:00,310
 

4035
01:13:58,780 --> 01:14:03,040
 this Benjen benjamine Ian Hochberg

4036
01:14:00,300 --> 01:14:03,040
 

4037
01:14:00,310 --> 01:14:04,570
 threshold okay and then we get a list of

4038
01:14:03,030 --> 01:14:04,570
 

4039
01:14:03,040 --> 01:14:06,460
 discoveries so we've taken like this

4040
01:14:04,560 --> 01:14:06,460
 

4041
01:14:04,570 --> 01:14:08,440
 huge number of biomarkers and we've

4042
01:14:06,450 --> 01:14:08,440
 

4043
01:14:06,460 --> 01:14:10,030
 reduced it to some number and then now

4044
01:14:08,430 --> 01:14:10,030
 

4045
01:14:08,440 --> 01:14:14,020
 we can maybe run follow-up Studies on

4046
01:14:10,020 --> 01:14:14,020
 

4047
01:14:10,030 --> 01:14:15,730
 that number to verify okay so um so this

4048
01:14:14,010 --> 01:14:15,730
 

4049
01:14:14,020 --> 01:14:17,680
 is really really really really commonly

4050
01:14:15,720 --> 01:14:17,680
 

4051
01:14:15,730 --> 01:14:19,870
 used it's very appealing in its

4052
01:14:17,670 --> 01:14:19,870
 

4053
01:14:17,680 --> 01:14:21,340
 simplicity it's quite scalable we can do

4054
01:14:19,860 --> 01:14:21,340
 

4055
01:14:19,870 --> 01:14:22,780
 this in parallel we're just doing a

4056
01:14:21,330 --> 01:14:22,780
 

4057
01:14:21,340 --> 01:14:25,300
 simple thing on little pieces of the

4058
01:14:22,770 --> 01:14:25,300
 

4059
01:14:22,780 --> 01:14:27,730
 data separately and so it's white quite

4060
01:14:25,290 --> 01:14:27,730
 

4061
01:14:25,300 --> 01:14:31,000
 nice in that sense it's really widely

4062
01:14:27,720 --> 01:14:31,000
 

4063
01:14:27,730 --> 01:14:33,370
 used there's a lot of false positives

4064
01:14:30,990 --> 01:14:33,370
 

4065
01:14:31,000 --> 01:14:35,410
 and negatives obviously for sparse data

4066
01:14:33,360 --> 01:14:35,410
 

4067
01:14:33,370 --> 01:14:38,550
 it's a it's a really big problem we

4068
01:14:35,400 --> 01:14:38,550
 

4069
01:14:35,410 --> 01:14:40,840
 might have no power to detect anything

4070
01:14:38,540 --> 01:14:40,840
 

4071
01:14:38,550 --> 01:14:43,240
 just looking at a pair of variables at a

4072
01:14:40,830 --> 01:14:43,240
 

4073
01:14:40,840 --> 01:14:44,770
 time leads to limited insights so let's

4074
01:14:43,230 --> 01:14:44,770
 

4075
01:14:43,240 --> 01:14:46,030
 kind of go into the the weeds here a

4076
01:14:44,760 --> 01:14:46,030
 

4077
01:14:44,770 --> 01:14:48,940
 little bit and so let's say we have a

4078
01:14:46,020 --> 01:14:48,940
 

4079
01:14:46,030 --> 01:14:51,640
 linear regression model okay so we have

4080
01:14:48,930 --> 01:14:51,640
 

4081
01:14:48,940 --> 01:14:53,410
 some response Y I on patient I and then

4082
01:14:51,630 --> 01:14:53,410
 

4083
01:14:51,640 --> 01:14:55,720
 we just say model the effect of all the

4084
01:14:53,400 --> 01:14:55,720
 

4085
01:14:53,410 --> 01:14:58,240
 biomarkers with some X I prime beta a

4086
01:14:55,710 --> 01:14:58,240
 

4087
01:14:55,720 --> 01:15:00,400
 bunch of features genetic features and

4088
01:14:58,230 --> 01:15:00,400
 

4089
01:14:58,240 --> 01:15:01,900
 then their coefficients or beta and

4090
01:15:00,390 --> 01:15:01,900
 

4091
01:15:00,400 --> 01:15:05,440
 let's just put a little Gaussian error

4092
01:15:01,890 --> 01:15:05,440
 

4093
01:15:01,900 --> 01:15:07,480
 on there okay and so um if we were you

4094
01:15:05,430 --> 01:15:07,480
 

4095
01:15:05,440 --> 01:15:09,130
 know around awhile ago before we started

4096
01:15:07,470 --> 01:15:09,130
 

4097
01:15:07,480 --> 01:15:10,600
 making P really big we might just have

4098
01:15:09,120 --> 01:15:10,600
 

4099
01:15:09,130 --> 01:15:12,850
 done linear regression or least squares

4100
01:15:10,590 --> 01:15:12,850
 

4101
01:15:10,600 --> 01:15:14,890
 okay and we would just get beta hat

4102
01:15:12,840 --> 01:15:14,890
 

4103
01:15:12,850 --> 01:15:17,050
 as X prime X inverse X prime Y okay

4104
01:15:14,880 --> 01:15:17,050
 

4105
01:15:14,890 --> 01:15:18,520
 everything's good because n is bigger

4106
01:15:17,040 --> 01:15:18,520
 

4107
01:15:17,050 --> 01:15:19,930
 than much bigger than P and we can

4108
01:15:18,510 --> 01:15:19,930
 

4109
01:15:18,520 --> 01:15:23,230
 estimate these coefficients reliably

4110
01:15:19,920 --> 01:15:23,230
 

4111
01:15:19,930 --> 01:15:25,870
 using least squares but what happens as

4112
01:15:23,220 --> 01:15:25,870
 

4113
01:15:23,230 --> 01:15:27,520
 P increases or the x x i's become more

4114
01:15:25,860 --> 01:15:27,520
 

4115
01:15:25,870 --> 01:15:28,990
 correlated the variance of that beta

4116
01:15:27,510 --> 01:15:28,990
 

4117
01:15:27,520 --> 01:15:31,780
 hats going to blow up and it's not going

4118
01:15:28,980 --> 01:15:31,780
 

4119
01:15:28,990 --> 01:15:33,190
 to be a very good estimator okay and so

4120
01:15:31,770 --> 01:15:33,190
 

4121
01:15:31,780 --> 01:15:35,440
 if P is bigger than n you're that's

4122
01:15:33,180 --> 01:15:35,440
 

4123
01:15:33,190 --> 01:15:37,210
 really not even going to exist so we

4124
01:15:35,430 --> 01:15:37,210
 

4125
01:15:35,440 --> 01:15:39,640
 need to include some sort of outsider

4126
01:15:37,200 --> 01:15:39,640
 

4127
01:15:37,210 --> 01:15:41,740
 prior information and doing this okay

4128
01:15:39,630 --> 01:15:41,740
 

4129
01:15:39,640 --> 01:15:43,210
 and so in a Bayesian approach what would

4130
01:15:41,730 --> 01:15:43,210
 

4131
01:15:41,740 --> 01:15:45,510
 we do well we would choose a prior

4132
01:15:43,200 --> 01:15:45,510
 

4133
01:15:43,210 --> 01:15:47,350
 probability distribution PI of beta

4134
01:15:45,500 --> 01:15:47,350
 

4135
01:15:45,510 --> 01:15:49,240
 characterizing our uncertainty in beta

4136
01:15:47,340 --> 01:15:49,240
 

4137
01:15:47,350 --> 01:15:52,180
 prior to observing the current data and

4138
01:15:49,230 --> 01:15:52,180
 

4139
01:15:49,240 --> 01:15:53,680
 then we would use our Bayes rule that I

4140
01:15:52,170 --> 01:15:53,680
 

4141
01:15:52,180 --> 01:15:55,060
 described earlier to update the prior

4142
01:15:53,670 --> 01:15:55,060
 

4143
01:15:53,680 --> 01:15:56,470
 with information in the likelihood so

4144
01:15:55,050 --> 01:15:56,470
 

4145
01:15:55,060 --> 01:15:59,320
 we'd have a posterior distribution of

4146
01:15:56,460 --> 01:15:59,320
 

4147
01:15:56,470 --> 01:16:01,720
 beta given our response data why are our

4148
01:15:59,310 --> 01:16:01,720
 

4149
01:15:59,320 --> 01:16:04,810
 features X and we would just plug it

4150
01:16:01,710 --> 01:16:04,810
 

4151
01:16:01,720 --> 01:16:06,250
 into Bayes rule as before okay so if we

4152
01:16:04,800 --> 01:16:06,250
 

4153
01:16:04,810 --> 01:16:08,070
 did if we have a normal linear

4154
01:16:06,240 --> 01:16:08,070
 

4155
01:16:06,250 --> 01:16:11,200
 regression model and we choose a

4156
01:16:08,060 --> 01:16:11,200
 

4157
01:16:08,070 --> 01:16:12,730
 Gaussian prior then we have conjugacy

4158
01:16:11,190 --> 01:16:12,730
 

4159
01:16:11,200 --> 01:16:14,670
 and we can just write down the posterior

4160
01:16:12,720 --> 01:16:14,670
 

4161
01:16:12,730 --> 01:16:17,350
 distribution of beta in this simple form

4162
01:16:14,660 --> 01:16:17,350
 

4163
01:16:14,670 --> 01:16:19,360
 it's just a Gaussian distribution and

4164
01:16:17,340 --> 01:16:19,360
 

4165
01:16:17,350 --> 01:16:21,400
 the posterior covariance is just going

4166
01:16:19,350 --> 01:16:21,400
 

4167
01:16:19,360 --> 01:16:23,350
 to be a part on this part contains

4168
01:16:21,390 --> 01:16:23,350
 

4169
01:16:21,400 --> 01:16:24,640
 information in the prior this part

4170
01:16:23,340 --> 01:16:24,640
 

4171
01:16:23,350 --> 01:16:25,930
 contains information in the likelihood

4172
01:16:24,630 --> 01:16:25,930
 

4173
01:16:24,640 --> 01:16:28,900
 it combines those two sources of

4174
01:16:25,920 --> 01:16:28,900
 

4175
01:16:25,930 --> 01:16:32,200
 information kind of shrinking back the

4176
01:16:28,890 --> 01:16:32,200
 

4177
01:16:28,900 --> 01:16:34,660
 the the MLE towards towards zero

4178
01:16:32,190 --> 01:16:34,660
 

4179
01:16:32,200 --> 01:16:36,040
 it's a type of shrinkage estimator ok so

4180
01:16:34,650 --> 01:16:36,040
 

4181
01:16:34,660 --> 01:16:39,430
 then that show that's shown in the

4182
01:16:36,030 --> 01:16:39,430
 

4183
01:16:36,040 --> 01:16:43,840
 posterior mean okay so we can get the

4184
01:16:39,420 --> 01:16:43,840
 

4185
01:16:39,430 --> 01:16:45,670
 same the same answer by just solving

4186
01:16:43,830 --> 01:16:45,670
 

4187
01:16:43,840 --> 01:16:47,440
 this penalized optimization problem

4188
01:16:45,660 --> 01:16:47,440
 

4189
01:16:45,670 --> 01:16:48,700
 which many people in this audience I'm

4190
01:16:47,430 --> 01:16:48,700
 

4191
01:16:47,440 --> 01:16:50,560
 sure I've seen a million of these types

4192
01:16:48,690 --> 01:16:50,560
 

4193
01:16:48,700 --> 01:16:52,480
 of things and so we just have you know

4194
01:16:50,550 --> 01:16:52,480
 

4195
01:16:50,560 --> 01:16:55,270
 we're minimizing some least squares plus

4196
01:16:52,470 --> 01:16:55,270
 

4197
01:16:52,480 --> 01:16:57,250
 some penalty and if we had a Gaussian

4198
01:16:55,260 --> 01:16:57,250
 

4199
01:16:55,270 --> 01:17:00,460
 prior that would correspond to a l2

4200
01:16:57,240 --> 01:17:00,460
 

4201
01:16:57,250 --> 01:17:02,890
 penalty so this is known as Ridge or l2

4202
01:17:00,450 --> 01:17:02,890
 

4203
01:17:00,460 --> 01:17:04,570
 panelized regression and so it has a

4204
01:17:02,880 --> 01:17:04,570
 

4205
01:17:02,890 --> 01:17:06,190
 dual interpretation as a bayesian

4206
01:17:04,560 --> 01:17:06,190
 

4207
01:17:04,570 --> 01:17:08,140
 estimator under Gaussian prior centered

4208
01:17:06,180 --> 01:17:08,140
 

4209
01:17:06,190 --> 01:17:09,940
 at zero and a least squares estimator

4210
01:17:08,130 --> 01:17:09,940
 

4211
01:17:08,140 --> 01:17:13,420
 with a penalty on large coefficients and

4212
01:17:09,930 --> 01:17:13,420
 

4213
01:17:09,940 --> 01:17:16,030
 l2 l2 penalty okay so the game here is

4214
01:17:13,410 --> 01:17:16,030
 

4215
01:17:13,420 --> 01:17:17,710
 to introduce some bias the maximum

4216
01:17:16,020 --> 01:17:17,710
 

4217
01:17:16,030 --> 01:17:19,450
 relative to maximum likelihood estimator

4218
01:17:17,700 --> 01:17:19,450
 

4219
01:17:17,710 --> 01:17:21,460
 while reducing the variance a lot to

4220
01:17:19,440 --> 01:17:21,460
 

4221
01:17:19,450 --> 01:17:24,849
 improve mean Square and I'll enable

4222
01:17:21,450 --> 01:17:24,849
 

4223
01:17:21,460 --> 01:17:27,940
 scaling to higher dimensional problems

4224
01:17:24,839 --> 01:17:27,940
 

4225
01:17:24,849 --> 01:17:30,130
 okay so of course we can generalize that

4226
01:17:27,930 --> 01:17:30,130
 

4227
01:17:27,940 --> 01:17:31,570
 to a broad class of penalize loss

4228
01:17:30,120 --> 01:17:31,570
 

4229
01:17:30,130 --> 01:17:34,270
 functions where we have a least squares

4230
01:17:31,560 --> 01:17:34,270
 

4231
01:17:31,570 --> 01:17:37,060
 part for good as a fit plus P of lambda

4232
01:17:34,260 --> 01:17:37,060
 

4233
01:17:34,270 --> 01:17:39,219
 of beta and we that's a penalty term and

4234
01:17:37,050 --> 01:17:39,219
 

4235
01:17:37,060 --> 01:17:41,380
 we could we could put we previously put

4236
01:17:39,209 --> 01:17:41,380
 

4237
01:17:39,219 --> 01:17:43,630
 l2 but we could put on an l1 penalty to

4238
01:17:41,370 --> 01:17:43,630
 

4239
01:17:41,380 --> 01:17:45,940
 have a lasso type procedure we could

4240
01:17:43,620 --> 01:17:45,940
 

4241
01:17:43,630 --> 01:17:48,639
 week and if we did that there was a

4242
01:17:45,930 --> 01:17:48,639
 

4243
01:17:45,940 --> 01:17:49,270
 question at the break about the bayesian

4244
01:17:48,629 --> 01:17:49,270
 

4245
01:17:48,639 --> 01:17:50,949
 lasso

4246
01:17:49,260 --> 01:17:50,949
 

4247
01:17:49,270 --> 01:17:52,989
 well the Bayesian lasso would just be

4248
01:17:50,939 --> 01:17:52,989
 

4249
01:17:50,949 --> 01:17:54,849
 you know doing this but we would put a

4250
01:17:52,979 --> 01:17:54,849
 

4251
01:17:52,989 --> 01:17:57,070
 prior on beta that would correspond to a

4252
01:17:54,839 --> 01:17:57,070
 

4253
01:17:54,849 --> 01:17:58,570
 double exponential distribution and then

4254
01:17:57,060 --> 01:17:58,570
 

4255
01:17:57,070 --> 01:17:59,739
 the mode of the posterior is going to be

4256
01:17:58,560 --> 01:17:59,739
 

4257
01:17:58,570 --> 01:18:03,159
 exactly the solution to this

4258
01:17:59,729 --> 01:18:03,159
 

4259
01:17:59,739 --> 01:18:04,810
 optimization problem okay the nice thing

4260
01:18:03,149 --> 01:18:04,810
 

4261
01:18:03,159 --> 01:18:07,060
 is that that mode is then going to be

4262
01:18:04,800 --> 01:18:07,060
 

4263
01:18:04,810 --> 01:18:10,659
 sparse and contains exactly row values

4264
01:18:07,050 --> 01:18:10,659
 

4265
01:18:07,060 --> 01:18:12,670
 hence the popularity of the lasso okay

4266
01:18:10,649 --> 01:18:12,670
 

4267
01:18:10,659 --> 01:18:14,849
 so um so there's a huge literature

4268
01:18:12,660 --> 01:18:14,849
 

4269
01:18:12,670 --> 01:18:18,460
 proposing many different penalties

4270
01:18:14,839 --> 01:18:18,460
 

4271
01:18:14,849 --> 01:18:21,070
 adaptive last abuse lasso blah blah blah

4272
01:18:18,450 --> 01:18:21,070
 

4273
01:18:18,460 --> 01:18:22,780
 in general the methods only produce a

4274
01:18:21,060 --> 01:18:22,780
 

4275
01:18:21,070 --> 01:18:24,940
 sparse point estimate and I would say

4276
01:18:22,770 --> 01:18:24,940
 

4277
01:18:22,780 --> 01:18:27,480
 are dangerous scientifically and I work

4278
01:18:24,930 --> 01:18:27,480
 

4279
01:18:24,940 --> 01:18:30,909
 a lot in real problems with scientists I

4280
01:18:27,470 --> 01:18:30,909
 

4281
01:18:27,480 --> 01:18:33,310
 funded by NIH and working on real trying

4282
01:18:30,899 --> 01:18:33,310
 

4283
01:18:30,909 --> 01:18:35,260
 to kind of solve health problems using

4284
01:18:33,300 --> 01:18:35,260
 

4285
01:18:33,310 --> 01:18:38,619
 machine learning methods and statistical

4286
01:18:35,250 --> 01:18:38,619
 

4287
01:18:35,260 --> 01:18:40,210
 methods and and a lot of the times these

4288
01:18:38,609 --> 01:18:40,210
 

4289
01:18:38,619 --> 01:18:42,159
 methods kind of pull you in you do this

4290
01:18:40,200 --> 01:18:42,159
 

4291
01:18:40,210 --> 01:18:43,750
 kind of sparse point estimation but then

4292
01:18:42,149 --> 01:18:43,750
 

4293
01:18:42,159 --> 01:18:46,320
 you kind of run into problems due to the

4294
01:18:43,740 --> 01:18:46,320
 

4295
01:18:43,750 --> 01:18:48,520
 lack of uncertainty quantification okay

4296
01:18:46,310 --> 01:18:48,520
 

4297
01:18:46,320 --> 01:18:51,639
 let's say there's a parallel Bayesian

4298
01:18:48,510 --> 01:18:51,639
 

4299
01:18:48,520 --> 01:18:53,440
 literature on shrinkage priors Bayes

4300
01:18:51,629 --> 01:18:53,440
 

4301
01:18:51,639 --> 01:18:55,030
 lasso is actually not a very good

4302
01:18:53,430 --> 01:18:55,030
 

4303
01:18:53,440 --> 01:18:56,500
 shrinkage prior I was talking about that

4304
01:18:55,020 --> 01:18:56,500
 

4305
01:18:55,030 --> 01:18:58,929
 over the break it's kind of a really bad

4306
01:18:56,490 --> 01:18:58,929
 

4307
01:18:56,500 --> 01:19:00,820
 shrinkage prior if we want to be able to

4308
01:18:58,919 --> 01:19:00,820
 

4309
01:18:58,929 --> 01:19:02,860
 deal with a sparse problem where we have

4310
01:19:00,810 --> 01:19:02,860
 

4311
01:19:00,820 --> 01:19:05,320
 you know very high dimensional

4312
01:19:02,850 --> 01:19:05,320
 

4313
01:19:02,860 --> 01:19:06,550
 predictors then the mode of the

4314
01:19:05,310 --> 01:19:06,550
 

4315
01:19:05,320 --> 01:19:08,110
 posterior is not something we

4316
01:19:06,540 --> 01:19:08,110
 

4317
01:19:06,550 --> 01:19:10,150
 particularly focus on we care about the

4318
01:19:08,100 --> 01:19:10,150
 

4319
01:19:08,110 --> 01:19:13,030
 full posterior distribution then we want

4320
01:19:10,140 --> 01:19:13,030
 

4321
01:19:10,150 --> 01:19:15,010
 a prior that actually has concentration

4322
01:19:13,020 --> 01:19:15,010
 

4323
01:19:13,030 --> 01:19:17,079
 near zero to shrink away the kind of

4324
01:19:15,000 --> 01:19:17,079
 

4325
01:19:15,010 --> 01:19:19,480
 noise parts or the small signals and

4326
01:19:17,069 --> 01:19:19,480
 

4327
01:19:17,079 --> 01:19:21,130
 then has heavy tails to avoid over

4328
01:19:19,470 --> 01:19:21,130
 

4329
01:19:19,480 --> 01:19:24,099
 shrinking the large signals and Bayes

4330
01:19:21,120 --> 01:19:24,099
 

4331
01:19:21,130 --> 01:19:25,329
 lasso has only one parameter controlling

4332
01:19:24,089 --> 01:19:25,329
 

4333
01:19:24,099 --> 01:19:27,429
 the tails in the concentration around

4334
01:19:25,319 --> 01:19:27,429
 

4335
01:19:25,329 --> 01:19:30,159
 zero so it's going to kind of over

4336
01:19:27,419 --> 01:19:30,159
 

4337
01:19:27,429 --> 01:19:32,710
 shrink the the the signals away from

4338
01:19:30,149 --> 01:19:32,710
 

4339
01:19:30,159 --> 01:19:35,650
 zero and then kind of could confound the

4340
01:19:32,700 --> 01:19:35,650
 

4341
01:19:32,710 --> 01:19:37,270
 kind of number of zero signals with with

4342
01:19:35,640 --> 01:19:37,270
 

4343
01:19:35,650 --> 01:19:38,389
 the number of non-zeros and so it

4344
01:19:37,260 --> 01:19:38,389
 

4345
01:19:37,270 --> 01:19:40,820
 doesn't have enough

4346
01:19:38,379 --> 01:19:40,820
 

4347
01:19:38,389 --> 01:19:43,310
 and some more the state of the art in

4348
01:19:40,810 --> 01:19:43,310
 

4349
01:19:40,820 --> 01:19:44,600
 Bayes based rink it's priors for high

4350
01:19:43,300 --> 01:19:44,600
 

4351
01:19:43,310 --> 01:19:46,610
 dimensional problems and we can apply

4352
01:19:44,590 --> 01:19:46,610
 

4353
01:19:44,600 --> 01:19:48,920
 them much much more broadly than in

4354
01:19:46,600 --> 01:19:48,920
 

4355
01:19:46,610 --> 01:19:51,170
 regression I would say horseshoe is the

4356
01:19:48,910 --> 01:19:51,170
 

4357
01:19:48,920 --> 01:19:52,850
 most popular I'm and then I'm also

4358
01:19:51,160 --> 01:19:52,850
 

4359
01:19:51,170 --> 01:19:55,610
 generalized double pret oh in dear sale

4360
01:19:52,840 --> 01:19:55,610
 

4361
01:19:52,850 --> 01:19:57,020
 applause okay and these priors are are

4362
01:19:55,600 --> 01:19:57,020
 

4363
01:19:55,610 --> 01:19:59,989
 designed to have this kind of more

4364
01:19:57,010 --> 01:19:59,989
 

4365
01:19:57,020 --> 01:20:03,560
 control over the shrinkage at zero

4366
01:19:59,979 --> 01:20:03,560
 

4367
01:19:59,989 --> 01:20:05,480
 versus the tails okay so I'm an

4368
01:20:03,550 --> 01:20:05,480
 

4369
01:20:03,560 --> 01:20:07,550
 appropriate sanity prior for a high

4370
01:20:05,470 --> 01:20:07,550
 

4371
01:20:05,480 --> 01:20:08,900
 dimensional vector of coefficients many

4372
01:20:07,540 --> 01:20:08,900
 

4373
01:20:07,550 --> 01:20:11,389
 of the priors can be written in this

4374
01:20:08,890 --> 01:20:11,389
 

4375
01:20:08,900 --> 01:20:13,460
 Nick pulled Polson had a nice paper on

4376
01:20:11,379 --> 01:20:13,460
 

4377
01:20:11,389 --> 01:20:15,530
 this and this global local scale mixture

4378
01:20:13,450 --> 01:20:15,530
 

4379
01:20:13,460 --> 01:20:18,409
 of gaussians framework where the beta

4380
01:20:15,520 --> 01:20:18,409
 

4381
01:20:15,530 --> 01:20:21,290
 J's are drawn iid conditionally iid from

4382
01:20:18,399 --> 01:20:21,290
 

4383
01:20:18,409 --> 01:20:23,659
 this normal 0 sy J lambda and then we

4384
01:20:21,280 --> 01:20:23,659
 

4385
01:20:21,290 --> 01:20:25,670
 put some sort of prior on sy j and on

4386
01:20:23,649 --> 01:20:25,670
 

4387
01:20:23,659 --> 01:20:27,409
 lambda so these are the local scales and

4388
01:20:25,660 --> 01:20:27,409
 

4389
01:20:25,670 --> 01:20:29,210
 this is a global scale and when we would

4390
01:20:27,399 --> 01:20:29,210
 

4391
01:20:27,409 --> 01:20:32,210
 choose an appropriate F and G this is

4392
01:20:29,200 --> 01:20:32,210
 

4393
01:20:29,210 --> 01:20:34,790
 the kind of game and we would like to

4394
01:20:32,200 --> 01:20:34,790
 

4395
01:20:32,210 --> 01:20:38,420
 choose lambda to be kind of close to

4396
01:20:34,780 --> 01:20:38,420
 

4397
01:20:34,790 --> 01:20:40,840
 zero to correspond to global sparsity

4398
01:20:38,410 --> 01:20:40,840
 

4399
01:20:38,420 --> 01:20:43,340
 and then then sigh J to have heavy tails

4400
01:20:40,830 --> 01:20:43,340
 

4401
01:20:40,840 --> 01:20:45,530
 okay so we have them we have really a

4402
01:20:43,330 --> 01:20:45,530
 

4403
01:20:43,340 --> 01:20:47,270
 rich literature now on these methods I'd

4404
01:20:45,520 --> 01:20:47,270
 

4405
01:20:45,530 --> 01:20:50,000
 like to kind of just highlight one paper

4406
01:20:47,260 --> 01:20:50,000
 

4407
01:20:47,270 --> 01:20:51,530
 which which is a you know biased a

4408
01:20:49,990 --> 01:20:51,530
 

4409
01:20:50,000 --> 01:20:53,119
 really excellent student of mine James

4410
01:20:51,520 --> 01:20:53,119
 

4411
01:20:51,530 --> 01:20:55,250
 John drew is now a Stein fellow at

4412
01:20:53,109 --> 01:20:55,250
 

4413
01:20:53,119 --> 01:20:56,810
 Stanford and on the job market he has

4414
01:20:55,240 --> 01:20:56,810
 

4415
01:20:55,250 --> 01:21:00,290
 this paper where he kind of shows that

4416
01:20:56,800 --> 01:21:00,290
 

4417
01:20:56,810 --> 01:21:03,350
 you can take a horseshoe prior and you

4418
01:21:00,280 --> 01:21:03,350
 

4419
01:21:00,290 --> 01:21:05,150
 can actually use some scalability tricks

4420
01:21:03,340 --> 01:21:05,150
 

4421
01:21:03,350 --> 01:21:07,070
 to show that theoretically you can scale

4422
01:21:05,140 --> 01:21:07,070
 

4423
01:21:05,150 --> 01:21:09,980
 up a prior to very high dimensional

4424
01:21:07,060 --> 01:21:09,980
 

4425
01:21:07,070 --> 01:21:12,199
 problems theoretically and practically

4426
01:21:09,970 --> 01:21:12,199
 

4427
01:21:09,980 --> 01:21:13,639
 and so you can now deal with certainly

4428
01:21:12,189 --> 01:21:13,639
 

4429
01:21:12,199 --> 01:21:15,230
 hundreds of thousands of predictors

4430
01:21:13,629 --> 01:21:15,230
 

4431
01:21:13,639 --> 01:21:17,719
 using these types of Markov chain Monte

4432
01:21:15,220 --> 01:21:17,719
 

4433
01:21:15,230 --> 01:21:20,119
 Carlo algorithms there's a really not

4434
01:21:17,709 --> 01:21:20,119
 

4435
01:21:17,719 --> 01:21:21,949
 another really nice paper is by Martin

4436
01:21:20,109 --> 01:21:21,949
 

4437
01:21:20,119 --> 01:21:23,360
 Wainwright and Mike Jordan and a former

4438
01:21:21,939 --> 01:21:23,360
 

4439
01:21:21,949 --> 01:21:25,280
 student of mine you and yang in the

4440
01:21:23,350 --> 01:21:25,280
 

4441
01:21:23,360 --> 01:21:27,040
 annals of statistics show it kind of

4442
01:21:25,270 --> 01:21:27,040
 

4443
01:21:25,280 --> 01:21:29,659
 have a similar result showing

4444
01:21:27,030 --> 01:21:29,659
 

4445
01:21:27,040 --> 01:21:33,350
 scalability for spikes lab priors we

4446
01:21:29,649 --> 01:21:33,350
 

4447
01:21:29,659 --> 01:21:35,030
 have a formal mass at zero Mik mixed

4448
01:21:33,340 --> 01:21:35,030
 

4449
01:21:33,350 --> 01:21:36,409
 with another another distribution and

4450
01:21:35,020 --> 01:21:36,409
 

4451
01:21:35,030 --> 01:21:39,409
 then you're doing a stochastic search in

4452
01:21:36,399 --> 01:21:39,409
 

4453
01:21:36,409 --> 01:21:41,630
 a very high dimensional space they show

4454
01:21:39,399 --> 01:21:41,630
 

4455
01:21:39,409 --> 01:21:43,820
 that that also can be designed to be to

4456
01:21:41,620 --> 01:21:43,820
 

4457
01:21:41,630 --> 01:21:48,159
 be technically scalable and in their CS

4458
01:21:43,810 --> 01:21:48,159
 

4459
01:21:43,820 --> 01:21:48,159
 sense okay

4460
01:21:48,250 --> 01:21:48,250
 

4461
01:21:48,260 --> 01:21:52,760
 so features of a Bayesian approach so a

4462
01:21:51,040 --> 01:21:52,760
 

4463
01:21:51,050 --> 01:21:54,320
 Bayesian approach provides a full

4464
01:21:52,750 --> 01:21:54,320
 

4465
01:21:52,760 --> 01:21:55,850
 posterior distribution characterizing

4466
01:21:54,310 --> 01:21:55,850
 

4467
01:21:54,320 --> 01:21:58,070
 uncertainty instead of just a sparse

4468
01:21:55,840 --> 01:21:58,070
 

4469
01:21:55,850 --> 01:22:01,280
 point estimate beta hat and that I think

4470
01:21:58,060 --> 01:22:01,280
 

4471
01:21:58,070 --> 01:22:05,239
 is really really really important and so

4472
01:22:01,270 --> 01:22:05,239
 

4473
01:22:01,280 --> 01:22:07,610
 by using MCMC we can get we can get

4474
01:22:05,229 --> 01:22:07,610
 

4475
01:22:05,239 --> 01:22:09,170
 credible bands and so those are like

4476
01:22:07,600 --> 01:22:09,170
 

4477
01:22:07,610 --> 01:22:11,000
 Bayesian versions of confidence

4478
01:22:09,160 --> 01:22:11,000
 

4479
01:22:09,170 --> 01:22:12,560
 intervals and so we can actually

4480
01:22:10,990 --> 01:22:12,560
 

4481
01:22:11,000 --> 01:22:14,420
 characterize uncertainty so these going

4482
01:22:12,550 --> 01:22:14,420
 

4483
01:22:12,560 --> 01:22:16,220
 to be confidence intervals for the beta

4484
01:22:14,410 --> 01:22:16,220
 

4485
01:22:14,420 --> 01:22:17,210
 beta J's and for any functional of

4486
01:22:16,210 --> 01:22:17,210
 

4487
01:22:16,220 --> 01:22:19,460
 interest like the predictive

4488
01:22:17,200 --> 01:22:19,460
 

4489
01:22:17,210 --> 01:22:20,660
 distribution and it's relatively

4490
01:22:19,450 --> 01:22:20,660
 

4491
01:22:19,460 --> 01:22:22,550
 straightforward to incorporate

4492
01:22:20,650 --> 01:22:22,550
 

4493
01:22:20,660 --> 01:22:24,470
 extensions to allow all sorts of like

4494
01:22:22,540 --> 01:22:24,470
 

4495
01:22:22,550 --> 01:22:26,510
 bells and whistles hierarchical

4496
01:22:24,460 --> 01:22:26,510
 

4497
01:22:24,470 --> 01:22:30,920
 dependent structures multivariate

4498
01:22:26,500 --> 01:22:30,920
 

4499
01:22:26,510 --> 01:22:33,170
 responses missing data etc etc but I

4500
01:22:30,910 --> 01:22:33,170
 

4501
01:22:30,920 --> 01:22:35,150
 would say that um you know a couple of

4502
01:22:33,160 --> 01:22:35,150
 

4503
01:22:33,170 --> 01:22:37,460
 caveats and motivations for people who

4504
01:22:35,140 --> 01:22:37,460
 

4505
01:22:35,150 --> 01:22:40,280
 might want to do some do some research

4506
01:22:37,450 --> 01:22:40,280
 

4507
01:22:37,460 --> 01:22:45,050
 in this area or that um I like still

4508
01:22:40,270 --> 01:22:45,050
 

4509
01:22:40,280 --> 01:22:46,130
 don't really trust these methods I'm

4510
01:22:45,040 --> 01:22:46,130
 

4511
01:22:45,050 --> 01:22:48,080
 gonna coffee but now I'm doing something

4512
01:22:46,120 --> 01:22:48,080
 

4513
01:22:46,130 --> 01:22:49,760
 I don't really don't trust these methods

4514
01:22:48,070 --> 01:22:49,760
 

4515
01:22:48,080 --> 01:22:52,270
 very well because what what we what we

4516
01:22:49,750 --> 01:22:52,270
 

4517
01:22:49,760 --> 01:22:54,860
 do in practice is in order to get good

4518
01:22:52,260 --> 01:22:54,860
 

4519
01:22:52,270 --> 01:22:57,920
 results you know provably good results

4520
01:22:54,850 --> 01:22:57,920
 

4521
01:22:54,860 --> 01:22:59,480
 for really big P relative to N then we

4522
01:22:57,910 --> 01:22:59,480
 

4523
01:22:57,920 --> 01:23:01,970
 kind of apply really an aggressive

4524
01:22:59,470 --> 01:23:01,970
 

4525
01:22:59,480 --> 01:23:03,410
 shrinkage prior okay and so the the

4526
01:23:01,960 --> 01:23:03,410
 

4527
01:23:01,970 --> 01:23:05,270
 prior is pretty shrinking really

4528
01:23:03,400 --> 01:23:05,270
 

4529
01:23:03,410 --> 01:23:06,950
 aggressively well what if the what if

4530
01:23:05,260 --> 01:23:06,950
 

4531
01:23:05,270 --> 01:23:10,489
 the truth is actually not that sparse

4532
01:23:06,940 --> 01:23:10,489
 

4533
01:23:06,950 --> 01:23:12,050
 for example well you know then our prior

4534
01:23:10,479 --> 01:23:12,050
 

4535
01:23:10,489 --> 01:23:13,640
 is actually really informative we've had

4536
01:23:12,040 --> 01:23:13,640
 

4537
01:23:12,050 --> 01:23:15,350
 to made it really informative because

4538
01:23:13,630 --> 01:23:15,350
 

4539
01:23:13,640 --> 01:23:17,570
 the dimension of the data is much bigger

4540
01:23:15,340 --> 01:23:17,570
 

4541
01:23:15,350 --> 01:23:19,580
 than the sample size if we don't make

4542
01:23:17,560 --> 01:23:19,580
 

4543
01:23:17,570 --> 01:23:21,020
 the prior really informative then the

4544
01:23:19,570 --> 01:23:21,020
 

4545
01:23:19,580 --> 01:23:22,430
 posterior is just gonna be really vague

4546
01:23:21,010 --> 01:23:22,430
 

4547
01:23:21,020 --> 01:23:23,720
 and it's not gonna concentrate around

4548
01:23:22,420 --> 01:23:23,720
 

4549
01:23:22,430 --> 01:23:25,400
 anything and then we can't make sense of

4550
01:23:23,710 --> 01:23:25,400
 

4551
01:23:23,720 --> 01:23:28,520
 it maybe that would be the appropriate

4552
01:23:25,390 --> 01:23:28,520
 

4553
01:23:25,400 --> 01:23:30,290
 uncertainty quantification but you know

4554
01:23:28,510 --> 01:23:30,290
 

4555
01:23:28,520 --> 01:23:31,550
 to try to get a good result we've

4556
01:23:30,280 --> 01:23:31,550
 

4557
01:23:30,290 --> 01:23:33,110
 because we've sort of put in a lot of

4558
01:23:31,540 --> 01:23:33,110
 

4559
01:23:31,550 --> 01:23:34,910
 information in the prior may be too much

4560
01:23:33,100 --> 01:23:34,910
 

4561
01:23:33,110 --> 01:23:36,230
 information in the prior and then we

4562
01:23:34,900 --> 01:23:36,230
 

4563
01:23:34,910 --> 01:23:38,180
 look at these intervals in there then

4564
01:23:36,220 --> 01:23:38,180
 

4565
01:23:36,230 --> 01:23:39,560
 they're really tight and so I get a

4566
01:23:38,170 --> 01:23:39,560
 

4567
01:23:38,180 --> 01:23:41,000
 little bit skeptical about those

4568
01:23:39,550 --> 01:23:41,000
 

4569
01:23:39,560 --> 01:23:43,220
 intervals because they're so tight and

4570
01:23:40,990 --> 01:23:43,220
 

4571
01:23:41,000 --> 01:23:45,350
 they're kind of putting in information

4572
01:23:43,210 --> 01:23:45,350
 

4573
01:23:43,220 --> 01:23:47,540
 in the prior as if the problem is really

4574
01:23:45,340 --> 01:23:47,540
 

4575
01:23:45,350 --> 01:23:49,040
 sparse and so I think we we all need to

4576
01:23:47,530 --> 01:23:49,040
 

4577
01:23:47,540 --> 01:23:51,110
 kind of think think carefully about

4578
01:23:49,030 --> 01:23:51,110
 

4579
01:23:49,040 --> 01:23:53,180
 about this kind of issue it gives you

4580
01:23:51,100 --> 01:23:53,180
 

4581
01:23:51,110 --> 01:23:55,280
 uncertainty quantification but it gives

4582
01:23:53,170 --> 01:23:55,280
 

4583
01:23:53,180 --> 01:23:56,360
 you it under a very informative prior if

4584
01:23:55,270 --> 01:23:56,360
 

4585
01:23:55,280 --> 01:23:58,750
 you don't have a really informative

4586
01:23:56,350 --> 01:23:58,750
 

4587
01:23:56,360 --> 01:24:01,130
 prior the posterior doesn't concentrate

4588
01:23:58,740 --> 01:24:01,130
 

4589
01:23:58,750 --> 01:24:03,750
 okay

4590
01:24:01,120 --> 01:24:03,750
 

4591
01:24:01,130 --> 01:24:05,760
 okay so in the in the in the remainder

4592
01:24:03,740 --> 01:24:05,760
 

4593
01:24:03,750 --> 01:24:07,410
 I'm gonna give a couple of kind of

4594
01:24:05,750 --> 01:24:07,410
 

4595
01:24:05,760 --> 01:24:09,000
 vignettes about you know kind of

4596
01:24:07,400 --> 01:24:09,000
 

4597
01:24:07,410 --> 01:24:11,970
 different types of approaches jumping

4598
01:24:08,990 --> 01:24:11,970
 

4599
01:24:09,000 --> 01:24:13,860
 into a couple of applications that can

4600
01:24:11,960 --> 01:24:13,860
 

4601
01:24:11,970 --> 01:24:18,840
 be made made scalable that I find kind

4602
01:24:13,850 --> 01:24:18,840
 

4603
01:24:13,860 --> 01:24:20,820
 of interesting so here's one really cool

4604
01:24:18,830 --> 01:24:20,820
 

4605
01:24:18,840 --> 01:24:23,070
 application so DNA methylation arrays

4606
01:24:20,810 --> 01:24:23,070
 

4607
01:24:20,820 --> 01:24:25,110
 and so what's DNA methylation and so um

4608
01:24:23,060 --> 01:24:25,110
 

4609
01:24:23,070 --> 01:24:27,930
 so we all have like our genome you know

4610
01:24:25,100 --> 01:24:27,930
 

4611
01:24:25,110 --> 01:24:29,880
 we have like our sequence but actually

4612
01:24:27,920 --> 01:24:29,880
 

4613
01:24:27,930 --> 01:24:32,250
 there's all these CPG sites along our

4614
01:24:29,870 --> 01:24:32,250
 

4615
01:24:29,880 --> 01:24:34,170
 genome that can become methylated and so

4616
01:24:32,240 --> 01:24:34,170
 

4617
01:24:32,250 --> 01:24:35,790
 like maybe by what we're exposed to I'm

4618
01:24:34,160 --> 01:24:35,790
 

4619
01:24:34,170 --> 01:24:37,890
 drinking too much coffee or something

4620
01:24:35,780 --> 01:24:37,890
 

4621
01:24:35,790 --> 01:24:39,810
 then I get certain regions methylated

4622
01:24:37,880 --> 01:24:39,810
 

4623
01:24:37,890 --> 01:24:41,940
 and then that's going to that's going to

4624
01:24:39,800 --> 01:24:41,940
 

4625
01:24:39,810 --> 01:24:43,710
 impact gene expression and so that can

4626
01:24:41,930 --> 01:24:43,710
 

4627
01:24:41,940 --> 01:24:45,300
 maybe impact you know my probability of

4628
01:24:43,700 --> 01:24:45,300
 

4629
01:24:43,710 --> 01:24:48,120
 getting diseases or other things about

4630
01:24:45,290 --> 01:24:48,120
 

4631
01:24:45,300 --> 01:24:49,830
 me okay and so we'd like to we'd like to

4632
01:24:48,110 --> 01:24:49,830
 

4633
01:24:48,120 --> 01:24:52,320
 understand the effect of this kind of

4634
01:24:49,820 --> 01:24:52,320
 

4635
01:24:49,830 --> 01:24:53,760
 epigenome corresponding to these DNA

4636
01:24:52,310 --> 01:24:53,760
 

4637
01:24:52,320 --> 01:24:55,860
 methylation sites okay

4638
01:24:53,750 --> 01:24:55,860
 

4639
01:24:53,760 --> 01:24:57,900
 and so now there's all sorts of like

4640
01:24:55,850 --> 01:24:57,900
 

4641
01:24:55,860 --> 01:24:59,220
 amazing biotechnology for measuring

4642
01:24:57,890 --> 01:24:59,220
 

4643
01:24:57,900 --> 01:25:01,680
 stuff like this and so we could measure

4644
01:24:59,210 --> 01:25:01,680
 

4645
01:24:59,220 --> 01:25:03,720
 it across the whole genome and all of

4646
01:25:01,670 --> 01:25:03,720
 

4647
01:25:01,680 --> 01:25:05,630
 these different CPG sites and so here

4648
01:25:03,710 --> 01:25:05,630
 

4649
01:25:03,720 --> 01:25:08,190
 we're analyzing data where we have

4650
01:25:05,620 --> 01:25:08,190
 

4651
01:25:05,630 --> 01:25:09,630
 450,000 different CPG sites there's even

4652
01:25:08,180 --> 01:25:09,630
 

4653
01:25:08,190 --> 01:25:10,950
 higher dimensional data available where

4654
01:25:09,620 --> 01:25:10,950
 

4655
01:25:09,630 --> 01:25:14,730
 we can look at across the entire genome

4656
01:25:10,940 --> 01:25:14,730
 

4657
01:25:10,950 --> 01:25:19,320
 for every individual in a study okay so

4658
01:25:14,720 --> 01:25:19,320
 

4659
01:25:14,730 --> 01:25:22,680
 um what we'd like to do is we'd like to

4660
01:25:19,310 --> 01:25:22,680
 

4661
01:25:19,320 --> 01:25:24,240
 identify differentially methylated CPG

4662
01:25:22,670 --> 01:25:24,240
 

4663
01:25:22,680 --> 01:25:27,180
 sites and there's there's a ton of them

4664
01:25:24,230 --> 01:25:27,180
 

4665
01:25:24,240 --> 01:25:30,240
 but if we zoom in at any given location

4666
01:25:27,170 --> 01:25:30,240
 

4667
01:25:27,180 --> 01:25:33,060
 like say these are the densities of DNA

4668
01:25:30,230 --> 01:25:33,060
 

4669
01:25:30,240 --> 01:25:35,180
 methylation for people with two

4670
01:25:33,050 --> 01:25:35,180
 

4671
01:25:33,060 --> 01:25:38,160
 different subtypes of cancer for example

4672
01:25:35,170 --> 01:25:38,160
 

4673
01:25:35,180 --> 01:25:40,200
 then we see that the densities are quite

4674
01:25:38,150 --> 01:25:40,200
 

4675
01:25:38,160 --> 01:25:42,540
 non Gaussian they're quite multimodal

4676
01:25:40,190 --> 01:25:42,540
 

4677
01:25:40,200 --> 01:25:44,130
 and strange-looking okay so how do we

4678
01:25:42,530 --> 01:25:44,130
 

4679
01:25:42,540 --> 01:25:49,440
 kind of develop some sort of statists

4680
01:25:44,120 --> 01:25:49,440
 

4681
01:25:44,130 --> 01:25:50,820
 callable Bayesian statistical method so

4682
01:25:49,430 --> 01:25:50,820
 

4683
01:25:49,440 --> 01:25:52,470
 the you know the met here the

4684
01:25:50,810 --> 01:25:52,470
 

4685
01:25:50,820 --> 01:25:54,390
 measurements aren't a zero one interval

4686
01:25:52,460 --> 01:25:54,390
 

4687
01:25:52,470 --> 01:25:57,750
 ranging from no methylation to fully

4688
01:25:54,380 --> 01:25:57,750
 

4689
01:25:54,390 --> 01:26:01,680
 methylated so the data from the Cancer

4690
01:25:57,740 --> 01:26:01,680
 

4691
01:25:57,750 --> 01:26:03,740
 Genome Atlas okay so we observe data

4692
01:26:01,670 --> 01:26:03,740
 

4693
01:26:01,680 --> 01:26:05,970
 like this at a huge number of CPG sites

4694
01:26:03,730 --> 01:26:05,970
 

4695
01:26:03,740 --> 01:26:08,670
 many distributions share common

4696
01:26:05,960 --> 01:26:08,670
 

4697
01:26:05,970 --> 01:26:11,280
 attributes modes etc and so we could we

4698
01:26:08,660 --> 01:26:11,280
 

4699
01:26:08,670 --> 01:26:13,170
 could you know what will we do if we

4700
01:26:11,270 --> 01:26:13,170
 

4701
01:26:11,280 --> 01:26:14,100
 tried to build like a religious fully

4702
01:26:13,160 --> 01:26:14,100
 

4703
01:26:13,170 --> 01:26:18,110
 Bayesian

4704
01:26:14,090 --> 01:26:18,110
 

4705
01:26:14,100 --> 01:26:20,850
 model then each individual would have a

4706
01:26:18,100 --> 01:26:20,850
 

4707
01:26:18,110 --> 01:26:22,980
 450 thousand dimensional response and

4708
01:26:20,840 --> 01:26:22,980
 

4709
01:26:20,850 --> 01:26:25,140
 then we could try to like what would you

4710
01:26:22,970 --> 01:26:25,140
 

4711
01:26:22,980 --> 01:26:27,480
 Canada to build a mixture model for a

4712
01:26:25,130 --> 01:26:27,480
 

4713
01:26:25,140 --> 01:26:32,490
 450 thousand dimensional response we

4714
01:26:27,470 --> 01:26:32,490
 

4715
01:26:27,480 --> 01:26:34,080
 really can't do that and so I'm what I'm

4716
01:26:32,480 --> 01:26:34,080
 

4717
01:26:32,490 --> 01:26:35,280
 gonna do is I'm gonna I talked earlier

4718
01:26:34,070 --> 01:26:35,280
 

4719
01:26:34,080 --> 01:26:38,190
 about sea bass

4720
01:26:35,270 --> 01:26:38,190
 

4721
01:26:35,280 --> 01:26:40,140
 another thing that I really like you've

4722
01:26:38,180 --> 01:26:40,140
 

4723
01:26:38,190 --> 01:26:41,730
 s been looked at in the recent

4724
01:26:40,130 --> 01:26:41,730
 

4725
01:26:40,140 --> 01:26:44,010
 literature is what's called modular

4726
01:26:41,720 --> 01:26:44,010
 

4727
01:26:41,730 --> 01:26:46,320
 Bay's and so that's taking the Bayesian

4728
01:26:44,000 --> 01:26:46,320
 

4729
01:26:44,010 --> 01:26:47,940
 framework but within a religious

4730
01:26:46,310 --> 01:26:47,940
 

4731
01:26:46,320 --> 01:26:49,830
 Bayesian framework you would have to

4732
01:26:47,930 --> 01:26:49,830
 

4733
01:26:47,940 --> 01:26:51,840
 model everything jointly you'd have to

4734
01:26:49,820 --> 01:26:51,840
 

4735
01:26:49,830 --> 01:26:54,330
 have like a likelihood you believe for

4736
01:26:51,830 --> 01:26:54,330
 

4737
01:26:51,840 --> 01:26:56,970
 everything jointly what modular Bayes

4738
01:26:54,320 --> 01:26:56,970
 

4739
01:26:54,330 --> 01:26:59,430
 does is it actually breaks up the data

4740
01:26:56,960 --> 01:26:59,430
 

4741
01:26:56,970 --> 01:27:01,800
 in the modules and you can like put a

4742
01:26:59,420 --> 01:27:01,800
 

4743
01:26:59,430 --> 01:27:03,450
 Bayesian model within each module but

4744
01:27:01,790 --> 01:27:03,450
 

4745
01:27:01,800 --> 01:27:05,640
 the modules don't necessarily talk to

4746
01:27:03,440 --> 01:27:05,640
 

4747
01:27:03,450 --> 01:27:08,010
 each other and there there's - there's a

4748
01:27:05,630 --> 01:27:08,010
 

4749
01:27:05,640 --> 01:27:10,740
 couple of reasons why that's really nice

4750
01:27:08,000 --> 01:27:10,740
 

4751
01:27:08,010 --> 01:27:12,540
 as a type of generalized Bayes procedure

4752
01:27:10,730 --> 01:27:12,540
 

4753
01:27:10,740 --> 01:27:14,970
 that does nice and scalable uncertainty

4754
01:27:12,530 --> 01:27:14,970
 

4755
01:27:12,540 --> 01:27:17,280
 quantification the first is that because

4756
01:27:14,960 --> 01:27:17,280
 

4757
01:27:14,970 --> 01:27:19,740
 the modules don't talk to each other the

4758
01:27:17,270 --> 01:27:19,740
 

4759
01:27:17,280 --> 01:27:22,290
 model can be much more robust and so

4760
01:27:19,730 --> 01:27:22,290
 

4761
01:27:19,740 --> 01:27:25,380
 sometimes fully base joint models for

4762
01:27:22,280 --> 01:27:25,380
 

4763
01:27:22,290 --> 01:27:27,570
 high dimensional data can be brittle to

4764
01:27:25,370 --> 01:27:27,570
 

4765
01:27:25,380 --> 01:27:30,000
 model Miss specification okay

4766
01:27:27,560 --> 01:27:30,000
 

4767
01:27:27,570 --> 01:27:33,330
 as I've talked about earlier but also

4768
01:27:29,990 --> 01:27:33,330
 

4769
01:27:30,000 --> 01:27:34,830
 and if we if we have they're much less

4770
01:27:33,320 --> 01:27:34,830
 

4771
01:27:33,330 --> 01:27:36,900
 brittle if we use a modular approach

4772
01:27:34,820 --> 01:27:36,900
 

4773
01:27:34,830 --> 01:27:39,690
 where we're modeling pieces of the data

4774
01:27:36,890 --> 01:27:39,690
 

4775
01:27:36,900 --> 01:27:41,370
 separately another really big reason of

4776
01:27:39,680 --> 01:27:41,370
 

4777
01:27:39,690 --> 01:27:43,320
 course is then scalability because if

4778
01:27:41,360 --> 01:27:43,320
 

4779
01:27:41,370 --> 01:27:45,200
 we're modularizing and we have piece of

4780
01:27:43,310 --> 01:27:45,200
 

4781
01:27:43,320 --> 01:27:47,790
 models for pieces of the data separately

4782
01:27:45,190 --> 01:27:47,790
 

4783
01:27:45,200 --> 01:27:54,120
 then we can potentially do computation

4784
01:27:47,780 --> 01:27:54,120
 

4785
01:27:47,790 --> 01:27:55,200
 separately as well so what I'm gonna do

4786
01:27:54,110 --> 01:27:55,200
 

4787
01:27:54,120 --> 01:27:57,120
 here is I'm gonna do a type of

4788
01:27:55,190 --> 01:27:57,120
 

4789
01:27:55,200 --> 01:28:00,990
 dictionary learning basically so I'd

4790
01:27:57,110 --> 01:28:00,990
 

4791
01:27:57,120 --> 01:28:02,610
 like to characterize hundreds or

4792
01:28:00,980 --> 01:28:02,610
 

4793
01:28:00,990 --> 01:28:05,280
 thousands or millions of these

4794
01:28:02,600 --> 01:28:05,280
 

4795
01:28:02,610 --> 01:28:07,380
 distributions using a small number of

4796
01:28:05,270 --> 01:28:07,380
 

4797
01:28:05,280 --> 01:28:09,330
 pieces and then reuse those pieces

4798
01:28:07,370 --> 01:28:09,330
 

4799
01:28:07,380 --> 01:28:10,770
 within a Bayesian model so I'm going to

4800
01:28:09,320 --> 01:28:10,770
 

4801
01:28:09,330 --> 01:28:13,290
 use what's called a shared kernel

4802
01:28:10,760 --> 01:28:13,290
 

4803
01:28:10,770 --> 01:28:14,910
 mixture model or shark okay so we're

4804
01:28:13,280 --> 01:28:14,910
 

4805
01:28:13,290 --> 01:28:16,470
 gonna use the same kernels across the

4806
01:28:14,900 --> 01:28:16,470
 

4807
01:28:14,910 --> 01:28:21,600
 sites and groups but allow the weights

4808
01:28:16,460 --> 01:28:21,600
 

4809
01:28:16,470 --> 01:28:24,780
 to vary okay so so the methylation

4810
01:28:21,590 --> 01:28:24,780
 

4811
01:28:21,600 --> 01:28:27,360
 density at site J in Group G so the site

4812
01:28:24,770 --> 01:28:27,360
 

4813
01:28:24,780 --> 01:28:29,070
 says different sites along the genome

4814
01:28:27,350 --> 01:28:29,070
 

4815
01:28:27,360 --> 01:28:31,320
 group is the different types of cancer

4816
01:28:29,060 --> 01:28:31,320
 

4817
01:28:29,070 --> 01:28:35,940
 group okay and so we could say well that

4818
01:28:31,310 --> 01:28:35,940
 

4819
01:28:31,320 --> 01:28:38,790
 that density is fjg so fjg of Y that's

4820
01:28:35,930 --> 01:28:38,790
 

4821
01:28:35,940 --> 01:28:40,650
 the density of methylation at that that

4822
01:28:38,780 --> 01:28:40,650
 

4823
01:28:38,790 --> 01:28:43,710
 site in that group we can write that as

4824
01:28:40,640 --> 01:28:43,710
 

4825
01:28:40,650 --> 01:28:44,940
 a kernel mixture okay of H components

4826
01:28:43,700 --> 01:28:44,940
 

4827
01:28:43,710 --> 01:28:46,110
 and then we can put down some kernels

4828
01:28:44,930 --> 01:28:46,110
 

4829
01:28:44,940 --> 01:28:48,360
 and now we're gonna let the weights vary

4830
01:28:46,100 --> 01:28:48,360
 

4831
01:28:46,110 --> 01:28:51,570
 but the kernels are common so when you

4832
01:28:48,350 --> 01:28:51,570
 

4833
01:28:48,360 --> 01:28:53,400
 do that we make the kernels common we

4834
01:28:51,560 --> 01:28:53,400
 

4835
01:28:51,570 --> 01:28:55,230
 basically have like a ridiculously

4836
01:28:53,390 --> 01:28:55,230
 

4837
01:28:53,400 --> 01:28:57,170
 enormous a bit essentially infinite

4838
01:28:55,220 --> 01:28:57,170
 

4839
01:28:55,230 --> 01:28:59,460
 sample size for learning the kernels

4840
01:28:57,160 --> 01:28:59,460
 

4841
01:28:57,170 --> 01:29:01,170
 because we get this kind of blessing of

4842
01:28:59,450 --> 01:29:01,170
 

4843
01:28:59,460 --> 01:29:03,210
 dimensionality across the different CPG

4844
01:29:01,160 --> 01:29:03,210
 

4845
01:29:01,170 --> 01:29:05,160
 sites and so we can we don't need to

4846
01:29:03,200 --> 01:29:05,160
 

4847
01:29:03,210 --> 01:29:07,740
 characterize uncertainty and learning

4848
01:29:05,150 --> 01:29:07,740
 

4849
01:29:05,160 --> 01:29:09,240
 the kernels because our data are so

4850
01:29:07,730 --> 01:29:09,240
 

4851
01:29:07,740 --> 01:29:10,590
 immense that if we characterize the

4852
01:29:09,230 --> 01:29:10,590
 

4853
01:29:09,240 --> 01:29:12,810
 uncertainty the posterior would be just

4854
01:29:10,580 --> 01:29:12,810
 

4855
01:29:10,590 --> 01:29:14,970
 a point mass super concentrated on one

4856
01:29:12,800 --> 01:29:14,970
 

4857
01:29:12,810 --> 01:29:17,310
 value anyway and so we're gonna learn a

4858
01:29:14,960 --> 01:29:17,310
 

4859
01:29:14,970 --> 01:29:19,320
 fixed set of kernels and then put a

4860
01:29:17,300 --> 01:29:19,320
 

4861
01:29:17,310 --> 01:29:20,910
 posterior distribution characterizing

4862
01:29:19,310 --> 01:29:20,910
 

4863
01:29:19,320 --> 01:29:22,590
 uncertainty on the weights with the

4864
01:29:20,900 --> 01:29:22,590
 

4865
01:29:20,910 --> 01:29:24,450
 weights vary across the different sites

4866
01:29:22,580 --> 01:29:24,450
 

4867
01:29:22,590 --> 01:29:28,470
 and groups okay and so that's gonna be

4868
01:29:24,440 --> 01:29:28,470
 

4869
01:29:24,450 --> 01:29:33,120
 the game so these weights 1 PI JG are

4870
01:29:28,460 --> 01:29:33,120
 

4871
01:29:28,470 --> 01:29:35,220
 weight specific to site J group G and so

4872
01:29:33,110 --> 01:29:35,220
 

4873
01:29:33,120 --> 01:29:37,230
 K is a shared kernel so here we use the

4874
01:29:35,210 --> 01:29:37,230
 

4875
01:29:35,220 --> 01:29:41,280
 truncated normal we can use any sort of

4876
01:29:37,220 --> 01:29:41,280
 

4877
01:29:37,230 --> 01:29:42,750
 like fancy kernel okay so we're gonna

4878
01:29:41,270 --> 01:29:42,750
 

4879
01:29:41,280 --> 01:29:44,700
 put a simple hierarchical model on the

4880
01:29:42,740 --> 01:29:44,700
 

4881
01:29:42,750 --> 01:29:47,430
 pi PI J G's the dearest lay in each

4882
01:29:44,690 --> 01:29:47,430
 

4883
01:29:44,700 --> 01:29:49,440
 group and then we we do a testing

4884
01:29:47,420 --> 01:29:49,440
 

4885
01:29:47,430 --> 01:29:51,300
 problem now we can do a scalable testing

4886
01:29:49,430 --> 01:29:51,300
 

4887
01:29:49,440 --> 01:29:53,580
 I think testing is some some problem

4888
01:29:51,290 --> 01:29:53,580
 

4889
01:29:51,300 --> 01:29:55,260
 that we often really have to do in

4890
01:29:53,570 --> 01:29:55,260
 

4891
01:29:53,580 --> 01:29:56,940
 scientific inferences but machine

4892
01:29:55,250 --> 01:29:56,940
 

4893
01:29:55,260 --> 01:29:59,850
 learning people haven't focused on as

4894
01:29:56,930 --> 01:29:59,850
 

4895
01:29:56,940 --> 01:30:01,170
 much as I'd like so I'm here we have two

4896
01:29:59,840 --> 01:30:01,170
 

4897
01:29:59,850 --> 01:30:03,420
 different possibilities need each site

4898
01:30:01,160 --> 01:30:03,420
 

4899
01:30:01,170 --> 01:30:05,400
 so the two different cancer subtype

4900
01:30:03,410 --> 01:30:05,400
 

4901
01:30:03,420 --> 01:30:07,890
 groups they either have the same

4902
01:30:05,390 --> 01:30:07,890
 

4903
01:30:05,400 --> 01:30:09,120
 distribution of methylation or they have

4904
01:30:07,880 --> 01:30:09,120
 

4905
01:30:07,890 --> 01:30:10,890
 a different distribution of methylation

4906
01:30:09,110 --> 01:30:10,890
 

4907
01:30:09,120 --> 01:30:13,740
 we'd like to identify the sites that are

4908
01:30:10,880 --> 01:30:13,740
 

4909
01:30:10,890 --> 01:30:16,170
 differentially methylated okay that's a

4910
01:30:13,730 --> 01:30:16,170
 

4911
01:30:13,740 --> 01:30:18,600
 multiple hypothesis testing problem so

4912
01:30:16,160 --> 01:30:18,600
 

4913
01:30:16,170 --> 01:30:21,270
 we're gonna put D relate priors on each

4914
01:30:18,590 --> 01:30:21,270
 

4915
01:30:18,600 --> 01:30:23,400
 of these probability vectors within this

4916
01:30:21,260 --> 01:30:23,400
 

4917
01:30:21,270 --> 01:30:25,230
 hypothesis testing problem and it's

4918
01:30:23,390 --> 01:30:25,230
 

4919
01:30:23,400 --> 01:30:27,750
 gonna be automatically adjusting for a

4920
01:30:25,220 --> 01:30:27,750
 

4921
01:30:25,230 --> 01:30:29,610
 multiple testing error controlling false

4922
01:30:27,740 --> 01:30:29,610
 

4923
01:30:27,750 --> 01:30:32,180
 discovery rate automatically within a

4924
01:30:29,600 --> 01:30:32,180
 

4925
01:30:29,610 --> 01:30:34,800
 Bayesian multiple testing framework and

4926
01:30:32,170 --> 01:30:34,800
 

4927
01:30:32,180 --> 01:30:37,530
 the computation is extremely fast

4928
01:30:34,790 --> 01:30:37,530
 

4929
01:30:34,800 --> 01:30:39,730
 because if we use fixed kernels we get

4930
01:30:37,520 --> 01:30:39,730
 

4931
01:30:37,530 --> 01:30:41,410
 conjugacy everywhere else except

4932
01:30:39,720 --> 01:30:41,410
 

4933
01:30:39,730 --> 01:30:43,600
 for one little piece where we can just

4934
01:30:41,400 --> 01:30:43,600
 

4935
01:30:41,410 --> 01:30:46,330
 use a simple parallel I skip sampler and

4936
01:30:43,590 --> 01:30:46,330
 

4937
01:30:43,600 --> 01:30:47,710
 so we can run you know this for hundreds

4938
01:30:46,320 --> 01:30:47,710
 

4939
01:30:46,330 --> 01:30:49,330
 and hundreds and hundreds of thousands

4940
01:30:47,700 --> 01:30:49,330
 

4941
01:30:47,710 --> 01:30:52,720
 or millions of different sites quite

4942
01:30:49,320 --> 01:30:52,720
 

4943
01:30:49,330 --> 01:30:55,270
 quite seamlessly and we have some theory

4944
01:30:52,710 --> 01:30:55,270
 

4945
01:30:52,720 --> 01:30:58,900
 I won't talk about okay so um so we're

4946
01:30:55,260 --> 01:30:58,900
 

4947
01:30:55,270 --> 01:31:01,000
 gonna we Illustrated this using of 597

4948
01:30:58,890 --> 01:31:01,000
 

4949
01:30:58,900 --> 01:31:04,000
 breast cancer samples let's just show it

4950
01:31:00,990 --> 01:31:04,000
 

4951
01:31:01,000 --> 01:31:07,090
 for 21,000 CBG sites from this Cancer

4952
01:31:03,990 --> 01:31:07,090
 

4953
01:31:04,000 --> 01:31:10,030
 Genome Atlas and here just shows the the

4954
01:31:07,080 --> 01:31:10,030
 

4955
01:31:07,090 --> 01:31:13,960
 probability of the null hypothesis at

4956
01:31:10,020 --> 01:31:13,960
 

4957
01:31:10,030 --> 01:31:15,580
 each of the sites so there's 21,000 22

4958
01:31:13,950 --> 01:31:15,580
 

4959
01:31:13,960 --> 01:31:17,530
 almost thousand sites and so this is

4960
01:31:15,570 --> 01:31:17,530
 

4961
01:31:15,580 --> 01:31:20,440
 just a histogram of the estimated

4962
01:31:17,520 --> 01:31:20,440
 

4963
01:31:17,530 --> 01:31:22,300
 posterior probabilities that that that

4964
01:31:20,430 --> 01:31:22,300
 

4965
01:31:20,440 --> 01:31:24,550
 there's no differential methylation and

4966
01:31:22,290 --> 01:31:24,550
 

4967
01:31:22,300 --> 01:31:26,470
 so you see most of most of the sites are

4968
01:31:24,540 --> 01:31:26,470
 

4969
01:31:24,550 --> 01:31:28,840
 like there's no difference between the

4970
01:31:26,460 --> 01:31:28,840
 

4971
01:31:26,470 --> 01:31:31,150
 cancer subtypes but here there's a

4972
01:31:28,830 --> 01:31:31,150
 

4973
01:31:28,840 --> 01:31:33,310
 subset of site sites having a high

4974
01:31:31,140 --> 01:31:33,310
 

4975
01:31:31,150 --> 01:31:36,190
 posterior probability of each not

4976
01:31:33,300 --> 01:31:36,190
 

4977
01:31:33,310 --> 01:31:37,960
 meaning a you know a low posterior

4978
01:31:36,180 --> 01:31:37,960
 

4979
01:31:36,190 --> 01:31:38,950
 probability H not meaning a high post

4980
01:31:37,950 --> 01:31:38,950
 

4981
01:31:37,960 --> 01:31:42,130
 your probability that there's a

4982
01:31:38,940 --> 01:31:42,130
 

4983
01:31:38,950 --> 01:31:44,170
 difference okay and so we can test

4984
01:31:42,120 --> 01:31:44,170
 

4985
01:31:42,130 --> 01:31:46,450
 differences between basal-like and not

4986
01:31:44,160 --> 01:31:46,450
 

4987
01:31:44,170 --> 01:31:50,050
 at each site and the global proportion

4988
01:31:46,440 --> 01:31:50,050
 

4989
01:31:46,450 --> 01:31:51,670
 is no difference was 0.8 to 1 okay so

4990
01:31:50,040 --> 01:31:51,670
 

4991
01:31:50,050 --> 01:31:54,190
 this is pretty cool and then we could we

4992
01:31:51,660 --> 01:31:54,190
 

4993
01:31:51,670 --> 01:31:56,980
 can look at relationships but with with

4994
01:31:54,180 --> 01:31:56,980
 

4995
01:31:54,190 --> 01:31:59,440
 held-out gene expression data and to

4996
01:31:56,970 --> 01:31:59,440
 

4997
01:31:56,980 --> 01:32:00,910
 kind of validate or have the essentially

4998
01:31:59,430 --> 01:32:00,910
 

4999
01:31:59,440 --> 01:32:03,130
 labels on the results and when we do

5000
01:32:00,900 --> 01:32:03,130
 

5001
01:32:00,910 --> 01:32:05,290
 that we can see that this does massively

5002
01:32:03,120 --> 01:32:05,290
 

5003
01:32:03,130 --> 01:32:07,120
 better than than other types of methods

5004
01:32:05,280 --> 01:32:07,120
 

5005
01:32:05,290 --> 01:32:09,040
 that people might use just doing

5006
01:32:07,110 --> 01:32:09,040
 

5007
01:32:07,120 --> 01:32:11,170
 independent testing and then false

5008
01:32:09,030 --> 01:32:11,170
 

5009
01:32:09,040 --> 01:32:13,090
 discovery rate control so there's kind

5010
01:32:11,160 --> 01:32:13,090
 

5011
01:32:11,170 --> 01:32:14,680
 of Bayesian it's really kind of Bayesian

5012
01:32:13,080 --> 01:32:14,680
 

5013
01:32:13,090 --> 01:32:16,600
 nonparametric multiple testing method

5014
01:32:14,670 --> 01:32:16,600
 

5015
01:32:14,680 --> 01:32:19,660
 kind of really has practical

5016
01:32:16,590 --> 01:32:19,660
 

5017
01:32:16,600 --> 01:32:21,190
 improvements and the usual multiple

5018
01:32:19,650 --> 01:32:21,190
 

5019
01:32:19,660 --> 01:32:23,230
 testing method for frequentist would

5020
01:32:21,180 --> 01:32:23,230
 

5021
01:32:21,190 --> 01:32:25,450
 like separately at each site get a

5022
01:32:23,220 --> 01:32:25,450
 

5023
01:32:23,230 --> 01:32:27,310
 p-value and here but they wouldn't be

5024
01:32:25,440 --> 01:32:27,310
 

5025
01:32:25,450 --> 01:32:29,290
 boring information across the sites and

5026
01:32:27,300 --> 01:32:29,290
 

5027
01:32:27,310 --> 01:32:32,800
 so here by using a Bayesian hierarchal

5028
01:32:29,280 --> 01:32:32,800
 

5029
01:32:29,290 --> 01:32:34,989
 model you're borrowing information okay

5030
01:32:32,790 --> 01:32:34,989
 

5031
01:32:32,800 --> 01:32:37,570
 so i'm gonna give one one last example

5032
01:32:34,979 --> 01:32:37,570
 

5033
01:32:34,989 --> 01:32:38,890
 before we run out of time where we can

5034
01:32:37,560 --> 01:32:38,890
 

5035
01:32:37,570 --> 01:32:43,300
 do this same thing for really

5036
01:32:38,880 --> 01:32:43,300
 

5037
01:32:38,890 --> 01:32:44,470
 complicated outcome data okay so um so

5038
01:32:43,290 --> 01:32:44,470
 

5039
01:32:43,300 --> 01:32:46,450
 what we're doing is we're characterizing

5040
01:32:44,460 --> 01:32:46,450
 

5041
01:32:44,470 --> 01:32:49,450
 nonparametric lis the distribution

5042
01:32:46,440 --> 01:32:49,450
 

5043
01:32:46,450 --> 01:32:50,980
 within each group at each site I'm using

5044
01:32:49,440 --> 01:32:50,980
 

5045
01:32:49,450 --> 01:32:52,690
 this kernel mixture model and then we're

5046
01:32:50,970 --> 01:32:52,690
 

5047
01:32:50,980 --> 01:32:53,440
 letting the weights vary so we're

5048
01:32:52,680 --> 01:32:53,440
 

5049
01:32:52,690 --> 01:32:55,570
 putting all the acts

5050
01:32:53,430 --> 01:32:55,570
 

5051
01:32:53,440 --> 01:32:57,790
 the weights to allow for to testing

5052
01:32:55,560 --> 01:32:57,790
 

5053
01:32:55,570 --> 01:32:59,530
 differences among groups covariance etc

5054
01:32:57,780 --> 01:32:59,530
 

5055
01:32:57,790 --> 01:33:02,290
 so we could do this in a really

5056
01:32:59,520 --> 01:33:02,290
 

5057
01:32:59,530 --> 01:33:05,020
 complicated settings so let's say we

5058
01:33:02,280 --> 01:33:05,020
 

5059
01:33:02,290 --> 01:33:07,360
 have you know brain connectomes for

5060
01:33:05,010 --> 01:33:07,360
 

5061
01:33:05,020 --> 01:33:09,760
 example and so here just shows like this

5062
01:33:07,350 --> 01:33:09,760
 

5063
01:33:07,360 --> 01:33:11,620
 is kind of a picture of you know in any

5064
01:33:09,750 --> 01:33:11,620
 

5065
01:33:09,760 --> 01:33:13,000
 given individual I've taken a brain scan

5066
01:33:11,610 --> 01:33:13,000
 

5067
01:33:11,620 --> 01:33:15,460
 of everyone in the room and then I

5068
01:33:12,990 --> 01:33:15,460
 

5069
01:33:13,000 --> 01:33:17,410
 applied all this processing and now I

5070
01:33:15,450 --> 01:33:17,410
 

5071
01:33:15,460 --> 01:33:19,420
 get you know the location of all these

5072
01:33:17,400 --> 01:33:19,420
 

5073
01:33:17,410 --> 01:33:21,400
 like white matter fiber tract bundles in

5074
01:33:19,410 --> 01:33:21,400
 

5075
01:33:19,420 --> 01:33:24,280
 your brain and I have a million of those

5076
01:33:21,390 --> 01:33:24,280
 

5077
01:33:21,400 --> 01:33:25,810
 and they're all spatially located and

5078
01:33:24,270 --> 01:33:25,810
 

5079
01:33:24,280 --> 01:33:27,520
 that's my like complicated data I would

5080
01:33:25,800 --> 01:33:27,520
 

5081
01:33:25,810 --> 01:33:29,500
 like to analyze and I would say that

5082
01:33:27,510 --> 01:33:29,500
 

5083
01:33:27,520 --> 01:33:31,900
 that's what the act where the actions

5084
01:33:29,490 --> 01:33:31,900
 

5085
01:33:29,500 --> 01:33:33,699
 going to be moving forward is like well

5086
01:33:31,890 --> 01:33:33,699
 

5087
01:33:31,900 --> 01:33:36,820
 how the hell do we like analyze this

5088
01:33:33,689 --> 01:33:36,820
 

5089
01:33:33,699 --> 01:33:40,000
 kind of amazingly rich data collected

5090
01:33:36,810 --> 01:33:40,000
 

5091
01:33:36,820 --> 01:33:41,290
 from this new biotechnology without just

5092
01:33:39,990 --> 01:33:41,290
 

5093
01:33:40,000 --> 01:33:43,180
 using kind of simple off-the-shelf

5094
01:33:41,280 --> 01:33:43,180
 

5095
01:33:41,290 --> 01:33:45,130
 methods but but having methods that are

5096
01:33:43,170 --> 01:33:45,130
 

5097
01:33:43,180 --> 01:33:49,510
 appropriate for really actually

5098
01:33:45,120 --> 01:33:49,510
 

5099
01:33:45,130 --> 01:33:51,940
 complicated data okay so um so let's say

5100
01:33:49,500 --> 01:33:51,940
 

5101
01:33:49,510 --> 01:33:53,890
 we represent the data as X I which is a

5102
01:33:51,930 --> 01:33:53,890
 

5103
01:33:51,940 --> 01:33:57,250
 network or a graph really for each

5104
01:33:53,880 --> 01:33:57,250
 

5105
01:33:53,890 --> 01:33:59,230
 individual so x iu b is one if there's

5106
01:33:57,240 --> 01:33:59,230
 

5107
01:33:57,250 --> 01:34:02,710
 any connection between regions U and V

5108
01:33:59,220 --> 01:34:02,710
 

5109
01:33:59,230 --> 01:34:05,770
 for individual I and x iu v equals zero

5110
01:34:02,700 --> 01:34:05,770
 

5111
01:34:02,710 --> 01:34:07,870
 otherwise okay so we need some sort of

5112
01:34:05,760 --> 01:34:07,870
 

5113
01:34:05,770 --> 01:34:09,580
 model for nonparametric lee

5114
01:34:07,860 --> 01:34:09,580
 

5115
01:34:07,870 --> 01:34:11,560
 characterizing variation in these brain

5116
01:34:09,570 --> 01:34:11,560
 

5117
01:34:09,580 --> 01:34:14,050
 networks across individuals okay so

5118
01:34:11,550 --> 01:34:14,050
 

5119
01:34:11,560 --> 01:34:16,239
 lacks eyes drawn some from p what the

5120
01:34:14,040 --> 01:34:16,239
 

5121
01:34:14,050 --> 01:34:19,120
 hell is P P is some distribution of of

5122
01:34:16,229 --> 01:34:19,120
 

5123
01:34:16,239 --> 01:34:20,590
 random graphs okay and they sure as hell

5124
01:34:19,110 --> 01:34:20,590
 

5125
01:34:19,120 --> 01:34:22,660
 don't follow some sort of stochastic

5126
01:34:20,580 --> 01:34:22,660
 

5127
01:34:20,590 --> 01:34:26,260
 block model or whatever the usual random

5128
01:34:22,650 --> 01:34:26,260
 

5129
01:34:22,660 --> 01:34:28,000
 graphs people use so we can define some

5130
01:34:26,250 --> 01:34:28,000
 

5131
01:34:26,260 --> 01:34:31,180
 type of model let's say for each brain

5132
01:34:27,990 --> 01:34:31,180
 

5133
01:34:28,000 --> 01:34:33,219
 region R and component H we can assign

5134
01:34:31,170 --> 01:34:33,219
 

5135
01:34:31,180 --> 01:34:35,350
 some sort of individual specific score

5136
01:34:33,209 --> 01:34:35,350
 

5137
01:34:33,219 --> 01:34:37,420
 and then we can put down a model a

5138
01:34:35,340 --> 01:34:37,420
 

5139
01:34:35,350 --> 01:34:40,210
 simple hierarchical model latent space

5140
01:34:37,410 --> 01:34:40,210
 

5141
01:34:37,420 --> 01:34:42,610
 model for brain networks and so here is

5142
01:34:40,200 --> 01:34:42,610
 

5143
01:34:40,210 --> 01:34:44,920
 the the logit of the probability that we

5144
01:34:42,600 --> 01:34:44,920
 

5145
01:34:42,610 --> 01:34:48,010
 have a connection between regions U and

5146
01:34:44,910 --> 01:34:48,010
 

5147
01:34:44,920 --> 01:34:50,230
 V in the brain of individual I we can

5148
01:34:48,000 --> 01:34:50,230
 

5149
01:34:48,010 --> 01:34:51,460
 write as an intercept part that's common

5150
01:34:50,220 --> 01:34:51,460
 

5151
01:34:50,230 --> 01:34:53,410
 across the different individuals

5152
01:34:51,450 --> 01:34:53,410
 

5153
01:34:51,460 --> 01:34:55,510
 characterizing commonalities and brain

5154
01:34:53,400 --> 01:34:55,510
 

5155
01:34:53,410 --> 01:34:57,280
 structure across different people I mean

5156
01:34:55,500 --> 01:34:57,280
 

5157
01:34:55,510 --> 01:34:59,320
 then we have this kind of a you know

5158
01:34:57,270 --> 01:34:59,320
 

5159
01:34:57,280 --> 01:35:02,350
 almost singular value decomposition or

5160
01:34:59,310 --> 01:35:02,350
 

5161
01:34:59,320 --> 01:35:04,179
 principal components type piece the data

5162
01:35:02,340 --> 01:35:04,179
 

5163
01:35:02,350 --> 01:35:06,970
 or cement symmetric and so we have like

5164
01:35:04,169 --> 01:35:06,970
 

5165
01:35:04,179 --> 01:35:09,490
 a sum from I equals 1 to K

5166
01:35:06,960 --> 01:35:09,490
 

5167
01:35:06,970 --> 01:35:12,040
 different latent components lambda IH

5168
01:35:09,480 --> 01:35:12,040
 

5169
01:35:09,490 --> 01:35:14,110
 some some weight specific to individual

5170
01:35:12,030 --> 01:35:14,110
 

5171
01:35:12,040 --> 01:35:16,570
 I component H and then we have some sort

5172
01:35:14,100 --> 01:35:16,570
 

5173
01:35:14,110 --> 01:35:18,000
 of ADA ADA part characterizing them

5174
01:35:16,560 --> 01:35:18,000
 

5175
01:35:16,570 --> 01:35:21,010
 specific to the different brain regions

5176
01:35:17,990 --> 01:35:21,010
 

5177
01:35:18,000 --> 01:35:22,870
 okay and then we can we can stack all

5178
01:35:21,000 --> 01:35:22,870
 

5179
01:35:21,010 --> 01:35:25,930
 the individual specific parameters and

5180
01:35:22,860 --> 01:35:25,930
 

5181
01:35:22,870 --> 01:35:27,700
 some theta I vector and and those are

5182
01:35:25,920 --> 01:35:27,700
 

5183
01:35:25,930 --> 01:35:30,100
 sort of random effects characterizing

5184
01:35:27,690 --> 01:35:30,100
 

5185
01:35:27,700 --> 01:35:32,890
 variation across individuals and so this

5186
01:35:30,090 --> 01:35:32,890
 

5187
01:35:30,100 --> 01:35:34,240
 is a sort of an example of Bayesian

5188
01:35:32,880 --> 01:35:34,240
 

5189
01:35:32,890 --> 01:35:36,370
 hierarchical modeling for high

5190
01:35:34,230 --> 01:35:36,370
 

5191
01:35:34,240 --> 01:35:38,410
 dimensional complicated data so we'd

5192
01:35:36,360 --> 01:35:38,410
 

5193
01:35:36,370 --> 01:35:41,500
 write down some model where we're

5194
01:35:38,400 --> 01:35:41,500
 

5195
01:35:38,410 --> 01:35:43,300
 characterizing variation and complicated

5196
01:35:41,490 --> 01:35:43,300
 

5197
01:35:41,500 --> 01:35:45,430
 data through some latent variables and

5198
01:35:43,290 --> 01:35:45,430
 

5199
01:35:43,300 --> 01:35:48,370
 so this is quite quite a common kind of

5200
01:35:45,420 --> 01:35:48,370
 

5201
01:35:45,430 --> 01:35:50,530
 canonical game in in Bayesian modeling

5202
01:35:48,360 --> 01:35:50,530
 

5203
01:35:48,370 --> 01:35:51,850
 of high dimensional data and then we

5204
01:35:50,520 --> 01:35:51,850
 

5205
01:35:50,530 --> 01:35:54,550
 don't know what the distribution of

5206
01:35:51,840 --> 01:35:54,550
 

5207
01:35:51,850 --> 01:35:55,720
 those random effects is and so here

5208
01:35:54,540 --> 01:35:55,720
 

5209
01:35:54,550 --> 01:35:58,000
 we're going to use Bayesian non /

5210
01:35:55,710 --> 01:35:58,000
 

5211
01:35:55,720 --> 01:35:59,380
 metrics to allow to allow Q the

5212
01:35:57,990 --> 01:35:59,380
 

5213
01:35:58,000 --> 01:36:01,660
 distribution of the random effects and

5214
01:35:59,370 --> 01:36:01,660
 

5215
01:35:59,380 --> 01:36:03,610
 hence P the distribution the brain

5216
01:36:01,650 --> 01:36:03,610
 

5217
01:36:01,660 --> 01:36:08,740
 networks across people to be unknown

5218
01:36:03,600 --> 01:36:08,740
 

5219
01:36:03,610 --> 01:36:10,150
 okay okay so um so based on this

5220
01:36:08,730 --> 01:36:10,150
 

5221
01:36:08,740 --> 01:36:11,440
 framework you're actually clustering

5222
01:36:10,140 --> 01:36:11,440
 

5223
01:36:10,150 --> 01:36:13,480
 individuals in terms of their brain

5224
01:36:11,430 --> 01:36:13,480
 

5225
01:36:11,440 --> 01:36:15,310
 structure which is pretty cool and we

5226
01:36:13,470 --> 01:36:15,310
 

5227
01:36:13,480 --> 01:36:21,280
 can test for relationships between brain

5228
01:36:15,300 --> 01:36:21,280
 

5229
01:36:15,310 --> 01:36:22,750
 structure and traits and genotype okay

5230
01:36:21,270 --> 01:36:22,750
 

5231
01:36:21,280 --> 01:36:24,520
 so we just allow the weights in our

5232
01:36:22,740 --> 01:36:24,520
 

5233
01:36:22,750 --> 01:36:27,070
 mixture model to vary with with with

5234
01:36:24,510 --> 01:36:27,070
 

5235
01:36:24,520 --> 01:36:28,510
 traits with these fixed kernels so the

5236
01:36:27,060 --> 01:36:28,510
 

5237
01:36:27,070 --> 01:36:31,120
 kernels are characterizing the kind of

5238
01:36:28,500 --> 01:36:31,120
 

5239
01:36:28,510 --> 01:36:34,750
 God provided dictionary with which we

5240
01:36:31,110 --> 01:36:34,750
 

5241
01:36:31,120 --> 01:36:36,760
 can characterize brain graphs okay and

5242
01:36:34,740 --> 01:36:36,760
 

5243
01:36:34,750 --> 01:36:38,620
 so this allows scientific inferences on

5244
01:36:36,750 --> 01:36:38,620
 

5245
01:36:36,760 --> 01:36:41,380
 global and local differences in network

5246
01:36:38,610 --> 01:36:41,380
 

5247
01:36:38,620 --> 01:36:43,440
 structure with traits and I really think

5248
01:36:41,370 --> 01:36:43,440
 

5249
01:36:41,380 --> 01:36:46,240
 we need more of these types of kind of

5250
01:36:43,430 --> 01:36:46,240
 

5251
01:36:43,440 --> 01:36:47,980
 scalable methods with uncertainty

5252
01:36:46,230 --> 01:36:47,980
 

5253
01:36:46,240 --> 01:36:49,720
 quantification for complex data and so I

5254
01:36:47,970 --> 01:36:49,720
 

5255
01:36:47,980 --> 01:36:51,100
 really encourage more people to work on

5256
01:36:49,710 --> 01:36:51,100
 

5257
01:36:49,720 --> 01:36:52,660
 the on this type of thing not

5258
01:36:51,090 --> 01:36:52,660
 

5259
01:36:51,100 --> 01:36:55,570
 necessarily for brain networks but for

5260
01:36:52,650 --> 01:36:55,570
 

5261
01:36:52,660 --> 01:36:57,700
 any kind of complicated data that just

5262
01:36:55,560 --> 01:36:57,700
 

5263
01:36:55,570 --> 01:37:00,100
 for a multiple testing as well okay so

5264
01:36:57,690 --> 01:37:00,100
 

5265
01:36:57,700 --> 01:37:01,780
 here here are some results and so we we

5266
01:37:00,090 --> 01:37:01,780
 

5267
01:37:00,100 --> 01:37:04,810
 applied the model to brain networks of

5268
01:37:01,770 --> 01:37:04,810
 

5269
01:37:01,780 --> 01:37:07,990
 36 subjects 19 with high creativity 17

5270
01:37:04,800 --> 01:37:07,990
 

5271
01:37:04,810 --> 01:37:11,350
 with low creativity measured with a the

5272
01:37:07,980 --> 01:37:11,350
 

5273
01:37:07,990 --> 01:37:13,450
 composite creativity index the it gives

5274
01:37:11,340 --> 01:37:13,450
 

5275
01:37:11,350 --> 01:37:15,250
 us an overall probability a posterior

5276
01:37:13,440 --> 01:37:15,250
 

5277
01:37:13,450 --> 01:37:18,160
 probability that there's any differences

5278
01:37:15,240 --> 01:37:18,160
 

5279
01:37:15,250 --> 01:37:20,830
 in brain structure with with creativity

5280
01:37:18,150 --> 01:37:20,830
 

5281
01:37:18,160 --> 01:37:23,950
 and that was 0.995 which is quite

5282
01:37:20,820 --> 01:37:23,950
 

5283
01:37:20,830 --> 01:37:27,340
 evidence and it also we also like you

5284
01:37:23,940 --> 01:37:27,340
 

5285
01:37:23,950 --> 01:37:31,030
 can see here what we did here is we we

5286
01:37:27,330 --> 01:37:31,030
 

5287
01:37:27,340 --> 01:37:33,300
 can flag statistically significant after

5288
01:37:31,020 --> 01:37:33,300
 

5289
01:37:31,030 --> 01:37:36,250
 adjusting from multiple testing

5290
01:37:33,290 --> 01:37:36,250
 

5291
01:37:33,300 --> 01:37:39,190
 differences in brain structure and so a

5292
01:37:36,240 --> 01:37:39,190
 

5293
01:37:36,250 --> 01:37:43,540
 red a red line would mean that people

5294
01:37:39,180 --> 01:37:43,540
 

5295
01:37:39,190 --> 01:37:46,420
 with with low creativity have more

5296
01:37:43,530 --> 01:37:46,420
 

5297
01:37:43,540 --> 01:37:49,180
 connections in that location and so a

5298
01:37:46,410 --> 01:37:49,180
 

5299
01:37:46,420 --> 01:37:50,980
 green line means less connections so so

5300
01:37:49,170 --> 01:37:50,980
 

5301
01:37:49,180 --> 01:37:53,140
 that what this green line here means is

5302
01:37:50,970 --> 01:37:53,140
 

5303
01:37:50,980 --> 01:37:56,950
 that that the individuals with low

5304
01:37:53,130 --> 01:37:56,950
 

5305
01:37:53,140 --> 01:37:59,080
 creativity have significantly less

5306
01:37:56,940 --> 01:37:59,080
 

5307
01:37:56,950 --> 01:38:01,150
 connections between these regions okay

5308
01:37:59,070 --> 01:38:01,150
 

5309
01:37:59,080 --> 01:38:03,130
 and so we can see a lot of green lines

5310
01:38:01,140 --> 01:38:03,130
 

5311
01:38:01,150 --> 01:38:05,110
 that are cross hemisphere connections in

5312
01:38:03,120 --> 01:38:05,110
 

5313
01:38:03,130 --> 01:38:07,930
 the frontal lobe of the brain and so

5314
01:38:05,100 --> 01:38:07,930
 

5315
01:38:05,110 --> 01:38:09,870
 what that means essentially is that that

5316
01:38:07,920 --> 01:38:09,870
 

5317
01:38:07,930 --> 01:38:12,700
 people who are highly create creative

5318
01:38:09,860 --> 01:38:12,700
 

5319
01:38:09,870 --> 01:38:14,680
 have more across hemisphere connections

5320
01:38:12,690 --> 01:38:14,680
 

5321
01:38:12,700 --> 01:38:18,610
 more more connections that occur in sort

5322
01:38:14,670 --> 01:38:18,610
 

5323
01:38:14,680 --> 01:38:21,430
 of unanticipated regions okay it was an

5324
01:38:18,600 --> 01:38:21,430
 

5325
01:38:18,610 --> 01:38:23,260
 interesting side comment is we had a we

5326
01:38:21,420 --> 01:38:23,260
 

5327
01:38:21,430 --> 01:38:25,540
 in a press release on this result and

5328
01:38:23,250 --> 01:38:25,540
 

5329
01:38:23,260 --> 01:38:27,880
 there you know the this left-brain

5330
01:38:25,530 --> 01:38:27,880
 

5331
01:38:25,540 --> 01:38:29,440
 right-brain hypothesis this whole thing

5332
01:38:27,870 --> 01:38:29,440
 

5333
01:38:27,880 --> 01:38:31,450
 we're like left brain people are like

5334
01:38:29,430 --> 01:38:31,450
 

5335
01:38:29,440 --> 01:38:33,910
 more mathematical and right brain people

5336
01:38:31,440 --> 01:38:33,910
 

5337
01:38:31,450 --> 01:38:35,800
 are more creative or whatever the the

5338
01:38:33,900 --> 01:38:35,800
 

5339
01:38:33,910 --> 01:38:38,740
 guy who came up with this he's deceased

5340
01:38:35,790 --> 01:38:38,740
 

5341
01:38:35,800 --> 01:38:40,990
 now but his son contacted us and said

5342
01:38:38,730 --> 01:38:40,990
 

5343
01:38:38,740 --> 01:38:42,070
 well actually his dad like ended up not

5344
01:38:40,980 --> 01:38:42,070
 

5345
01:38:40,990 --> 01:38:43,990
 believing the right brain left brain

5346
01:38:42,060 --> 01:38:43,990
 

5347
01:38:42,070 --> 01:38:45,400
 thing at all and what he ended up

5348
01:38:43,980 --> 01:38:45,400
 

5349
01:38:43,990 --> 01:38:47,050
 believing was much more consistent with

5350
01:38:45,390 --> 01:38:47,050
 

5351
01:38:45,400 --> 01:38:48,250
 our thing which was there's not left

5352
01:38:47,040 --> 01:38:48,250
 

5353
01:38:47,050 --> 01:38:50,140
 brain right brain it's more

5354
01:38:48,240 --> 01:38:50,140
 

5355
01:38:48,250 --> 01:38:52,330
 interconnected brains that are more

5356
01:38:50,130 --> 01:38:52,330
 

5357
01:38:50,140 --> 01:38:56,800
 creative the left brain right brain

5358
01:38:52,320 --> 01:38:56,800
 

5359
01:38:52,330 --> 01:38:59,650
 thing is just bogus okay so um so the

5360
01:38:56,790 --> 01:38:59,650
 

5361
01:38:56,800 --> 01:39:02,280
 the idea of just wrapping up is that in

5362
01:38:59,640 --> 01:39:02,280
 

5363
01:38:59,650 --> 01:39:05,470
 this kind of modularization strategy

5364
01:39:02,270 --> 01:39:05,470
 

5365
01:39:02,280 --> 01:39:08,200
 which is rated through that that that

5366
01:39:05,460 --> 01:39:08,200
 

5367
01:39:05,470 --> 01:39:10,060
 that DNA methylation example is we don't

5368
01:39:08,190 --> 01:39:10,060
 

5369
01:39:08,200 --> 01:39:13,450
 allow for all the dependencies implied

5370
01:39:10,050 --> 01:39:13,450
 

5371
01:39:10,060 --> 01:39:14,710
 by the joint Bayesian model i think i

5372
01:39:13,440 --> 01:39:14,710
 

5373
01:39:13,450 --> 01:39:17,140
 kind of said that already

5374
01:39:14,700 --> 01:39:17,140
 

5375
01:39:14,710 --> 01:39:18,580
 i think we have like five minutes i'd

5376
01:39:17,130 --> 01:39:18,580
 

5377
01:39:17,140 --> 01:39:21,850
 like to i take some questions and so i'm

5378
01:39:18,570 --> 01:39:21,850
 

5379
01:39:18,580 --> 01:39:24,670
 just gonna skip this so that i just the

5380
01:39:21,840 --> 01:39:24,670
 

5381
01:39:21,850 --> 01:39:27,310
 bottom line here is that we can scale up

5382
01:39:24,660 --> 01:39:27,310
 

5383
01:39:24,670 --> 01:39:30,750
 bayesian inferences for genomic problems

5384
01:39:27,300 --> 01:39:30,750
 

5385
01:39:27,310 --> 01:39:33,700
 with with tens of millions of snips

5386
01:39:30,740 --> 01:39:33,700
 

5387
01:39:30,750 --> 01:39:34,659
 using these types same types of methods

5388
01:39:33,690 --> 01:39:34,659
 

5389
01:39:33,700 --> 01:39:36,940
 that i've been talking about

5390
01:39:34,649 --> 01:39:36,940
 

5391
01:39:34,659 --> 01:39:38,500
 and so it's not true at all that

5392
01:39:36,930 --> 01:39:38,500
 

5393
01:39:36,940 --> 01:39:40,690
 Bayesian inference isn't scalable we can

5394
01:39:38,490 --> 01:39:40,690
 

5395
01:39:38,500 --> 01:39:43,840
 deal with hundreds of millions of snaps

5396
01:39:40,680 --> 01:39:43,840
 

5397
01:39:40,690 --> 01:39:46,170
 even or features in regression problems

5398
01:39:43,830 --> 01:39:46,170
 

5399
01:39:43,840 --> 01:39:50,440
 and we can use these types of methods

5400
01:39:46,160 --> 01:39:50,440
 

5401
01:39:46,170 --> 01:39:52,000
 okay so discussion so so I kind of gave

5402
01:39:50,430 --> 01:39:52,000
 

5403
01:39:50,440 --> 01:39:55,000
 a brief intro to Bayesian methods for

5404
01:39:51,990 --> 01:39:55,000
 

5405
01:39:52,000 --> 01:39:56,530
 large P problems at the end of this talk

5406
01:39:54,990 --> 01:39:56,530
 

5407
01:39:55,000 --> 01:39:58,480
 highlighted some recent work using

5408
01:39:56,520 --> 01:39:58,480
 

5409
01:39:56,530 --> 01:40:00,130
 shared kernels and modularization I

5410
01:39:58,470 --> 01:40:00,130
 

5411
01:39:58,480 --> 01:40:01,929
 would say there's a really a rich

5412
01:40:00,120 --> 01:40:01,929
 

5413
01:40:00,130 --> 01:40:04,570
 literature that's just kind of beginning

5414
01:40:01,919 --> 01:40:04,570
 

5415
01:40:01,929 --> 01:40:06,130
 an increasing focus on scalability and I

5416
01:40:04,560 --> 01:40:06,130
 

5417
01:40:04,570 --> 01:40:09,580
 hope I hope to inspire some people to

5418
01:40:06,120 --> 01:40:09,580
 

5419
01:40:06,130 --> 01:40:11,139
 work more more in these areas and one

5420
01:40:09,570 --> 01:40:11,139
 

5421
01:40:09,580 --> 01:40:12,310
 important direction is obtain methods

5422
01:40:11,129 --> 01:40:12,310
 

5423
01:40:11,139 --> 01:40:14,110
 for assessing when we're attempting

5424
01:40:12,300 --> 01:40:14,110
 

5425
01:40:12,310 --> 01:40:16,870
 inferences on to find a scale for our

5426
01:40:14,100 --> 01:40:16,870
 

5427
01:40:14,110 --> 01:40:18,760
 data I think that's really important to

5428
01:40:16,860 --> 01:40:18,760
 

5429
01:40:16,870 --> 01:40:21,070
 come up with more negative results I

5430
01:40:18,750 --> 01:40:21,070
 

5431
01:40:18,760 --> 01:40:22,330
 mean like and these problems work with

5432
01:40:21,060 --> 01:40:22,330
 

5433
01:40:21,070 --> 01:40:24,040
 scientists all the time I want to be

5434
01:40:22,320 --> 01:40:24,040
 

5435
01:40:22,330 --> 01:40:26,560
 able to tell them like actually the

5436
01:40:24,030 --> 01:40:26,560
 

5437
01:40:24,040 --> 01:40:28,270
 questions you're trying to ask are not

5438
01:40:26,550 --> 01:40:28,270
 

5439
01:40:26,560 --> 01:40:30,489
 possible given the data you've provided

5440
01:40:28,260 --> 01:40:30,489
 

5441
01:40:28,270 --> 01:40:32,260
 me and the prior information that we

5442
01:40:30,479 --> 01:40:32,260
 

5443
01:40:30,489 --> 01:40:34,659
 have and so if I'm going to answer your

5444
01:40:32,250 --> 01:40:34,659
 

5445
01:40:32,260 --> 01:40:36,940
 questions then I have to put more prior

5446
01:40:34,649 --> 01:40:36,940
 

5447
01:40:34,659 --> 01:40:39,429
 information in the analysis than I

5448
01:40:36,930 --> 01:40:39,429
 

5449
01:40:36,940 --> 01:40:43,150
 actually have which is probably a bad

5450
01:40:39,419 --> 01:40:43,150
 

5451
01:40:39,429 --> 01:40:45,880
 idea and so therefore maybe maybe I can

5452
01:40:43,140 --> 01:40:45,880
 

5453
01:40:43,150 --> 01:40:48,820
 come up with a method that will say no

5454
01:40:45,870 --> 01:40:48,820
 

5455
01:40:45,880 --> 01:40:51,760
 you can't answer those questions maybe

5456
01:40:48,810 --> 01:40:51,760
 

5457
01:40:48,820 --> 01:40:53,320
 it can then even go beyond that almost

5458
01:40:51,750 --> 01:40:53,320
 

5459
01:40:51,760 --> 01:40:55,630
 like in an artificial intelligence way

5460
01:40:53,310 --> 01:40:55,630
 

5461
01:40:53,320 --> 01:40:56,889
 and say well here are these questions

5462
01:40:55,620 --> 01:40:56,889
 

5463
01:40:55,630 --> 01:40:58,719
 you can answer you can have like an

5464
01:40:56,879 --> 01:40:58,719
 

5465
01:40:56,889 --> 01:41:00,790
 assistant like a Bayesian assistant that

5466
01:40:58,709 --> 01:41:00,790
 

5467
01:40:58,719 --> 01:41:03,429
 can say well the questions you're trying

5468
01:41:00,780 --> 01:41:03,429
 

5469
01:41:00,790 --> 01:41:04,929
 to ask are impossible why don't you why

5470
01:41:03,419 --> 01:41:04,929
 

5471
01:41:03,429 --> 01:41:06,070
 do you ask these types of questions ease

5472
01:41:04,919 --> 01:41:06,070
 

5473
01:41:04,929 --> 01:41:09,040
 are the types of questions you can ask

5474
01:41:06,060 --> 01:41:09,040
 

5475
01:41:06,070 --> 01:41:10,690
 you can't identify individual snips that

5476
01:41:09,030 --> 01:41:10,690
 

5477
01:41:09,040 --> 01:41:12,969
 are related to the phenotype you can

5478
01:41:10,680 --> 01:41:12,969
 

5479
01:41:10,690 --> 01:41:14,560
 only identify regions of the genome for

5480
01:41:12,959 --> 01:41:14,560
 

5481
01:41:12,969 --> 01:41:16,179
 example so that would be incredibly

5482
01:41:14,550 --> 01:41:16,179
 

5483
01:41:14,560 --> 01:41:19,630
 useful and there's really nothing like

5484
01:41:16,169 --> 01:41:19,630
 

5485
01:41:16,179 --> 01:41:21,340
 that out there now so one way is the

5486
01:41:19,620 --> 01:41:21,340
 

5487
01:41:19,630 --> 01:41:24,280
 course on the scale of the data here's

5488
01:41:21,330 --> 01:41:24,280
 

5489
01:41:21,340 --> 01:41:26,889
 some some references on large and I'm

5490
01:41:24,270 --> 01:41:26,889
 

5491
01:41:24,280 --> 01:41:29,080
 gonna I'm happy to provide the slides to

5492
01:41:26,879 --> 01:41:29,080
 

5493
01:41:26,889 --> 01:41:30,880
 anyone I can post them on my website and

5494
01:41:29,070 --> 01:41:30,880
 

5495
01:41:29,080 --> 01:41:34,000
 so and then please email me if you have

5496
01:41:30,870 --> 01:41:34,000
 

5497
01:41:30,880 --> 01:41:36,880
 any comments etc the take-home message

5498
01:41:33,990 --> 01:41:36,880
 

5499
01:41:34,000 --> 01:41:39,010
 is just that Bayes is scalable and MCMC

5500
01:41:36,870 --> 01:41:39,010
 

5501
01:41:36,880 --> 01:41:40,650
 is scalable but in big and high

5502
01:41:39,000 --> 01:41:40,650
 

5503
01:41:39,010 --> 01:41:43,270
 dimensional problems you can't just use

5504
01:41:40,640 --> 01:41:43,270
 

5505
01:41:40,650 --> 01:41:45,400
 off-the-shelf algorithms you have to be

5506
01:41:43,260 --> 01:41:45,400
 

5507
01:41:43,270 --> 01:41:47,199
 smart about it and we need to think

5508
01:41:45,390 --> 01:41:47,199
 

5509
01:41:45,400 --> 01:41:48,310
 carefully about how to exploit parallel

5510
01:41:47,189 --> 01:41:48,310
 

5511
01:41:47,199 --> 01:41:51,130
 processing accurate

5512
01:41:48,300 --> 01:41:51,130
 

5513
01:41:48,310 --> 01:41:52,810
 approximations to reduce bottlenecks we

5514
01:41:51,120 --> 01:41:52,810
 

5515
01:41:51,130 --> 01:41:55,000
 might also need to take a step away from

5516
01:41:52,800 --> 01:41:55,000
 

5517
01:41:52,810 --> 01:41:57,160
 fully base frame works by using mod

5518
01:41:54,990 --> 01:41:57,160
 

5519
01:41:55,000 --> 01:42:01,090
 realization composite likelihood C Bay's

5520
01:41:57,150 --> 01:42:01,090
 

5521
01:41:57,160 --> 01:42:02,920
 etc and we'd like to be able to design

5522
01:42:01,080 --> 01:42:02,920
 

5523
01:42:01,090 --> 01:42:05,680
 algorithms and and an inference

5524
01:42:02,910 --> 01:42:05,680
 

5525
01:42:02,920 --> 01:42:07,240
 frameworks that are improved in a

5526
01:42:05,670 --> 01:42:07,240
 

5527
01:42:05,680 --> 01:42:11,070
 computational way and in terms of

5528
01:42:07,230 --> 01:42:11,070
 

5529
01:42:07,240 --> 01:42:11,070
 robustness no it's not there

5530
01:42:11,240 --> 01:42:11,240
 

5531
01:42:11,250 --> 01:42:14,140
[Applause]

5532
01:42:13,210 --> 01:42:14,140
 

5533
01:42:13,220 --> 01:42:15,090
[Music]

5534
01:42:14,130 --> 01:42:15,090
 

5535
01:42:14,140 --> 01:42:18,530
[Applause]

5536
01:42:15,080 --> 01:42:18,530
 

5537
01:42:15,090 --> 01:42:21,120
[Music]

5538
01:42:18,520 --> 01:42:21,120
 

5539
01:42:18,530 --> 01:42:23,310
 thanks a lot David very very inspiring

5540
01:42:21,110 --> 01:42:23,310
 

5541
01:42:21,120 --> 01:42:25,050
 talk so we have time for a few questions

5542
01:42:23,300 --> 01:42:25,050
 

5543
01:42:23,310 --> 01:42:27,720
 if there are any there are microphones

5544
01:42:25,040 --> 01:42:27,720
 

5545
01:42:25,050 --> 01:42:29,960
 here in the front so please PLEASE use

5546
01:42:27,710 --> 01:42:29,960
 

5547
01:42:27,720 --> 01:42:29,960
 them

5548
01:42:32,260 --> 01:42:32,260
 

5549
01:42:32,270 --> 01:42:39,000
 okay there are no questions I'm sorry

5550
01:42:36,110 --> 01:42:39,000
 

5551
01:42:36,120 --> 01:42:41,580
 beautiful hi thank you very much it was

5552
01:42:38,990 --> 01:42:41,580
 

5553
01:42:39,000 --> 01:42:44,730
 very illuminating talk my question is

5554
01:42:41,570 --> 01:42:44,730
 

5555
01:42:41,580 --> 01:42:48,900
 about collaboration so any Bayesian

5556
01:42:44,720 --> 01:42:48,900
 

5557
01:42:44,730 --> 01:42:50,790
 framework starts with likelihood

5558
01:42:48,890 --> 01:42:50,790
 

5559
01:42:48,900 --> 01:42:52,680
 function which is a condition all of you

5560
01:42:50,780 --> 01:42:52,680
 

5561
01:42:50,790 --> 01:42:54,960
 observed data on whatever latent

5562
01:42:52,670 --> 01:42:54,960
 

5563
01:42:52,680 --> 01:42:57,390
 parameters theta you choose then you

5564
01:42:54,950 --> 01:42:57,390
 

5565
01:42:54,960 --> 01:42:59,730
 start with the priors and then through

5566
01:42:57,380 --> 01:42:59,730
 

5567
01:42:57,390 --> 01:43:01,980
 multi-car law whatever you essentially

5568
01:42:59,720 --> 01:43:01,980
 

5569
01:42:59,730 --> 01:43:05,130
 can describe mysterious and you can also

5570
01:43:01,970 --> 01:43:05,130
 

5571
01:43:01,980 --> 01:43:07,590
 just describe the marginals so and quite

5572
01:43:05,120 --> 01:43:07,590
 

5573
01:43:05,130 --> 01:43:11,520
 often it happens that yeah and of course

5574
01:43:07,580 --> 01:43:11,520
 

5575
01:43:07,590 --> 01:43:14,130
 this likelihood function is a parametric

5576
01:43:11,510 --> 01:43:14,130
 

5577
01:43:11,520 --> 01:43:15,900
 model and you know the real problems are

5578
01:43:14,120 --> 01:43:15,900
 

5579
01:43:14,130 --> 01:43:18,300
 very complex it's very hard to actual

5580
01:43:15,890 --> 01:43:18,300
 

5581
01:43:15,900 --> 01:43:20,010
 analytically write that likelihood

5582
01:43:18,290 --> 01:43:20,010
 

5583
01:43:18,300 --> 01:43:22,380
 function so in the end of the day when

5584
01:43:20,000 --> 01:43:22,380
 

5585
01:43:20,010 --> 01:43:25,020
 everything is done and one computes the

5586
01:43:22,370 --> 01:43:25,020
 

5587
01:43:22,380 --> 01:43:27,090
 marginal of the data and computes the

5588
01:43:25,010 --> 01:43:27,090
 

5589
01:43:25,020 --> 01:43:28,620
 quantiles for example you had an example

5590
01:43:27,080 --> 01:43:28,620
 

5591
01:43:27,090 --> 01:43:30,810
 of single dimensional distribution at

5592
01:43:28,610 --> 01:43:30,810
 

5593
01:43:28,620 --> 01:43:32,940
 the end marginalized everything and for

5594
01:43:30,800 --> 01:43:32,940
 

5595
01:43:30,810 --> 01:43:34,470
 single quantile function then you gets

5596
01:43:32,930 --> 01:43:34,470
 

5597
01:43:32,940 --> 01:43:36,090
 the quantiles which are different from

5598
01:43:34,460 --> 01:43:36,090
 

5599
01:43:34,470 --> 01:43:37,380
 what you observe in the data so

5600
01:43:36,080 --> 01:43:37,380
 

5601
01:43:36,090 --> 01:43:39,930
 essentially the model is miscalibrated

5602
01:43:37,370 --> 01:43:39,930
 

5603
01:43:37,380 --> 01:43:42,420
 and because model had like hundreds of

5604
01:43:39,920 --> 01:43:42,420
 

5605
01:43:39,930 --> 01:43:46,460
 the parameters so what exactly should be

5606
01:43:42,410 --> 01:43:46,460
 

5607
01:43:42,420 --> 01:43:48,830
 the approach to recalibrate model back

5608
01:43:46,450 --> 01:43:48,830
 

5609
01:43:46,460 --> 01:43:51,570
 yeah that's a really great question the

5610
01:43:48,820 --> 01:43:51,570
 

5611
01:43:48,830 --> 01:43:54,330
 first note that it's really important to

5612
01:43:51,560 --> 01:43:54,330
 

5613
01:43:51,570 --> 01:43:56,970
 look at calibration and so I'm often

5614
01:43:54,320 --> 01:43:56,970
 

5615
01:43:54,330 --> 01:43:59,160
 using you know out-of-sample assessments

5616
01:43:56,960 --> 01:43:59,160
 

5617
01:43:56,970 --> 01:44:01,800
 and then calibration in a sense of not

5618
01:43:59,150 --> 01:44:01,800
 

5619
01:43:59,160 --> 01:44:03,600
 just point point estimates but that the

5620
01:44:01,790 --> 01:44:03,600
 

5621
01:44:01,800 --> 01:44:06,030
 probability distribution is

5622
01:44:03,590 --> 01:44:06,030
 

5623
01:44:03,600 --> 01:44:08,310
 characterizing uncertainty well in terms

5624
01:44:06,020 --> 01:44:08,310
 

5625
01:44:06,030 --> 01:44:10,650
 of predictive distributions when the

5626
01:44:08,300 --> 01:44:10,650
 

5627
01:44:08,310 --> 01:44:13,440
 model is not well calibrated I often

5628
01:44:10,640 --> 01:44:13,440
 

5629
01:44:10,650 --> 01:44:15,150
 think to try to put in more more more

5630
01:44:13,430 --> 01:44:15,150
 

5631
01:44:13,440 --> 01:44:17,850
 flexibilities somehow I don't know that

5632
01:44:15,140 --> 01:44:17,850
 

5633
01:44:15,150 --> 01:44:20,490
 there's a great answer for you know okay

5634
01:44:17,840 --> 01:44:20,490
 

5635
01:44:17,850 --> 01:44:22,920
 well where did I go wrong exactly it's

5636
01:44:20,480 --> 01:44:22,920
 

5637
01:44:20,490 --> 01:44:25,920
 in a complicated setting it's not well

5638
01:44:22,910 --> 01:44:25,920
 

5639
01:44:22,920 --> 01:44:27,870
 calibrated but the kind of rule of thumb

5640
01:44:25,910 --> 01:44:27,870
 

5641
01:44:25,920 --> 01:44:29,940
 in some sense is - well maybe I need

5642
01:44:27,860 --> 01:44:29,940
 

5643
01:44:27,870 --> 01:44:31,519
 some more I need some more for the

5644
01:44:29,930 --> 01:44:31,519
 

5645
01:44:29,940 --> 01:44:33,260
 flexibility and so then I can

5646
01:44:31,509 --> 01:44:33,260
 

5647
01:44:31,519 --> 01:44:35,269
 back and try to do some Diagnostics to

5648
01:44:33,250 --> 01:44:35,269
 

5649
01:44:33,260 --> 01:44:37,279
 try to figure out where my model is not

5650
01:44:35,259 --> 01:44:37,279
 

5651
01:44:35,269 --> 01:44:44,749
 fitting very well and kind of iterate

5652
01:44:37,269 --> 01:44:44,749
 

5653
01:44:37,279 --> 01:44:47,299
 from there thank you hi two short

5654
01:44:44,739 --> 01:44:47,299
 

5655
01:44:44,749 --> 01:44:49,789
 questions one question concerning the

5656
01:44:47,289 --> 01:44:49,789
 

5657
01:44:47,299 --> 01:44:53,689
 example that you make on the Bernoulli

5658
01:44:49,779 --> 01:44:53,689
 

5659
01:44:49,789 --> 01:44:57,109
 case conflicting in that case the test

5660
01:44:53,679 --> 01:44:57,109
 

5661
01:44:53,689 --> 01:44:59,539
 is a certain point telling you that your

5662
01:44:57,099 --> 01:44:59,539
 

5663
01:44:57,109 --> 01:45:02,239
 hypothesis is false so isn't that just

5664
01:44:59,529 --> 01:45:02,239
 

5665
01:44:59,539 --> 01:45:04,729
 the right answer I mean at some point we

5666
01:45:02,229 --> 01:45:04,729
 

5667
01:45:02,239 --> 01:45:06,829
 have enough power to review to refute

5668
01:45:04,719 --> 01:45:06,829
 

5669
01:45:04,729 --> 01:45:09,499
 the apologies the power the hypothesis

5670
01:45:06,819 --> 01:45:09,499
 

5671
01:45:06,829 --> 01:45:11,510
 is false and so that's a great question

5672
01:45:09,489 --> 01:45:11,510
 

5673
01:45:09,499 --> 01:45:14,029
 I think that that's one of the

5674
01:45:11,500 --> 01:45:14,029
 

5675
01:45:11,510 --> 01:45:15,439
 philosophical issues a lot of people

5676
01:45:14,019 --> 01:45:15,439
 

5677
01:45:14,029 --> 01:45:18,379
 don't believe in testing because they

5678
01:45:15,429 --> 01:45:18,379
 

5679
01:45:15,439 --> 01:45:20,089
 never believe in precisely that exact

5680
01:45:18,369 --> 01:45:20,089
 

5681
01:45:18,379 --> 01:45:21,739
 specification of hypothesis and it's

5682
01:45:20,079 --> 01:45:21,739
 

5683
01:45:20,089 --> 01:45:23,989
 related to this model of specification

5684
01:45:21,729 --> 01:45:23,989
 

5685
01:45:21,739 --> 01:45:26,479
 it was also doing the right thing in

5686
01:45:23,979 --> 01:45:26,479
 

5687
01:45:23,989 --> 01:45:28,489
 terms of the mixture model in adding

5688
01:45:26,469 --> 01:45:28,489
 

5689
01:45:26,479 --> 01:45:30,319
 more components because it wasn't

5690
01:45:28,479 --> 01:45:30,319
 

5691
01:45:28,489 --> 01:45:32,089
 precisely two components that were true

5692
01:45:30,309 --> 01:45:32,089
 

5693
01:45:30,319 --> 01:45:35,989
 when I would say that you know

5694
01:45:32,079 --> 01:45:35,989
 

5695
01:45:32,089 --> 01:45:37,909
 practically allowing a tiny bit a small

5696
01:45:35,979 --> 01:45:37,909
 

5697
01:45:35,989 --> 01:45:40,159
 amount of miss specification of

5698
01:45:37,899 --> 01:45:40,159
 

5699
01:45:37,909 --> 01:45:42,019
 hypothesis in the model is going to give

5700
01:45:40,149 --> 01:45:42,019
 

5701
01:45:40,159 --> 01:45:44,149
 you much better performance because in

5702
01:45:42,009 --> 01:45:44,149
 

5703
01:45:42,019 --> 01:45:47,119
 reality you know you're never gonna have

5704
01:45:44,139 --> 01:45:47,119
 

5705
01:45:44,149 --> 01:45:49,069
 exactly the data you observed or from

5706
01:45:47,109 --> 01:45:49,069
 

5707
01:45:47,119 --> 01:45:50,539
 exactly a particular hypothesis or

5708
01:45:49,059 --> 01:45:50,539
 

5709
01:45:49,069 --> 01:45:53,089
 exactly a particular model and so you'd

5710
01:45:50,529 --> 01:45:53,089
 

5711
01:45:50,539 --> 01:45:54,919
 like to allow some sort of fudge factor

5712
01:45:53,079 --> 01:45:54,919
 

5713
01:45:53,089 --> 01:45:57,769
 there to allow them to be close but not

5714
01:45:54,909 --> 01:45:57,769
 

5715
01:45:54,919 --> 01:46:02,089
 exactly okay the other question we are

5716
01:45:57,759 --> 01:46:02,089
 

5717
01:45:57,769 --> 01:46:05,419
 doing we are applying the sorry merry

5718
01:46:02,079 --> 01:46:05,419
 

5719
01:46:02,089 --> 01:46:08,299
 way and repin it they beckon you convoy

5720
01:46:05,409 --> 01:46:08,299
 

5721
01:46:05,419 --> 01:46:11,539
 we are applying the Kennedy and Hagin

5722
01:46:08,289 --> 01:46:11,539
 

5723
01:46:08,299 --> 01:46:14,359
 framework to a pair so it's Bayesian

5724
01:46:11,529 --> 01:46:14,359
 

5725
01:46:11,539 --> 01:46:17,809
 calibration for computer models okay to

5726
01:46:14,349 --> 01:46:17,809
 

5727
01:46:14,359 --> 01:46:19,939
 a problem where basically we know that

5728
01:46:17,799 --> 01:46:19,939
 

5729
01:46:17,809 --> 01:46:22,129
 Gaussian process regression is not

5730
01:46:19,929 --> 01:46:22,129
 

5731
01:46:19,939 --> 01:46:25,369
 appropriate or at least a standard

5732
01:46:22,119 --> 01:46:25,369
 

5733
01:46:22,129 --> 01:46:28,429
 Gaussian process because we are sure

5734
01:46:25,359 --> 01:46:28,429
 

5735
01:46:25,369 --> 01:46:30,679
 that it's not the model is the variance

5736
01:46:28,419 --> 01:46:30,679
 

5737
01:46:28,429 --> 01:46:32,929
 is not isotropic a great problem in this

5738
01:46:30,669 --> 01:46:32,929
 

5739
01:46:30,679 --> 01:46:35,329
 computer model emulation literature is

5740
01:46:32,919 --> 01:46:35,329
 

5741
01:46:32,929 --> 01:46:37,309
 really cool but often the Gaussian

5742
01:46:35,319 --> 01:46:37,309
 

5743
01:46:35,329 --> 01:46:40,449
 process falls flat and so better models

5744
01:46:37,299 --> 01:46:40,449
 

5745
01:46:37,309 --> 01:46:43,570
 formulation or is a good important area

5746
01:46:40,439 --> 01:46:43,570
 

5747
01:46:40,449 --> 01:46:46,179
 so yeah in that case what could be an

5748
01:46:43,560 --> 01:46:46,179
 

5749
01:46:43,570 --> 01:46:48,610
 alternative to using variational bass

5750
01:46:46,169 --> 01:46:48,610
 

5751
01:46:46,179 --> 01:46:51,040
 because it's like we have let's say

5752
01:46:48,600 --> 01:46:51,040
 

5753
01:46:48,610 --> 01:46:54,190
 alpha million samples and it's a

5754
01:46:51,030 --> 01:46:54,190
 

5755
01:46:51,040 --> 01:46:57,820
 fourteen dimensional problem so it's I

5756
01:46:54,180 --> 01:46:57,820
 

5757
01:46:54,190 --> 01:47:00,580
 mean not big P but relatively large and

5758
01:46:57,810 --> 01:47:00,580
 

5759
01:46:57,820 --> 01:47:02,679
 so instead then trying variational base

5760
01:47:00,570 --> 01:47:02,679
 

5761
01:47:00,580 --> 01:47:05,590
 that usually works certainly you could

5762
01:47:02,669 --> 01:47:05,590
 

5763
01:47:02,679 --> 01:47:07,360
 use these EPM CMC kind of methods in

5764
01:47:05,580 --> 01:47:07,360
 

5765
01:47:05,590 --> 01:47:08,920
 that setting they've been used for

5766
01:47:07,350 --> 01:47:08,920
 

5767
01:47:07,360 --> 01:47:10,989
 Gaussian processes but even if the

5768
01:47:08,910 --> 01:47:10,989
 

5769
01:47:08,920 --> 01:47:12,790
 Gaussian process wasn't it didn't hold

5770
01:47:10,979 --> 01:47:12,790
 

5771
01:47:10,989 --> 01:47:15,070
 exactly you could even use them for

5772
01:47:12,780 --> 01:47:15,070
 

5773
01:47:12,790 --> 01:47:16,510
 something like I know you know one of

5774
01:47:15,060 --> 01:47:16,510
 

5775
01:47:15,070 --> 01:47:19,270
 our projects is using deep neural

5776
01:47:16,500 --> 01:47:19,270
 

5777
01:47:16,510 --> 01:47:20,949
 networks for emulation instead and you

5778
01:47:19,260 --> 01:47:20,949
 

5779
01:47:19,270 --> 01:47:23,020
 can even run em Sam Seaver deep deep

5780
01:47:20,939 --> 01:47:23,020
 

5781
01:47:20,949 --> 01:47:23,800
 neural networks and you could do that in

5782
01:47:23,010 --> 01:47:23,800
 

5783
01:47:23,020 --> 01:47:25,900
 a parallel way

5784
01:47:23,790 --> 01:47:25,900
 

5785
01:47:23,800 --> 01:47:27,760
 now that's something like that okay

5786
01:47:25,890 --> 01:47:27,760
 

5787
01:47:25,900 --> 01:47:30,580
 thank you

5788
01:47:27,750 --> 01:47:30,580
 

5789
01:47:27,760 --> 01:47:32,739
 I was also gonna ask about the sea bass

5790
01:47:30,570 --> 01:47:32,739
 

5791
01:47:30,580 --> 01:47:35,710
 I found that quite intriguing and I

5792
01:47:32,729 --> 01:47:35,710
 

5793
01:47:32,739 --> 01:47:38,080
 guess it seems like what you've actually

5794
01:47:35,700 --> 01:47:38,080
 

5795
01:47:35,710 --> 01:47:41,050
 done there is replace having an explicit

5796
01:47:38,070 --> 01:47:41,050
 

5797
01:47:38,080 --> 01:47:44,170
 noise model with something nonparametric

5798
01:47:41,040 --> 01:47:44,170
 

5799
01:47:41,050 --> 01:47:46,060
 so is is is actually what this is doing

5800
01:47:44,160 --> 01:47:46,060
 

5801
01:47:44,170 --> 01:47:48,400
 is corresponding to having some

5802
01:47:46,050 --> 01:47:48,400
 

5803
01:47:46,060 --> 01:47:48,940
 nonparametric noise model but then you

5804
01:47:48,390 --> 01:47:48,940
 

5805
01:47:48,400 --> 01:47:50,560
 know it's just

5806
01:47:48,930 --> 01:47:50,560
 

5807
01:47:48,940 --> 01:47:52,510
 that's a great question yeah then we

5808
01:47:50,550 --> 01:47:52,510
 

5809
01:47:50,560 --> 01:47:54,280
 often get a lot of questions on sea bass

5810
01:47:52,500 --> 01:47:54,280
 

5811
01:47:52,510 --> 01:47:56,320
 whether it's like exactly the same as an

5812
01:47:54,270 --> 01:47:56,320
 

5813
01:47:54,280 --> 01:47:58,869
 explicit noise model but it it's sort of

5814
01:47:56,310 --> 01:47:58,869
 

5815
01:47:56,320 --> 01:48:00,790
 like avoiding specifying the noise model

5816
01:47:58,859 --> 01:48:00,790
 

5817
01:47:58,869 --> 01:48:03,280
 because the noise model might also be

5818
01:48:00,780 --> 01:48:03,280
 

5819
01:48:00,790 --> 01:48:05,199
 miss specified etc so it is like being

5820
01:48:03,270 --> 01:48:05,199
 

5821
01:48:03,280 --> 01:48:07,000
 nonparametric but not nonparametric in

5822
01:48:05,189 --> 01:48:07,000
 

5823
01:48:05,199 --> 01:48:08,920
 the Bayesian nonparametric way which

5824
01:48:06,990 --> 01:48:08,920
 

5825
01:48:07,000 --> 01:48:10,540
 means a really flexible model but more

5826
01:48:08,910 --> 01:48:10,540
 

5827
01:48:08,920 --> 01:48:16,960
 nonparametric in the classical way where

5828
01:48:10,530 --> 01:48:16,960
 

5829
01:48:10,540 --> 01:48:18,850
 you avoid modeling something my question

5830
01:48:16,950 --> 01:48:18,850
 

5831
01:48:16,960 --> 01:48:21,190
 is mostly referred in terms of

5832
01:48:18,840 --> 01:48:21,190
 

5833
01:48:18,850 --> 01:48:23,320
 scalability usually you say that

5834
01:48:21,180 --> 01:48:23,320
 

5835
01:48:21,190 --> 01:48:26,199
 probably most of the problems are kind

5836
01:48:23,310 --> 01:48:26,199
 

5837
01:48:23,320 --> 01:48:28,030
 of scalable in terms of my robustness in

5838
01:48:26,189 --> 01:48:28,030
 

5839
01:48:26,199 --> 01:48:30,400
 which cases you have faced this

5840
01:48:28,020 --> 01:48:30,400
 

5841
01:48:28,030 --> 01:48:33,760
 challenge to a scale to make it a scale

5842
01:48:30,390 --> 01:48:33,760
 

5843
01:48:30,400 --> 01:48:36,150
 or to where the actual training the full

5844
01:48:33,750 --> 01:48:36,150
 

5845
01:48:33,760 --> 01:48:39,460
 training is much much accurate that

5846
01:48:36,140 --> 01:48:39,460
 

5847
01:48:36,150 --> 01:48:41,500
 trying to too many bootstrap for example

5848
01:48:39,450 --> 01:48:41,500
 

5849
01:48:39,460 --> 01:48:44,020
 yeah that's a great question of it so

5850
01:48:41,490 --> 01:48:44,020
 

5851
01:48:41,500 --> 01:48:46,570
 the so this is really a new literature

5852
01:48:44,010 --> 01:48:46,570
 

5853
01:48:44,020 --> 01:48:47,820
 and so there are definitely areas that

5854
01:48:46,560 --> 01:48:47,820
 

5855
01:48:46,570 --> 01:48:50,130
 are not developed

5856
01:48:47,810 --> 01:48:50,130
 

5857
01:48:47,820 --> 01:48:52,890
 and I would say that that two of those

5858
01:48:50,120 --> 01:48:52,890
 

5859
01:48:50,130 --> 01:48:54,960
 are you know network structured data and

5860
01:48:52,880 --> 01:48:54,960
 

5861
01:48:52,890 --> 01:48:57,390
 time series data and so if you have like

5862
01:48:54,950 --> 01:48:57,390
 

5863
01:48:54,960 --> 01:48:59,280
 a really like super long time series and

5864
01:48:57,380 --> 01:48:59,280
 

5865
01:48:57,390 --> 01:49:01,560
 you're trying to scale it up like scale

5866
01:48:59,270 --> 01:49:01,560
 

5867
01:48:59,280 --> 01:49:03,600
 up these MCMC methods there's almost no

5868
01:49:01,550 --> 01:49:03,600
 

5869
01:49:01,560 --> 01:49:05,370
 literature on that there's a tiny bit by

5870
01:49:03,590 --> 01:49:05,370
 

5871
01:49:03,600 --> 01:49:07,290
 Emily Fox's group on hidden Markov

5872
01:49:05,360 --> 01:49:07,290
 

5873
01:49:05,370 --> 01:49:09,570
 models but and we just posted a paper on

5874
01:49:07,280 --> 01:49:09,570
 

5875
01:49:07,290 --> 01:49:12,000
 an archive on hit 4 hidden Markov models

5876
01:49:09,560 --> 01:49:12,000
 

5877
01:49:09,570 --> 01:49:14,520
 but there's no like useful general

5878
01:49:11,990 --> 01:49:14,520
 

5879
01:49:12,000 --> 01:49:17,490
 methods and that's even more true in the

5880
01:49:14,510 --> 01:49:17,490
 

5881
01:49:14,520 --> 01:49:19,710
 network case you know we have billions

5882
01:49:17,480 --> 01:49:19,710
 

5883
01:49:17,490 --> 01:49:21,930
 of you know users and products in this

5884
01:49:19,700 --> 01:49:21,930
 

5885
01:49:19,710 --> 01:49:23,640
 Alibaba collaboration and we're kind of

5886
01:49:21,920 --> 01:49:23,640
 

5887
01:49:21,930 --> 01:49:25,950
 trying to trying to develop scalable

5888
01:49:23,630 --> 01:49:25,950
 

5889
01:49:23,640 --> 01:49:27,660
 Bayesian methods in that case is a

5890
01:49:25,940 --> 01:49:27,660
 

5891
01:49:25,950 --> 01:49:29,370
 challenge because if you're just kind of

5892
01:49:27,650 --> 01:49:29,370
 

5893
01:49:27,660 --> 01:49:30,690
 subsampling of breaking up the data well

5894
01:49:29,360 --> 01:49:30,690
 

5895
01:49:29,370 --> 01:49:32,340
 how do you do that if the data really

5896
01:49:30,680 --> 01:49:32,340
 

5897
01:49:30,690 --> 01:49:35,430
 dependent and like a network context

5898
01:49:32,330 --> 01:49:35,430
 

5899
01:49:32,340 --> 01:49:37,530
 it's an open problem so do you have a

5900
01:49:35,420 --> 01:49:37,530
 

5901
01:49:35,430 --> 01:49:40,860
 kind of initial guesses about that

5902
01:49:37,520 --> 01:49:40,860
 

5903
01:49:37,530 --> 01:49:42,420
 especially for the time series data sure

5904
01:49:40,850 --> 01:49:42,420
 

5905
01:49:40,860 --> 01:49:44,820
 yeah yeah time series data the the

5906
01:49:42,410 --> 01:49:44,820
 

5907
01:49:42,420 --> 01:49:46,380
 initial guess would be tube in time you

5908
01:49:44,810 --> 01:49:46,380
 

5909
01:49:44,820 --> 01:49:48,540
 know and see if you have a choice chunks

5910
01:49:46,370 --> 01:49:48,540
 

5911
01:49:46,380 --> 01:49:51,030
 and of adjacent time points and then you

5912
01:49:48,530 --> 01:49:51,030
 

5913
01:49:48,540 --> 01:49:52,980
 can do dumb computation in parallel for

5914
01:49:51,020 --> 01:49:52,980
 

5915
01:49:51,030 --> 01:49:54,630
 adjacent time points that then you can

5916
01:49:52,970 --> 01:49:54,630
 

5917
01:49:52,980 --> 01:49:56,430
 kind of do a lot in terms of scalability

5918
01:49:54,620 --> 01:49:56,430
 

5919
01:49:54,630 --> 01:50:02,820
 doing that I think that that'll end up

5920
01:49:56,420 --> 01:50:02,820
 

5921
01:49:56,430 --> 01:50:04,790
 working quite well okay thank hi thanks

5922
01:50:02,810 --> 01:50:04,790
 

5923
01:50:02,820 --> 01:50:07,670
 for your talk it's very interesting I

5924
01:50:04,780 --> 01:50:07,670
 

5925
01:50:04,790 --> 01:50:11,130
 guess what wasn't really clear to me is

5926
01:50:07,660 --> 01:50:11,130
 

5927
01:50:07,670 --> 01:50:13,920
 if we have a lot of data are you really

5928
01:50:11,120 --> 01:50:13,920
 

5929
01:50:11,130 --> 01:50:15,780
 getting a different conclusion using

5930
01:50:13,910 --> 01:50:15,780
 

5931
01:50:13,920 --> 01:50:18,510
 Bayesian methods versus using some more

5932
01:50:15,770 --> 01:50:18,510
 

5933
01:50:15,780 --> 01:50:20,610
 classical methods yeah that's a great

5934
01:50:18,500 --> 01:50:20,610
 

5935
01:50:18,510 --> 01:50:22,320
 question I mean I think that uh you

5936
01:50:20,600 --> 01:50:22,320
 

5937
01:50:20,610 --> 01:50:23,940
 should hopefully shouldn't you know if

5938
01:50:22,310 --> 01:50:23,940
 

5939
01:50:22,320 --> 01:50:26,850
 you have if you're like in a setting

5940
01:50:23,930 --> 01:50:26,850
 

5941
01:50:23,940 --> 01:50:28,590
 where you have a ton of data and the

5942
01:50:26,840 --> 01:50:28,590
 

5943
01:50:26,850 --> 01:50:30,810
 sort of number of parameters isn't that

5944
01:50:28,580 --> 01:50:30,810
 

5945
01:50:28,590 --> 01:50:32,790
 giant so if I have a logistic regression

5946
01:50:30,800 --> 01:50:32,790
 

5947
01:50:30,810 --> 01:50:34,920
 and I have maybe a hundred hundred

5948
01:50:32,780 --> 01:50:34,920
 

5949
01:50:32,790 --> 01:50:36,480
 predictors but my sample size is under

5950
01:50:34,910 --> 01:50:36,480
 

5951
01:50:34,920 --> 01:50:38,490
 million or something hopefully I'm

5952
01:50:36,470 --> 01:50:38,490
 

5953
01:50:36,480 --> 01:50:40,020
 getting exactly the same results and and

5954
01:50:38,480 --> 01:50:40,020
 

5955
01:50:38,490 --> 01:50:41,760
 we can show that theoretically actually

5956
01:50:40,010 --> 01:50:41,760
 

5957
01:50:40,020 --> 01:50:43,440
 with that the Bayesian central limit

5958
01:50:41,750 --> 01:50:43,440
 

5959
01:50:41,760 --> 01:50:45,150
 theorem or bernstein von Mises result

5960
01:50:43,430 --> 01:50:45,150
 

5961
01:50:43,440 --> 01:50:47,850
 the Bayesian posterior is actually going

5962
01:50:45,140 --> 01:50:47,850
 

5963
01:50:45,150 --> 01:50:49,440
 to converge to exactly the around the

5964
01:50:47,840 --> 01:50:49,440
 

5965
01:50:47,850 --> 01:50:50,790
 maximum likelihood estimates with the

5966
01:50:49,430 --> 01:50:50,790
 

5967
01:50:49,440 --> 01:50:52,100
 covariance being inverse Fisher

5968
01:50:50,780 --> 01:50:52,100
 

5969
01:50:50,790 --> 01:50:54,870
 information and so there's this

5970
01:50:52,090 --> 01:50:54,870
 

5971
01:50:52,100 --> 01:50:57,540
 well-known equivalence between between

5972
01:50:54,860 --> 01:50:57,540
 

5973
01:50:54,870 --> 01:50:58,950
 bays and frequentist methods where where

5974
01:50:57,530 --> 01:50:58,950
 

5975
01:50:57,540 --> 01:50:59,520
 the equivalence doesn't hold is when

5976
01:50:58,940 --> 01:50:59,520
 

5977
01:50:58,950 --> 01:51:01,800
 you're in these

5978
01:50:59,510 --> 01:51:01,800
 

5979
01:50:59,520 --> 01:51:04,590
 challenging problems certainly where you

5980
01:51:01,790 --> 01:51:04,590
 

5981
01:51:01,800 --> 01:51:07,350
 have you know you you have uncertainty

5982
01:51:04,580 --> 01:51:07,350
 

5983
01:51:04,590 --> 01:51:09,890
 remaining it's not well characterized by

5984
01:51:07,340 --> 01:51:09,890
 

5985
01:51:07,350 --> 01:51:12,320
 a large sample Gaussian distribution

5986
01:51:09,880 --> 01:51:12,320
 

5987
01:51:09,890 --> 01:51:15,150
 thanks

5988
01:51:12,310 --> 01:51:15,150
 

5989
01:51:12,320 --> 01:51:16,380
 you mentioned that there's orders of

5990
01:51:15,140 --> 01:51:16,380
 

5991
01:51:15,150 --> 01:51:18,140
 magnitude more people working on

5992
01:51:16,370 --> 01:51:18,140
 

5993
01:51:16,380 --> 01:51:20,690
 optimization distribute optimization

5994
01:51:18,130 --> 01:51:20,690
 

5995
01:51:18,140 --> 01:51:22,950
 techniques is there any way our

5996
01:51:20,680 --> 01:51:22,950
 

5997
01:51:20,690 --> 01:51:24,720
 literature of applying these

5998
01:51:22,940 --> 01:51:24,720
 

5999
01:51:22,950 --> 01:51:27,870
 optimization distribute optimization

6000
01:51:24,710 --> 01:51:27,870
 

6001
01:51:24,720 --> 01:51:29,820
 techniques to scaleable Bayesian MCMC

6002
01:51:27,860 --> 01:51:29,820
 

6003
01:51:27,870 --> 01:51:31,260
 methods that's a great question I mean

6004
01:51:29,810 --> 01:51:31,260
 

6005
01:51:29,820 --> 01:51:34,080
 that that's actually what we were trying

6006
01:51:31,250 --> 01:51:34,080
 

6007
01:51:31,260 --> 01:51:35,370
 to do and that that wass method or

6008
01:51:34,070 --> 01:51:35,370
 

6009
01:51:34,080 --> 01:51:38,190
 something like so it's that we do

6010
01:51:35,360 --> 01:51:38,190
 

6011
01:51:35,370 --> 01:51:39,780
 separate sampling but then use modern

6012
01:51:38,180 --> 01:51:39,780
 

6013
01:51:38,190 --> 01:51:42,450
 optimization techniques for doing the

6014
01:51:39,770 --> 01:51:42,450
 

6015
01:51:39,780 --> 01:51:44,550
 doing the combining but there's also a

6016
01:51:42,440 --> 01:51:44,550
 

6017
01:51:42,450 --> 01:51:45,900
 you know a really amazingly cool

6018
01:51:44,540 --> 01:51:45,900
 

6019
01:51:44,550 --> 01:51:47,790
 literature that I didn't have time to

6020
01:51:45,890 --> 01:51:47,790
 

6021
01:51:45,900 --> 01:51:51,090
 talk about like by people like Yusef

6022
01:51:47,780 --> 01:51:51,090
 

6023
01:51:47,790 --> 01:51:53,330
 more Zook and at MIT you know trying to

6024
01:51:51,080 --> 01:51:53,330
 

6025
01:51:51,090 --> 01:51:55,860
 do things like oh let's try to sample

6026
01:51:53,320 --> 01:51:55,860
 

6027
01:51:53,330 --> 01:51:57,750
 iid from a multivariate Gaussian but

6028
01:51:55,850 --> 01:51:57,750
 

6029
01:51:55,860 --> 01:51:59,550
 defined some optimal transportation plan

6030
01:51:57,740 --> 01:51:59,550
 

6031
01:51:57,750 --> 01:52:00,900
 to the true posterior distribution and

6032
01:51:59,540 --> 01:52:00,900
 

6033
01:51:59,550 --> 01:52:02,160
 then defined some corresponding

6034
01:52:00,890 --> 01:52:02,160
 

6035
01:52:00,900 --> 01:52:04,140
 optimization problems solved an

6036
01:52:02,150 --> 01:52:04,140
 

6037
01:52:02,160 --> 01:52:05,940
 optimization problem well if I have the

6038
01:52:04,130 --> 01:52:05,940
 

6039
01:52:04,140 --> 01:52:07,590
 map that I just you know take those

6040
01:52:05,930 --> 01:52:07,590
 

6041
01:52:05,940 --> 01:52:08,580
 independent samples and I transform them

6042
01:52:07,580 --> 01:52:08,580
 

6043
01:52:07,590 --> 01:52:10,500
 to get perfect samples from the

6044
01:52:08,570 --> 01:52:10,500
 

6045
01:52:08,580 --> 01:52:12,390
 posterior so there's a number of people

6046
01:52:10,490 --> 01:52:12,390
 

6047
01:52:10,500 --> 01:52:14,130
 working on that Guangcheng add up Purdue

6048
01:52:12,380 --> 01:52:14,130
 

6049
01:52:12,390 --> 01:52:19,110
 is another one yeah it's a great

6050
01:52:14,120 --> 01:52:19,110
 

6051
01:52:14,130 --> 01:52:22,410
 question thanks you briefly mentioned

6052
01:52:19,100 --> 01:52:22,410
 

6053
01:52:19,110 --> 01:52:23,940
 that sorry I think we need to break for

6054
01:52:22,400 --> 01:52:23,940
 

6055
01:52:22,410 --> 01:52:25,800
 the for the coffee break but feel free

6056
01:52:23,930 --> 01:52:25,800
 

6057
01:52:23,940 --> 01:52:27,900
 to come come forward that we can chat

6058
01:52:25,790 --> 01:52:27,900
 

6059
01:52:25,800 --> 01:52:31,040
 that offline afterwards but let's thank

6060
01:52:27,890 --> 01:52:31,040
 

6061
01:52:27,900 --> 01:52:31,040
 David again thanks a lot

6062
01:52:33,210 --> 01:52:33,210
 

6063
01:52:33,220 --> 01:53:04,319
[Music]