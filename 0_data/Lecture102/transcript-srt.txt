1
00:00:00,000 --> 00:00:35,279
[Music]

2
00:00:42,020 --> 00:00:42,020
 

3
00:00:42,030 --> 00:00:59,249
[Music]

4
00:01:06,030 --> 00:01:06,030
 

5
00:01:06,040 --> 00:01:19,310
[Music]

6
00:01:30,020 --> 00:01:30,020
 

7
00:01:30,030 --> 00:01:35,310
[Music]

8
00:01:39,520 --> 00:01:39,520
 

9
00:01:39,530 --> 00:01:44,549
[Music]

10
00:01:54,030 --> 00:01:54,030
 

11
00:01:54,040 --> 00:02:07,310
[Music]

12
00:02:21,420 --> 00:02:21,420
 

13
00:02:21,430 --> 00:02:43,210
[Music]

14
00:02:50,020 --> 00:02:50,020
 

15
00:02:50,030 --> 00:03:03,309
[Music]

16
00:03:14,040 --> 00:03:14,040
 

17
00:03:14,050 --> 00:03:27,310
[Music]

18
00:03:38,040 --> 00:03:38,040
 

19
00:03:38,050 --> 00:03:43,309
[Music]

20
00:03:47,510 --> 00:03:47,510
 

21
00:03:47,520 --> 00:03:51,159
[Music]

22
00:04:02,040 --> 00:04:02,040
 

23
00:04:02,050 --> 00:04:15,309
[Music]

24
00:04:29,400 --> 00:04:29,400
 

25
00:04:29,410 --> 00:05:39,690
[Music]

26
00:05:37,000 --> 00:05:39,690
 

27
00:05:37,010 --> 00:05:43,650
 it is my great pleasure and honor to

28
00:05:39,680 --> 00:05:43,650
 

29
00:05:39,690 --> 00:05:44,910
 introduce our last keynote speaker kun

30
00:05:43,640 --> 00:05:44,910
 

31
00:05:43,650 --> 00:05:47,580
 lai Oh looka tun

32
00:05:44,900 --> 00:05:47,580
 

33
00:05:44,910 --> 00:05:48,870
 he's the cadence design professor of

34
00:05:47,570 --> 00:05:48,870
 

35
00:05:47,580 --> 00:05:52,110
 Electrical Engineering and computer

36
00:05:48,860 --> 00:05:52,110
 

37
00:05:48,870 --> 00:05:54,360
 science at Stanford University he's also

38
00:05:52,100 --> 00:05:54,360
 

39
00:05:52,110 --> 00:05:56,160
 the chief scientists technologists and

40
00:05:54,350 --> 00:05:56,160
 

41
00:05:54,360 --> 00:05:58,290
 co-founder of Samba Nova systems

42
00:05:56,150 --> 00:05:58,290
 

43
00:05:56,160 --> 00:06:00,389
 developing invent systems for machine

44
00:05:58,280 --> 00:06:00,389
 

45
00:05:58,290 --> 00:06:03,180
 learning based on configurable AI

46
00:06:00,379 --> 00:06:03,180
 

47
00:06:00,389 --> 00:06:05,160
 processors coonley is well known as a

48
00:06:03,170 --> 00:06:05,160
 

49
00:06:03,180 --> 00:06:08,040
 pioneer and multi-core processor design

50
00:06:05,150 --> 00:06:08,040
 

51
00:06:05,160 --> 00:06:09,389
 he found that the fara web systems to

52
00:06:08,030 --> 00:06:09,389
 

53
00:06:08,040 --> 00:06:10,860
 develop a throughput low-power

54
00:06:09,379 --> 00:06:10,860
 

55
00:06:09,389 --> 00:06:13,410
 multi-core processors for service

56
00:06:10,850 --> 00:06:13,410
 

57
00:06:10,860 --> 00:06:15,650
 systems and some of that work is now

58
00:06:13,400 --> 00:06:15,650
 

59
00:06:13,410 --> 00:06:18,060
 pouring all oracle spark based servers

60
00:06:15,640 --> 00:06:18,060
 

61
00:06:15,650 --> 00:06:22,020
 he currently directs the Stanford

62
00:06:18,050 --> 00:06:22,020
 

63
00:06:18,060 --> 00:06:24,270
 pervasive parallelism lab and he is also

64
00:06:22,010 --> 00:06:24,270
 

65
00:06:22,020 --> 00:06:27,120
 a member of the data analytics for

66
00:06:24,260 --> 00:06:27,120
 

67
00:06:24,270 --> 00:06:28,950
 what's next or dawn lab which is

68
00:06:27,110 --> 00:06:28,950
 

69
00:06:27,120 --> 00:06:31,800
 developing infrastructure for usable

70
00:06:28,940 --> 00:06:31,800
 

71
00:06:28,950 --> 00:06:34,289
 machine learning families and ACM fellow

72
00:06:31,790 --> 00:06:34,289
 

73
00:06:31,800 --> 00:06:37,020
 and I Triple E fellow for contributions

74
00:06:34,279 --> 00:06:37,020
 

75
00:06:34,289 --> 00:06:38,610
 to multi processors on chip and

76
00:06:37,010 --> 00:06:38,610
 

77
00:06:37,020 --> 00:06:41,400
 multi-threaded processor design and

78
00:06:38,600 --> 00:06:41,400
 

79
00:06:38,610 --> 00:06:44,430
 these recipient of the 2018 I Triple E

80
00:06:41,390 --> 00:06:44,430
 

81
00:06:41,400 --> 00:06:45,930
 hair eh good Memorial Award and so it is

82
00:06:44,420 --> 00:06:45,930
 

83
00:06:44,430 --> 00:06:48,130
 great pleasure that I introduce to you

84
00:06:45,920 --> 00:06:48,130
 

85
00:06:45,930 --> 00:06:55,560
 coming

86
00:06:48,120 --> 00:06:55,560
 

87
00:06:48,130 --> 00:06:59,889
[Applause]

88
00:06:55,550 --> 00:06:59,889
 

89
00:06:55,560 --> 00:07:03,039
 good afternoon so it's a pleasure to be

90
00:06:59,879 --> 00:07:03,039
 

91
00:06:59,889 --> 00:07:05,020
 here and thank you Hugo for that kind

92
00:07:03,029 --> 00:07:05,020
 

93
00:07:03,039 --> 00:07:07,180
 introduction and thank you to the

94
00:07:05,010 --> 00:07:07,180
 

95
00:07:05,020 --> 00:07:10,689
 organizing committee of Naropa for

96
00:07:07,170 --> 00:07:10,689
 

97
00:07:07,180 --> 00:07:16,300
 inviting me this is a huge conference

98
00:07:10,679 --> 00:07:16,300
 

99
00:07:10,689 --> 00:07:18,849
 and I'm glad in order to be here so I'm

100
00:07:16,290 --> 00:07:18,849
 

101
00:07:16,300 --> 00:07:21,039
 a computer system designer have done

102
00:07:18,839 --> 00:07:21,039
 

103
00:07:18,849 --> 00:07:24,120
 computer systems design for many years

104
00:07:21,029 --> 00:07:24,120
 

105
00:07:21,039 --> 00:07:27,279
 and it's a particularly interesting time

106
00:07:24,110 --> 00:07:27,279
 

107
00:07:24,120 --> 00:07:29,710
 in the kind of evolution of computer

108
00:07:27,269 --> 00:07:29,710
 

109
00:07:27,279 --> 00:07:32,680
 systems design and that's because it's

110
00:07:29,700 --> 00:07:32,680
 

111
00:07:29,710 --> 00:07:35,770
 characterized by two important trends

112
00:07:32,670 --> 00:07:35,770
 

113
00:07:32,680 --> 00:07:38,889
 the first of which you are quite aware

114
00:07:35,760 --> 00:07:38,889
 

115
00:07:35,770 --> 00:07:41,199
 of that's the broad success of machine

116
00:07:38,879 --> 00:07:41,199
 

117
00:07:38,889 --> 00:07:43,229
 learning and you know that this

118
00:07:41,189 --> 00:07:43,229
 

119
00:07:41,199 --> 00:07:45,159
 conference is an example or

120
00:07:43,219 --> 00:07:45,159
 

121
00:07:43,229 --> 00:07:47,279
 representative of how much interest

122
00:07:45,149 --> 00:07:47,279
 

123
00:07:45,159 --> 00:07:49,360
 there is in the machine learning space

124
00:07:47,269 --> 00:07:49,360
 

125
00:07:47,279 --> 00:07:52,419
 when I first came to this conference

126
00:07:49,350 --> 00:07:52,419
 

127
00:07:49,360 --> 00:07:55,870
 about 12 years ago it was much much

128
00:07:52,409 --> 00:07:55,870
 

129
00:07:52,419 --> 00:07:58,360
 smaller and so the fact that the

130
00:07:55,860 --> 00:07:58,360
 

131
00:07:55,870 --> 00:08:00,870
 interest in machine learning is being

132
00:07:58,350 --> 00:08:00,870
 

133
00:07:58,360 --> 00:08:04,479
 driven by the incredible advances in

134
00:08:00,860 --> 00:08:04,479
 

135
00:08:00,870 --> 00:08:07,120
 image recognition and language

136
00:08:04,469 --> 00:08:07,120
 

137
00:08:04,479 --> 00:08:10,810
 understanding and you know knowledge

138
00:08:07,110 --> 00:08:10,810
 

139
00:08:07,120 --> 00:08:13,569
 base creation and these sorts of

140
00:08:10,800 --> 00:08:13,569
 

141
00:08:10,810 --> 00:08:16,899
 innovations and advances are having you

142
00:08:13,559 --> 00:08:16,899
 

143
00:08:13,569 --> 00:08:19,649
 know society societal scale impact right

144
00:08:16,889 --> 00:08:19,649
 

145
00:08:16,899 --> 00:08:22,029
 so they're enabling autonomous vehicles

146
00:08:19,639 --> 00:08:22,029
 

147
00:08:19,649 --> 00:08:25,149
 you know greater new scientific

148
00:08:22,019 --> 00:08:25,149
 

149
00:08:22,029 --> 00:08:28,649
 discovery and all sorts of innovation in

150
00:08:25,139 --> 00:08:28,649
 

151
00:08:25,149 --> 00:08:32,529
 the medical and personal medicine sphere

152
00:08:28,639 --> 00:08:32,529
 

153
00:08:28,649 --> 00:08:34,120
 so the interesting thing from a from a

154
00:08:32,519 --> 00:08:34,120
 

155
00:08:32,529 --> 00:08:36,430
 computing point of view is is that you

156
00:08:34,110 --> 00:08:36,430
 

157
00:08:34,120 --> 00:08:38,829
 know building these evermore interesting

158
00:08:36,420 --> 00:08:38,829
 

159
00:08:36,430 --> 00:08:41,260
 complex machine learning models takes a

160
00:08:38,819 --> 00:08:41,260
 

161
00:08:38,829 --> 00:08:43,719
 huge amount of computation so you often

162
00:08:41,250 --> 00:08:43,719
 

163
00:08:41,260 --> 00:08:46,540
 want to train on terabytes size data

164
00:08:43,709 --> 00:08:46,540
 

165
00:08:43,719 --> 00:08:49,360
 sets you often want to you know if

166
00:08:46,530 --> 00:08:49,360
 

167
00:08:46,540 --> 00:08:53,110
 you're a large web company you want to

168
00:08:49,350 --> 00:08:53,110
 

169
00:08:49,360 --> 00:08:55,389
 serve millions of your your users using

170
00:08:53,100 --> 00:08:55,389
 

171
00:08:53,110 --> 00:08:57,490
 these machine learning models and so

172
00:08:55,379 --> 00:08:57,490
 

173
00:08:55,389 --> 00:09:00,790
 there's huge amounts of or almost

174
00:08:57,480 --> 00:09:00,790
 

175
00:08:57,490 --> 00:09:02,230
 insatiable demand for computation so

176
00:09:00,780 --> 00:09:02,230
 

177
00:09:00,790 --> 00:09:04,180
 that's the trend that you're all

178
00:09:02,220 --> 00:09:04,180
 

179
00:09:02,230 --> 00:09:05,709
 familiar with we have a trend which you

180
00:09:04,170 --> 00:09:05,709
 

181
00:09:04,180 --> 00:09:07,269
 might not be as familiar with is the

182
00:09:05,699 --> 00:09:07,269
 

183
00:09:05,709 --> 00:09:09,430
 fact that

184
00:09:07,259 --> 00:09:09,430
 

185
00:09:07,269 --> 00:09:11,019
 Moore's law which is kind of driven the

186
00:09:09,420 --> 00:09:11,019
 

187
00:09:09,430 --> 00:09:13,600
 computer industry for the last 50 years

188
00:09:11,009 --> 00:09:13,600
 

189
00:09:11,019 --> 00:09:16,240
 is basically slowing down right so

190
00:09:13,590 --> 00:09:16,240
 

191
00:09:13,600 --> 00:09:18,220
 Moore's law of course says that every 18

192
00:09:16,230 --> 00:09:18,220
 

193
00:09:16,240 --> 00:09:20,620
 months or two years we double the number

194
00:09:18,210 --> 00:09:20,620
 

195
00:09:18,220 --> 00:09:21,970
 of transistors on a chip which means of

196
00:09:20,610 --> 00:09:21,970
 

197
00:09:20,620 --> 00:09:24,310
 course the systems that you can build

198
00:09:21,960 --> 00:09:24,310
 

199
00:09:21,970 --> 00:09:27,129
 with these chips become cheaper and more

200
00:09:24,300 --> 00:09:27,129
 

201
00:09:24,310 --> 00:09:28,959
 capable but what is happening is of

202
00:09:27,119 --> 00:09:28,959
 

203
00:09:27,129 --> 00:09:33,879
 course you know we're stretching out

204
00:09:28,949 --> 00:09:33,879
 

205
00:09:28,959 --> 00:09:36,009
 beyond every two years per circuit

206
00:09:33,869 --> 00:09:36,009
 

207
00:09:33,879 --> 00:09:38,740
 generation and we can argue how much

208
00:09:35,999 --> 00:09:38,740
 

209
00:09:36,009 --> 00:09:41,680
 stretching is going on but what's clear

210
00:09:38,730 --> 00:09:41,680
 

211
00:09:38,740 --> 00:09:43,389
 is the companion' law to Moore's law

212
00:09:41,670 --> 00:09:43,389
 

213
00:09:41,680 --> 00:09:45,730
 which is known as Dennard scaling is

214
00:09:43,379 --> 00:09:45,730
 

215
00:09:43,389 --> 00:09:48,430
 absolutely dead right so Dennard scaling

216
00:09:45,720 --> 00:09:48,430
 

217
00:09:45,730 --> 00:09:50,850
 says if our double the number of

218
00:09:48,420 --> 00:09:50,850
 

219
00:09:48,430 --> 00:09:53,379
 transistors on a chip I can basically

220
00:09:50,840 --> 00:09:53,379
 

221
00:09:50,850 --> 00:09:55,930
 put you use the same amount of power

222
00:09:53,369 --> 00:09:55,930
 

223
00:09:53,379 --> 00:09:57,129
 and so if Dennard scaling is dead if I

224
00:09:55,920 --> 00:09:57,129
 

225
00:09:55,930 --> 00:09:59,110
 double the number of transistors on the

226
00:09:57,119 --> 00:09:59,110
 

227
00:09:57,129 --> 00:10:00,939
 chip then I double the power and that

228
00:09:59,100 --> 00:10:00,939
 

229
00:09:59,110 --> 00:10:03,910
 basically means that all computing is

230
00:10:00,929 --> 00:10:03,910
 

231
00:10:00,939 --> 00:10:06,279
 power limited and what that means is

232
00:10:03,900 --> 00:10:06,279
 

233
00:10:03,910 --> 00:10:09,430
 that the conventional way of designing

234
00:10:06,269 --> 00:10:09,430
 

235
00:10:06,279 --> 00:10:10,899
 computers using CPUs basically is

236
00:10:09,420 --> 00:10:10,899
 

237
00:10:09,430 --> 00:10:12,610
 stagnating you can't get more

238
00:10:10,889 --> 00:10:12,610
 

239
00:10:10,899 --> 00:10:14,170
 performance because anything that you

240
00:10:12,600 --> 00:10:14,170
 

241
00:10:12,610 --> 00:10:17,639
 want to do is basically power

242
00:10:14,160 --> 00:10:17,639
 

243
00:10:14,170 --> 00:10:20,860
 inefficient okay so that's the backdrop

244
00:10:17,629 --> 00:10:20,860
 

245
00:10:17,639 --> 00:10:24,610
 you know the interesting thing of course

246
00:10:20,850 --> 00:10:24,610
 

247
00:10:20,860 --> 00:10:26,709
 then is that given the huge demands from

248
00:10:24,600 --> 00:10:26,709
 

249
00:10:24,610 --> 00:10:28,959
 from machine learning for performance

250
00:10:26,699 --> 00:10:28,959
 

251
00:10:26,709 --> 00:10:30,519
 and the fact that that the traditional

252
00:10:28,949 --> 00:10:30,519
 

253
00:10:28,959 --> 00:10:32,680
 ways of providing that performance have

254
00:10:30,509 --> 00:10:32,680
 

255
00:10:30,519 --> 00:10:35,110
 run out of steam what we need then is a

256
00:10:32,670 --> 00:10:35,110
 

257
00:10:32,680 --> 00:10:37,809
 new approach to designing computer

258
00:10:35,100 --> 00:10:37,809
 

259
00:10:35,110 --> 00:10:41,550
 systems for machine learning so the

260
00:10:37,799 --> 00:10:41,550
 

261
00:10:37,809 --> 00:10:45,509
 intertwine the mingling intervening of

262
00:10:41,540 --> 00:10:45,509
 

263
00:10:41,550 --> 00:10:49,360
 machine learning and more computer power

264
00:10:45,499 --> 00:10:49,360
 

265
00:10:45,509 --> 00:10:51,279
 actually it's quite has been going on

266
00:10:49,350 --> 00:10:51,279
 

267
00:10:49,360 --> 00:10:53,500
 for quite some time as you all know

268
00:10:51,269 --> 00:10:53,500
 

269
00:10:51,279 --> 00:10:55,779
 neural nets has been around for a long

270
00:10:53,490 --> 00:10:55,779
 

271
00:10:53,500 --> 00:10:58,269
 time 50 plus years and training them

272
00:10:55,769 --> 00:10:58,269
 

273
00:10:55,779 --> 00:11:00,550
 using back property around since 1980s

274
00:10:58,259 --> 00:11:00,550
 

275
00:10:58,269 --> 00:11:03,339
 and so the question is you know why did

276
00:11:00,540 --> 00:11:03,339
 

277
00:11:00,550 --> 00:11:05,199
 they become so dominant and clearly this

278
00:11:03,329 --> 00:11:05,199
 

279
00:11:03,339 --> 00:11:08,139
 graph kind of kind of gives you a sense

280
00:11:05,189 --> 00:11:08,139
 

281
00:11:05,199 --> 00:11:11,620
 of what's going what went on on the

282
00:11:08,129 --> 00:11:11,620
 

283
00:11:08,139 --> 00:11:13,259
 x-axis we have the data set size and the

284
00:11:11,610 --> 00:11:13,259
 

285
00:11:11,620 --> 00:11:16,600
 model complexity which of course is

286
00:11:13,249 --> 00:11:16,600
 

287
00:11:13,259 --> 00:11:19,420
 grown over time and on the y-axis you

288
00:11:16,590 --> 00:11:19,420
 

289
00:11:16,600 --> 00:11:20,800
 have the accuracy on some toss B it's a

290
00:11:19,410 --> 00:11:20,800
 

291
00:11:19,420 --> 00:11:22,930
 image recognition and

292
00:11:20,790 --> 00:11:22,930
 

293
00:11:20,800 --> 00:11:25,930
 language translation and so in the 1980s

294
00:11:22,920 --> 00:11:25,930
 

295
00:11:22,930 --> 00:11:28,720
 the datasets was so small and that the

296
00:11:25,920 --> 00:11:28,720
 

297
00:11:25,930 --> 00:11:30,310
 models were one was so simple that they

298
00:11:28,710 --> 00:11:30,310
 

299
00:11:28,720 --> 00:11:33,459
 couldn't beat conventional algorithms

300
00:11:30,300 --> 00:11:33,459
 

301
00:11:30,310 --> 00:11:35,649
 but fast forward to today and you've got

302
00:11:33,449 --> 00:11:35,649
 

303
00:11:33,459 --> 00:11:38,440
 a million times more computation much

304
00:11:35,639 --> 00:11:38,440
 

305
00:11:35,649 --> 00:11:40,649
 bigger models much more data and now of

306
00:11:38,430 --> 00:11:40,649
 

307
00:11:38,440 --> 00:11:43,540
 course the neural network approaches are

308
00:11:40,639 --> 00:11:43,540
 

309
00:11:40,649 --> 00:11:46,390
 surpassing conventional idea commercial

310
00:11:43,530 --> 00:11:46,390
 

311
00:11:43,540 --> 00:11:48,610
 algorithms by a large margin and on some

312
00:11:46,380 --> 00:11:48,610
 

313
00:11:46,390 --> 00:11:51,490
 of these tasks that humans do they're

314
00:11:48,600 --> 00:11:51,490
 

315
00:11:48,610 --> 00:11:59,050
 approaching or surpassing human accuracy

316
00:11:51,480 --> 00:11:59,050
 

317
00:11:51,490 --> 00:12:01,930
 and so this notion of training of

318
00:11:59,040 --> 00:12:01,930
 

319
00:11:59,050 --> 00:12:04,390
 developing software by by use doing a

320
00:12:01,920 --> 00:12:04,390
 

321
00:12:01,930 --> 00:12:08,320
 neural net training has been

322
00:12:04,380 --> 00:12:08,320
 

323
00:12:04,390 --> 00:12:11,950
 characterized by and raghupathy of uber

324
00:12:08,310 --> 00:12:11,950
 

325
00:12:08,320 --> 00:12:16,029
 as formally a Stanford PhD student as

326
00:12:11,940 --> 00:12:16,029
 

327
00:12:11,950 --> 00:12:18,399
 scaled a sari and as software 2.0 right

328
00:12:16,019 --> 00:12:18,399
 

329
00:12:16,029 --> 00:12:19,930
 so software 2.0 when I first heard the

330
00:12:18,389 --> 00:12:19,930
 

331
00:12:18,399 --> 00:12:21,730
 term I said this is pretty pretentious

332
00:12:19,920 --> 00:12:21,730
 

333
00:12:19,930 --> 00:12:24,880
 you're gonna change completely the way

334
00:12:21,720 --> 00:12:24,880
 

335
00:12:21,730 --> 00:12:28,300
 that we develop software but I've it's

336
00:12:24,870 --> 00:12:28,300
 

337
00:12:24,880 --> 00:12:30,490
 grown on me and the whole idea in Eisley

338
00:12:28,290 --> 00:12:30,490
 

339
00:12:28,300 --> 00:12:33,430
 captures the idea of you know instead of

340
00:12:30,480 --> 00:12:33,430
 

341
00:12:30,490 --> 00:12:36,820
 developing algorithms in the traditional

342
00:12:33,420 --> 00:12:36,820
 

343
00:12:33,430 --> 00:12:40,390
 way you instead you you train neural net

344
00:12:36,810 --> 00:12:40,390
 

345
00:12:36,820 --> 00:12:43,870
 weights based on optimization and this

346
00:12:40,380 --> 00:12:43,870
 

347
00:12:40,390 --> 00:12:47,380
 approach to developing software software

348
00:12:43,860 --> 00:12:47,380
 

349
00:12:43,870 --> 00:12:48,940
 2.0 has a lot of fundamental advantages

350
00:12:47,370 --> 00:12:48,940
 

351
00:12:47,380 --> 00:12:53,860
 right so it's first of all of course

352
00:12:48,930 --> 00:12:53,860
 

353
00:12:48,940 --> 00:12:55,829
 it's often easier to develop a to

354
00:12:53,850 --> 00:12:55,829
 

355
00:12:53,860 --> 00:13:00,579
 trainer model than it is to actually

356
00:12:55,819 --> 00:13:00,579
 

357
00:12:55,829 --> 00:13:03,970
 manually develop algorithms but I think

358
00:13:00,569 --> 00:13:03,970
 

359
00:13:00,579 --> 00:13:06,940
 more than the development of the model

360
00:13:03,960 --> 00:13:06,940
 

361
00:13:03,970 --> 00:13:09,430
 one of the big benefits of deploying

362
00:13:06,930 --> 00:13:09,430
 

363
00:13:06,940 --> 00:13:11,500
 these neural network models is they are

364
00:13:09,420 --> 00:13:11,500
 

365
00:13:09,430 --> 00:13:13,860
 much more predictable in terms of

366
00:13:11,490 --> 00:13:13,860
 

367
00:13:11,500 --> 00:13:15,760
 runtime and memory usage compared to

368
00:13:13,850 --> 00:13:15,760
 

369
00:13:13,860 --> 00:13:18,100
 conventional algorithms and this is a

370
00:13:15,750 --> 00:13:18,100
 

371
00:13:15,760 --> 00:13:20,019
 lot of benefits from a deployment point

372
00:13:18,090 --> 00:13:20,019
 

373
00:13:18,100 --> 00:13:22,060
 point of view because of course you want

374
00:13:20,009 --> 00:13:22,060
 

375
00:13:20,019 --> 00:13:24,970
 things to be predictable an example of

376
00:13:22,050 --> 00:13:24,970
 

377
00:13:22,060 --> 00:13:28,600
 course is Google's language translation

378
00:13:24,960 --> 00:13:28,600
 

379
00:13:24,970 --> 00:13:32,430
 and they shrank their implementation for

380
00:13:28,590 --> 00:13:32,430
 

381
00:13:28,600 --> 00:13:34,279
 500,000 lines of code to just 500

382
00:13:32,420 --> 00:13:34,279
 

383
00:13:32,430 --> 00:13:36,680
 however it's not just

384
00:13:34,269 --> 00:13:36,680
 

385
00:13:34,279 --> 00:13:38,930
 these high-end applications of language

386
00:13:36,670 --> 00:13:38,930
 

387
00:13:36,680 --> 00:13:41,149
 translation and image recognition that

388
00:13:38,920 --> 00:13:41,149
 

389
00:13:38,930 --> 00:13:44,209
 can benefit from the software 2.0

390
00:13:41,139 --> 00:13:44,209
 

391
00:13:41,149 --> 00:13:47,329
 approach even classical problems in

392
00:13:44,199 --> 00:13:47,329
 

393
00:13:44,209 --> 00:13:49,910
 computer systems such as networking and

394
00:13:47,319 --> 00:13:49,910
 

395
00:13:47,329 --> 00:13:52,279
 databases and compilers can benefit from

396
00:13:49,900 --> 00:13:52,279
 

397
00:13:49,910 --> 00:13:54,110
 the software 2.0 approach and people who

398
00:13:52,269 --> 00:13:54,110
 

399
00:13:52,279 --> 00:13:55,930
 are taking this approach are radically

400
00:13:54,100 --> 00:13:55,930
 

401
00:13:54,110 --> 00:13:59,269
 changing what they're doing right so

402
00:13:55,920 --> 00:13:59,269
 

403
00:13:55,930 --> 00:14:01,850
 instead of developing algorithms they

404
00:13:59,259 --> 00:14:01,850
 

405
00:13:59,269 --> 00:14:04,730
 are spending a lot of their time dealing

406
00:14:01,840 --> 00:14:04,730
 

407
00:14:01,850 --> 00:14:06,740
 with training data and this means that

408
00:14:04,720 --> 00:14:06,740
 

409
00:14:04,730 --> 00:14:11,779
 they are fundamentally changing how they

410
00:14:06,730 --> 00:14:11,779
 

411
00:14:06,740 --> 00:14:15,110
 spend their time so so this idea of kind

412
00:14:11,769 --> 00:14:15,110
 

413
00:14:11,779 --> 00:14:17,959
 of creating and engineering training

414
00:14:15,100 --> 00:14:17,959
 

415
00:14:15,110 --> 00:14:20,959
 data is is the new challenge then for

416
00:14:17,949 --> 00:14:20,959
 

417
00:14:17,959 --> 00:14:22,309
 developing software 2.0 applications and

418
00:14:20,949 --> 00:14:22,309
 

419
00:14:20,959 --> 00:14:24,980
 their number of things of course that

420
00:14:22,299 --> 00:14:24,980
 

421
00:14:22,309 --> 00:14:27,499
 you do when developing training sets you

422
00:14:24,970 --> 00:14:27,499
 

423
00:14:24,980 --> 00:14:31,009
 might come up with noisy labeling

424
00:14:27,489 --> 00:14:31,009
 

425
00:14:27,499 --> 00:14:32,809
 schemes using weak supervision you might

426
00:14:30,999 --> 00:14:32,809
 

427
00:14:31,009 --> 00:14:34,850
 spend a lot of your time doing data

428
00:14:32,799 --> 00:14:34,850
 

429
00:14:32,809 --> 00:14:37,939
 augmentation to generalize your models

430
00:14:34,840 --> 00:14:37,939
 

431
00:14:34,850 --> 00:14:42,500
 and you might do data reshaping so

432
00:14:37,929 --> 00:14:42,500
 

433
00:14:37,939 --> 00:14:45,170
 snorkle is a research prototype that's

434
00:14:42,490 --> 00:14:45,170
 

435
00:14:42,500 --> 00:14:48,009
 been developed at Stanford by Alex

436
00:14:45,160 --> 00:14:48,009
 

437
00:14:45,170 --> 00:14:50,870
 Ratner who's on the job market

438
00:14:47,999 --> 00:14:50,870
 

439
00:14:48,009 --> 00:14:55,759
 academically by the way and his advisor

440
00:14:50,860 --> 00:14:55,759
 

441
00:14:50,870 --> 00:15:00,220
 Chris Rea and so snorkle provides a way

442
00:14:55,749 --> 00:15:00,220
 

443
00:14:55,759 --> 00:15:02,959
 of accelerating and characterizing and

444
00:15:00,210 --> 00:15:02,959
 

445
00:15:00,220 --> 00:15:06,980
 formalizing these workflows that you use

446
00:15:02,949 --> 00:15:06,980
 

447
00:15:02,959 --> 00:15:08,899
 in data programming and it can makes it

448
00:15:06,970 --> 00:15:08,899
 

449
00:15:06,980 --> 00:15:10,939
 possible to combine these workflow flows

450
00:15:08,889 --> 00:15:10,939
 

451
00:15:08,899 --> 00:15:15,170
 and these workflows are often noisy and

452
00:15:10,929 --> 00:15:15,170
 

453
00:15:10,939 --> 00:15:16,550
 so typically you need to denoise of the

454
00:15:15,160 --> 00:15:16,550
 

455
00:15:15,170 --> 00:15:18,920
 workflows in order to generate

456
00:15:16,540 --> 00:15:18,920
 

457
00:15:16,550 --> 00:15:22,819
 high-quality training data that could be

458
00:15:18,910 --> 00:15:22,819
 

459
00:15:18,920 --> 00:15:26,089
 used to generate high-quality models so

460
00:15:22,809 --> 00:15:26,089
 

461
00:15:22,819 --> 00:15:27,769
 one of the implications of the snorkel

462
00:15:26,079 --> 00:15:27,769
 

463
00:15:26,089 --> 00:15:36,490
 approach well one of the implications is

464
00:15:27,759 --> 00:15:36,490
 

465
00:15:27,769 --> 00:15:36,490
 the intermingling of what was data

466
00:15:37,710 --> 00:15:37,710
 

467
00:15:37,720 --> 00:15:44,360
 data analysis and machine learning right

468
00:15:40,780 --> 00:15:44,360
 

469
00:15:40,790 --> 00:15:46,610
 so in this example in this case you're

470
00:15:44,350 --> 00:15:46,610
 

471
00:15:44,360 --> 00:15:48,040
 doing some weak supervision and so

472
00:15:46,600 --> 00:15:48,040
 

473
00:15:46,610 --> 00:15:50,199
 rather than simply loading

474
00:15:48,030 --> 00:15:50,199
 

475
00:15:48,040 --> 00:15:52,420
 miles sequentially from someplace in

476
00:15:50,189 --> 00:15:52,420
 

477
00:15:50,199 --> 00:15:54,850
 memory you're actually doing an SQL

478
00:15:52,410 --> 00:15:54,850
 

479
00:15:52,420 --> 00:15:57,009
 query right in the middle of your

480
00:15:54,840 --> 00:15:57,009
 

481
00:15:54,850 --> 00:15:59,920
 machine learning training loop and so

482
00:15:56,999 --> 00:15:59,920
 

483
00:15:57,009 --> 00:16:03,100
 this is fundamentally adding data

484
00:15:59,910 --> 00:16:03,100
 

485
00:15:59,920 --> 00:16:05,290
 processing to machine learning which is

486
00:16:03,090 --> 00:16:05,290
 

487
00:16:03,100 --> 00:16:07,750
 a change from what typically was done

488
00:16:05,280 --> 00:16:07,750
 

489
00:16:05,290 --> 00:16:12,670
 before the other thing of course that

490
00:16:07,740 --> 00:16:12,670
 

491
00:16:07,750 --> 00:16:14,470
 you're seeing is the use of sparsity in

492
00:16:12,660 --> 00:16:14,470
 

493
00:16:12,670 --> 00:16:17,110
 machine learning models right and so

494
00:16:14,460 --> 00:16:17,110
 

495
00:16:14,470 --> 00:16:19,839
 sparsity is becoming a design objective

496
00:16:17,100 --> 00:16:19,839
 

497
00:16:17,110 --> 00:16:21,750
 right so for instance in some large

498
00:16:19,829 --> 00:16:21,750
 

499
00:16:19,839 --> 00:16:26,500
 neural networks in the fully connected

500
00:16:21,740 --> 00:16:26,500
 

501
00:16:21,750 --> 00:16:28,180
 layer the number of parameters grows

502
00:16:26,490 --> 00:16:28,180
 

503
00:16:26,500 --> 00:16:30,759
 quadratically with the number of neurons

504
00:16:28,170 --> 00:16:30,759
 

505
00:16:28,180 --> 00:16:33,009
 right and so recently this piece of work

506
00:16:30,749 --> 00:16:33,009
 

507
00:16:30,759 --> 00:16:35,500
 that shows that hey if you you sparsity

508
00:16:32,999 --> 00:16:35,500
 

509
00:16:33,009 --> 00:16:37,569
 you can make that growth be linear

510
00:16:35,490 --> 00:16:37,569
 

511
00:16:35,500 --> 00:16:39,699
 rather than quadratic and this of course

512
00:16:37,559 --> 00:16:39,699
 

513
00:16:37,569 --> 00:16:42,639
 will dramatically improve the time it

514
00:16:39,689 --> 00:16:42,639
 

515
00:16:39,699 --> 00:16:44,350
 takes to train the model another example

516
00:16:42,629 --> 00:16:44,350
 

517
00:16:42,639 --> 00:16:46,839
 where sparsity comes up is when you're

518
00:16:44,340 --> 00:16:46,839
 

519
00:16:44,350 --> 00:16:48,490
 trying to model the real world people's

520
00:16:46,829 --> 00:16:48,490
 

521
00:16:46,839 --> 00:16:51,730
 places things and you're trying to use

522
00:16:48,480 --> 00:16:51,730
 

523
00:16:48,490 --> 00:16:54,970
 this to improve your training of your of

524
00:16:51,720 --> 00:16:54,970
 

525
00:16:51,730 --> 00:16:57,339
 your models and so you end up with you

526
00:16:54,960 --> 00:16:57,339
 

527
00:16:54,970 --> 00:17:00,010
 know a fundamentally sparse network

528
00:16:57,329 --> 00:17:00,010
 

529
00:16:57,339 --> 00:17:03,639
 structure and and so sparsity again

530
00:17:00,000 --> 00:17:03,639
 

531
00:17:00,010 --> 00:17:08,439
 shows up in the modeling of your

532
00:17:03,629 --> 00:17:08,439
 

533
00:17:03,639 --> 00:17:11,110
 networks the other of course a big trend

534
00:17:08,429 --> 00:17:11,110
 

535
00:17:08,439 --> 00:17:13,299
 in in machine learning is of course is

536
00:17:11,100 --> 00:17:13,299
 

537
00:17:11,110 --> 00:17:15,220
 greater complexity for the models so the

538
00:17:13,289 --> 00:17:15,220
 

539
00:17:13,299 --> 00:17:17,589
 models are growing in complexity because

540
00:17:15,210 --> 00:17:17,589
 

541
00:17:15,220 --> 00:17:20,500
 of course by having more complex models

542
00:17:17,579 --> 00:17:20,500
 

543
00:17:17,589 --> 00:17:23,740
 you can get higher accuracy so this just

544
00:17:20,490 --> 00:17:23,740
 

545
00:17:20,500 --> 00:17:26,650
 shows that you know from over the period

546
00:17:23,730 --> 00:17:26,650
 

547
00:17:23,740 --> 00:17:30,070
 of two years the growth of the model

548
00:17:26,640 --> 00:17:30,070
 

549
00:17:26,650 --> 00:17:33,270
 size from sixty million parameters to

550
00:17:30,060 --> 00:17:33,270
 

551
00:17:30,070 --> 00:17:37,799
 8.7 million parameters and this is a

552
00:17:33,260 --> 00:17:37,799
 

553
00:17:33,270 --> 00:17:40,090
 commensurate with a order of magnitude

554
00:17:37,789 --> 00:17:40,090
 

555
00:17:37,799 --> 00:17:43,260
 increase in the amount of computation

556
00:17:40,080 --> 00:17:43,260
 

557
00:17:40,090 --> 00:17:46,000
 required to train the model so

558
00:17:43,250 --> 00:17:46,000
 

559
00:17:43,260 --> 00:17:48,970
 fundamentally then machine learning

560
00:17:45,990 --> 00:17:48,970
 

561
00:17:46,000 --> 00:17:51,549
 training is limited by computation so

562
00:17:48,960 --> 00:17:51,549
 

563
00:17:48,970 --> 00:17:55,809
 there's a quote from Greg Thomas who was

564
00:17:51,539 --> 00:17:55,809
 

565
00:17:51,549 --> 00:17:58,299
 or Tommy made the quote a a senior

566
00:17:55,799 --> 00:17:58,299
 

567
00:17:55,809 --> 00:18:00,790
 researcher at Baidu and he basically

568
00:17:58,289 --> 00:18:00,790
 

569
00:17:58,299 --> 00:18:01,350
 says that the job of training machine

570
00:18:00,780 --> 00:18:01,350
 

571
00:18:00,790 --> 00:18:03,120
 learning

572
00:18:01,340 --> 00:18:03,120
 

573
00:18:01,350 --> 00:18:05,130
 models is limited by compute and

574
00:18:03,110 --> 00:18:05,130
 

575
00:18:03,120 --> 00:18:06,750
 basically if we had more compute we

576
00:18:05,120 --> 00:18:06,750
 

577
00:18:05,130 --> 00:18:09,690
 would train more complex models and get

578
00:18:06,740 --> 00:18:09,690
 

579
00:18:06,750 --> 00:18:12,299
 higher accuracy and you know what we'd

580
00:18:09,680 --> 00:18:12,299
 

581
00:18:09,690 --> 00:18:16,380
 like is a 100x improvement in the

582
00:18:12,289 --> 00:18:16,380
 

583
00:18:12,299 --> 00:18:19,320
 performance of our computers so that we

584
00:18:16,370 --> 00:18:19,320
 

585
00:18:16,380 --> 00:18:22,340
 could train more interesting models okay

586
00:18:19,310 --> 00:18:22,340
 

587
00:18:19,320 --> 00:18:24,779
 so that's the story for machine learning

588
00:18:22,330 --> 00:18:24,779
 

589
00:18:22,340 --> 00:18:29,460
 from a computation point of view well

590
00:18:24,769 --> 00:18:29,460
 

591
00:18:24,779 --> 00:18:31,850
 what about the story of computation well

592
00:18:29,450 --> 00:18:31,850
 

593
00:18:29,460 --> 00:18:36,179
 so this is a graph that kind of shows

594
00:18:31,840 --> 00:18:36,179
 

595
00:18:31,850 --> 00:18:40,200
 you the trends of micro processors over

596
00:18:36,169 --> 00:18:40,200
 

597
00:18:36,179 --> 00:18:43,350
 the last 45 years and what we see is a

598
00:18:40,190 --> 00:18:43,350
 

599
00:18:40,200 --> 00:18:47,879
 number of curves the top curve which is

600
00:18:43,340 --> 00:18:47,879
 

601
00:18:43,350 --> 00:18:49,350
 the golden triangles show the transistor

602
00:18:47,869 --> 00:18:49,350
 

603
00:18:47,879 --> 00:18:52,610
 growth so this transistor growth

604
00:18:49,340 --> 00:18:52,610
 

605
00:18:49,350 --> 00:19:03,029
 basically follows Moore's law and then

606
00:18:52,600 --> 00:19:03,029
 

607
00:18:52,610 --> 00:19:06,600
 the red show that the power that is

608
00:19:03,019 --> 00:19:06,600
 

609
00:19:03,029 --> 00:19:07,860
 dissipated and so basically on a chip if

610
00:19:06,590 --> 00:19:07,860
 

611
00:19:06,600 --> 00:19:10,799
 you dissipate more than a few hundred

612
00:19:07,850 --> 00:19:10,799
 

613
00:19:07,860 --> 00:19:12,840
 watts and you don't have very exotic

614
00:19:10,789 --> 00:19:12,840
 

615
00:19:10,799 --> 00:19:15,960
 cooling then typically your chip will

616
00:19:12,830 --> 00:19:15,960
 

617
00:19:12,840 --> 00:19:19,139
 melt and so you're limited to a few

618
00:19:15,950 --> 00:19:19,139
 

619
00:19:15,960 --> 00:19:22,019
 hundred watts yeah of dissipation and so

620
00:19:19,129 --> 00:19:22,019
 

621
00:19:19,139 --> 00:19:24,690
 that cap with the end of Dennard scaling

622
00:19:22,009 --> 00:19:24,690
 

623
00:19:22,019 --> 00:19:27,120
 means that the frequency as shown by the

624
00:19:24,680 --> 00:19:27,120
 

625
00:19:24,690 --> 00:19:29,730
 green squares has basically plateaued

626
00:19:27,110 --> 00:19:29,730
 

627
00:19:27,120 --> 00:19:32,909
 and that means that your single core

628
00:19:29,720 --> 00:19:32,909
 

629
00:19:29,730 --> 00:19:36,019
 performance plateaus and the only way to

630
00:19:32,899 --> 00:19:36,019
 

631
00:19:32,909 --> 00:19:39,450
 get more performance going forward is

632
00:19:36,009 --> 00:19:39,450
 

633
00:19:36,019 --> 00:19:42,899
 well you know what the industry did was

634
00:19:39,440 --> 00:19:42,899
 

635
00:19:39,450 --> 00:19:45,720
 was turn to multi-core right so Moore's

636
00:19:42,889 --> 00:19:45,720
 

637
00:19:42,899 --> 00:19:47,639
 law power war was so that it's this

638
00:19:45,710 --> 00:19:47,639
 

639
00:19:45,720 --> 00:19:51,000
 plateauing in the amount of power is

640
00:19:47,629 --> 00:19:51,000
 

641
00:19:47,639 --> 00:19:53,580
 known as a power wall and back in the

642
00:19:50,990 --> 00:19:53,580
 

643
00:19:51,000 --> 00:19:55,679
 mid-90s my research group did a lot a

644
00:19:53,570 --> 00:19:55,679
 

645
00:19:53,580 --> 00:19:57,840
 lot of work on figuring out how to do

646
00:19:55,669 --> 00:19:57,840
 

647
00:19:55,679 --> 00:20:00,600
 multi-core it didn't actually take off

648
00:19:57,830 --> 00:20:00,600
 

649
00:19:57,840 --> 00:20:04,230
 in the industry until you know you know

650
00:20:00,590 --> 00:20:04,230
 

651
00:20:00,600 --> 00:20:07,290
 mid 2000s but you know

652
00:20:04,220 --> 00:20:07,290
 

653
00:20:04,230 --> 00:20:09,090
 that was basically the last time that we

654
00:20:07,280 --> 00:20:09,090
 

655
00:20:07,290 --> 00:20:10,860
 changed the paradigm in which we

656
00:20:09,080 --> 00:20:10,860
 

657
00:20:09,090 --> 00:20:14,070
 designed micro processes fundamentally

658
00:20:10,850 --> 00:20:14,070
 

659
00:20:10,860 --> 00:20:16,500
 but notice instead of the the software

660
00:20:14,060 --> 00:20:16,500
 

661
00:20:14,070 --> 00:20:17,820
 really didn't change right so it might

662
00:20:16,490 --> 00:20:17,820
 

663
00:20:16,500 --> 00:20:20,880
 have become more parallel but we were

664
00:20:17,810 --> 00:20:20,880
 

665
00:20:17,820 --> 00:20:24,660
 still basically you know do using

666
00:20:20,870 --> 00:20:24,660
 

667
00:20:20,880 --> 00:20:28,740
 instructions as our mode of of executing

668
00:20:24,650 --> 00:20:28,740
 

669
00:20:24,660 --> 00:20:32,790
 programs okay so so let me say a little

670
00:20:28,730 --> 00:20:32,790
 

671
00:20:28,740 --> 00:20:34,440
 bit about power and so power is as we

672
00:20:32,780 --> 00:20:34,440
 

673
00:20:32,790 --> 00:20:37,770
 said you know it's a very simple

674
00:20:34,430 --> 00:20:37,770
 

675
00:20:34,440 --> 00:20:40,850
 equation which which relates power and

676
00:20:37,760 --> 00:20:40,850
 

677
00:20:37,770 --> 00:20:43,350
 to performance and so power is equal to

678
00:20:40,840 --> 00:20:43,350
 

679
00:20:40,850 --> 00:20:45,960
 operations per second times Joule per

680
00:20:43,340 --> 00:20:45,960
 

681
00:20:43,350 --> 00:20:48,330
 operation now we said that the amount of

682
00:20:45,950 --> 00:20:48,330
 

683
00:20:45,960 --> 00:20:50,190
 power you can dissipate is fixed and so

684
00:20:48,320 --> 00:20:50,190
 

685
00:20:48,330 --> 00:20:52,169
 of course you want more performance

686
00:20:50,180 --> 00:20:52,169
 

687
00:20:50,190 --> 00:20:54,690
 because we just said machine learning

688
00:20:52,159 --> 00:20:54,690
 

689
00:20:52,169 --> 00:20:57,090
 needs a lot more performance and so then

690
00:20:54,680 --> 00:20:57,090
 

691
00:20:54,690 --> 00:21:00,030
 the only way to to make this equation

692
00:20:57,080 --> 00:21:00,030
 

693
00:20:57,090 --> 00:21:02,429
 stay balanced of course is to become

694
00:21:00,020 --> 00:21:02,429
 

695
00:21:00,030 --> 00:21:05,100
 more energy efficient we have to

696
00:21:02,419 --> 00:21:05,100
 

697
00:21:02,429 --> 00:21:09,030
 decrease the amount of power dissipated

698
00:21:05,090 --> 00:21:09,030
 

699
00:21:05,100 --> 00:21:10,740
 per operation and so the way to do that

700
00:21:09,020 --> 00:21:10,740
 

701
00:21:09,030 --> 00:21:12,419
 is potentially to become more

702
00:21:10,730 --> 00:21:12,419
 

703
00:21:10,740 --> 00:21:14,940
 specialized right so if you do something

704
00:21:12,409 --> 00:21:14,940
 

705
00:21:12,419 --> 00:21:18,929
 in a fixed function way you make it less

706
00:21:14,930 --> 00:21:18,929
 

707
00:21:14,940 --> 00:21:21,600
 flexible less dynamic less general then

708
00:21:18,919 --> 00:21:21,600
 

709
00:21:18,929 --> 00:21:23,360
 you can make it more efficient and so

710
00:21:21,590 --> 00:21:23,360
 

711
00:21:21,600 --> 00:21:26,669
 better energy energy efficiency

712
00:21:23,350 --> 00:21:26,669
 

713
00:21:23,360 --> 00:21:30,500
 basically comes from specialization or

714
00:21:26,659 --> 00:21:30,500
 

715
00:21:26,669 --> 00:21:33,030
 fixed function so given the set up now

716
00:21:30,490 --> 00:21:33,030
 

717
00:21:30,500 --> 00:21:35,700
 you know what are the key questions we

718
00:21:33,020 --> 00:21:35,700
 

719
00:21:33,030 --> 00:21:37,650
 want to address well first of all we

720
00:21:35,690 --> 00:21:37,650
 

721
00:21:35,700 --> 00:21:40,440
 certainly want to understand how we can

722
00:21:37,640 --> 00:21:40,440
 

723
00:21:37,650 --> 00:21:42,240
 get 100x improvement in performance so

724
00:21:40,430 --> 00:21:42,240
 

725
00:21:40,440 --> 00:21:44,880
 that you machine learning researchers

726
00:21:42,230 --> 00:21:44,880
 

727
00:21:42,240 --> 00:21:46,770
 can do more interesting things however

728
00:21:44,870 --> 00:21:46,770
 

729
00:21:44,880 --> 00:21:48,270
 as I said the only way to do that is to

730
00:21:46,760 --> 00:21:48,270
 

731
00:21:46,770 --> 00:21:50,540
 become more power efficient so we have

732
00:21:48,260 --> 00:21:50,540
 

733
00:21:48,270 --> 00:21:53,100
 to become more powerful so we need

734
00:21:50,530 --> 00:21:53,100
 

735
00:21:50,540 --> 00:21:57,330
 improvement in performance per watt and

736
00:21:53,090 --> 00:21:57,330
 

737
00:21:53,100 --> 00:21:59,040
 we want to do this without giving up the

738
00:21:57,320 --> 00:21:59,040
 

739
00:21:57,330 --> 00:22:01,470
 flexibility that you've traditionally

740
00:21:59,030 --> 00:22:01,470
 

741
00:21:59,040 --> 00:22:03,450
 have been accustomed to right so we

742
00:22:01,460 --> 00:22:03,450
 

743
00:22:01,470 --> 00:22:05,520
 don't want to do this by fixing what you

744
00:22:03,440 --> 00:22:05,520
 

745
00:22:03,450 --> 00:22:07,620
 do because then you won't get to

746
00:22:05,510 --> 00:22:07,620
 

747
00:22:05,520 --> 00:22:10,200
 innovate right and so we want to be able

748
00:22:07,610 --> 00:22:10,200
 

749
00:22:07,620 --> 00:22:14,179
 to essentially give you fixed functions

750
00:22:10,190 --> 00:22:14,179
 

751
00:22:10,200 --> 00:22:16,710
 like performance with processor like

752
00:22:14,169 --> 00:22:16,710
 

753
00:22:14,179 --> 00:22:17,590
 flexibility that's that's the hard

754
00:22:16,700 --> 00:22:17,590
 

755
00:22:16,710 --> 00:22:20,890
 question

756
00:22:17,580 --> 00:22:20,890
 

757
00:22:17,590 --> 00:22:23,620
 that we want to address okay and I would

758
00:22:20,880 --> 00:22:23,620
 

759
00:22:20,890 --> 00:22:26,289
 contend that in order to answer these

760
00:22:23,610 --> 00:22:26,289
 

761
00:22:23,620 --> 00:22:30,130
 questions we need a full vertically

762
00:22:26,279 --> 00:22:30,130
 

763
00:22:26,289 --> 00:22:32,440
 integrated solution that combines new

764
00:22:30,120 --> 00:22:32,440
 

765
00:22:30,130 --> 00:22:35,830
 machine learning algorithms new

766
00:22:32,430 --> 00:22:35,830
 

767
00:22:32,440 --> 00:22:39,750
 languages and compilers and new hardware

768
00:22:35,820 --> 00:22:39,750
 

769
00:22:35,830 --> 00:22:42,220
 all designed or co.design and together

770
00:22:39,740 --> 00:22:42,220
 

771
00:22:39,750 --> 00:22:44,950
 alright so let me say a few words about

772
00:22:42,210 --> 00:22:44,950
 

773
00:22:42,220 --> 00:22:47,350
 ml algorithms and now basically the

774
00:22:44,940 --> 00:22:47,350
 

775
00:22:44,950 --> 00:22:49,690
 outline of the talk will follow the

776
00:22:47,340 --> 00:22:49,690
 

777
00:22:47,350 --> 00:22:52,870
 three points I just mentioned so ml

778
00:22:49,680 --> 00:22:52,870
 

779
00:22:49,690 --> 00:22:55,390
 algorithms right so the insight here is

780
00:22:52,860 --> 00:22:55,390
 

781
00:22:52,870 --> 00:22:58,029
 that machine learning is this brand new

782
00:22:55,380 --> 00:22:58,029
 

783
00:22:55,390 --> 00:23:00,100
 type of computation that follows a

784
00:22:58,019 --> 00:23:00,100
 

785
00:22:58,029 --> 00:23:04,809
 different model from the software 1.0

786
00:23:00,090 --> 00:23:04,809
 

787
00:23:00,100 --> 00:23:06,970
 model software 1.0 model the computation

788
00:23:04,799 --> 00:23:06,970
 

789
00:23:04,809 --> 00:23:09,909
 had to be deterministic and repeatable

790
00:23:06,960 --> 00:23:09,909
 

791
00:23:06,970 --> 00:23:11,710
 because humans were programming you know

792
00:23:09,899 --> 00:23:11,710
 

793
00:23:09,909 --> 00:23:13,659
 with algorithms and of course they want

794
00:23:11,700 --> 00:23:13,659
 

795
00:23:11,710 --> 00:23:15,909
 to get the same answer every time they

796
00:23:13,649 --> 00:23:15,909
 

797
00:23:13,659 --> 00:23:17,590
 run the program with the same input and

798
00:23:15,899 --> 00:23:17,590
 

799
00:23:15,909 --> 00:23:20,320
 they want to be able to bug their

800
00:23:17,580 --> 00:23:20,320
 

801
00:23:17,590 --> 00:23:22,960
 program so things have to be predictable

802
00:23:20,310 --> 00:23:22,960
 

803
00:23:20,320 --> 00:23:24,520
 and stur monistic software 2.0 what

804
00:23:22,950 --> 00:23:24,520
 

805
00:23:22,960 --> 00:23:27,850
 you're really trying to do of course is

806
00:23:24,510 --> 00:23:27,850
 

807
00:23:24,520 --> 00:23:30,460
 learn these probabilistic models trained

808
00:23:27,840 --> 00:23:30,460
 

809
00:23:27,850 --> 00:23:32,740
 on data and you only have to be as

810
00:23:30,450 --> 00:23:32,740
 

811
00:23:30,460 --> 00:23:35,529
 accurate or you only have to be

812
00:23:32,730 --> 00:23:35,529
 

813
00:23:32,740 --> 00:23:38,020
 statistically correct in order to get

814
00:23:35,519 --> 00:23:38,020
 

815
00:23:35,529 --> 00:23:41,529
 the requisite accuracy in your models

816
00:23:38,010 --> 00:23:41,529
 

817
00:23:38,020 --> 00:23:44,020
 and so by taking advantage of this you

818
00:23:41,519 --> 00:23:44,020
 

819
00:23:41,529 --> 00:23:46,600
 creates lots of opportunities for

820
00:23:44,010 --> 00:23:46,600
 

821
00:23:44,020 --> 00:23:48,190
 improved performance right so machine

822
00:23:46,590 --> 00:23:48,190
 

823
00:23:46,600 --> 00:23:50,140
 learning basically is an architect's

824
00:23:48,180 --> 00:23:50,140
 

825
00:23:48,190 --> 00:23:54,190
 computer architects dream right it's

826
00:23:50,130 --> 00:23:54,190
 

827
00:23:50,140 --> 00:23:56,200
 this world in which fundamentally you

828
00:23:54,180 --> 00:23:56,200
 

829
00:23:54,190 --> 00:23:57,820
 need lots of computation but there's all

830
00:23:56,190 --> 00:23:57,820
 

831
00:23:56,200 --> 00:24:01,029
 sorts of ways of getting to the right

832
00:23:57,810 --> 00:24:01,029
 

833
00:23:57,820 --> 00:24:03,760
 answer right and so that creates lots of

834
00:24:01,019 --> 00:24:03,760
 

835
00:24:01,029 --> 00:24:06,100
 opportunity for innovation and you're

836
00:24:03,750 --> 00:24:06,100
 

837
00:24:03,760 --> 00:24:07,929
 seeing that if you you know if I

838
00:24:06,090 --> 00:24:07,929
 

839
00:24:06,100 --> 00:24:09,820
 transported you to a computer

840
00:24:07,919 --> 00:24:09,820
 

841
00:24:07,929 --> 00:24:11,409
 architecture conference there'd be many

842
00:24:09,810 --> 00:24:11,409
 

843
00:24:09,820 --> 00:24:13,000
 many sessions on how to improve

844
00:24:11,399 --> 00:24:13,000
 

845
00:24:11,409 --> 00:24:14,350
 performance for machine learning because

846
00:24:12,990 --> 00:24:14,350
 

847
00:24:13,000 --> 00:24:16,360
 it's something that of course has

848
00:24:14,340 --> 00:24:16,360
 

849
00:24:14,350 --> 00:24:19,710
 captured a lot of people's imaginations

850
00:24:16,350 --> 00:24:19,710
 

851
00:24:16,360 --> 00:24:22,929
 for the reasons I just described ok so

852
00:24:19,700 --> 00:24:22,929
 

853
00:24:19,710 --> 00:24:25,149
 the core algorithm in machine learning

854
00:24:22,919 --> 00:24:25,149
 

855
00:24:22,929 --> 00:24:27,220
 training of course is stochastic grand

856
00:24:25,139 --> 00:24:27,220
 

857
00:24:25,149 --> 00:24:29,530
 gradient descent you might use other

858
00:24:27,210 --> 00:24:29,530
 

859
00:24:27,220 --> 00:24:30,860
 techniques but but this is one is it's

860
00:24:29,520 --> 00:24:30,860
 

861
00:24:29,530 --> 00:24:36,049
 basically the most

862
00:24:30,850 --> 00:24:36,049
 

863
00:24:30,860 --> 00:24:38,330
 one and of course the key idea is you

864
00:24:36,039 --> 00:24:38,330
 

865
00:24:36,049 --> 00:24:40,370
 know to take some element of the

866
00:24:38,320 --> 00:24:40,370
 

867
00:24:38,330 --> 00:24:42,950
 training set compute the gradient and

868
00:24:40,360 --> 00:24:42,950
 

869
00:24:40,370 --> 00:24:48,110
 move in the opposite direction all right

870
00:24:42,940 --> 00:24:48,110
 

871
00:24:42,950 --> 00:24:50,660
 so one way of characterizing SGD is in

872
00:24:48,100 --> 00:24:50,660
 

873
00:24:48,110 --> 00:24:52,670
 terms of efficiency there two types of

874
00:24:50,650 --> 00:24:52,670
 

875
00:24:50,660 --> 00:24:54,980
 efficiency it's a statistical efficiency

876
00:24:52,660 --> 00:24:54,980
 

877
00:24:52,670 --> 00:24:56,929
 which is how many iterations it takes

878
00:24:54,970 --> 00:24:56,929
 

879
00:24:54,980 --> 00:24:59,299
 for you to converge to a particular

880
00:24:56,919 --> 00:24:59,299
 

881
00:24:56,929 --> 00:25:02,090
 level of accuracy for your model and

882
00:24:59,289 --> 00:25:02,090
 

883
00:24:59,299 --> 00:25:04,370
 then hardware efficiency which is how

884
00:25:02,080 --> 00:25:04,370
 

885
00:25:02,090 --> 00:25:06,410
 long each of these iterations take now

886
00:25:04,360 --> 00:25:06,410
 

887
00:25:04,370 --> 00:25:08,480
 clearly you know in order to understand

888
00:25:06,400 --> 00:25:08,480
 

889
00:25:06,410 --> 00:25:10,669
 how long the whole SGD album is going to

890
00:25:08,470 --> 00:25:10,669
 

891
00:25:08,480 --> 00:25:14,049
 take you got to take the product of

892
00:25:10,659 --> 00:25:14,049
 

893
00:25:10,669 --> 00:25:17,000
 these two efficiencies however from a

894
00:25:14,039 --> 00:25:17,000
 

895
00:25:14,049 --> 00:25:18,559
 system optimization point of view what

896
00:25:16,990 --> 00:25:18,559
 

897
00:25:17,000 --> 00:25:22,700
 would what you'd like to do is to

898
00:25:18,549 --> 00:25:22,700
 

899
00:25:18,559 --> 00:25:25,460
 trade-off some reduction in statistical

900
00:25:22,690 --> 00:25:25,460
 

901
00:25:22,700 --> 00:25:28,610
 efficiency for maybe a much larger

902
00:25:25,450 --> 00:25:28,610
 

903
00:25:25,460 --> 00:25:32,840
 improvement in hardware efficiency so

904
00:25:28,600 --> 00:25:32,840
 

905
00:25:28,610 --> 00:25:35,230
 you know one of the places where you

906
00:25:32,830 --> 00:25:35,230
 

907
00:25:32,840 --> 00:25:37,640
 know there is some debate within the

908
00:25:35,220 --> 00:25:37,640
 

909
00:25:35,230 --> 00:25:40,340
 machine learning community is on back

910
00:25:37,630 --> 00:25:40,340
 

911
00:25:37,640 --> 00:25:42,169
 size so this is a fundamental trade-off

912
00:25:40,330 --> 00:25:42,169
 

913
00:25:40,340 --> 00:25:44,179
 between hardware efficiency and

914
00:25:42,159 --> 00:25:44,179
 

915
00:25:42,169 --> 00:25:47,900
 statistical efficiency and you know you

916
00:25:44,169 --> 00:25:47,900
 

917
00:25:44,179 --> 00:25:49,549
 can decide sort of where you want to to

918
00:25:47,890 --> 00:25:49,549
 

919
00:25:47,900 --> 00:25:52,070
 be but let me say let me talk about a

920
00:25:49,539 --> 00:25:52,070
 

921
00:25:49,549 --> 00:25:54,230
 def another kind of trade-off and that

922
00:25:52,060 --> 00:25:54,230
 

923
00:25:52,070 --> 00:25:57,410
 is lower precision right so low

924
00:25:54,220 --> 00:25:57,410
 

925
00:25:54,230 --> 00:25:59,960
 precision is an area where you can make

926
00:25:57,400 --> 00:25:59,960
 

927
00:25:57,410 --> 00:26:02,780
 this trade-off and fundamentally if you

928
00:25:59,950 --> 00:26:02,780
 

929
00:25:59,960 --> 00:26:04,549
 compute with integers instead of

930
00:26:02,770 --> 00:26:04,549
 

931
00:26:02,780 --> 00:26:07,220
 floating point numbers or you compute

932
00:26:04,539 --> 00:26:07,220
 

933
00:26:04,549 --> 00:26:09,559
 with you know 4-bit integers instead of

934
00:26:07,210 --> 00:26:09,559
 

935
00:26:07,220 --> 00:26:11,929
 64-bit integers you will save a lot of

936
00:26:09,549 --> 00:26:11,929
 

937
00:26:09,559 --> 00:26:14,510
 energy so quadratic in this in the size

938
00:26:11,919 --> 00:26:14,510
 

939
00:26:11,929 --> 00:26:17,150
 of your your integers and you know

940
00:26:14,500 --> 00:26:17,150
 

941
00:26:14,510 --> 00:26:20,419
 they're multiples of power save you go

942
00:26:17,140 --> 00:26:20,419
 

943
00:26:17,150 --> 00:26:23,090
 from floating point to fixed point also

944
00:26:20,409 --> 00:26:23,090
 

945
00:26:20,419 --> 00:26:25,309
 of course the sizes of your data asset

946
00:26:23,080 --> 00:26:25,309
 

947
00:26:23,090 --> 00:26:27,590
 size both the activations and the the

948
00:26:25,299 --> 00:26:27,590
 

949
00:26:25,309 --> 00:26:30,380
 weights are going to get smaller if you

950
00:26:27,580 --> 00:26:30,380
 

951
00:26:27,590 --> 00:26:31,730
 use lower precision and this means of

952
00:26:30,370 --> 00:26:31,730
 

953
00:26:30,380 --> 00:26:34,070
 course you're going to use less memory

954
00:26:31,720 --> 00:26:34,070
 

955
00:26:31,730 --> 00:26:35,330
 capacity but also when transferring that

956
00:26:34,060 --> 00:26:35,330
 

957
00:26:34,070 --> 00:26:38,299
 data you're going to use less memory

958
00:26:35,320 --> 00:26:38,299
 

959
00:26:35,330 --> 00:26:41,929
 bandwidth when you do that and so it has

960
00:26:38,289 --> 00:26:41,929
 

961
00:26:38,299 --> 00:26:43,550
 memory advantages also of course it has

962
00:26:41,919 --> 00:26:43,550
 

963
00:26:41,929 --> 00:26:44,340
 throughput computational throughput

964
00:26:43,540 --> 00:26:44,340
 

965
00:26:43,550 --> 00:26:46,620
 advantages

966
00:26:44,330 --> 00:26:46,620
 

967
00:26:44,340 --> 00:26:49,559
 you can use specialized instructions

968
00:26:46,610 --> 00:26:49,559
 

969
00:26:46,620 --> 00:26:51,990
 that operate on multiple data elements

970
00:26:49,549 --> 00:26:51,990
 

971
00:26:49,559 --> 00:26:54,900
 at the same time these instructions are

972
00:26:51,980 --> 00:26:54,900
 

973
00:26:51,990 --> 00:26:58,289
 called sim D instructions or SIMD

974
00:26:54,890 --> 00:26:58,289
 

975
00:26:54,900 --> 00:27:01,860
 instructions and and many people take

976
00:26:58,279 --> 00:27:01,860
 

977
00:26:58,289 --> 00:27:04,140
 many processes take advantage of that so

978
00:27:01,850 --> 00:27:04,140
 

979
00:27:01,860 --> 00:27:06,080
 examples of where lower precision has

980
00:27:04,130 --> 00:27:06,080
 

981
00:27:04,140 --> 00:27:09,120
 been used being used in the Google TPU

982
00:27:06,070 --> 00:27:09,120
 

983
00:27:06,080 --> 00:27:12,840
 intel has specialized instructs in D

984
00:27:09,110 --> 00:27:12,840
 

985
00:27:09,120 --> 00:27:15,510
 instructions for exploiting smaller data

986
00:27:12,830 --> 00:27:15,510
 

987
00:27:12,840 --> 00:27:18,630
 types than then 32-bit floating-point

988
00:27:15,500 --> 00:27:18,630
 

989
00:27:15,510 --> 00:27:20,940
 numbers and of course if you look at the

990
00:27:18,620 --> 00:27:20,940
 

991
00:27:18,630 --> 00:27:23,039
 Microsoft brainwave project which uses a

992
00:27:20,930 --> 00:27:23,039
 

993
00:27:20,940 --> 00:27:26,149
 technology called field programmable

994
00:27:23,029 --> 00:27:26,149
 

995
00:27:23,039 --> 00:27:29,309
 gate arrays which sometimes people call

996
00:27:26,139 --> 00:27:29,309
 

997
00:27:26,149 --> 00:27:31,740
 programmable dates or programmable

998
00:27:29,299 --> 00:27:31,740
 

999
00:27:29,309 --> 00:27:33,630
 Hardware then again the only way that

1000
00:27:31,730 --> 00:27:33,630
 

1001
00:27:31,740 --> 00:27:36,000
 that works well is by using low

1002
00:27:33,620 --> 00:27:36,000
 

1003
00:27:33,630 --> 00:27:37,919
 precision so what's the downside to

1004
00:27:35,990 --> 00:27:37,919
 

1005
00:27:36,000 --> 00:27:41,520
 lower precision well you've got lower

1006
00:27:37,909 --> 00:27:41,520
 

1007
00:27:37,919 --> 00:27:43,649
 accuracy and that means that potentially

1008
00:27:41,510 --> 00:27:43,649
 

1009
00:27:41,520 --> 00:27:46,919
 you might hurt your statistical

1010
00:27:43,639 --> 00:27:46,919
 

1011
00:27:43,649 --> 00:27:49,350
 efficiency and the sort of conventional

1012
00:27:46,909 --> 00:27:49,350
 

1013
00:27:46,919 --> 00:27:52,350
 wisdom is that you need at least 16 bits

1014
00:27:49,340 --> 00:27:52,350
 

1015
00:27:49,350 --> 00:27:56,669
 of floating-point numbers with 32 bit

1016
00:27:52,340 --> 00:27:56,669
 

1017
00:27:52,350 --> 00:27:59,279
 accumulate to actually train using lower

1018
00:27:56,659 --> 00:27:59,279
 

1019
00:27:56,669 --> 00:28:01,230
 precision recently we come up with an

1020
00:27:59,269 --> 00:28:01,230
 

1021
00:27:59,279 --> 00:28:04,230
 algorithm that does somewhat better it's

1022
00:28:01,220 --> 00:28:04,230
 

1023
00:28:01,230 --> 00:28:06,779
 called high accuracy low precision and

1024
00:28:04,220 --> 00:28:06,779
 

1025
00:28:04,230 --> 00:28:09,480
 the key idea is to use a technique

1026
00:28:06,769 --> 00:28:09,480
 

1027
00:28:06,779 --> 00:28:10,679
 called bit centering and basically what

1028
00:28:09,470 --> 00:28:10,679
 

1029
00:28:09,480 --> 00:28:13,770
 you're doing is you're dynamically

1030
00:28:10,669 --> 00:28:13,770
 

1031
00:28:10,679 --> 00:28:16,350
 moving some fixed number of bits around

1032
00:28:13,760 --> 00:28:16,350
 

1033
00:28:13,770 --> 00:28:19,289
 so that you get the least amount of

1034
00:28:16,340 --> 00:28:19,289
 

1035
00:28:16,350 --> 00:28:24,450
 error and so the idea is to bound the

1036
00:28:19,279 --> 00:28:24,450
 

1037
00:28:19,289 --> 00:28:27,779
 solution of space and then then move the

1038
00:28:24,440 --> 00:28:27,779
 

1039
00:28:24,450 --> 00:28:30,299
 bits to cover that space and that's the

1040
00:28:27,769 --> 00:28:30,299
 

1041
00:28:27,779 --> 00:28:32,460
 Reese entering and rescaling component

1042
00:28:30,289 --> 00:28:32,460
 

1043
00:28:30,299 --> 00:28:35,190
 of the algorithm there's also a

1044
00:28:32,450 --> 00:28:35,190
 

1045
00:28:32,460 --> 00:28:37,470
 statistical variance reduction component

1046
00:28:35,180 --> 00:28:37,470
 

1047
00:28:35,190 --> 00:28:43,440
 of the the algorithm and so if you

1048
00:28:37,460 --> 00:28:43,440
 

1049
00:28:37,470 --> 00:28:48,390
 compare help which is shown in orange or

1050
00:28:43,430 --> 00:28:48,390
 

1051
00:28:43,440 --> 00:28:51,330
 or pink with the 64 bit variant

1052
00:28:48,380 --> 00:28:51,330
 

1053
00:28:48,390 --> 00:28:53,190
 reduction algorithm SVR G you see that

1054
00:28:51,320 --> 00:28:53,190
 

1055
00:28:51,330 --> 00:28:57,290
 it comes pretty close to the test

1056
00:28:53,180 --> 00:28:57,290
 

1057
00:28:53,190 --> 00:29:01,050
 accuracy of the 64 bit

1058
00:28:57,280 --> 00:29:01,050
 

1059
00:28:57,290 --> 00:29:03,180
 SVO G algorithm using ten bits of help

1060
00:29:01,040 --> 00:29:03,180
 

1061
00:29:01,050 --> 00:29:07,830
 and it turns out of course if you are

1062
00:29:03,170 --> 00:29:07,830
 

1063
00:29:03,180 --> 00:29:10,260
 using a model that is convex you can

1064
00:29:07,820 --> 00:29:10,260
 

1065
00:29:07,830 --> 00:29:12,330
 prove that help will converge at a

1066
00:29:10,250 --> 00:29:12,330
 

1067
00:29:10,260 --> 00:29:14,730
 linear rate but of course most

1068
00:29:12,320 --> 00:29:14,730
 

1069
00:29:12,330 --> 00:29:16,350
 interesting models are not complex and

1070
00:29:14,720 --> 00:29:16,350
 

1071
00:29:14,730 --> 00:29:18,990
 so you don't have the same sort of

1072
00:29:16,340 --> 00:29:18,990
 

1073
00:29:16,350 --> 00:29:21,809
 guarantees however you do see in in

1074
00:29:18,980 --> 00:29:21,809
 

1075
00:29:18,990 --> 00:29:25,170
 practice that help works pretty well

1076
00:29:21,799 --> 00:29:25,170
 

1077
00:29:21,809 --> 00:29:27,570
 this is just an example from a 14 layer

1078
00:29:25,160 --> 00:29:27,570
 

1079
00:29:25,170 --> 00:29:31,350
 ResNet on the C far

1080
00:29:27,560 --> 00:29:31,350
 

1081
00:29:27,570 --> 00:29:34,800
 ten data set it shows that a help shown

1082
00:29:31,340 --> 00:29:34,800
 

1083
00:29:31,350 --> 00:29:37,530
 in orange and get you lower training

1084
00:29:34,790 --> 00:29:37,530
 

1085
00:29:34,800 --> 00:29:40,470
 loss and high validation accuracy

1086
00:29:37,520 --> 00:29:40,470
 

1087
00:29:37,530 --> 00:29:42,950
 compared to stochastic gradient descent

1088
00:29:40,460 --> 00:29:42,950
 

1089
00:29:40,470 --> 00:29:48,300
 with 32-bit floating-point numbers and

1090
00:29:42,940 --> 00:29:48,300
 

1091
00:29:42,950 --> 00:29:51,650
 so lower precision is a good thing but

1092
00:29:48,290 --> 00:29:51,650
 

1093
00:29:48,300 --> 00:29:55,370
 there are other ways of trading off

1094
00:29:51,640 --> 00:29:55,370
 

1095
00:29:51,650 --> 00:29:58,650
 better Hardware efficiency for lower

1096
00:29:55,360 --> 00:29:58,650
 

1097
00:29:55,370 --> 00:30:00,750
 better harbor efficiency for some small

1098
00:29:58,640 --> 00:30:00,750
 

1099
00:29:58,650 --> 00:30:10,050
 reduction in statistical efficiency the

1100
00:30:00,740 --> 00:30:10,050
 

1101
00:30:00,750 --> 00:30:11,960
 general sense or idea that I talk to

1102
00:30:10,040 --> 00:30:11,960
 

1103
00:30:10,050 --> 00:30:15,270
 when I talk to my computer architecture

1104
00:30:11,950 --> 00:30:15,270
 

1105
00:30:11,960 --> 00:30:17,220
 friends as I say just relax it's only

1106
00:30:15,260 --> 00:30:17,220
 

1107
00:30:15,270 --> 00:30:18,840
 machine learning and there's all sorts

1108
00:30:17,210 --> 00:30:18,840
 

1109
00:30:17,220 --> 00:30:22,070
 of things you can do you can relax

1110
00:30:18,830 --> 00:30:22,070
 

1111
00:30:18,840 --> 00:30:23,309
 precision as I just showed you you can

1112
00:30:22,060 --> 00:30:23,309
 

1113
00:30:22,070 --> 00:30:25,170
 relax

1114
00:30:23,299 --> 00:30:25,170
 

1115
00:30:23,309 --> 00:30:29,010
 synchronization if you write a parallel

1116
00:30:25,160 --> 00:30:29,010
 

1117
00:30:25,170 --> 00:30:31,260
 program you can ignore the races and get

1118
00:30:29,000 --> 00:30:31,260
 

1119
00:30:29,010 --> 00:30:32,940
 rid of synchronization and ignore the

1120
00:30:31,250 --> 00:30:32,940
 

1121
00:30:31,260 --> 00:30:35,070
 races which is something which is

1122
00:30:32,930 --> 00:30:35,070
 

1123
00:30:32,940 --> 00:30:36,870
 anathema to any parallel programmer

1124
00:30:35,060 --> 00:30:36,870
 

1125
00:30:35,070 --> 00:30:39,120
 right I teach parallel programming and

1126
00:30:36,860 --> 00:30:39,120
 

1127
00:30:36,870 --> 00:30:42,030
 you know if I tell students say stuck

1128
00:30:39,110 --> 00:30:42,030
 

1129
00:30:39,120 --> 00:30:44,370
 rule number one if you touch shared data

1130
00:30:42,020 --> 00:30:44,370
 

1131
00:30:42,030 --> 00:30:45,950
 put in synchronization well you know you

1132
00:30:44,360 --> 00:30:45,950
 

1133
00:30:44,370 --> 00:30:49,920
 can get rid of synchronization and

1134
00:30:45,940 --> 00:30:49,920
 

1135
00:30:45,950 --> 00:30:52,890
 things will still work correctly now if

1136
00:30:49,910 --> 00:30:52,890
 

1137
00:30:49,920 --> 00:30:55,440
 you look at a a shared memory

1138
00:30:52,880 --> 00:30:55,440
 

1139
00:30:52,890 --> 00:30:57,990
 multiprocessor on chip or multiple chips

1140
00:30:55,430 --> 00:30:57,990
 

1141
00:30:55,440 --> 00:31:00,510
 one of the key pieces of technology is

1142
00:30:57,980 --> 00:31:00,510
 

1143
00:30:57,990 --> 00:31:03,150
 called the cache coherence algorithm

1144
00:31:00,500 --> 00:31:03,150
 

1145
00:31:00,510 --> 00:31:05,760
 right it keeps all the caches coherent

1146
00:31:03,140 --> 00:31:05,760
 

1147
00:31:03,150 --> 00:31:09,510
 or consistent and this is a really

1148
00:31:05,750 --> 00:31:09,510
 

1149
00:31:05,760 --> 00:31:10,620
 gnarly piece of logic it's hard to

1150
00:31:09,500 --> 00:31:10,620
 

1151
00:31:09,510 --> 00:31:12,630
 design and

1152
00:31:10,610 --> 00:31:12,630
 

1153
00:31:10,620 --> 00:31:16,050
 and may perform correctly it's hard to

1154
00:31:12,620 --> 00:31:16,050
 

1155
00:31:12,630 --> 00:31:19,050
 get right and always is a rich source of

1156
00:31:16,040 --> 00:31:19,050
 

1157
00:31:16,050 --> 00:31:21,030
 bugs well we found out that for machine

1158
00:31:19,040 --> 00:31:21,030
 

1159
00:31:19,050 --> 00:31:22,380
 learning algorithms at least SGD you

1160
00:31:21,020 --> 00:31:22,380
 

1161
00:31:21,030 --> 00:31:24,390
 don't really need it you don't really

1162
00:31:22,370 --> 00:31:24,390
 

1163
00:31:22,380 --> 00:31:26,670
 need it most of the time like you can

1164
00:31:24,380 --> 00:31:26,670
 

1165
00:31:24,390 --> 00:31:28,260
 throw away you can throw it away 99% of

1166
00:31:26,660 --> 00:31:28,260
 

1167
00:31:26,670 --> 00:31:31,410
 the time and you still get the right

1168
00:31:28,250 --> 00:31:31,410
 

1169
00:31:28,260 --> 00:31:34,040
 answer so you can relax cache coherence

1170
00:31:31,400 --> 00:31:34,040
 

1171
00:31:31,410 --> 00:31:36,240
 in a distributed environment you can

1172
00:31:34,030 --> 00:31:36,240
 

1173
00:31:34,040 --> 00:31:38,430
 reduce the amount of communication you

1174
00:31:36,230 --> 00:31:38,430
 

1175
00:31:36,240 --> 00:31:40,110
 do by analyzing the gradients and only

1176
00:31:38,420 --> 00:31:40,110
 

1177
00:31:38,430 --> 00:31:42,030
 transferring the gradients when they're

1178
00:31:40,100 --> 00:31:42,030
 

1179
00:31:40,110 --> 00:31:44,220
 big enough of course if you do this

1180
00:31:42,020 --> 00:31:44,220
 

1181
00:31:42,030 --> 00:31:46,470
 naively your simple efficiency will all

1182
00:31:44,210 --> 00:31:46,470
 

1183
00:31:44,220 --> 00:31:48,960
 go into the toilet however if you

1184
00:31:46,460 --> 00:31:48,960
 

1185
00:31:46,470 --> 00:31:50,730
 compensate by playing with the momentum

1186
00:31:48,950 --> 00:31:50,730
 

1187
00:31:48,960 --> 00:31:52,890
 you can still make things work correctly

1188
00:31:50,720 --> 00:31:52,890
 

1189
00:31:50,730 --> 00:31:56,220
 and you can get rid of a lot of

1190
00:31:52,880 --> 00:31:56,220
 

1191
00:31:52,890 --> 00:31:58,230
 communication by doing this okay so all

1192
00:31:56,210 --> 00:31:58,230
 

1193
00:31:56,220 --> 00:32:02,309
 of these tricks fall under this idea of

1194
00:31:58,220 --> 00:32:02,309
 

1195
00:31:58,230 --> 00:32:04,620
 sort of how can we get much better

1196
00:32:02,299 --> 00:32:04,620
 

1197
00:32:02,309 --> 00:32:06,710
 Hardware efficiency for some small

1198
00:32:04,610 --> 00:32:06,710
 

1199
00:32:04,620 --> 00:32:09,780
 reduction or maybe even no reduction in

1200
00:32:06,700 --> 00:32:09,780
 

1201
00:32:06,710 --> 00:32:12,929
 statistical efficiency okay and here are

1202
00:32:09,770 --> 00:32:12,929
 

1203
00:32:09,780 --> 00:32:14,220
 three recent Stanford PhD students who

1204
00:32:12,919 --> 00:32:14,220
 

1205
00:32:12,929 --> 00:32:17,010
 have kind of done a lot of work in this

1206
00:32:14,210 --> 00:32:17,010
 

1207
00:32:14,220 --> 00:32:18,270
 area okay so now we've got new

1208
00:32:17,000 --> 00:32:18,270
 

1209
00:32:17,010 --> 00:32:20,970
 algorithms about algorithms that are

1210
00:32:18,260 --> 00:32:20,970
 

1211
00:32:18,270 --> 00:32:23,820
 optimized for modern hardware and now

1212
00:32:20,960 --> 00:32:23,820
 

1213
00:32:20,970 --> 00:32:25,650
 we'd like to make those algorithms run

1214
00:32:23,810 --> 00:32:25,650
 

1215
00:32:23,820 --> 00:32:28,320
 for actually run faster modern hardware

1216
00:32:25,640 --> 00:32:28,320
 

1217
00:32:25,650 --> 00:32:30,690
 and we want to do so without having to

1218
00:32:28,310 --> 00:32:30,690
 

1219
00:32:28,320 --> 00:32:32,970
 understand all the details about what

1220
00:32:30,680 --> 00:32:32,970
 

1221
00:32:30,690 --> 00:32:34,440
 happens underneath with modern hardware

1222
00:32:32,960 --> 00:32:34,440
 

1223
00:32:32,970 --> 00:32:36,770
 and so that's is that this is the realm

1224
00:32:34,430 --> 00:32:36,770
 

1225
00:32:34,440 --> 00:32:39,990
 of domain-specific languages and

1226
00:32:36,760 --> 00:32:39,990
 

1227
00:32:36,770 --> 00:32:42,480
 sophisticated compilers so everybody is

1228
00:32:39,980 --> 00:32:42,480
 

1229
00:32:39,990 --> 00:32:45,120
 aware of domain-specific languages right

1230
00:32:42,470 --> 00:32:45,120
 

1231
00:32:42,480 --> 00:32:50,190
 so the idea is that I want to do a

1232
00:32:45,110 --> 00:32:50,190
 

1233
00:32:45,120 --> 00:32:52,140
 particular task and if I can write in a

1234
00:32:50,180 --> 00:32:52,140
 

1235
00:32:50,190 --> 00:32:54,690
 language that has operators and data

1236
00:32:52,130 --> 00:32:54,690
 

1237
00:32:52,140 --> 00:32:56,790
 types which are just well matched or

1238
00:32:54,680 --> 00:32:56,790
 

1239
00:32:54,690 --> 00:32:59,220
 tuned to that task then I can be a lot

1240
00:32:56,780 --> 00:32:59,220
 

1241
00:32:56,790 --> 00:33:01,730
 more productive okay it's so a big

1242
00:32:59,210 --> 00:33:01,730
 

1243
00:32:59,220 --> 00:33:03,900
 example of course of that in the area of

1244
00:33:01,720 --> 00:33:03,900
 

1245
00:33:01,730 --> 00:33:05,790
 matrix and linear algebra is MATLAB

1246
00:33:03,890 --> 00:33:05,790
 

1247
00:33:03,900 --> 00:33:08,340
 which you've all used and there are

1248
00:33:05,780 --> 00:33:08,340
 

1249
00:33:05,790 --> 00:33:11,220
 others like sequel in in the area of

1250
00:33:08,330 --> 00:33:11,220
 

1251
00:33:08,340 --> 00:33:13,080
 database processing so these languages

1252
00:33:11,210 --> 00:33:13,080
 

1253
00:33:11,220 --> 00:33:14,730
 are usually high-level and declarative

1254
00:33:13,070 --> 00:33:14,730
 

1255
00:33:13,080 --> 00:33:17,460
 they say we know what you should do

1256
00:33:14,720 --> 00:33:17,460
 

1257
00:33:14,730 --> 00:33:19,460
 rather than how to do it and you know

1258
00:33:17,450 --> 00:33:19,460
 

1259
00:33:17,460 --> 00:33:22,230
 traditionally they have been focused on

1260
00:33:19,450 --> 00:33:22,230
 

1261
00:33:19,460 --> 00:33:24,120
 you know raising the productivity of the

1262
00:33:22,220 --> 00:33:24,120
 

1263
00:33:22,230 --> 00:33:27,270
 application developer

1264
00:33:24,110 --> 00:33:27,270
 

1265
00:33:24,120 --> 00:33:29,040
 a few years ago we decided that hey the

1266
00:33:27,260 --> 00:33:29,040
 

1267
00:33:27,270 --> 00:33:30,450
 real one of the other benefits that you

1268
00:33:29,030 --> 00:33:30,450
 

1269
00:33:29,040 --> 00:33:33,110
 could get from the mazes of Atlanta jizz

1270
00:33:30,440 --> 00:33:33,110
 

1271
00:33:30,450 --> 00:33:35,280
 is as a driver for high performance

1272
00:33:33,100 --> 00:33:35,280
 

1273
00:33:33,110 --> 00:33:36,990
 implementations especially in

1274
00:33:35,270 --> 00:33:36,990
 

1275
00:33:35,280 --> 00:33:39,960
 heterogeneous computing environments

1276
00:33:36,980 --> 00:33:39,960
 

1277
00:33:36,990 --> 00:33:42,630
 composed of GPU CPUs classes and alike

1278
00:33:39,950 --> 00:33:42,630
 

1279
00:33:39,960 --> 00:33:45,690
 right so you'd like to program once and

1280
00:33:42,620 --> 00:33:45,690
 

1281
00:33:42,630 --> 00:33:48,540
 and optimize on all on multiple

1282
00:33:45,680 --> 00:33:48,540
 

1283
00:33:45,690 --> 00:33:51,410
 different platforms so we come in came

1284
00:33:48,530 --> 00:33:51,410
 

1285
00:33:48,540 --> 00:33:55,770
 up with a language that we called

1286
00:33:51,400 --> 00:33:55,770
 

1287
00:33:51,410 --> 00:33:58,500
 optimal for machine learning some some

1288
00:33:55,760 --> 00:33:58,500
 

1289
00:33:55,770 --> 00:34:02,250
 say look at it and say opti ml but it's

1290
00:33:58,490 --> 00:34:02,250
 

1291
00:33:58,500 --> 00:34:03,980
 actually optimal and and the idea is

1292
00:34:02,240 --> 00:34:03,980
 

1293
00:34:02,250 --> 00:34:05,880
 that this language was embedded in a

1294
00:34:03,970 --> 00:34:05,880
 

1295
00:34:03,980 --> 00:34:07,770
 general-purpose programming language

1296
00:34:05,870 --> 00:34:07,770
 

1297
00:34:05,880 --> 00:34:11,280
 called Scala right so this is just an

1298
00:34:07,760 --> 00:34:11,280
 

1299
00:34:07,770 --> 00:34:14,160
 example of k-means clustering so it's a

1300
00:34:11,270 --> 00:34:14,160
 

1301
00:34:11,280 --> 00:34:18,000
 few lines you calculate the distances to

1302
00:34:14,150 --> 00:34:18,000
 

1303
00:34:14,160 --> 00:34:19,950
 current means you assign you come up

1304
00:34:17,990 --> 00:34:19,950
 

1305
00:34:18,000 --> 00:34:24,270
 with with you you come up with clusters

1306
00:34:19,940 --> 00:34:24,270
 

1307
00:34:19,950 --> 00:34:27,420
 and then you you come up with with the

1308
00:34:24,260 --> 00:34:27,420
 

1309
00:34:24,270 --> 00:34:30,090
 means sort with new means by finding the

1310
00:34:27,410 --> 00:34:30,090
 

1311
00:34:27,420 --> 00:34:33,180
 centroids of the clusters and so the

1312
00:34:30,080 --> 00:34:33,180
 

1313
00:34:30,090 --> 00:34:35,280
 point is is that it's just a few lines

1314
00:34:33,170 --> 00:34:35,280
 

1315
00:34:33,180 --> 00:34:37,500
 of code there's no explicit parallelism

1316
00:34:35,270 --> 00:34:37,500
 

1317
00:34:35,280 --> 00:34:39,150
 there's no distributed data structures

1318
00:34:37,490 --> 00:34:39,150
 

1319
00:34:37,500 --> 00:34:41,340
 as you would see in a language and

1320
00:34:39,140 --> 00:34:41,340
 

1321
00:34:39,150 --> 00:34:43,820
 environment like spark there's nothing

1322
00:34:41,330 --> 00:34:43,820
 

1323
00:34:41,340 --> 00:34:46,260
 that says this is how you run on a GPU

1324
00:34:43,810 --> 00:34:46,260
 

1325
00:34:43,820 --> 00:34:49,070
 this is how you run on a cluster but yet

1326
00:34:46,250 --> 00:34:49,070
 

1327
00:34:46,260 --> 00:34:51,150
 from this description we can get very

1328
00:34:49,060 --> 00:34:51,150
 

1329
00:34:49,070 --> 00:34:53,310
 high-performance implementations on all

1330
00:34:51,140 --> 00:34:53,310
 

1331
00:34:51,150 --> 00:34:55,080
 of these platforms so fundamentally of

1332
00:34:53,300 --> 00:34:55,080
 

1333
00:34:53,310 --> 00:34:57,690
 course what we're going to get from this

1334
00:34:55,070 --> 00:34:57,690
 

1335
00:34:55,080 --> 00:34:59,000
 domain-specific language is a graph but

1336
00:34:57,680 --> 00:34:59,000
 

1337
00:34:57,690 --> 00:35:02,310
 we're going to create this graph

1338
00:34:58,990 --> 00:35:02,310
 

1339
00:34:59,000 --> 00:35:03,990
 implicitly right as opposed to tensor

1340
00:35:02,300 --> 00:35:03,990
 

1341
00:35:02,310 --> 00:35:05,700
 flow which fundamentally is the same

1342
00:35:03,980 --> 00:35:05,700
 

1343
00:35:03,990 --> 00:35:08,490
 sort of idea it's a domain-specific

1344
00:35:05,690 --> 00:35:08,490
 

1345
00:35:05,700 --> 00:35:10,230
 language for machine learning but here

1346
00:35:08,480 --> 00:35:10,230
 

1347
00:35:08,490 --> 00:35:13,170
 you're doing explicit graph construction

1348
00:35:10,220 --> 00:35:13,170
 

1349
00:35:10,230 --> 00:35:16,520
 so you know what we see for the same

1350
00:35:13,160 --> 00:35:16,520
 

1351
00:35:13,170 --> 00:35:19,650
 sort of algorithm is some slightly more

1352
00:35:16,510 --> 00:35:19,650
 

1353
00:35:16,520 --> 00:35:23,250
 longer description which looks somewhat

1354
00:35:19,640 --> 00:35:23,250
 

1355
00:35:19,650 --> 00:35:24,510
 more cryptic now you know there's

1356
00:35:23,240 --> 00:35:24,510
 

1357
00:35:23,250 --> 00:35:26,730
 nothing to stop you from kind of

1358
00:35:24,500 --> 00:35:26,730
 

1359
00:35:24,510 --> 00:35:30,240
 applying the approach that we took in

1360
00:35:26,720 --> 00:35:30,240
 

1361
00:35:26,730 --> 00:35:33,540
 optimal to tensor flow and in fact I

1362
00:35:30,230 --> 00:35:33,540
 

1363
00:35:30,240 --> 00:35:35,700
 think one of my collaborators is looking

1364
00:35:33,530 --> 00:35:35,700
 

1365
00:35:33,540 --> 00:35:36,230
 at doing that and so it is possible to

1366
00:35:35,690 --> 00:35:36,230
 

1367
00:35:35,700 --> 00:35:38,600
 do bad

1368
00:35:36,220 --> 00:35:38,600
 

1369
00:35:36,230 --> 00:35:41,030
 intend to flow by using some of the

1370
00:35:38,590 --> 00:35:41,030
 

1371
00:35:38,600 --> 00:35:42,470
 techniques that we use in optimal if

1372
00:35:41,020 --> 00:35:42,470
 

1373
00:35:41,030 --> 00:35:46,750
 you're interested in there might be glad

1374
00:35:42,460 --> 00:35:46,750
 

1375
00:35:42,470 --> 00:35:46,750
 to talk to you about them afterwards

1376
00:35:46,830 --> 00:35:46,830
 

1377
00:35:46,840 --> 00:35:53,420
 okay so what you'd like then is you know

1378
00:35:50,860 --> 00:35:53,420
 

1379
00:35:50,870 --> 00:35:57,020
 you'd like a your machine learning

1380
00:35:53,410 --> 00:35:57,020
 

1381
00:35:53,420 --> 00:36:02,990
 application composed of a bunch of DSL

1382
00:35:57,010 --> 00:36:02,990
 

1383
00:35:57,020 --> 00:36:05,120
 and you know as I said in environments

1384
00:36:02,980 --> 00:36:05,120
 

1385
00:36:02,990 --> 00:36:07,430
 like snorkel you would need to combine

1386
00:36:05,110 --> 00:36:07,430
 

1387
00:36:05,120 --> 00:36:11,690
 some sequel processing with machine

1388
00:36:07,420 --> 00:36:11,690
 

1389
00:36:07,430 --> 00:36:12,650
 learning processing and what that would

1390
00:36:11,680 --> 00:36:12,650
 

1391
00:36:11,690 --> 00:36:14,900
 mean is that you would have an

1392
00:36:12,640 --> 00:36:14,900
 

1393
00:36:12,650 --> 00:36:16,550
 application that had multiple DSL and

1394
00:36:14,890 --> 00:36:16,550
 

1395
00:36:14,900 --> 00:36:20,090
 then the question is how do you optimize

1396
00:36:16,540 --> 00:36:20,090
 

1397
00:36:16,550 --> 00:36:22,760
 your application together well that

1398
00:36:20,080 --> 00:36:22,760
 

1399
00:36:20,090 --> 00:36:24,200
 requires that you convert all of the DSL

1400
00:36:22,750 --> 00:36:24,200
 

1401
00:36:22,760 --> 00:36:26,720
 x' into a common intermediate

1402
00:36:24,190 --> 00:36:26,720
 

1403
00:36:24,200 --> 00:36:30,220
 representation so that's what we did and

1404
00:36:26,710 --> 00:36:30,220
 

1405
00:36:26,720 --> 00:36:33,140
 we came up with a common intermediate

1406
00:36:30,210 --> 00:36:33,140
 

1407
00:36:30,220 --> 00:36:34,850
 representation based on what we call

1408
00:36:33,130 --> 00:36:34,850
 

1409
00:36:33,140 --> 00:36:36,320
 parallel patterns or what people call

1410
00:36:34,840 --> 00:36:36,320
 

1411
00:36:34,850 --> 00:36:40,150
 parallel patterns right so these are

1412
00:36:36,310 --> 00:36:40,150
 

1413
00:36:36,320 --> 00:36:42,170
 basically functional data parallel

1414
00:36:40,140 --> 00:36:42,170
 

1415
00:36:40,150 --> 00:36:44,560
 transformations on collections the

1416
00:36:42,160 --> 00:36:44,560
 

1417
00:36:42,170 --> 00:36:48,440
 collections could be sets arrays tables

1418
00:36:44,550 --> 00:36:48,440
 

1419
00:36:44,560 --> 00:36:50,960
 and dimensional matrices and here are

1420
00:36:48,430 --> 00:36:50,960
 

1421
00:36:48,440 --> 00:36:53,510
 some patterns listed here a map which

1422
00:36:50,950 --> 00:36:53,510
 

1423
00:36:50,960 --> 00:36:58,280
 you're familiar with zip reduced group

1424
00:36:53,500 --> 00:36:58,280
 

1425
00:36:53,510 --> 00:37:04,670
 flat map and group by and so here is the

1426
00:36:58,270 --> 00:37:04,670
 

1427
00:36:58,280 --> 00:37:08,720
 representation of k-means using our

1428
00:37:04,660 --> 00:37:08,720
 

1429
00:37:04,670 --> 00:37:10,760
 Prowler pattern representation and so

1430
00:37:08,710 --> 00:37:10,760
 

1431
00:37:08,720 --> 00:37:12,470
 one thing to notice is is of course it's

1432
00:37:10,750 --> 00:37:12,470
 

1433
00:37:10,760 --> 00:37:14,810
 more lines of code because this is a

1434
00:37:12,460 --> 00:37:14,810
 

1435
00:37:12,470 --> 00:37:18,710
 lower level of abstraction the other

1436
00:37:14,800 --> 00:37:18,710
 

1437
00:37:14,810 --> 00:37:21,170
 thing to notice is that the patterns

1438
00:37:18,700 --> 00:37:21,170
 

1439
00:37:18,710 --> 00:37:24,110
 which are shown in blue are actually

1440
00:37:21,160 --> 00:37:24,110
 

1441
00:37:21,170 --> 00:37:26,000
 nested so a flat representation like Map

1442
00:37:24,100 --> 00:37:26,000
 

1443
00:37:24,110 --> 00:37:30,410
 Reduce won't do the trick you need

1444
00:37:25,990 --> 00:37:30,410
 

1445
00:37:26,000 --> 00:37:34,280
 nesting in order to capture the to be

1446
00:37:30,400 --> 00:37:34,280
 

1447
00:37:30,410 --> 00:37:35,210
 expressive enough to capture the what

1448
00:37:34,270 --> 00:37:35,210
 

1449
00:37:34,280 --> 00:37:37,700
 you want for machine learning

1450
00:37:35,200 --> 00:37:37,700
 

1451
00:37:35,210 --> 00:37:39,260
 applications okay so once you've got a

1452
00:37:37,690 --> 00:37:39,260
 

1453
00:37:37,700 --> 00:37:40,190
 representation like parallel patterns

1454
00:37:39,250 --> 00:37:40,190
 

1455
00:37:39,260 --> 00:37:42,920
 you can do a lot of number of

1456
00:37:40,180 --> 00:37:42,920
 

1457
00:37:40,190 --> 00:37:44,630
 optimizations you can optimize locality

1458
00:37:42,910 --> 00:37:44,630
 

1459
00:37:42,920 --> 00:37:47,720
 those of you who know about parallelism

1460
00:37:44,620 --> 00:37:47,720
 

1461
00:37:44,630 --> 00:37:49,790
 know that figuring out how to use local

1462
00:37:47,710 --> 00:37:49,790
 

1463
00:37:47,720 --> 00:37:51,980
 memory efficiently is key

1464
00:37:49,780 --> 00:37:51,980
 

1465
00:37:49,790 --> 00:37:53,300
 performance right so you need to be able

1466
00:37:51,970 --> 00:37:53,300
 

1467
00:37:51,980 --> 00:37:56,090
 to do that and you need to be able to do

1468
00:37:53,290 --> 00:37:56,090
 

1469
00:37:53,300 --> 00:37:58,910
 that automatically and of course this is

1470
00:37:56,080 --> 00:37:58,910
 

1471
00:37:56,090 --> 00:38:00,980
 an area of work that compiler or people

1472
00:37:58,900 --> 00:38:00,980
 

1473
00:37:58,910 --> 00:38:02,930
 have worked on for a long time by

1474
00:38:00,970 --> 00:38:02,930
 

1475
00:38:00,980 --> 00:38:05,180
 looking at Luke nests and optimizing

1476
00:38:02,920 --> 00:38:05,180
 

1477
00:38:02,930 --> 00:38:07,310
 them turns out but that the techniques

1478
00:38:05,170 --> 00:38:07,310
 

1479
00:38:05,180 --> 00:38:09,440
 that have been pioneered traditionally

1480
00:38:07,300 --> 00:38:09,440
 

1481
00:38:07,310 --> 00:38:11,870
 been pioneered in the compiler community

1482
00:38:09,430 --> 00:38:11,870
 

1483
00:38:09,440 --> 00:38:13,790
 don't really work on power patents turns

1484
00:38:11,860 --> 00:38:13,790
 

1485
00:38:11,870 --> 00:38:16,550
 out parallel patents are a little more

1486
00:38:13,780 --> 00:38:16,550
 

1487
00:38:13,790 --> 00:38:20,750
 complex they're not what we call a fine

1488
00:38:16,540 --> 00:38:20,750
 

1489
00:38:16,550 --> 00:38:23,540
 loops they've got to flow control in

1490
00:38:20,740 --> 00:38:23,540
 

1491
00:38:20,750 --> 00:38:25,130
 there you know so turns out that you

1492
00:38:23,530 --> 00:38:25,130
 

1493
00:38:23,540 --> 00:38:27,530
 need different techniques but you can

1494
00:38:25,120 --> 00:38:27,530
 

1495
00:38:25,130 --> 00:38:30,170
 make things work and do the sort of

1496
00:38:27,520 --> 00:38:30,170
 

1497
00:38:27,530 --> 00:38:33,080
 optimization you need for local memory

1498
00:38:30,160 --> 00:38:33,080
 

1499
00:38:30,170 --> 00:38:35,570
 the techniques are called tiling and of

1500
00:38:33,070 --> 00:38:35,570
 

1501
00:38:33,080 --> 00:38:37,880
 course fusion and fusion is this really

1502
00:38:35,560 --> 00:38:37,880
 

1503
00:38:35,570 --> 00:38:40,700
 powerful technique that can get rid of a

1504
00:38:37,870 --> 00:38:40,700
 

1505
00:38:37,880 --> 00:38:42,680
 lot of intermediate memory the other

1506
00:38:40,690 --> 00:38:42,680
 

1507
00:38:40,700 --> 00:38:44,780
 thing you want to do is create as much

1508
00:38:42,670 --> 00:38:44,780
 

1509
00:38:42,680 --> 00:38:46,400
 parallelism as possible because you know

1510
00:38:44,770 --> 00:38:46,400
 

1511
00:38:44,780 --> 00:38:48,680
 you might need it right you might have

1512
00:38:46,390 --> 00:38:48,680
 

1513
00:38:46,400 --> 00:38:50,840
 some accelerator or some hardware that

1514
00:38:48,670 --> 00:38:50,840
 

1515
00:38:48,680 --> 00:38:52,520
 can use all this parallelism so you want

1516
00:38:50,830 --> 00:38:52,520
 

1517
00:38:50,840 --> 00:38:54,440
 to be able to generate a lot of

1518
00:38:52,510 --> 00:38:54,440
 

1519
00:38:52,520 --> 00:38:56,200
 parallelism and you might want to be

1520
00:38:54,430 --> 00:38:56,200
 

1521
00:38:54,440 --> 00:38:59,600
 able to generate this parallelism in

1522
00:38:56,190 --> 00:38:59,600
 

1523
00:38:56,200 --> 00:39:02,720
 instances where the components of your

1524
00:38:59,590 --> 00:39:02,720
 

1525
00:38:59,600 --> 00:39:04,070
 program have dependencies and the way to

1526
00:39:02,710 --> 00:39:04,070
 

1527
00:39:02,720 --> 00:39:07,490
 do this is using a technique called

1528
00:39:04,060 --> 00:39:07,490
 

1529
00:39:04,070 --> 00:39:10,700
 pipelining so pipelining is is a really

1530
00:39:07,480 --> 00:39:10,700
 

1531
00:39:07,490 --> 00:39:12,040
 important technique and so pipelining is

1532
00:39:10,690 --> 00:39:12,040
 

1533
00:39:10,700 --> 00:39:14,420
 a technique that we've used and

1534
00:39:12,030 --> 00:39:14,420
 

1535
00:39:12,040 --> 00:39:18,170
 basically we show that that using

1536
00:39:14,410 --> 00:39:18,170
 

1537
00:39:14,420 --> 00:39:19,790
 pipelining we can using these techniques

1538
00:39:18,160 --> 00:39:19,790
 

1539
00:39:18,170 --> 00:39:21,590
 we can get dramatic improvements in

1540
00:39:19,780 --> 00:39:21,590
 

1541
00:39:19,790 --> 00:39:23,110
 performance this is one example is

1542
00:39:21,580 --> 00:39:23,110
 

1543
00:39:21,590 --> 00:39:31,150
 working with a colleague of mine

1544
00:39:23,100 --> 00:39:31,150
 

1545
00:39:23,110 --> 00:39:31,150
 Vijay Pandey and he he was looking at

1546
00:39:31,860 --> 00:39:31,860
 

1547
00:39:31,870 --> 00:39:36,590
 folding protein folding and using a

1548
00:39:35,110 --> 00:39:36,590
 

1549
00:39:35,120 --> 00:39:38,570
 technique called Markov state models

1550
00:39:36,580 --> 00:39:38,570
 

1551
00:39:36,590 --> 00:39:40,490
 originally they wrote it in Python it

1552
00:39:38,560 --> 00:39:40,490
 

1553
00:39:38,570 --> 00:39:42,830
 was way too slow so they got a CS

1554
00:39:40,480 --> 00:39:42,830
 

1555
00:39:40,490 --> 00:39:47,470
 undergrad to take that Python code and

1556
00:39:42,820 --> 00:39:47,470
 

1557
00:39:42,830 --> 00:39:50,570
 convert it to C++ and then a s assembly

1558
00:39:47,460 --> 00:39:50,570
 

1559
00:39:47,470 --> 00:39:53,420
 x86 assembly language and it sped up by

1560
00:39:50,560 --> 00:39:53,420
 

1561
00:39:50,570 --> 00:39:55,130
 1,500 times then that student left for

1562
00:39:53,410 --> 00:39:55,130
 

1563
00:39:53,420 --> 00:39:57,260
 Google and they didn't know what to do

1564
00:39:55,120 --> 00:39:57,260
 

1565
00:39:55,130 --> 00:39:58,340
 with the code right and so what we did

1566
00:39:57,250 --> 00:39:58,340
 

1567
00:39:57,260 --> 00:40:00,620
 is we came along we took the original

1568
00:39:58,330 --> 00:40:00,620
 

1569
00:39:58,340 --> 00:40:01,460
 Python code converted it to optimal

1570
00:40:00,610 --> 00:40:01,460
 

1571
00:40:00,620 --> 00:40:03,619
 optimal

1572
00:40:01,450 --> 00:40:03,619
 

1573
00:40:01,460 --> 00:40:08,960
 and we were able to get the same

1574
00:40:03,609 --> 00:40:08,960
 

1575
00:40:03,619 --> 00:40:10,880
 performance on a Intel processor as the

1576
00:40:08,950 --> 00:40:10,880
 

1577
00:40:08,960 --> 00:40:13,609
 hand coded result and then we were able

1578
00:40:10,870 --> 00:40:13,609
 

1579
00:40:10,880 --> 00:40:16,190
 to take that same optimal code and run

1580
00:40:13,599 --> 00:40:16,190
 

1581
00:40:13,609 --> 00:40:18,530
 it and generate a GPU code and get

1582
00:40:16,180 --> 00:40:18,530
 

1583
00:40:16,190 --> 00:40:20,260
 another two and a half times performance

1584
00:40:18,520 --> 00:40:20,260
 

1585
00:40:18,530 --> 00:40:23,050
 improvement

1586
00:40:20,250 --> 00:40:23,050
 

1587
00:40:20,260 --> 00:40:26,740
 okay so we have sophisticated

1588
00:40:23,040 --> 00:40:26,740
 

1589
00:40:23,050 --> 00:40:29,839
 compilation approach that can generate

1590
00:40:26,730 --> 00:40:29,839
 

1591
00:40:26,740 --> 00:40:33,710
 code for instruction set processes what

1592
00:40:29,829 --> 00:40:33,710
 

1593
00:40:29,839 --> 00:40:36,589
 about hardware acceleration so today the

1594
00:40:33,700 --> 00:40:36,589
 

1595
00:40:33,710 --> 00:40:38,390
 current approaches are CPU which uses

1596
00:40:36,579 --> 00:40:38,390
 

1597
00:40:36,589 --> 00:40:41,450
 thread and specialized sim D

1598
00:40:38,380 --> 00:40:41,450
 

1599
00:40:38,390 --> 00:40:44,270
 instructions GPUs which uses a massive

1600
00:40:41,440 --> 00:40:44,270
 

1601
00:40:41,450 --> 00:40:46,339
 number of threads specialize in the

1602
00:40:44,260 --> 00:40:46,339
 

1603
00:40:44,270 --> 00:40:50,780
 instructions and of course specialized

1604
00:40:46,329 --> 00:40:50,780
 

1605
00:40:46,339 --> 00:40:54,920
 memories and an example of TPU which

1606
00:40:50,770 --> 00:40:54,920
 

1607
00:40:50,780 --> 00:40:58,069
 uses a big multiply unit matrix multiply

1608
00:40:54,910 --> 00:40:58,069
 

1609
00:40:54,920 --> 00:41:00,440
 unit and software manage cache and the

1610
00:40:58,059 --> 00:41:00,440
 

1611
00:40:58,069 --> 00:41:03,260
 question is what's next right so what do

1612
00:41:00,430 --> 00:41:03,260
 

1613
00:41:00,440 --> 00:41:05,599
 we need to do to get you know 10 to 100x

1614
00:41:03,250 --> 00:41:05,599
 

1615
00:41:03,260 --> 00:41:07,750
 improvement over these accelerators well

1616
00:41:05,589 --> 00:41:07,750
 

1617
00:41:05,599 --> 00:41:10,640
 you could do something very specialized

1618
00:41:07,740 --> 00:41:10,640
 

1619
00:41:07,750 --> 00:41:13,880
 but the problem is is there a lot of

1620
00:41:10,630 --> 00:41:13,880
 

1621
00:41:10,640 --> 00:41:16,339
 ideas right so this graph from Jeff Dean

1622
00:41:13,870 --> 00:41:16,339
 

1623
00:41:13,880 --> 00:41:19,250
 shows that the number of machine

1624
00:41:16,329 --> 00:41:19,250
 

1625
00:41:16,339 --> 00:41:22,309
 learning papers on archive is outpacing

1626
00:41:19,240 --> 00:41:22,309
 

1627
00:41:19,250 --> 00:41:25,099
 Moore's law so more than doubling every

1628
00:41:22,299 --> 00:41:25,099
 

1629
00:41:22,309 --> 00:41:26,390
 two years and you know somewhere in

1630
00:41:25,089 --> 00:41:26,390
 

1631
00:41:25,099 --> 00:41:28,520
 there you know probably there are lots

1632
00:41:26,380 --> 00:41:28,520
 

1633
00:41:26,390 --> 00:41:32,420
 of good ideas but you don't know exactly

1634
00:41:28,510 --> 00:41:32,420
 

1635
00:41:28,520 --> 00:41:34,250
 which ones and so figuring out how to to

1636
00:41:32,410 --> 00:41:34,250
 

1637
00:41:32,420 --> 00:41:35,960
 translate those ideas into a hardware

1638
00:41:34,240 --> 00:41:35,960
 

1639
00:41:34,250 --> 00:41:38,900
 implementation will take you two years

1640
00:41:35,950 --> 00:41:38,900
 

1641
00:41:35,960 --> 00:41:41,420
 and by that time there'll be a thousand

1642
00:41:38,890 --> 00:41:41,420
 

1643
00:41:38,900 --> 00:41:43,910
 new ideas right so this is not really an

1644
00:41:41,410 --> 00:41:43,910
 

1645
00:41:41,420 --> 00:41:46,369
 approach of translating papers directly

1646
00:41:43,900 --> 00:41:46,369
 

1647
00:41:43,910 --> 00:41:48,470
 into hardware and so if you kind of look

1648
00:41:46,359 --> 00:41:48,470
 

1649
00:41:46,369 --> 00:41:50,809
 into your crystal ball about the future

1650
00:41:48,460 --> 00:41:50,809
 

1651
00:41:48,470 --> 00:41:52,640
 of machine learning algorithms you know

1652
00:41:50,799 --> 00:41:52,640
 

1653
00:41:50,809 --> 00:41:56,540
 it may be clear or it might might be

1654
00:41:52,630 --> 00:41:56,540
 

1655
00:41:52,640 --> 00:41:58,130
 cloudy but it's not you're gonna you're

1656
00:41:56,530 --> 00:41:58,130
 

1657
00:41:56,540 --> 00:42:00,380
 not gonna know exactly what's gonna come

1658
00:41:58,120 --> 00:42:00,380
 

1659
00:41:58,130 --> 00:42:01,730
 down the pike but we know that there are

1660
00:42:00,370 --> 00:42:01,730
 

1661
00:42:00,380 --> 00:42:03,890
 a number of things that that look

1662
00:42:01,720 --> 00:42:03,890
 

1663
00:42:01,730 --> 00:42:06,559
 interesting first of all this whole idea

1664
00:42:03,880 --> 00:42:06,559
 

1665
00:42:03,890 --> 00:42:09,760
 of hierarchical parallel patterns seems

1666
00:42:06,549 --> 00:42:09,760
 

1667
00:42:06,559 --> 00:42:12,230
 to be very central we can cut we can

1668
00:42:09,750 --> 00:42:12,230
 

1669
00:42:09,760 --> 00:42:14,560
 express a lot all of machine learning

1670
00:42:12,220 --> 00:42:14,560
 

1671
00:42:12,230 --> 00:42:17,170
 and and all of data analytics you

1672
00:42:14,550 --> 00:42:17,170
 

1673
00:42:14,560 --> 00:42:20,200
 these Prowler pans dynamic precision

1674
00:42:17,160 --> 00:42:20,200
 

1675
00:42:17,170 --> 00:42:23,140
 seems to be an important idea for

1676
00:42:20,190 --> 00:42:23,140
 

1677
00:42:20,200 --> 00:42:24,910
 supporting algorithms like help sparsity

1678
00:42:23,130 --> 00:42:24,910
 

1679
00:42:23,140 --> 00:42:27,640
 is going to be very important because of

1680
00:42:24,900 --> 00:42:27,640
 

1681
00:42:24,910 --> 00:42:30,550
 course we're going to want to reduce the

1682
00:42:27,630 --> 00:42:30,550
 

1683
00:42:27,640 --> 00:42:33,370
 amount of computation we do and then as

1684
00:42:30,540 --> 00:42:33,370
 

1685
00:42:30,550 --> 00:42:36,280
 I said data processing in the inner loop

1686
00:42:33,360 --> 00:42:36,280
 

1687
00:42:33,370 --> 00:42:38,560
 of MLL training means that that will

1688
00:42:36,270 --> 00:42:38,560
 

1689
00:42:36,280 --> 00:42:41,110
 also be important the other thing to

1690
00:42:38,550 --> 00:42:41,110
 

1691
00:42:38,560 --> 00:42:43,540
 note is that I think the current

1692
00:42:41,100 --> 00:42:43,540
 

1693
00:42:41,110 --> 00:42:45,910
 interface to hardware accelerators is

1694
00:42:43,530 --> 00:42:45,910
 

1695
00:42:43,540 --> 00:42:47,530
 too narrow and too restrictive right

1696
00:42:45,900 --> 00:42:47,530
 

1697
00:42:45,910 --> 00:42:49,360
 traditionally you think of parallel

1698
00:42:47,520 --> 00:42:49,360
 

1699
00:42:47,530 --> 00:42:51,400
 programming models some interface also

1700
00:42:49,350 --> 00:42:51,400
 

1701
00:42:49,360 --> 00:42:54,390
 some programming model some interface

1702
00:42:51,390 --> 00:42:54,390
 

1703
00:42:51,400 --> 00:42:56,950
 too and then hardware so traditional

1704
00:42:54,380 --> 00:42:56,950
 

1705
00:42:54,390 --> 00:43:00,220
 interfaces of course an instruction set

1706
00:42:56,940 --> 00:43:00,220
 

1707
00:42:56,950 --> 00:43:02,860
 architecture like x86 or arm so you

1708
00:43:00,210 --> 00:43:02,860
 

1709
00:43:00,220 --> 00:43:04,840
 could start with C++ go through an

1710
00:43:02,850 --> 00:43:04,840
 

1711
00:43:02,860 --> 00:43:07,180
 instruction set architecture and that

1712
00:43:04,830 --> 00:43:07,180
 

1713
00:43:04,840 --> 00:43:10,090
 would be implemented by CPU if we're

1714
00:43:07,170 --> 00:43:10,090
 

1715
00:43:07,180 --> 00:43:13,450
 using GPUs you could start with CUDA go

1716
00:43:10,080 --> 00:43:13,450
 

1717
00:43:10,090 --> 00:43:14,920
 through PT X and end up on a GPU the

1718
00:43:13,440 --> 00:43:14,920
 

1719
00:43:13,450 --> 00:43:17,590
 problem with the instruction set

1720
00:43:14,910 --> 00:43:17,590
 

1721
00:43:14,920 --> 00:43:19,480
 architectures are that you have a fixed

1722
00:43:17,580 --> 00:43:19,480
 

1723
00:43:17,590 --> 00:43:21,820
 set of operations and they're usually

1724
00:43:19,470 --> 00:43:21,820
 

1725
00:43:19,480 --> 00:43:23,410
 defined at the low-level and serve you

1726
00:43:21,810 --> 00:43:23,410
 

1727
00:43:21,820 --> 00:43:24,780
 want more interesting things you've got

1728
00:43:23,400 --> 00:43:24,780
 

1729
00:43:23,410 --> 00:43:27,220
 to combine them together and

1730
00:43:24,770 --> 00:43:27,220
 

1731
00:43:24,780 --> 00:43:29,140
 fundamentally executing instructions is

1732
00:43:27,210 --> 00:43:29,140
 

1733
00:43:27,220 --> 00:43:31,270
 inefficient got all kinds of overhead

1734
00:43:29,130 --> 00:43:31,270
 

1735
00:43:29,140 --> 00:43:33,310
 and we said efficiency is really the key

1736
00:43:31,260 --> 00:43:33,310
 

1737
00:43:31,270 --> 00:43:34,810
 going forward because we're really power

1738
00:43:33,300 --> 00:43:34,810
 

1739
00:43:33,310 --> 00:43:37,690
 limited so we want to make things as

1740
00:43:34,800 --> 00:43:37,690
 

1741
00:43:34,810 --> 00:43:39,880
 efficient as possible and so the

1742
00:43:37,680 --> 00:43:39,880
 

1743
00:43:37,690 --> 00:43:41,440
 approach that we would advocate is

1744
00:43:39,870 --> 00:43:41,440
 

1745
00:43:39,880 --> 00:43:43,870
 something which is much closer to this

1746
00:43:41,430 --> 00:43:43,870
 

1747
00:43:41,440 --> 00:43:46,480
 fundamental programming model that I

1748
00:43:43,860 --> 00:43:46,480
 

1749
00:43:43,870 --> 00:43:48,100
 said underlies all of machine learning

1750
00:43:46,470 --> 00:43:48,100
 

1751
00:43:46,480 --> 00:43:52,440
 and data analytics this idea of

1752
00:43:48,090 --> 00:43:52,440
 

1753
00:43:48,100 --> 00:43:55,480
 hierarchical parallel pans and the

1754
00:43:52,430 --> 00:43:55,480
 

1755
00:43:52,440 --> 00:43:58,780
 interface is what we're going to call

1756
00:43:55,470 --> 00:43:58,780
 

1757
00:43:55,480 --> 00:44:03,610
 hierarchical coarse-grained dataflow and

1758
00:43:58,770 --> 00:44:03,610
 

1759
00:43:58,780 --> 00:44:05,500
 the idea is that it's much closer to the

1760
00:44:03,600 --> 00:44:05,500
 

1761
00:44:03,610 --> 00:44:08,280
 algorithms and allows you a much wider

1762
00:44:05,490 --> 00:44:08,280
 

1763
00:44:05,500 --> 00:44:13,030
 interface between the programming model

1764
00:44:08,270 --> 00:44:13,030
 

1765
00:44:08,280 --> 00:44:19,630
 and the hardware so we have a

1766
00:44:13,020 --> 00:44:19,630
 

1767
00:44:13,030 --> 00:44:22,990
 representation of this interface and

1768
00:44:19,620 --> 00:44:22,990
 

1769
00:44:19,630 --> 00:44:25,450
 we're calling it spatial and it's a open

1770
00:44:22,980 --> 00:44:25,450
 

1771
00:44:22,990 --> 00:44:26,690
 source language it represents the

1772
00:44:25,440 --> 00:44:26,690
 

1773
00:44:25,450 --> 00:44:29,810
 prowler patterns

1774
00:44:26,680 --> 00:44:29,810
 

1775
00:44:26,690 --> 00:44:33,380
 as a hierarchy of pipeline data types

1776
00:44:29,800 --> 00:44:33,380
 

1777
00:44:29,810 --> 00:44:35,690
 data paths and it has an explicit memory

1778
00:44:33,370 --> 00:44:35,690
 

1779
00:44:33,380 --> 00:44:37,970
 hierarchy so you can control with

1780
00:44:35,680 --> 00:44:37,970
 

1781
00:44:35,690 --> 00:44:40,810
 software how the memory gets moved

1782
00:44:37,960 --> 00:44:40,810
 

1783
00:44:37,970 --> 00:44:42,980
 between the different memories in your

1784
00:44:40,800 --> 00:44:42,980
 

1785
00:44:40,810 --> 00:44:44,900
 accelerator your hardware accelerator

1786
00:44:42,970 --> 00:44:44,900
 

1787
00:44:42,980 --> 00:44:48,200
 just to show you a really quick example

1788
00:44:44,890 --> 00:44:48,200
 

1789
00:44:44,900 --> 00:44:50,180
 of spatial suppose we want to do dot

1790
00:44:48,190 --> 00:44:50,180
 

1791
00:44:48,200 --> 00:44:52,160
 product of course the core of every

1792
00:44:50,170 --> 00:44:52,160
 

1793
00:44:50,180 --> 00:44:54,829
 machine learn learning algorithm is dot

1794
00:44:52,150 --> 00:44:54,829
 

1795
00:44:52,160 --> 00:44:58,040
 product right and so here's dot product

1796
00:44:54,819 --> 00:44:58,040
 

1797
00:44:54,829 --> 00:45:01,069
 with using power patents it's just a zip

1798
00:44:58,030 --> 00:45:01,069
 

1799
00:44:58,040 --> 00:45:05,650
 and a reduce if you tile it then of

1800
00:45:01,059 --> 00:45:05,650
 

1801
00:45:01,069 --> 00:45:05,650
 course you're gonna get a two-level

1802
00:45:06,000 --> 00:45:06,000
 

1803
00:45:06,010 --> 00:45:11,119
 representation where you have a reduce

1804
00:45:08,319 --> 00:45:11,119
 

1805
00:45:08,329 --> 00:45:14,900
 and inside the reduce is a map and a

1806
00:45:11,109 --> 00:45:14,900
 

1807
00:45:11,119 --> 00:45:16,640
 reduce if you look at the spatial code

1808
00:45:14,890 --> 00:45:16,640
 

1809
00:45:14,900 --> 00:45:22,400
 what will happen is you're going to

1810
00:45:16,630 --> 00:45:22,400
 

1811
00:45:16,640 --> 00:45:25,339
 declare two vectors in DRAM and you

1812
00:45:22,390 --> 00:45:25,339
 

1813
00:45:22,400 --> 00:45:27,109
 declare two tiles in SRAM and then

1814
00:45:25,329 --> 00:45:27,109
 

1815
00:45:25,339 --> 00:45:31,040
 you're going to move a piece or a block

1816
00:45:27,099 --> 00:45:31,040
 

1817
00:45:27,109 --> 00:45:34,010
 of the DRAM into one of the tiles tile a

1818
00:45:31,030 --> 00:45:34,010
 

1819
00:45:31,040 --> 00:45:37,940
 and then a block of the second vector

1820
00:45:34,000 --> 00:45:37,940
 

1821
00:45:34,010 --> 00:45:40,010
 vector B into tile B and then you're

1822
00:45:37,930 --> 00:45:40,010
 

1823
00:45:37,940 --> 00:45:42,290
 going to do a reduction within the tile

1824
00:45:40,000 --> 00:45:42,290
 

1825
00:45:40,010 --> 00:45:44,270
 and then you're going to do for in the

1826
00:45:42,280 --> 00:45:44,270
 

1827
00:45:42,290 --> 00:45:46,910
 outer reduce or a reduction across the

1828
00:45:44,260 --> 00:45:46,910
 

1829
00:45:44,270 --> 00:45:49,250
 tiles okay so this two-level reduce

1830
00:45:46,900 --> 00:45:49,250
 

1831
00:45:46,910 --> 00:45:53,560
 turns out that the hardware you get

1832
00:45:49,240 --> 00:45:53,560
 

1833
00:45:49,250 --> 00:45:58,010
 looks somewhat like this right and if

1834
00:45:53,550 --> 00:45:58,010
 

1835
00:45:53,560 --> 00:46:00,200
 you convert that into hardware then

1836
00:45:58,000 --> 00:46:00,200
 

1837
00:45:58,010 --> 00:46:02,720
 you've got lots of design parameters

1838
00:46:00,190 --> 00:46:02,720
 

1839
00:46:00,200 --> 00:46:05,060
 that you can play with the sizes of the

1840
00:46:02,710 --> 00:46:05,060
 

1841
00:46:02,720 --> 00:46:08,030
 buffers the amount of banking in the

1842
00:46:05,050 --> 00:46:08,030
 

1843
00:46:05,060 --> 00:46:10,780
 buffers the amount of parallelism and

1844
00:46:08,020 --> 00:46:10,780
 

1845
00:46:08,030 --> 00:46:13,609
 pipelining you do in the outer reduce

1846
00:46:10,770 --> 00:46:13,609
 

1847
00:46:10,780 --> 00:46:16,250
 the amount of things you fetch at the

1848
00:46:13,599 --> 00:46:16,250
 

1849
00:46:13,609 --> 00:46:18,500
 same time from the DRAM into the SRAM

1850
00:46:16,240 --> 00:46:18,500
 

1851
00:46:16,250 --> 00:46:22,640
 the amount of parallelism you exploit

1852
00:46:18,490 --> 00:46:22,640
 

1853
00:46:18,500 --> 00:46:24,770
 when doing the the inner reduce and what

1854
00:46:22,630 --> 00:46:24,770
 

1855
00:46:22,640 --> 00:46:26,960
 you want is a environment that can

1856
00:46:24,760 --> 00:46:26,960
 

1857
00:46:24,770 --> 00:46:28,970
 explore the design space and come up

1858
00:46:26,950 --> 00:46:28,970
 

1859
00:46:26,960 --> 00:46:31,310
 with something that's optimal for the

1860
00:46:28,960 --> 00:46:31,310
 

1861
00:46:28,970 --> 00:46:34,069
 particular hardware target that you are

1862
00:46:31,300 --> 00:46:34,069
 

1863
00:46:31,310 --> 00:46:37,069
 after so now if you're looking at the

1864
00:46:34,059 --> 00:46:37,069
 

1865
00:46:34,069 --> 00:46:40,580
 compiler pile or architecture as I said

1866
00:46:37,059 --> 00:46:40,580
 

1867
00:46:37,069 --> 00:46:42,290
 you start with high-level dsls

1868
00:46:40,570 --> 00:46:42,290
 

1869
00:46:40,580 --> 00:46:43,730
 you use a common intermediate

1870
00:46:42,280 --> 00:46:43,730
 

1871
00:46:42,290 --> 00:46:45,560
 representation that can allow you to

1872
00:46:43,720 --> 00:46:45,560
 

1873
00:46:43,730 --> 00:46:49,250
 optimize across the boundaries of these

1874
00:46:45,550 --> 00:46:49,250
 

1875
00:46:45,560 --> 00:46:52,040
 dsls you optimize the locality and

1876
00:46:49,240 --> 00:46:52,040
 

1877
00:46:49,250 --> 00:46:54,680
 parallelism using parallel patterns you

1878
00:46:52,030 --> 00:46:54,680
 

1879
00:46:52,040 --> 00:46:56,870
 generate spatial which is wide interface

1880
00:46:54,670 --> 00:46:56,870
 

1881
00:46:54,680 --> 00:46:57,950
 to hardware accelerators and then of

1882
00:46:56,860 --> 00:46:57,950
 

1883
00:46:56,870 --> 00:47:00,140
 course you need to map it to an

1884
00:46:57,940 --> 00:47:00,140
 

1885
00:46:57,950 --> 00:47:04,480
 accelerator so here's an example it's

1886
00:47:00,130 --> 00:47:04,480
 

1887
00:47:00,140 --> 00:47:07,700
 one called plasticine so we're in Canada

1888
00:47:04,470 --> 00:47:07,700
 

1889
00:47:04,480 --> 00:47:08,900
 so Canadians know what plasticine is I

1890
00:47:07,690 --> 00:47:08,900
 

1891
00:47:07,700 --> 00:47:11,120
 grew up in London so I know what

1892
00:47:08,890 --> 00:47:11,120
 

1893
00:47:08,900 --> 00:47:14,000
 plasticine is Americans don't but it's

1894
00:47:11,110 --> 00:47:14,000
 

1895
00:47:11,120 --> 00:47:15,860
 not like play-doh the key thing that's

1896
00:47:13,990 --> 00:47:15,860
 

1897
00:47:14,000 --> 00:47:18,590
 different about it is it never hardens

1898
00:47:15,850 --> 00:47:18,590
 

1899
00:47:15,860 --> 00:47:22,880
 right so it's always pliable so that's

1900
00:47:18,580 --> 00:47:22,880
 

1901
00:47:18,590 --> 00:47:26,180
 plasticine so unlike so plasticine then

1902
00:47:22,870 --> 00:47:26,180
 

1903
00:47:22,880 --> 00:47:30,110
 is a architecture that implements

1904
00:47:26,170 --> 00:47:30,110
 

1905
00:47:26,180 --> 00:47:35,030
 parallel patterns it it's key components

1906
00:47:30,100 --> 00:47:35,030
 

1907
00:47:30,110 --> 00:47:36,620
 are a configurable compute unit called a

1908
00:47:35,020 --> 00:47:36,620
 

1909
00:47:35,030 --> 00:47:39,110
 patent compute unit and in a

1910
00:47:36,610 --> 00:47:39,110
 

1911
00:47:36,620 --> 00:47:41,210
 configurable memory unit called a patent

1912
00:47:39,100 --> 00:47:41,210
 

1913
00:47:39,110 --> 00:47:43,420
 memory unit and you put these things

1914
00:47:41,200 --> 00:47:43,420
 

1915
00:47:41,210 --> 00:47:46,340
 together and then you can create

1916
00:47:43,410 --> 00:47:46,340
 

1917
00:47:43,420 --> 00:47:47,930
 whatever Hardware you want for the

1918
00:47:46,330 --> 00:47:47,930
 

1919
00:47:46,340 --> 00:47:50,330
 particular machine learning algorithm

1920
00:47:47,920 --> 00:47:50,330
 

1921
00:47:47,930 --> 00:47:52,910
 that's off interest by going through

1922
00:47:50,320 --> 00:47:52,910
 

1923
00:47:50,330 --> 00:47:56,870
 spatial right so here here's a way in

1924
00:47:52,900 --> 00:47:56,870
 

1925
00:47:52,910 --> 00:47:58,790
 which you get to design hardware without

1926
00:47:56,860 --> 00:47:58,790
 

1927
00:47:56,870 --> 00:48:00,920
 actually going through the process of

1928
00:47:58,780 --> 00:48:00,920
 

1929
00:47:58,790 --> 00:48:03,230
 fabrication and the question is of

1930
00:48:00,910 --> 00:48:03,230
 

1931
00:48:00,920 --> 00:48:07,430
 course how much overhead did I give you

1932
00:48:03,220 --> 00:48:07,430
 

1933
00:48:03,230 --> 00:48:10,280
 in providing this performance okay so in

1934
00:48:07,420 --> 00:48:10,280
 

1935
00:48:07,430 --> 00:48:12,140
 order for you to map to this hard way of

1936
00:48:10,270 --> 00:48:12,140
 

1937
00:48:10,280 --> 00:48:14,690
 course we need a way of going from some

1938
00:48:12,130 --> 00:48:14,690
 

1939
00:48:12,140 --> 00:48:16,550
 spatial representation which I just

1940
00:48:14,680 --> 00:48:16,550
 

1941
00:48:14,690 --> 00:48:18,800
 showed you the example for for dot

1942
00:48:16,540 --> 00:48:18,800
 

1943
00:48:16,550 --> 00:48:21,710
 product into something that can actually

1944
00:48:18,790 --> 00:48:21,710
 

1945
00:48:18,800 --> 00:48:25,610
 program this array of patent compute

1946
00:48:21,700 --> 00:48:25,610
 

1947
00:48:21,710 --> 00:48:27,920
 units and patent memory units and so you

1948
00:48:25,600 --> 00:48:27,920
 

1949
00:48:25,610 --> 00:48:29,960
 know here's a very simple example I'm

1950
00:48:27,910 --> 00:48:29,960
 

1951
00:48:27,920 --> 00:48:33,620
 gonna I've got some address generation

1952
00:48:29,950 --> 00:48:33,620
 

1953
00:48:29,960 --> 00:48:37,520
 Hardware I'm gonna map my the stuff that

1954
00:48:33,610 --> 00:48:37,520
 

1955
00:48:33,620 --> 00:48:41,930
 moves data from DRAM to SRAM in that I'm

1956
00:48:37,510 --> 00:48:41,930
 

1957
00:48:37,520 --> 00:48:44,660
 gonna map my compute to one of the

1958
00:48:41,920 --> 00:48:44,660
 

1959
00:48:41,930 --> 00:48:46,940
 patent compute units and that's gonna

1960
00:48:44,650 --> 00:48:46,940
 

1961
00:48:44,660 --> 00:48:48,710
 not fit in one so I'm going to have to

1962
00:48:46,930 --> 00:48:48,710
 

1963
00:48:46,940 --> 00:48:50,540
 go to another I'm going to connect all

1964
00:48:48,700 --> 00:48:50,540
 

1965
00:48:48,710 --> 00:48:54,010
 the pieces together and then the

1966
00:48:50,530 --> 00:48:54,010
 

1967
00:48:50,540 --> 00:48:58,660
 performance I'm gonna get is gonna be

1968
00:48:54,000 --> 00:48:58,660
 

1969
00:48:54,010 --> 00:49:01,060
 better than a GP or a CPU because we're

1970
00:48:58,650 --> 00:49:01,060
 

1971
00:48:58,660 --> 00:49:03,820
 going to more directly implement the

1972
00:49:01,050 --> 00:49:03,820
 

1973
00:49:01,060 --> 00:49:08,340
 algorithm and the question is how close

1974
00:49:03,810 --> 00:49:08,340
 

1975
00:49:03,820 --> 00:49:11,350
 we get to an ASIC so this graph shows

1976
00:49:08,330 --> 00:49:11,350
 

1977
00:49:08,340 --> 00:49:15,010
 energy efficiency in terms of mega ops

1978
00:49:11,340 --> 00:49:15,010
 

1979
00:49:11,350 --> 00:49:16,990
 per milli watt versus reprogramming time

1980
00:49:15,000 --> 00:49:16,990
 

1981
00:49:15,010 --> 00:49:21,250
 which is some measure of flexibility and

1982
00:49:16,980 --> 00:49:21,250
 

1983
00:49:16,990 --> 00:49:22,450
 what we see is that GPUs and CPUs are

1984
00:49:21,240 --> 00:49:22,450
 

1985
00:49:21,250 --> 00:49:26,410
 basically three orders of magnitude

1986
00:49:22,440 --> 00:49:26,410
 

1987
00:49:22,450 --> 00:49:30,430
 worse than a fixed function ASIC and the

1988
00:49:26,400 --> 00:49:30,430
 

1989
00:49:26,410 --> 00:49:33,100
 goal of the software-defined hardware an

1990
00:49:30,420 --> 00:49:33,100
 

1991
00:49:30,430 --> 00:49:36,850
 example being plasticine is to get

1992
00:49:33,090 --> 00:49:36,850
 

1993
00:49:33,100 --> 00:49:40,960
 within a factor of five to ten of the

1994
00:49:36,840 --> 00:49:40,960
 

1995
00:49:36,850 --> 00:49:43,630
 ASIC wall being much more flexible while

1996
00:49:40,950 --> 00:49:43,630
 

1997
00:49:40,960 --> 00:49:47,980
 being only a little less flexible than a

1998
00:49:43,620 --> 00:49:47,980
 

1999
00:49:43,630 --> 00:49:50,410
 GPU so let me conclude by saying you

2000
00:49:47,970 --> 00:49:50,410
 

2001
00:49:47,980 --> 00:49:55,210
 know this is a very interesting time in

2002
00:49:50,400 --> 00:49:55,210
 

2003
00:49:50,410 --> 00:49:59,980
 the evolution of computer systems and is

2004
00:49:55,200 --> 00:49:59,980
 

2005
00:49:55,210 --> 00:50:03,370
 possible you know given that the the end

2006
00:49:59,970 --> 00:50:03,370
 

2007
00:49:59,980 --> 00:50:05,620
 of Moore's Law and the the great demands

2008
00:50:03,360 --> 00:50:05,620
 

2009
00:50:03,370 --> 00:50:09,040
 in computer performance required by

2010
00:50:05,610 --> 00:50:09,040
 

2011
00:50:05,620 --> 00:50:11,080
 machine learning and it's time in which

2012
00:50:09,030 --> 00:50:11,080
 

2013
00:50:09,040 --> 00:50:13,480
 we can in fact have it all with software

2014
00:50:11,070 --> 00:50:13,480
 

2015
00:50:11,080 --> 00:50:19,410
 2.0 and having it all makes having all

2016
00:50:13,470 --> 00:50:19,410
 

2017
00:50:13,480 --> 00:50:22,170
 the peas right having productivity power

2018
00:50:19,400 --> 00:50:22,170
 

2019
00:50:19,410 --> 00:50:24,820
 performance product efficiency

2020
00:50:22,160 --> 00:50:24,820
 

2021
00:50:22,170 --> 00:50:28,210
 performance programmability and

2022
00:50:24,810 --> 00:50:28,210
 

2023
00:50:24,820 --> 00:50:31,420
 portability so productivity will get by

2024
00:50:28,200 --> 00:50:31,420
 

2025
00:50:28,210 --> 00:50:34,630
 you having people develop applications

2026
00:50:31,410 --> 00:50:34,630
 

2027
00:50:31,420 --> 00:50:38,700
 using by by doing training rather than

2028
00:50:34,620 --> 00:50:38,700
 

2029
00:50:34,630 --> 00:50:40,870
 conventional algorithms and these these

2030
00:50:38,690 --> 00:50:40,870
 

2031
00:50:38,700 --> 00:50:42,700
 training will be done by using new

2032
00:50:40,860 --> 00:50:42,700
 

2033
00:50:40,870 --> 00:50:47,740
 machine learning algorithms that

2034
00:50:42,690 --> 00:50:47,740
 

2035
00:50:42,700 --> 00:50:49,350
 trade-off reduce statistical efficiency

2036
00:50:47,730 --> 00:50:49,350
 

2037
00:50:47,740 --> 00:50:51,760
 for much greater Hardware efficiency

2038
00:50:49,340 --> 00:50:51,760
 

2039
00:50:49,350 --> 00:50:53,980
 these algorithms will be implemented

2040
00:50:51,750 --> 00:50:53,980
 

2041
00:50:51,760 --> 00:50:56,800
 using high performance domain-specific

2042
00:50:53,970 --> 00:50:56,800
 

2043
00:50:53,980 --> 00:50:58,780
 languages and these languages with

2044
00:50:56,790 --> 00:50:58,780
 

2045
00:50:56,800 --> 00:51:00,430
 sophisticated compilers will be used to

2046
00:50:58,770 --> 00:51:00,430
 

2047
00:50:58,780 --> 00:51:05,020
 generate intermediate representations

2048
00:51:00,420 --> 00:51:05,020
 

2049
00:51:00,430 --> 00:51:06,640
 that can drive optimized implementations

2050
00:51:05,010 --> 00:51:06,640
 

2051
00:51:05,020 --> 00:51:12,640
 of hard

2052
00:51:06,630 --> 00:51:12,640
 

2053
00:51:06,640 --> 00:51:15,039
 and I hope that you know in the

2054
00:51:12,630 --> 00:51:15,039
 

2055
00:51:12,640 --> 00:51:19,690
 description of a vertically integrated

2056
00:51:15,029 --> 00:51:19,690
 

2057
00:51:15,039 --> 00:51:21,400
 stack for software 2.0 you are convinced

2058
00:51:19,680 --> 00:51:21,400
 

2059
00:51:19,690 --> 00:51:23,440
 that you know in order to actually have

2060
00:51:21,390 --> 00:51:23,440
 

2061
00:51:21,400 --> 00:51:26,109
 it all you do need the vertical

2062
00:51:23,430 --> 00:51:26,109
 

2063
00:51:23,440 --> 00:51:28,670
 integration and at this point I'll end

2064
00:51:26,099 --> 00:51:28,670
 

2065
00:51:26,109 --> 00:51:35,139
 and ask if there any questions

2066
00:51:28,660 --> 00:51:35,139
 

2067
00:51:28,670 --> 00:51:35,139
[Applause]

2068
00:51:36,580 --> 00:51:36,580
 

2069
00:51:36,590 --> 00:51:44,070
 so that was awesome I happen to have

2070
00:51:41,300 --> 00:51:44,070
 

2071
00:51:41,310 --> 00:51:46,580
 dinner with Bill Dahle one of your

2072
00:51:44,060 --> 00:51:46,580
 

2073
00:51:44,070 --> 00:51:49,290
 colleagues at Stanford and I asked him

2074
00:51:46,570 --> 00:51:49,290
 

2075
00:51:46,580 --> 00:51:50,850
 how many hardware companies you know

2076
00:51:49,280 --> 00:51:50,850
 

2077
00:51:49,290 --> 00:51:53,820
 startups are out there and he said that

2078
00:51:50,840 --> 00:51:53,820
 

2079
00:51:50,850 --> 00:51:56,430
 he's lost count at 50 right and and

2080
00:51:53,810 --> 00:51:56,430
 

2081
00:51:53,820 --> 00:51:57,780
 here's the question the question is can

2082
00:51:56,420 --> 00:51:57,780
 

2083
00:51:56,430 --> 00:51:59,640
 you give an estimate for how much

2084
00:51:57,770 --> 00:51:59,640
 

2085
00:51:57,780 --> 00:52:01,860
 investment is being made into this

2086
00:51:59,630 --> 00:52:01,860
 

2087
00:51:59,640 --> 00:52:03,270
 sector of hardware I mean you know

2088
00:52:01,850 --> 00:52:03,270
 

2089
00:52:01,860 --> 00:52:05,010
 overall I know Intel all the big

2090
00:52:03,260 --> 00:52:05,010
 

2091
00:52:03,270 --> 00:52:06,390
 companies are also making investments

2092
00:52:05,000 --> 00:52:06,390
 

2093
00:52:05,010 --> 00:52:10,590
 and what's the scale here

2094
00:52:06,380 --> 00:52:10,590
 

2095
00:52:06,390 --> 00:52:13,080
 Oh billions of dollars billions at least

2096
00:52:10,580 --> 00:52:13,080
 

2097
00:52:10,590 --> 00:52:15,360
 a billion yeah probably if you if you

2098
00:52:13,070 --> 00:52:15,360
 

2099
00:52:13,080 --> 00:52:18,780
 look across the US and China it's

2100
00:52:15,350 --> 00:52:18,780
 

2101
00:52:15,360 --> 00:52:21,570
 definitely billions oh yeah that's the

2102
00:52:18,770 --> 00:52:21,570
 

2103
00:52:18,780 --> 00:52:22,800
 China of gearing up - it's obviously

2104
00:52:21,560 --> 00:52:22,800
 

2105
00:52:21,570 --> 00:52:32,310
 that it's going to have a huge impact

2106
00:52:22,790 --> 00:52:32,310
 

2107
00:52:22,800 --> 00:52:35,280
 there - yeah another question so I worry

2108
00:52:32,300 --> 00:52:35,280
 

2109
00:52:32,310 --> 00:52:38,490
 about applying machine learning in high

2110
00:52:35,270 --> 00:52:38,490
 

2111
00:52:35,280 --> 00:52:40,140
 high risk applications and tried to

2112
00:52:38,480 --> 00:52:40,140
 

2113
00:52:38,490 --> 00:52:42,150
 quantify all of the sources of

2114
00:52:40,130 --> 00:52:42,150
 

2115
00:52:40,140 --> 00:52:45,960
 statistical uncertainty in the answers

2116
00:52:42,140 --> 00:52:45,960
 

2117
00:52:42,150 --> 00:52:47,850
 we give and so the question is how do I

2118
00:52:45,950 --> 00:52:47,850
 

2119
00:52:45,960 --> 00:52:49,290
 get my hands on how much uncertainty is

2120
00:52:47,840 --> 00:52:49,290
 

2121
00:52:47,850 --> 00:52:52,500
 being introduced by these various

2122
00:52:49,280 --> 00:52:52,500
 

2123
00:52:49,290 --> 00:52:55,920
 Hardware hacks that are each maybe

2124
00:52:52,490 --> 00:52:55,920
 

2125
00:52:52,500 --> 00:52:57,960
 invisible to my software layer that's a

2126
00:52:55,910 --> 00:52:57,960
 

2127
00:52:55,920 --> 00:53:00,000
 very good question and you know I'm

2128
00:52:57,950 --> 00:53:00,000
 

2129
00:52:57,960 --> 00:53:03,720
 probably not the right person to answer

2130
00:52:59,990 --> 00:53:03,720
 

2131
00:53:00,000 --> 00:53:05,820
 it I mean I think if you are working in

2132
00:53:03,710 --> 00:53:05,820
 

2133
00:53:03,720 --> 00:53:07,620
 with algorithms that are non convex then

2134
00:53:05,810 --> 00:53:07,620
 

2135
00:53:05,820 --> 00:53:10,860
 they're no real guarantees that I know

2136
00:53:07,610 --> 00:53:10,860
 

2137
00:53:07,620 --> 00:53:13,890
 of maybe someone that knows more can can

2138
00:53:10,850 --> 00:53:13,890
 

2139
00:53:10,860 --> 00:53:16,140
 answer but that that's well so often we

2140
00:53:13,880 --> 00:53:16,140
 

2141
00:53:13,890 --> 00:53:18,570
 might want to train using all of the

2142
00:53:16,130 --> 00:53:18,570
 

2143
00:53:16,140 --> 00:53:19,860
 tricks that you're giving but then take

2144
00:53:18,560 --> 00:53:19,860
 

2145
00:53:18,570 --> 00:53:23,400
 a validation set and run it through

2146
00:53:19,850 --> 00:53:23,400
 

2147
00:53:19,860 --> 00:53:26,130
 somehow very cleanly without you know

2148
00:53:23,390 --> 00:53:26,130
 

2149
00:53:23,400 --> 00:53:27,750
 without the cash relaxations and all of

2150
00:53:26,120 --> 00:53:27,750
 

2151
00:53:26,130 --> 00:53:29,160
 those things so we might want to have

2152
00:53:27,740 --> 00:53:29,160
 

2153
00:53:27,750 --> 00:53:30,990
 two modes for operating the hardware

2154
00:53:29,150 --> 00:53:30,990
 

2155
00:53:29,160 --> 00:53:32,730
 yeah I mean that's perfectly possible

2156
00:53:30,980 --> 00:53:32,730
 

2157
00:53:30,990 --> 00:53:35,840
 and you want hardware that can support

2158
00:53:32,720 --> 00:53:35,840
 

2159
00:53:32,730 --> 00:53:35,840
 that yeah

2160
00:53:38,270 --> 00:53:38,270
 

2161
00:53:38,280 --> 00:53:43,440
 can these innovations serve a niche

2162
00:53:40,490 --> 00:53:43,440
 

2163
00:53:40,500 --> 00:53:45,990
 market of model training and inference

2164
00:53:43,430 --> 00:53:45,990
 

2165
00:53:43,440 --> 00:53:47,970
 or is the expectation that overall

2166
00:53:45,980 --> 00:53:47,970
 

2167
00:53:45,990 --> 00:53:49,980
 training and inference workloads grow to

2168
00:53:47,960 --> 00:53:49,980
 

2169
00:53:47,970 --> 00:53:53,310
 some substantial portion

2170
00:53:49,970 --> 00:53:53,310
 

2171
00:53:49,980 --> 00:53:56,280
 of overall compute workloads in order to

2172
00:53:53,300 --> 00:53:56,280
 

2173
00:53:53,310 --> 00:54:01,980
 make this innovation vital and give it

2174
00:53:56,270 --> 00:54:01,980
 

2175
00:53:56,280 --> 00:54:04,380
 momentum is this a niche market for

2176
00:54:01,970 --> 00:54:04,380
 

2177
00:54:01,980 --> 00:54:12,050
 accelerators or does it no that's the

2178
00:54:04,370 --> 00:54:12,050
 

2179
00:54:04,380 --> 00:54:12,050
 whole point so in fact the original when

2180
00:54:12,760 --> 00:54:12,760
 

2181
00:54:12,770 --> 00:54:17,850
 the original approach was to was to come

2182
00:54:16,340 --> 00:54:17,850
 

2183
00:54:16,350 --> 00:54:20,640
 up with a general-purpose accelerator

2184
00:54:17,840 --> 00:54:20,640
 

2185
00:54:17,850 --> 00:54:23,750
 right so you know as I said it's not

2186
00:54:20,630 --> 00:54:23,750
 

2187
00:54:20,640 --> 00:54:26,810
 just machine learning it's state Alec's

2188
00:54:23,740 --> 00:54:26,810
 

2189
00:54:23,750 --> 00:54:29,340
 broadly so it's sequel it's it's Graaf

2190
00:54:26,800 --> 00:54:29,340
 

2191
00:54:26,810 --> 00:54:31,260
 anything that's data intensive could be

2192
00:54:29,330 --> 00:54:31,260
 

2193
00:54:29,340 --> 00:54:34,260
 accelerated using this approach thank

2194
00:54:31,250 --> 00:54:34,260
 

2195
00:54:31,260 --> 00:54:38,010
 you yeah I'd like to connect a couple

2196
00:54:34,250 --> 00:54:38,010
 

2197
00:54:34,260 --> 00:54:40,700
 dots following up Tom's question yeah of

2198
00:54:38,000 --> 00:54:40,700
 

2199
00:54:38,010 --> 00:54:42,990
 a moment ago at the same mic here

2200
00:54:40,690 --> 00:54:42,990
 

2201
00:54:40,700 --> 00:54:44,760
 hardware efficients our computational

2202
00:54:42,980 --> 00:54:44,760
 

2203
00:54:42,990 --> 00:54:47,250
 efficiency feeds back to become

2204
00:54:44,750 --> 00:54:47,250
 

2205
00:54:44,760 --> 00:54:48,840
 statistical efficiency when your

2206
00:54:47,240 --> 00:54:48,840
 

2207
00:54:47,250 --> 00:54:50,940
 algorithms require a lot of hyper

2208
00:54:48,830 --> 00:54:50,940
 

2209
00:54:48,840 --> 00:54:53,790
 parameter search other configuration

2210
00:54:50,930 --> 00:54:53,790
 

2211
00:54:50,940 --> 00:54:55,830
 search and you know tuning in that sense

2212
00:54:53,780 --> 00:54:55,830
 

2213
00:54:53,790 --> 00:54:59,030
 so the greater the computational

2214
00:54:55,820 --> 00:54:59,030
 

2215
00:54:55,830 --> 00:55:01,980
 efficiency you may be increasing your

2216
00:54:59,020 --> 00:55:01,980
 

2217
00:54:59,030 --> 00:55:03,540
 statistical efficiency so even the

2218
00:55:01,970 --> 00:55:03,540
 

2219
00:55:01,980 --> 00:55:06,990
 notion of kind of what the trade-off is

2220
00:55:03,530 --> 00:55:06,990
 

2221
00:55:03,540 --> 00:55:09,840
 here I think requires a bit of nuance

2222
00:55:06,980 --> 00:55:09,840
 

2223
00:55:06,990 --> 00:55:11,460
 yeah you know to be responsive to the

2224
00:55:09,830 --> 00:55:11,460
 

2225
00:55:09,840 --> 00:55:14,820
 fact that overall the machine learning

2226
00:55:11,450 --> 00:55:14,820
 

2227
00:55:11,460 --> 00:55:16,380
 problem includes the metal level of the

2228
00:55:14,810 --> 00:55:16,380
 

2229
00:55:14,820 --> 00:55:17,730
 learning so I was just wondering if

2230
00:55:16,370 --> 00:55:17,730
 

2231
00:55:16,380 --> 00:55:19,920
 you've considered that or have any

2232
00:55:17,720 --> 00:55:19,920
 

2233
00:55:17,730 --> 00:55:22,980
 comments about that because I think that

2234
00:55:19,910 --> 00:55:22,980
 

2235
00:55:19,920 --> 00:55:28,340
 makes your story even stronger I think

2236
00:55:22,970 --> 00:55:28,340
 

2237
00:55:22,980 --> 00:55:32,010
 that makes your story even stronger yeah

2238
00:55:28,330 --> 00:55:32,010
 

2239
00:55:28,340 --> 00:55:36,480
 sorry I would agree that there is this

2240
00:55:32,000 --> 00:55:36,480
 

2241
00:55:32,010 --> 00:55:42,510
 matter issue that you need to you know

2242
00:55:36,470 --> 00:55:42,510
 

2243
00:55:36,480 --> 00:55:46,650
 that however I haven't given it a huge

2244
00:55:42,500 --> 00:55:46,650
 

2245
00:55:42,510 --> 00:55:50,880
 amount of thought yeah okay yeah but we

2246
00:55:46,640 --> 00:55:50,880
 

2247
00:55:46,650 --> 00:55:53,850
 can take it offline Thanks yeah sure hi

2248
00:55:50,870 --> 00:55:53,850
 

2249
00:55:50,880 --> 00:55:56,790
 so I think one of the key components of

2250
00:55:53,840 --> 00:55:56,790
 

2251
00:55:53,850 --> 00:55:59,220
 software development is testing and I

2252
00:55:56,780 --> 00:55:59,220
 

2253
00:55:56,790 --> 00:56:01,710
 think when it comes to machine learning

2254
00:55:59,210 --> 00:56:01,710
 

2255
00:55:59,220 --> 00:56:03,109
 as opposed to standard testing

2256
00:56:01,700 --> 00:56:03,109
 

2257
00:56:01,710 --> 00:56:05,269
 mechanisms it

2258
00:56:03,099 --> 00:56:05,269
 

2259
00:56:03,109 --> 00:56:07,609
 is quite hard because machine learning

2260
00:56:05,259 --> 00:56:07,609
 

2261
00:56:05,269 --> 00:56:09,499
 is stochastic by nature and if you think

2262
00:56:07,599 --> 00:56:09,499
 

2263
00:56:07,609 --> 00:56:12,410
 about things like continuous integration

2264
00:56:09,489 --> 00:56:12,410
 

2265
00:56:09,499 --> 00:56:15,440
 it's even harder because the dataset

2266
00:56:12,400 --> 00:56:15,440
 

2267
00:56:12,410 --> 00:56:19,279
 that's being flown into the algorithms

2268
00:56:15,430 --> 00:56:19,279
 

2269
00:56:15,440 --> 00:56:21,650
 will change quite dynamically so I was

2270
00:56:19,269 --> 00:56:21,650
 

2271
00:56:19,279 --> 00:56:24,410
 cured too curious to see to hear your

2272
00:56:21,640 --> 00:56:24,410
 

2273
00:56:21,650 --> 00:56:29,059
 opinions about testing for emiltt in

2274
00:56:24,400 --> 00:56:29,059
 

2275
00:56:24,410 --> 00:56:31,130
 general yeah I think you know you you're

2276
00:56:29,049 --> 00:56:31,130
 

2277
00:56:29,059 --> 00:56:33,559
 gonna have to do the same sorts of

2278
00:56:31,120 --> 00:56:33,559
 

2279
00:56:31,130 --> 00:56:34,880
 things that you do today with machine

2280
00:56:33,549 --> 00:56:34,880
 

2281
00:56:33,559 --> 00:56:39,920
 learning algorithms I think you know

2282
00:56:34,870 --> 00:56:39,920
 

2283
00:56:34,880 --> 00:56:45,789
 don't fundamentally you know make it any

2284
00:56:39,910 --> 00:56:45,789
 

2285
00:56:39,920 --> 00:56:48,230
 better by having I mean you can you can

2286
00:56:45,779 --> 00:56:48,230
 

2287
00:56:45,789 --> 00:56:51,619
 with greater performance potentially you

2288
00:56:48,220 --> 00:56:51,619
 

2289
00:56:48,230 --> 00:56:55,960
 can look at more cases but I don't see

2290
00:56:51,609 --> 00:56:55,960
 

2291
00:56:51,619 --> 00:57:02,900
 fundamentally what to do there I mean

2292
00:56:55,950 --> 00:57:02,900
 

2293
00:56:55,960 --> 00:57:06,619
 yeah hi

2294
00:57:02,890 --> 00:57:06,619
 

2295
00:57:02,900 --> 00:57:09,589
 I'm curious about the choice of Scala

2296
00:57:06,609 --> 00:57:09,589
 

2297
00:57:06,619 --> 00:57:13,009
 programming language yeah is there any

2298
00:57:09,579 --> 00:57:13,009
 

2299
00:57:09,589 --> 00:57:16,759
 particular characteristic about salat is

2300
00:57:12,999 --> 00:57:16,759
 

2301
00:57:13,009 --> 00:57:18,259
 that well so Scala is an artifact of

2302
00:57:16,749 --> 00:57:18,259
 

2303
00:57:16,759 --> 00:57:19,999
 some of the research that we did earlier

2304
00:57:18,249 --> 00:57:19,999
 

2305
00:57:18,259 --> 00:57:21,890
 right and so the reason to use scholar

2306
00:57:19,989 --> 00:57:21,890
 

2307
00:57:19,999 --> 00:57:25,369
 has had a very expressive type system

2308
00:57:21,880 --> 00:57:25,369
 

2309
00:57:21,890 --> 00:57:27,739
 and it allowed this technique called

2310
00:57:25,359 --> 00:57:27,739
 

2311
00:57:25,369 --> 00:57:30,039
 lightweight modulus staging which

2312
00:57:27,729 --> 00:57:30,039
 

2313
00:57:27,739 --> 00:57:34,400
 allowed you to be able to basically

2314
00:57:30,029 --> 00:57:34,400
 

2315
00:57:30,039 --> 00:57:36,769
 virtualize the program that you wrote so

2316
00:57:34,390 --> 00:57:36,769
 

2317
00:57:34,400 --> 00:57:40,279
 you could get the whole IR including

2318
00:57:36,759 --> 00:57:40,279
 

2319
00:57:36,769 --> 00:57:41,749
 control flow into the representation

2320
00:57:40,269 --> 00:57:41,749
 

2321
00:57:40,279 --> 00:57:45,829
 right so you could you could apply this

2322
00:57:41,739 --> 00:57:45,829
 

2323
00:57:41,749 --> 00:57:48,769
 approach of staging where you could

2324
00:57:45,819 --> 00:57:48,769
 

2325
00:57:45,829 --> 00:57:52,220
 essentially put the compiler inside of

2326
00:57:48,759 --> 00:57:52,220
 

2327
00:57:48,769 --> 00:57:54,890
 the library and it made it easy to do

2328
00:57:52,210 --> 00:57:54,890
 

2329
00:57:52,220 --> 00:57:57,190
 and so that's why we use Scala due today

2330
00:57:54,880 --> 00:57:57,190
 

2331
00:57:54,890 --> 00:58:00,460
 of course in the machine learning space

2332
00:57:57,180 --> 00:58:00,460
 

2333
00:57:57,190 --> 00:58:03,349
 you you know the front end has to be

2334
00:58:00,450 --> 00:58:03,349
 

2335
00:58:00,460 --> 00:58:05,509
 Python yeah that's my question about

2336
00:58:03,339 --> 00:58:05,509
 

2337
00:58:03,349 --> 00:58:07,999
 what you do in the back end you could do

2338
00:58:05,499 --> 00:58:07,999
 

2339
00:58:05,509 --> 00:58:10,400
 use whatever language you like right as

2340
00:58:07,989 --> 00:58:10,400
 

2341
00:58:07,999 --> 00:58:13,249
 long as I didn't expose it to to the to

2342
00:58:10,390 --> 00:58:13,249
 

2343
00:58:10,400 --> 00:58:15,130
 the programs okay thanks yeah all right

2344
00:58:13,239 --> 00:58:15,130
 

2345
00:58:13,249 --> 00:58:16,989
 we're out of time but I'm sure

2346
00:58:15,120 --> 00:58:16,989
 

2347
00:58:15,130 --> 00:58:18,630
 we'll be happy to take questions offline

2348
00:58:16,979 --> 00:58:18,630
 

2349
00:58:16,989 --> 00:58:25,030
 so let's thank only again

2350
00:58:18,620 --> 00:58:25,030
 

2351
00:58:18,630 --> 00:58:49,530
[Applause]

2352
00:58:25,020 --> 00:58:49,530
 

2353
00:58:25,030 --> 00:58:49,530
[Music]

2354
00:58:56,290 --> 00:58:56,290
 

2355
00:58:56,300 --> 00:58:59,689
[Music]