1
00:00:10,160 --> 00:00:13,240
[Music]

2
00:01:27,410 --> 00:01:27,410
 

3
00:01:27,420 --> 00:01:30,680
[Music]

4
00:02:37,980 --> 00:02:37,980
 

5
00:02:37,990 --> 00:02:41,209
[Music]

6
00:03:20,770 --> 00:03:20,770
 

7
00:03:20,780 --> 00:03:23,930
[Music]

8
00:03:31,580 --> 00:03:31,580
 

9
00:03:31,590 --> 00:03:36,419
[Music]

10
00:05:07,780 --> 00:05:07,780
 

11
00:05:07,790 --> 00:05:11,009
[Music]

12
00:06:36,080 --> 00:06:36,080
 

13
00:06:36,090 --> 00:06:43,240
 okay so I guess we should get started

14
00:06:38,010 --> 00:06:43,240
 

15
00:06:38,020 --> 00:06:46,930
 while people are still walking in so

16
00:06:43,230 --> 00:06:46,930
 

17
00:06:43,240 --> 00:06:49,810
 welcome everybody to the to trial and

18
00:06:46,920 --> 00:06:49,810
 

19
00:06:46,930 --> 00:06:52,420
 statistical learning theory which is

20
00:06:49,800 --> 00:06:52,420
 

21
00:06:49,810 --> 00:06:56,260
 gonna be led by John Shaw Taylor and I'm

22
00:06:52,410 --> 00:06:56,260
 

23
00:06:52,420 --> 00:06:57,690
 Omar Eva's plotter I guess John Taylor

24
00:06:56,250 --> 00:06:57,690
 

25
00:06:56,260 --> 00:07:00,400
 doesn't really need an introduction but

26
00:06:57,680 --> 00:07:00,400
 

27
00:06:57,690 --> 00:07:02,200
 quickly introduced him regardless he's a

28
00:07:00,390 --> 00:07:02,200
 

29
00:07:00,400 --> 00:07:04,150
 professor and the director for the

30
00:07:02,190 --> 00:07:04,150
 

31
00:07:02,200 --> 00:07:06,010
 center of computational statistics and

32
00:07:04,140 --> 00:07:06,010
 

33
00:07:04,150 --> 00:07:09,700
 machine learning at University College

34
00:07:06,000 --> 00:07:09,700
 

35
00:07:06,010 --> 00:07:11,830
 London he has been doing research on a

36
00:07:09,690 --> 00:07:11,830
 

37
00:07:09,700 --> 00:07:14,530
 wide range of topics I'm starting from

38
00:07:11,820 --> 00:07:14,530
 

39
00:07:11,830 --> 00:07:16,110
 graph theory and cryptography work on

40
00:07:14,520 --> 00:07:16,110
 

41
00:07:14,530 --> 00:07:19,390
 neural networks but I guess he's most

42
00:07:16,100 --> 00:07:19,390
 

43
00:07:16,110 --> 00:07:21,250
 well-known for work on kernel methods

44
00:07:19,380 --> 00:07:21,250
 

45
00:07:19,390 --> 00:07:24,130
 and support vector machines he has

46
00:07:21,240 --> 00:07:24,130
 

47
00:07:21,250 --> 00:07:27,490
 published over 300 research papers and

48
00:07:24,120 --> 00:07:27,490
 

49
00:07:24,130 --> 00:07:29,860
 two books on support vector machines and

50
00:07:27,480 --> 00:07:29,860
 

51
00:07:27,490 --> 00:07:32,170
 and kernel methods he's also been very

52
00:07:29,850 --> 00:07:32,170
 

53
00:07:29,860 --> 00:07:35,080
 influential in fostering and bringing

54
00:07:32,160 --> 00:07:35,080
 

55
00:07:32,170 --> 00:07:37,960
 together the research community in

56
00:07:35,070 --> 00:07:37,960
 

57
00:07:35,080 --> 00:07:40,630
 Europe on machine learning years one of

58
00:07:37,950 --> 00:07:40,630
 

59
00:07:37,960 --> 00:07:43,180
 the Pascal Network India's also I've

60
00:07:40,620 --> 00:07:43,180
 

61
00:07:40,630 --> 00:07:45,460
 been told found it actually the euro

62
00:07:43,170 --> 00:07:45,460
 

63
00:07:43,180 --> 00:07:48,030
 called conference which doesn't exist

64
00:07:45,450 --> 00:07:48,030
 

65
00:07:45,460 --> 00:07:51,640
 anymore but tweaks to its success has

66
00:07:48,020 --> 00:07:51,640
 

67
00:07:48,030 --> 00:07:53,590
 forced the cult community in the code

68
00:07:51,630 --> 00:07:53,590
 

69
00:07:51,640 --> 00:07:57,730
 conference to become more international

70
00:07:53,580 --> 00:07:57,730
 

71
00:07:53,590 --> 00:08:00,400
 and to move out of just being a purely

72
00:07:57,720 --> 00:08:00,400
 

73
00:07:57,730 --> 00:08:05,820
 us-based conference to now be a more

74
00:08:00,390 --> 00:08:05,820
 

75
00:08:00,400 --> 00:08:10,060
 international also European conference

76
00:08:05,810 --> 00:08:10,060
 

77
00:08:05,820 --> 00:08:13,300
 Martha's plateau is currently a PhD

78
00:08:10,050 --> 00:08:13,300
 

79
00:08:10,060 --> 00:08:15,460
 student at University College London he

80
00:08:13,290 --> 00:08:15,460
 

81
00:08:13,300 --> 00:08:17,080
 has already earned a degree in

82
00:08:15,450 --> 00:08:17,080
 

83
00:08:15,460 --> 00:08:19,180
 mathematics actually I'm from the

84
00:08:17,070 --> 00:08:19,180
 

85
00:08:17,080 --> 00:08:21,790
 University of Alberta West when he has

86
00:08:19,170 --> 00:08:21,790
 

87
00:08:19,180 --> 00:08:24,430
 started to work with Chaves Chaves

88
00:08:21,780 --> 00:08:24,430
 

89
00:08:21,790 --> 00:08:29,650
 Valley who is now also currently working

90
00:08:24,420 --> 00:08:29,650
 

91
00:08:24,430 --> 00:08:32,500
 with a deep mind so we he is both in

92
00:08:29,640 --> 00:08:32,500
 

93
00:08:29,650 --> 00:08:34,360
 academia at UCL and and working as a

94
00:08:32,490 --> 00:08:34,360
 

95
00:08:32,500 --> 00:08:36,250
 researcher and deep mind

96
00:08:34,350 --> 00:08:36,250
 

97
00:08:34,360 --> 00:08:39,160
 and maybe last but not least I've been

98
00:08:36,240 --> 00:08:39,160
 

99
00:08:36,250 --> 00:08:41,170
 told that John's father was it was a

100
00:08:39,150 --> 00:08:41,170
 

101
00:08:39,160 --> 00:08:43,540
 racing car driver in that that is very

102
00:08:41,160 --> 00:08:43,540
 

103
00:08:41,170 --> 00:08:45,940
 noticeable if one ever gets a ride with

104
00:08:43,530 --> 00:08:45,940
 

105
00:08:43,540 --> 00:08:50,310
 with John's so let's see what happens if

106
00:08:45,930 --> 00:08:50,310
 

107
00:08:45,940 --> 00:08:50,310
 we I'm hitchhike with job

108
00:08:54,870 --> 00:08:54,870
 

109
00:08:54,880 --> 00:09:00,630
 thank you very much thank you and thank

110
00:08:58,080 --> 00:09:00,630
 

111
00:08:58,090 --> 00:09:04,000
 you for for showing an interest so

112
00:09:00,620 --> 00:09:04,000
 

113
00:09:00,630 --> 00:09:08,350
 statistical learning theory Hitchhiker's

114
00:09:03,990 --> 00:09:08,350
 

115
00:09:04,000 --> 00:09:12,520
 Guide so I'm going to start with a look

116
00:09:08,340 --> 00:09:12,520
 

117
00:09:08,350 --> 00:09:14,560
 at a kind of overview of why and what

118
00:09:12,510 --> 00:09:14,560
 

119
00:09:12,520 --> 00:09:19,300
 the aim of statistical learning theory

120
00:09:14,550 --> 00:09:19,300
 

121
00:09:14,560 --> 00:09:22,680
 is so I want to just start with a plot

122
00:09:19,290 --> 00:09:22,680
 

123
00:09:19,300 --> 00:09:25,600
 of two distributions

124
00:09:22,670 --> 00:09:25,600
 

125
00:09:22,680 --> 00:09:27,610
 these are distributions for two

126
00:09:25,590 --> 00:09:27,610
 

127
00:09:25,600 --> 00:09:30,160
 different algorithms a red algorithm and

128
00:09:27,600 --> 00:09:30,160
 

129
00:09:27,610 --> 00:09:32,890
 a blue algorithm and what they plot are

130
00:09:30,150 --> 00:09:32,890
 

131
00:09:30,160 --> 00:09:39,550
 the generalization errors so the test

132
00:09:32,880 --> 00:09:39,550
 

133
00:09:32,890 --> 00:09:41,920
 errors as a random draw is made of

134
00:09:39,540 --> 00:09:41,920
 

135
00:09:39,550 --> 00:09:44,620
 different training sets so you can

136
00:09:41,910 --> 00:09:44,620
 

137
00:09:41,920 --> 00:09:47,350
 imagine you've got a problem you're

138
00:09:44,610 --> 00:09:47,350
 

139
00:09:44,620 --> 00:09:50,080
 trying to solve and you generate random

140
00:09:47,340 --> 00:09:50,080
 

141
00:09:47,350 --> 00:09:52,360
 training sets you run your algorithm on

142
00:09:50,070 --> 00:09:52,360
 

143
00:09:50,080 --> 00:09:54,910
 the training sets and you find out what

144
00:09:52,350 --> 00:09:54,910
 

145
00:09:52,360 --> 00:09:57,400
 test error it has and then you do it

146
00:09:54,900 --> 00:09:57,400
 

147
00:09:54,910 --> 00:09:59,860
 again and again and you create a

148
00:09:57,390 --> 00:09:59,860
 

149
00:09:57,400 --> 00:10:03,340
 distribution of the performance of the

150
00:09:59,850 --> 00:10:03,340
 

151
00:09:59,860 --> 00:10:04,870
 algorithm as you you know that is the

152
00:10:03,330 --> 00:10:04,870
 

153
00:10:03,340 --> 00:10:06,790
 random variable and this is the

154
00:10:04,860 --> 00:10:06,790
 

155
00:10:04,870 --> 00:10:11,710
 distribution of that random variable

156
00:10:06,780 --> 00:10:11,710
 

157
00:10:06,790 --> 00:10:15,690
 over many many draws and I think if you

158
00:10:11,700 --> 00:10:15,690
 

159
00:10:11,710 --> 00:10:18,280
 have a look at it I've also plotted the

160
00:10:15,680 --> 00:10:18,280
 

161
00:10:15,690 --> 00:10:19,450
 the mean of the two distributions so

162
00:10:18,270 --> 00:10:19,450
 

163
00:10:18,280 --> 00:10:22,690
 this is the mean of the blue

164
00:10:19,440 --> 00:10:22,690
 

165
00:10:19,450 --> 00:10:25,510
 distribution this is the mean of the red

166
00:10:22,680 --> 00:10:25,510
 

167
00:10:22,690 --> 00:10:28,830
 distribution these two different

168
00:10:25,500 --> 00:10:28,830
 

169
00:10:25,510 --> 00:10:32,110
 algorithms and they look pretty similar

170
00:10:28,820 --> 00:10:32,110
 

171
00:10:28,830 --> 00:10:34,930
 but I think you can already see from the

172
00:10:32,100 --> 00:10:34,930
 

173
00:10:32,110 --> 00:10:37,150
 plot that actually these algorithms are

174
00:10:34,920 --> 00:10:37,150
 

175
00:10:34,930 --> 00:10:40,240
 not so similar in terms of the potential

176
00:10:37,140 --> 00:10:40,240
 

177
00:10:37,150 --> 00:10:42,130
 performance that you might get when you

178
00:10:40,230 --> 00:10:42,130
 

179
00:10:40,240 --> 00:10:44,710
 run them on a particular training set

180
00:10:42,120 --> 00:10:44,710
 

181
00:10:42,130 --> 00:10:47,080
 remember we only have one training set

182
00:10:44,700 --> 00:10:47,080
 

183
00:10:44,710 --> 00:10:50,290
 and if we want to look at the kind of

184
00:10:47,070 --> 00:10:50,290
 

185
00:10:47,080 --> 00:10:53,440
 worst-case or sort of you know tail of

186
00:10:50,280 --> 00:10:53,440
 

187
00:10:50,290 --> 00:10:55,300
 the performances we might expect if we

188
00:10:53,430 --> 00:10:55,300
 

189
00:10:53,440 --> 00:10:58,840
 were to run this algorithm on a real

190
00:10:55,290 --> 00:10:58,840
 

191
00:10:55,300 --> 00:11:03,400
 data set the blue algorithm comes out at

192
00:10:58,830 --> 00:11:03,400
 

193
00:10:58,840 --> 00:11:05,650
 a 95% confidence that test error plotted

194
00:11:03,390 --> 00:11:05,650
 

195
00:11:03,400 --> 00:11:08,220
 there while the red algorithm is

196
00:11:05,640 --> 00:11:08,220
 

197
00:11:05,650 --> 00:11:11,010
 significantly better

198
00:11:08,210 --> 00:11:11,010
 

199
00:11:08,220 --> 00:11:14,639
 so in fact this is these two plots are

200
00:11:11,000 --> 00:11:14,639
 

201
00:11:11,010 --> 00:11:16,290
 made for a parson window algorithm just

202
00:11:14,629 --> 00:11:16,290
 

203
00:11:14,639 --> 00:11:19,610
 taking an average of the positive

204
00:11:16,280 --> 00:11:19,610
 

205
00:11:16,290 --> 00:11:21,810
 average of the negative and drawing a a

206
00:11:19,600 --> 00:11:21,810
 

207
00:11:19,610 --> 00:11:26,040
 vector between them so it's a linear

208
00:11:21,800 --> 00:11:26,040
 

209
00:11:21,810 --> 00:11:29,250
 function and it just is very simple and

210
00:11:26,030 --> 00:11:29,250
 

211
00:11:26,040 --> 00:11:31,230
 the second is a linear SVM so they're

212
00:11:29,240 --> 00:11:31,230
 

213
00:11:29,250 --> 00:11:33,600
 actually learning with exactly the same

214
00:11:31,220 --> 00:11:33,600
 

215
00:11:31,230 --> 00:11:35,550
 set of functions the algorithms differ

216
00:11:33,590 --> 00:11:35,550
 

217
00:11:33,600 --> 00:11:40,230
 only in the way they choose the function

218
00:11:35,540 --> 00:11:40,230
 

219
00:11:35,550 --> 00:11:43,320
 that they that they use on from the

220
00:11:40,220 --> 00:11:43,320
 

221
00:11:40,230 --> 00:11:46,620
 training data so I think what I wanted

222
00:11:43,310 --> 00:11:46,620
 

223
00:11:43,320 --> 00:11:48,540
 to draw from this this plot is a few

224
00:11:46,610 --> 00:11:48,540
 

225
00:11:46,620 --> 00:11:50,519
 kind of key observations about the

226
00:11:48,530 --> 00:11:50,519
 

227
00:11:48,540 --> 00:11:57,930
 motivation for statistical learning

228
00:11:50,509 --> 00:11:57,930
 

229
00:11:50,519 --> 00:12:00,990
 theory so what is a learning algorithm

230
00:11:57,920 --> 00:12:00,990
 

231
00:11:57,930 --> 00:12:03,420
 doing for a fixed algorithm function

232
00:12:00,980 --> 00:12:03,420
 

233
00:12:00,990 --> 00:12:06,440
 class and sample size the generating

234
00:12:03,410 --> 00:12:06,440
 

235
00:12:03,420 --> 00:12:09,029
 random samples those the training set

236
00:12:06,430 --> 00:12:09,029
 

237
00:12:06,440 --> 00:12:12,570
 gives this distribution of test errors

238
00:12:09,019 --> 00:12:12,570
 

239
00:12:09,029 --> 00:12:14,910
 and the point that comes out of that

240
00:12:12,560 --> 00:12:14,910
 

241
00:12:12,570 --> 00:12:16,560
 plot is if we just look at the mean of

242
00:12:14,900 --> 00:12:16,560
 

243
00:12:14,910 --> 00:12:20,190
 that distribution the mean of the error

244
00:12:16,550 --> 00:12:20,190
 

245
00:12:16,560 --> 00:12:22,230
 distribution it can be misleading the

246
00:12:20,180 --> 00:12:22,230
 

247
00:12:20,190 --> 00:12:25,350
 point being when we actually run the

248
00:12:22,220 --> 00:12:25,350
 

249
00:12:22,230 --> 00:12:27,240
 algorithm we will only have one training

250
00:12:25,340 --> 00:12:27,240
 

251
00:12:25,350 --> 00:12:30,029
 set to use that will be the data we

252
00:12:27,230 --> 00:12:30,029
 

253
00:12:27,240 --> 00:12:32,910
 actually got for training that algorithm

254
00:12:30,019 --> 00:12:32,910
 

255
00:12:30,029 --> 00:12:34,199
 and so the mean of that distribution may

256
00:12:32,900 --> 00:12:34,199
 

257
00:12:32,910 --> 00:12:35,970
 not be much used to us

258
00:12:34,189 --> 00:12:35,970
 

259
00:12:34,199 --> 00:12:39,510
 what we want to know is how we perform

260
00:12:35,960 --> 00:12:39,510
 

261
00:12:35,970 --> 00:12:40,949
 on our training set and this is what

262
00:12:39,500 --> 00:12:40,949
 

263
00:12:39,510 --> 00:12:43,050
 statistical learning theory is

264
00:12:40,939 --> 00:12:43,050
 

265
00:12:40,949 --> 00:12:45,779
 attempting to do it's looking at the

266
00:12:43,040 --> 00:12:45,779
 

267
00:12:43,050 --> 00:12:47,820
 tail of the distribution and finding

268
00:12:45,769 --> 00:12:47,820
 

269
00:12:45,779 --> 00:12:52,560
 bounds which hold with high probability

270
00:12:47,810 --> 00:12:52,560
 

271
00:12:47,820 --> 00:12:55,290
 over the random sample of training data

272
00:12:52,550 --> 00:12:55,290
 

273
00:12:52,560 --> 00:12:58,319
 so we want to be confident of the

274
00:12:55,280 --> 00:12:58,319
 

275
00:12:55,290 --> 00:13:00,630
 performance that we're going to get in

276
00:12:58,309 --> 00:13:00,630
 

277
00:12:58,319 --> 00:13:03,630
 that sense it's called worst-case

278
00:13:00,620 --> 00:13:03,630
 

279
00:13:00,630 --> 00:13:05,760
 performance but I think that term may be

280
00:13:03,620 --> 00:13:05,760
 

281
00:13:03,630 --> 00:13:11,699
 misleading and now we'll come back to it

282
00:13:05,750 --> 00:13:11,699
 

283
00:13:05,760 --> 00:13:14,190
 in the following slides so we with

284
00:13:11,689 --> 00:13:14,190
 

285
00:13:11,699 --> 00:13:16,350
 similar in sense to a statistical test

286
00:13:14,180 --> 00:13:16,350
 

287
00:13:14,190 --> 00:13:18,839
 we want some sort of confidence level

288
00:13:16,340 --> 00:13:18,839
 

289
00:13:16,350 --> 00:13:20,819
 which will bound the chances that the

290
00:13:18,829 --> 00:13:20,819
 

291
00:13:18,839 --> 00:13:24,299
 conclusion we have drawn from

292
00:13:20,809 --> 00:13:24,299
 

293
00:13:20,819 --> 00:13:27,899
 data is not true so the possibility of

294
00:13:24,289 --> 00:13:27,899
 

295
00:13:24,299 --> 00:13:30,569
 us being misled and this has given rise

296
00:13:27,889 --> 00:13:30,569
 

297
00:13:27,899 --> 00:13:33,589
 to the acronym probably approximately

298
00:13:30,559 --> 00:13:33,589
 

299
00:13:30,569 --> 00:13:35,879
 correct which doesn't sound very

300
00:13:33,579 --> 00:13:35,879
 

301
00:13:33,589 --> 00:13:38,069
 confidence inspiring but what it means

302
00:13:35,869 --> 00:13:38,069
 

303
00:13:35,879 --> 00:13:40,079
 by that is you have a confidence

304
00:13:38,059 --> 00:13:40,079
 

305
00:13:38,069 --> 00:13:43,289
 parameter and what you want to be able

306
00:13:40,069 --> 00:13:43,289
 

307
00:13:40,079 --> 00:13:46,109
 to bound is the probability that you

308
00:13:43,279 --> 00:13:46,109
 

309
00:13:43,289 --> 00:13:48,419
 have large error will be less than that

310
00:13:46,099 --> 00:13:48,419
 

311
00:13:46,109 --> 00:13:51,119
 confidence that confidence is that tail

312
00:13:48,409 --> 00:13:51,119
 

313
00:13:48,419 --> 00:13:54,600
 bound on the distribution we want to be

314
00:13:51,109 --> 00:13:54,600
 

315
00:13:51,119 --> 00:13:58,229
 confident that we are not going to be

316
00:13:54,590 --> 00:13:58,229
 

317
00:13:54,600 --> 00:14:00,539
 having worse performance than the bound

318
00:13:58,219 --> 00:14:00,539
 

319
00:13:58,229 --> 00:14:05,309
 is giving us or the chances of that

320
00:14:00,529 --> 00:14:05,309
 

321
00:14:00,539 --> 00:14:08,069
 happening are very small and hence we

322
00:14:05,299 --> 00:14:08,069
 

323
00:14:05,309 --> 00:14:10,169
 say high confidence bound the

324
00:14:08,059 --> 00:14:10,169
 

325
00:14:08,069 --> 00:14:12,479
 probability of being approximately

326
00:14:10,159 --> 00:14:12,479
 

327
00:14:10,169 --> 00:14:15,689
 correct is high so we're probably

328
00:14:12,469 --> 00:14:15,689
 

329
00:14:12,479 --> 00:14:18,289
 approximately correct that's the if I

330
00:14:15,679 --> 00:14:18,289
 

331
00:14:15,689 --> 00:14:23,189
 show you the whole thing again the

332
00:14:18,279 --> 00:14:23,189
 

333
00:14:18,289 --> 00:14:26,100
 bounds here for the the parson window

334
00:14:23,179 --> 00:14:26,100
 

335
00:14:23,189 --> 00:14:29,279
 this is of quite weak performance in the

336
00:14:26,090 --> 00:14:29,279
 

337
00:14:26,100 --> 00:14:31,409
 probably approximately correct sense 0.3

338
00:14:29,269 --> 00:14:31,409
 

339
00:14:29,279 --> 00:14:35,329
 error whereas the bounds for the SVM in

340
00:14:31,399 --> 00:14:35,329
 

341
00:14:31,409 --> 00:14:38,069
 this case are around point two two or

342
00:14:35,319 --> 00:14:38,069
 

343
00:14:35,329 --> 00:14:40,859
 significantly better so that's just

344
00:14:38,059 --> 00:14:40,859
 

345
00:14:38,069 --> 00:14:43,039
 framing what statistical learning theory

346
00:14:40,849 --> 00:14:43,039
 

347
00:14:40,859 --> 00:14:47,720
 is trying to do is trying to give us

348
00:14:43,029 --> 00:14:47,720
 

349
00:14:43,039 --> 00:14:51,720
 reliable estimates that we can use to

350
00:14:47,710 --> 00:14:51,720
 

351
00:14:47,720 --> 00:14:54,389
 perform an understanding of algorithms

352
00:14:51,710 --> 00:14:54,389
 

353
00:14:51,720 --> 00:14:56,220
 and also of their performance so I'm

354
00:14:54,379 --> 00:14:56,220
 

355
00:14:54,389 --> 00:15:00,359
 going to now move to an overview of what

356
00:14:56,210 --> 00:15:00,359
 

357
00:14:56,220 --> 00:15:03,029
 we would like to cover so we I'll be

358
00:15:00,349 --> 00:15:03,029
 

359
00:15:00,359 --> 00:15:04,889
 giving some definitions and notation

360
00:15:03,019 --> 00:15:04,889
 

361
00:15:03,029 --> 00:15:06,119
 risk measures and generalizations some

362
00:15:04,879 --> 00:15:06,119
 

363
00:15:04,889 --> 00:15:08,850
 of the definitions that are required

364
00:15:06,109 --> 00:15:08,850
 

365
00:15:06,119 --> 00:15:12,449
 I'll then be handing over to Omar to

366
00:15:08,840 --> 00:15:12,449
 

367
00:15:08,850 --> 00:15:14,669
 give an introduction to what I'm calling

368
00:15:12,439 --> 00:15:14,669
 

369
00:15:12,449 --> 00:15:15,749
 first-generation statistical learning

370
00:15:14,659 --> 00:15:15,749
 

371
00:15:14,669 --> 00:15:18,419
 theory

372
00:15:15,739 --> 00:15:18,419
 

373
00:15:15,749 --> 00:15:19,559
 so it's worst case Uniform bounds and

374
00:15:18,409 --> 00:15:19,559
 

375
00:15:18,419 --> 00:15:21,749
 the vac nature of and incus

376
00:15:19,549 --> 00:15:21,749
 

377
00:15:19,559 --> 00:15:24,929
 characterization of learnability

378
00:15:21,739 --> 00:15:24,929
 

379
00:15:21,749 --> 00:15:26,730
 so this is a sort of historical context

380
00:15:24,919 --> 00:15:26,730
 

381
00:15:24,929 --> 00:15:27,999
 that will then set the scene for

382
00:15:26,720 --> 00:15:27,999
 

383
00:15:26,730 --> 00:15:30,799
[Music]

384
00:15:27,989 --> 00:15:30,799
 

385
00:15:27,999 --> 00:15:32,029
 the second-generation statistical

386
00:15:30,789 --> 00:15:32,029
 

387
00:15:30,799 --> 00:15:34,670
 learning theory which I'll I'll be

388
00:15:32,019 --> 00:15:34,670
 

389
00:15:32,029 --> 00:15:36,470
 presenting which is where we arrived at

390
00:15:34,660 --> 00:15:36,470
 

391
00:15:34,670 --> 00:15:39,739
 so called hypothesis dependent

392
00:15:36,460 --> 00:15:39,739
 

393
00:15:36,470 --> 00:15:42,529
 complexity and structural risk

394
00:15:39,729 --> 00:15:42,529
 

395
00:15:39,739 --> 00:15:45,769
 minimization margin bounds and the plant

396
00:15:42,519 --> 00:15:45,769
 

397
00:15:42,529 --> 00:15:49,189
 Bayes framework and then finally Omar

398
00:15:45,759 --> 00:15:49,189
 

399
00:15:45,769 --> 00:15:51,499
 will then give some ideas about next

400
00:15:49,179 --> 00:15:51,499
 

401
00:15:49,189 --> 00:15:53,720
 generation statistical learning theory

402
00:15:51,489 --> 00:15:53,720
 

403
00:15:51,499 --> 00:15:56,119
 looking at stability deep neural

404
00:15:53,710 --> 00:15:56,119
 

405
00:15:53,720 --> 00:15:59,059
 networks and future directions so that's

406
00:15:56,109 --> 00:15:59,059
 

407
00:15:56,119 --> 00:16:03,529
 the overview of what we plan to cover in

408
00:15:59,049 --> 00:16:03,529
 

409
00:15:59,059 --> 00:16:06,499
 this tutorial we'll try and leave time

410
00:16:03,519 --> 00:16:06,499
 

411
00:16:03,529 --> 00:16:08,589
 for questions at the end if if we

412
00:16:06,489 --> 00:16:08,589
 

413
00:16:06,499 --> 00:16:12,350
 succeed

414
00:16:08,579 --> 00:16:12,350
 

415
00:16:08,589 --> 00:16:17,540
 so things we will do we'll focus on the

416
00:16:12,340 --> 00:16:17,540
 

417
00:16:12,350 --> 00:16:21,439
 aims the methods and the key ideas will

418
00:16:17,530 --> 00:16:21,439
 

419
00:16:17,540 --> 00:16:22,489
 give outlines of some proofs and we're

420
00:16:21,429 --> 00:16:22,489
 

421
00:16:21,439 --> 00:16:24,230
 aiming at what we're calling a

422
00:16:22,479 --> 00:16:24,230
 

423
00:16:22,489 --> 00:16:25,669
 Hitchhiker's Guide in the sense that we

424
00:16:24,220 --> 00:16:25,669
 

425
00:16:24,230 --> 00:16:27,619
 want to give you an understanding of

426
00:16:25,659 --> 00:16:27,619
 

427
00:16:25,669 --> 00:16:30,100
 what you can get out of statistical

428
00:16:27,609 --> 00:16:30,100
 

429
00:16:27,619 --> 00:16:33,009
 learning theory how it can help you in

430
00:16:30,090 --> 00:16:33,009
 

431
00:16:30,100 --> 00:16:37,189
 may be thinking about algorithms

432
00:16:32,999 --> 00:16:37,189
 

433
00:16:33,009 --> 00:16:40,569
 thinking about performance and and using

434
00:16:37,179 --> 00:16:40,569
 

435
00:16:37,189 --> 00:16:42,699
 the exploiting the methods that

436
00:16:40,559 --> 00:16:42,699
 

437
00:16:40,569 --> 00:16:45,739
 statistical learning theory can provide

438
00:16:42,689 --> 00:16:45,739
 

439
00:16:42,699 --> 00:16:49,699
 what we will not be doing is giving

440
00:16:45,729 --> 00:16:49,699
 

441
00:16:45,739 --> 00:16:53,600
 detailed proofs or a full literature and

442
00:16:49,689 --> 00:16:53,600
 

443
00:16:49,699 --> 00:16:55,639
 I can apologize right away to the many

444
00:16:53,590 --> 00:16:55,639
 

445
00:16:53,600 --> 00:16:58,939
 many papers we will fail to reference I

446
00:16:55,629 --> 00:16:58,939
 

447
00:16:55,639 --> 00:17:01,100
 have given that we've given a short sort

448
00:16:58,929 --> 00:17:01,100
 

449
00:16:58,939 --> 00:17:03,230
 of bibliography at the end but it is no

450
00:17:01,090 --> 00:17:03,230
 

451
00:17:01,100 --> 00:17:05,120
 even remotely attempting to be

452
00:17:03,220 --> 00:17:05,120
 

453
00:17:03,230 --> 00:17:07,519
 comprehensive so please accept my

454
00:17:05,110 --> 00:17:07,519
 

455
00:17:05,120 --> 00:17:10,159
 apologies if your papers are not

456
00:17:07,509 --> 00:17:10,159
 

457
00:17:07,519 --> 00:17:13,159
 included we've tried to at least give

458
00:17:10,149 --> 00:17:13,159
 

459
00:17:10,159 --> 00:17:17,209
 some starting references for people to

460
00:17:13,149 --> 00:17:17,209
 

461
00:17:13,159 --> 00:17:19,429
 link to we're certainly not going to aim

462
00:17:17,199 --> 00:17:19,429
 

463
00:17:17,209 --> 00:17:23,179
 at a complete history and we're focusing

464
00:17:19,419 --> 00:17:23,179
 

465
00:17:19,429 --> 00:17:26,929
 down onto a particular learning paradigm

466
00:17:23,169 --> 00:17:26,929
 

467
00:17:23,179 --> 00:17:29,210
 we will allude to others but the main

468
00:17:26,919 --> 00:17:29,210
 

469
00:17:26,929 --> 00:17:32,149
 focus will be this statistical learning

470
00:17:29,200 --> 00:17:32,149
 

471
00:17:29,210 --> 00:17:34,549
 approach and but we're not going to give

472
00:17:32,139 --> 00:17:34,549
 

473
00:17:32,149 --> 00:17:37,180
 an exact rÃ¡pida coverage of statistical

474
00:17:34,539 --> 00:17:37,180
 

475
00:17:34,549 --> 00:17:40,600
 learning theory so just those

476
00:17:37,170 --> 00:17:40,600
 

477
00:17:37,180 --> 00:17:46,030
 the ants okay so let me get into this

478
00:17:40,590 --> 00:17:46,030
 

479
00:17:40,600 --> 00:17:47,170
 just introducing some of the notation so

480
00:17:46,020 --> 00:17:47,170
 

481
00:17:46,030 --> 00:17:49,300
 we're thinking of a learning algorithm

482
00:17:47,160 --> 00:17:49,300
 

483
00:17:47,170 --> 00:17:52,330
 as being a mapping from a training set

484
00:17:49,290 --> 00:17:52,330
 

485
00:17:49,300 --> 00:17:54,820
 and we're gonna fix on M as being the

486
00:17:52,320 --> 00:17:54,820
 

487
00:17:52,330 --> 00:17:57,760
 size of the training set and we're

488
00:17:54,810 --> 00:17:57,760
 

489
00:17:54,820 --> 00:18:00,760
 typically thinking of pairs input-output

490
00:17:57,750 --> 00:18:00,760
 

491
00:17:57,760 --> 00:18:03,190
 pairs in the training set input and

492
00:18:00,750 --> 00:18:03,190
 

493
00:18:00,760 --> 00:18:04,810
 corresponding label in some cases there

494
00:18:03,180 --> 00:18:04,810
 

495
00:18:03,190 --> 00:18:07,690
 might not be a label but we're going to

496
00:18:04,800 --> 00:18:07,690
 

497
00:18:04,810 --> 00:18:10,830
 leave that in there anyway as a sort of

498
00:18:07,680 --> 00:18:10,830
 

499
00:18:07,690 --> 00:18:13,150
 standard notation we're thinking of a

500
00:18:10,820 --> 00:18:13,150
 

501
00:18:10,830 --> 00:18:16,060
 hypothesis class which is a set of

502
00:18:13,140 --> 00:18:16,060
 

503
00:18:13,150 --> 00:18:17,560
 predictors for example classifiers in

504
00:18:16,050 --> 00:18:17,560
 

505
00:18:16,060 --> 00:18:21,010
 the case where the labels might be

506
00:18:17,550 --> 00:18:21,010
 

507
00:18:17,560 --> 00:18:24,730
 binary labels but there are other

508
00:18:21,000 --> 00:18:24,730
 

509
00:18:21,010 --> 00:18:28,930
 possibilities of course we're expecting

510
00:18:24,720 --> 00:18:28,930
 

511
00:18:24,730 --> 00:18:33,480
 to have a training set which is a set of

512
00:18:28,920 --> 00:18:33,480
 

513
00:18:28,930 --> 00:18:36,340
 these pairs input-output pairs of size M

514
00:18:33,470 --> 00:18:36,340
 

515
00:18:33,480 --> 00:18:39,580
 so it's a finite sequence of input

516
00:18:36,330 --> 00:18:39,580
 

517
00:18:36,340 --> 00:18:43,060
 labeled examples and the key assumptions

518
00:18:39,570 --> 00:18:43,060
 

519
00:18:39,580 --> 00:18:45,010
 that are made by that we're going to use

520
00:18:43,050 --> 00:18:45,010
 

521
00:18:43,060 --> 00:18:47,410
 in this presentation are that the

522
00:18:45,000 --> 00:18:47,410
 

523
00:18:45,010 --> 00:18:49,750
 there's a data generating distribution

524
00:18:47,400 --> 00:18:49,750
 

525
00:18:47,410 --> 00:18:53,470
 which we're going to denote by P over

526
00:18:49,740 --> 00:18:53,470
 

527
00:18:49,750 --> 00:18:56,830
 these pairs the learner doesn't know

528
00:18:53,460 --> 00:18:56,830
 

529
00:18:53,470 --> 00:19:00,640
 what P is and only learns about P

530
00:18:56,820 --> 00:19:00,640
 

531
00:18:56,830 --> 00:19:03,220
 through seeing this training data and it

532
00:19:00,630 --> 00:19:03,220
 

533
00:19:00,640 --> 00:19:06,940
 gives insight into P because the

534
00:19:03,210 --> 00:19:06,940
 

535
00:19:03,220 --> 00:19:09,340
 training data is examples a generated a

536
00:19:06,930 --> 00:19:09,340
 

537
00:19:06,940 --> 00:19:14,890
 ID from that distribution so that's the

538
00:19:09,330 --> 00:19:14,890
 

539
00:19:09,340 --> 00:19:16,330
 key link so we're having to our test

540
00:19:14,880 --> 00:19:16,330
 

541
00:19:14,890 --> 00:19:18,940
 performance will be the performance

542
00:19:16,320 --> 00:19:18,940
 

543
00:19:16,330 --> 00:19:21,030
 under that distribution of a randomly

544
00:19:18,930 --> 00:19:21,030
 

545
00:19:18,940 --> 00:19:24,190
 generated example on that distribution

546
00:19:21,020 --> 00:19:24,190
 

547
00:19:21,030 --> 00:19:26,230
 but we don't the only access we have to

548
00:19:24,180 --> 00:19:26,230
 

549
00:19:24,190 --> 00:19:29,170
 that distribution is through those M

550
00:19:26,220 --> 00:19:29,170
 

551
00:19:26,230 --> 00:19:33,310
 training examples generated iid

552
00:19:29,160 --> 00:19:33,310
 

553
00:19:29,170 --> 00:19:36,190
 according to that distribution these

554
00:19:33,300 --> 00:19:36,190
 

555
00:19:33,310 --> 00:19:39,220
 assumptions can be relaxed in many

556
00:19:36,180 --> 00:19:39,220
 

557
00:19:36,190 --> 00:19:42,010
 respects but we won't be discussing that

558
00:19:39,210 --> 00:19:42,010
 

559
00:19:39,220 --> 00:19:45,520
 in this tutorial so for example the iid

560
00:19:42,000 --> 00:19:45,520
 

561
00:19:42,010 --> 00:19:48,250
 assumption can be weakened and and so on

562
00:19:45,510 --> 00:19:48,250
 

563
00:19:45,520 --> 00:19:51,360
 so we're not going to address that but

564
00:19:48,240 --> 00:19:51,360
 

565
00:19:48,250 --> 00:19:51,360
 just for infamy

566
00:19:52,710 --> 00:19:52,710
 

567
00:19:52,720 --> 00:19:59,929
 okay so the things that we would like to

568
00:19:56,310 --> 00:19:59,929
 

569
00:19:56,320 --> 00:20:02,659
 do with a sample is clearly learn a

570
00:19:59,919 --> 00:20:02,659
 

571
00:19:59,929 --> 00:20:05,210
 predictor but also we would like to

572
00:20:02,649 --> 00:20:05,210
 

573
00:20:02,659 --> 00:20:08,330
 certify the predictors performance we'd

574
00:20:05,200 --> 00:20:08,330
 

575
00:20:05,210 --> 00:20:10,750
 like to give some if you like a

576
00:20:08,320 --> 00:20:10,750
 

577
00:20:08,330 --> 00:20:13,669
 certificate that will tell us the

578
00:20:10,740 --> 00:20:13,669
 

579
00:20:10,750 --> 00:20:15,860
 performance we can expect as I said with

580
00:20:13,659 --> 00:20:15,860
 

581
00:20:13,669 --> 00:20:17,750
 high probability some sort of tail bound

582
00:20:15,850 --> 00:20:17,750
 

583
00:20:15,860 --> 00:20:19,899
 will say okay there's a chance we've

584
00:20:17,740 --> 00:20:19,899
 

585
00:20:17,750 --> 00:20:23,059
 been misled by this training set but

586
00:20:19,889 --> 00:20:23,059
 

587
00:20:19,899 --> 00:20:28,039
 modular that probability we're expecting

588
00:20:23,049 --> 00:20:28,039
 

589
00:20:23,059 --> 00:20:30,169
 this performance so the two parts

590
00:20:28,029 --> 00:20:30,169
 

591
00:20:28,039 --> 00:20:32,179
 learning a predictor we would like to

592
00:20:30,159 --> 00:20:32,179
 

593
00:20:30,169 --> 00:20:34,669
 have algorithms driven by some learning

594
00:20:32,169 --> 00:20:34,669
 

595
00:20:32,179 --> 00:20:36,950
 principle and they will be informed by

596
00:20:34,659 --> 00:20:36,950
 

597
00:20:34,669 --> 00:20:39,830
 prior knowledge that be will be

598
00:20:36,940 --> 00:20:39,830
 

599
00:20:36,950 --> 00:20:43,760
 resulting in inductive bias and we'll be

600
00:20:39,820 --> 00:20:43,760
 

601
00:20:39,830 --> 00:20:46,010
 trying to understand the effect of that

602
00:20:43,750 --> 00:20:46,010
 

603
00:20:43,760 --> 00:20:48,710
 bias on the performance of the algorithm

604
00:20:46,000 --> 00:20:48,710
 

605
00:20:46,010 --> 00:20:51,080
 and this will be giving us the

606
00:20:48,700 --> 00:20:51,080
 

607
00:20:48,710 --> 00:20:53,210
 certification of performance which is

608
00:20:51,070 --> 00:20:53,210
 

609
00:20:51,080 --> 00:20:55,070
 really what's happening beyond the

610
00:20:53,200 --> 00:20:55,070
 

611
00:20:53,210 --> 00:20:58,460
 training set so we can clearly observe

612
00:20:55,060 --> 00:20:58,460
 

613
00:20:55,070 --> 00:21:00,260
 what happens on the training set but

614
00:20:58,450 --> 00:21:00,260
 

615
00:20:58,460 --> 00:21:02,870
 what we're interested in is how we're

616
00:21:00,250 --> 00:21:02,870
 

617
00:21:00,260 --> 00:21:04,549
 going to perform in the wild if you like

618
00:21:02,860 --> 00:21:04,549
 

619
00:21:02,870 --> 00:21:07,159
 or beyond the training set when we

620
00:21:04,539 --> 00:21:07,159
 

621
00:21:04,549 --> 00:21:08,960
 actually run this algorithm on new data

622
00:21:07,149 --> 00:21:08,960
 

623
00:21:07,159 --> 00:21:11,120
 generated according to the same

624
00:21:08,950 --> 00:21:11,120
 

625
00:21:08,960 --> 00:21:14,750
 distribution and we refer to that as

626
00:21:11,110 --> 00:21:14,750
 

627
00:21:11,120 --> 00:21:16,940
 generalization bounds but actually I

628
00:21:14,740 --> 00:21:16,940
 

629
00:21:14,750 --> 00:21:19,490
 think we're gonna highlight I hope in

630
00:21:16,930 --> 00:21:19,490
 

631
00:21:16,940 --> 00:21:23,270
 this presentation that these two things

632
00:21:19,480 --> 00:21:23,270
 

633
00:21:19,490 --> 00:21:26,419
 interact a good bound will give rise to

634
00:21:23,260 --> 00:21:26,419
 

635
00:21:23,270 --> 00:21:28,820
 an algorithm that optimizes that bound

636
00:21:26,409 --> 00:21:28,820
 

637
00:21:26,419 --> 00:21:32,029
 so it's not that we're just doing this

638
00:21:28,810 --> 00:21:32,029
 

639
00:21:28,820 --> 00:21:34,580
 theory for theories sake potentially we

640
00:21:32,019 --> 00:21:34,580
 

641
00:21:32,029 --> 00:21:38,419
 will translate that theoretical insight

642
00:21:34,570 --> 00:21:38,419
 

643
00:21:34,580 --> 00:21:41,210
 into an improved algorithm that will as

644
00:21:38,409 --> 00:21:41,210
 

645
00:21:38,419 --> 00:21:44,419
 it were drive as we saw in that case of

646
00:21:41,200 --> 00:21:44,419
 

647
00:21:41,210 --> 00:21:46,399
 the linear SVM an improved distribution

648
00:21:44,409 --> 00:21:46,399
 

649
00:21:44,419 --> 00:21:48,049
 of errors and hence an improved

650
00:21:46,389 --> 00:21:48,049
 

651
00:21:46,399 --> 00:21:51,830
 performance that we would hope to

652
00:21:48,039 --> 00:21:51,830
 

653
00:21:48,049 --> 00:21:54,080
 achieve using that algorithm on the data

654
00:21:51,820 --> 00:21:54,080
 

655
00:21:51,830 --> 00:21:56,600
 set that we actually get given so these

656
00:21:54,070 --> 00:21:56,600
 

657
00:21:54,080 --> 00:21:59,630
 are the the things we would like to

658
00:21:56,590 --> 00:21:59,630
 

659
00:21:56,600 --> 00:22:02,360
 achieve the measures of performance will

660
00:21:59,620 --> 00:22:02,360
 

661
00:21:59,630 --> 00:22:03,430
 always be through a loss function which

662
00:22:02,350 --> 00:22:03,430
 

663
00:22:02,360 --> 00:22:05,800
 is measuring

664
00:22:03,420 --> 00:22:05,800
 

665
00:22:03,430 --> 00:22:09,310
 the discrepancy between the prediction

666
00:22:05,790 --> 00:22:09,310
 

667
00:22:05,800 --> 00:22:13,390
 that we make on the input with the

668
00:22:09,300 --> 00:22:13,390
 

669
00:22:09,310 --> 00:22:17,050
 function H and the true label y so we

670
00:22:13,380 --> 00:22:17,050
 

671
00:22:13,390 --> 00:22:19,810
 have two measures of performance the

672
00:22:17,040 --> 00:22:19,810
 

673
00:22:17,050 --> 00:22:21,880
 in-sample performance which is the

674
00:22:19,800 --> 00:22:21,880
 

675
00:22:19,810 --> 00:22:25,660
 empirical risk which is basically the

676
00:22:21,870 --> 00:22:25,660
 

677
00:22:21,880 --> 00:22:28,450
 average of the loss that we experienced

678
00:22:25,650 --> 00:22:28,450
 

679
00:22:25,660 --> 00:22:30,850
 on the training data so the M training

680
00:22:28,440 --> 00:22:30,850
 

681
00:22:28,450 --> 00:22:33,390
 samples and the theoretical risk or

682
00:22:30,840 --> 00:22:33,390
 

683
00:22:30,850 --> 00:22:35,380
 out-of-sample risk which is the

684
00:22:33,380 --> 00:22:35,380
 

685
00:22:33,390 --> 00:22:37,720
 generalization error the expected

686
00:22:35,370 --> 00:22:37,720
 

687
00:22:35,380 --> 00:22:40,690
 performance the expected risk that we

688
00:22:37,710 --> 00:22:40,690
 

689
00:22:37,720 --> 00:22:44,500
 would see from a randomly chosen test

690
00:22:40,680 --> 00:22:44,500
 

691
00:22:40,690 --> 00:22:47,320
 example and just as examples of the

692
00:22:44,490 --> 00:22:47,320
 

693
00:22:44,500 --> 00:22:49,930
 types of loss function we could consider

694
00:22:47,310 --> 00:22:49,930
 

695
00:22:47,320 --> 00:22:53,320
 zero one loss which would be for

696
00:22:49,920 --> 00:22:53,320
 

697
00:22:49,930 --> 00:22:56,380
 classification would say one if we make

698
00:22:53,310 --> 00:22:56,380
 

699
00:22:53,320 --> 00:23:00,100
 an error zero if we don't squared loss

700
00:22:56,370 --> 00:23:00,100
 

701
00:22:56,380 --> 00:23:03,220
 appropriate for regression hinge loss

702
00:23:00,090 --> 00:23:03,220
 

703
00:23:00,100 --> 00:23:05,440
 which is the one that's used in an SVM

704
00:23:03,210 --> 00:23:05,440
 

705
00:23:03,220 --> 00:23:07,800
 and we'll come back to that later which

706
00:23:05,430 --> 00:23:07,800
 

707
00:23:05,440 --> 00:23:12,790
 is some sort of proxy for or a

708
00:23:07,790 --> 00:23:12,790
 

709
00:23:07,800 --> 00:23:15,490
 intermediate between the continuous loss

710
00:23:12,780 --> 00:23:15,490
 

711
00:23:12,790 --> 00:23:17,800
 and the zero one loss and then for

712
00:23:15,480 --> 00:23:17,800
 

713
00:23:15,490 --> 00:23:19,840
 example we could have a log loss if we

714
00:23:17,790 --> 00:23:19,840
 

715
00:23:17,800 --> 00:23:21,430
 were doing density estimation where

716
00:23:19,830 --> 00:23:21,430
 

717
00:23:19,840 --> 00:23:24,220
 actually there isn't a label so the

718
00:23:21,420 --> 00:23:24,220
 

719
00:23:21,430 --> 00:23:30,040
 label there Y is a dummy variable in

720
00:23:24,210 --> 00:23:30,040
 

721
00:23:24,220 --> 00:23:32,590
 that example so generalization if H does

722
00:23:30,030 --> 00:23:32,590
 

723
00:23:30,040 --> 00:23:35,590
 well on the in-sample pairs will it do

724
00:23:32,580 --> 00:23:35,590
 

725
00:23:32,590 --> 00:23:37,450
 well on the out-of-sample pass and the

726
00:23:35,580 --> 00:23:37,450
 

727
00:23:35,590 --> 00:23:39,990
 thing that we're looking at in terms of

728
00:23:37,440 --> 00:23:39,990
 

729
00:23:37,450 --> 00:23:42,550
 performance is the gap between the

730
00:23:39,980 --> 00:23:42,550
 

731
00:23:39,990 --> 00:23:44,230
 in-sample performance that average on

732
00:23:42,540 --> 00:23:44,230
 

733
00:23:42,550 --> 00:23:46,750
 the training set and the out-of-sample

734
00:23:44,220 --> 00:23:46,750
 

735
00:23:44,230 --> 00:23:50,470
 performance the performance we can

736
00:23:46,740 --> 00:23:50,470
 

737
00:23:46,750 --> 00:23:52,720
 expect on new data and what we would

738
00:23:50,460 --> 00:23:52,720
 

739
00:23:50,470 --> 00:23:56,410
 like to have our upper bounds on that

740
00:23:52,710 --> 00:23:56,410
 

741
00:23:52,720 --> 00:23:59,350
 which hold with high probability and the

742
00:23:56,400 --> 00:23:59,350
 

743
00:23:56,410 --> 00:24:02,200
 Delta is as I mentioned already used to

744
00:23:59,340 --> 00:24:02,200
 

745
00:23:59,350 --> 00:24:04,960
 sort of quantify that probability so

746
00:24:02,190 --> 00:24:04,960
 

747
00:24:02,200 --> 00:24:08,200
 Delta H being less than or equal to some

748
00:24:04,950 --> 00:24:08,200
 

749
00:24:04,960 --> 00:24:13,750
 function epsilon of the sample size and

750
00:24:08,190 --> 00:24:13,750
 

751
00:24:08,200 --> 00:24:16,590
 that Delta confidence and that would

752
00:24:13,740 --> 00:24:16,590
 

753
00:24:13,750 --> 00:24:19,770
 correspond to the abound and up

754
00:24:16,580 --> 00:24:19,770
 

755
00:24:16,590 --> 00:24:22,140
 and on the actual error rate that we

756
00:24:19,760 --> 00:24:22,140
 

757
00:24:19,770 --> 00:24:25,279
 would expect on test data in terms of

758
00:24:22,130 --> 00:24:25,279
 

759
00:24:22,140 --> 00:24:30,029
 the empirical plus this gap function

760
00:24:25,269 --> 00:24:30,029
 

761
00:24:25,279 --> 00:24:34,250
 epsilon we're also interested in lower

762
00:24:30,019 --> 00:24:34,250
 

763
00:24:30,029 --> 00:24:36,960
 bounds and this is the possibility of

764
00:24:34,240 --> 00:24:36,960
 

765
00:24:34,250 --> 00:24:39,419
 getting an estimate of the minimum gap

766
00:24:36,950 --> 00:24:39,419
 

767
00:24:36,960 --> 00:24:41,250
 that we can expect we'll come back to

768
00:24:39,409 --> 00:24:41,250
 

769
00:24:39,419 --> 00:24:43,950
 that again later

770
00:24:41,240 --> 00:24:43,950
 

771
00:24:41,250 --> 00:24:47,039
 so flavors of these bounds we might

772
00:24:43,940 --> 00:24:47,039
 

773
00:24:43,950 --> 00:24:50,429
 expect distribution free distribution

774
00:24:47,029 --> 00:24:50,429
 

775
00:24:47,039 --> 00:24:53,580
 dependent algorithm free algorithm

776
00:24:50,419 --> 00:24:53,580
 

777
00:24:50,429 --> 00:24:55,230
 dependent and clearly you know there

778
00:24:53,570 --> 00:24:55,230
 

779
00:24:53,580 --> 00:24:57,630
 will be trade-offs with the amount of

780
00:24:55,220 --> 00:24:57,630
 

781
00:24:55,230 --> 00:25:00,330
 generality that we're able to achieve in

782
00:24:57,620 --> 00:25:00,330
 

783
00:24:57,630 --> 00:25:03,059
 terms of the performance sorry in terms

784
00:25:00,320 --> 00:25:03,059
 

785
00:25:00,330 --> 00:25:07,020
 yeah the quality of the bounds again

786
00:25:03,049 --> 00:25:07,020
 

787
00:25:03,059 --> 00:25:09,840
 we'll be mentioning that later okay so

788
00:25:07,010 --> 00:25:09,840
 

789
00:25:07,020 --> 00:25:11,580
 having set the scene in that way I'm

790
00:25:09,830 --> 00:25:11,580
 

791
00:25:09,840 --> 00:25:13,940
 going to hand over now to Omar who will

792
00:25:11,570 --> 00:25:13,940
 

793
00:25:11,580 --> 00:25:17,390
 take you through the first generation

794
00:25:13,930 --> 00:25:17,390
 

795
00:25:13,940 --> 00:25:17,390
 statistical learning theory

796
00:25:24,590 --> 00:25:24,590
 

797
00:25:24,600 --> 00:25:34,870
[Applause]

798
00:25:30,080 --> 00:25:34,870
 

799
00:25:30,090 --> 00:25:38,429
 thank you John so I would like to start

800
00:25:34,860 --> 00:25:38,429
 

801
00:25:34,870 --> 00:25:42,039
 by presenting the easiest possible

802
00:25:38,419 --> 00:25:42,039
 

803
00:25:38,429 --> 00:25:44,230
 setting of generalization it might seem

804
00:25:42,029 --> 00:25:44,230
 

805
00:25:42,039 --> 00:25:46,750
 like a very unrealistic but just four

806
00:25:44,220 --> 00:25:46,750
 

807
00:25:44,230 --> 00:25:48,940
 stars you know thinking from a very

808
00:25:46,740 --> 00:25:48,940
 

809
00:25:46,750 --> 00:25:53,320
 simple-minded point of view let's say

810
00:25:48,930 --> 00:25:53,320
 

811
00:25:48,940 --> 00:25:55,450
 that we have just one function that's

812
00:25:53,310 --> 00:25:55,450
 

813
00:25:53,320 --> 00:25:58,690
 all that we have access to one function

814
00:25:55,440 --> 00:25:58,690
 

815
00:25:55,450 --> 00:26:03,760
 and then we're going to use it as our

816
00:25:58,680 --> 00:26:03,760
 

817
00:25:58,690 --> 00:26:05,950
 building block generalization so if I

818
00:26:03,750 --> 00:26:05,950
 

819
00:26:03,760 --> 00:26:09,159
 have one fixed non data dependent

820
00:26:05,940 --> 00:26:09,159
 

821
00:26:05,950 --> 00:26:11,980
 function a non random function and I

822
00:26:09,149 --> 00:26:11,980
 

823
00:26:09,159 --> 00:26:15,580
 look at the empirical risk of that

824
00:26:11,970 --> 00:26:15,580
 

825
00:26:11,980 --> 00:26:17,710
 function I look at its expectation turns

826
00:26:15,570 --> 00:26:17,710
 

827
00:26:15,580 --> 00:26:21,700
 out to be equal to the theoretical to

828
00:26:17,700 --> 00:26:21,700
 

829
00:26:17,710 --> 00:26:24,909
 the out-of-sample risk so in this

830
00:26:21,690 --> 00:26:24,909
 

831
00:26:21,700 --> 00:26:27,280
 setting the problem of bounding the

832
00:26:24,899 --> 00:26:27,280
 

833
00:26:24,909 --> 00:26:30,460
 probability that the generalization gap

834
00:26:27,270 --> 00:26:30,460
 

835
00:26:27,280 --> 00:26:32,470
 is large turns out to be the problem of

836
00:26:30,450 --> 00:26:32,470
 

837
00:26:30,460 --> 00:26:34,179
 looking at the difference between a

838
00:26:32,460 --> 00:26:34,179
 

839
00:26:32,470 --> 00:26:36,130
 random variable and its expectation

840
00:26:34,169 --> 00:26:36,130
 

841
00:26:34,179 --> 00:26:39,390
 that's something that in probability is

842
00:26:36,120 --> 00:26:39,390
 

843
00:26:36,130 --> 00:26:41,830
 called the deviation inequality

844
00:26:39,380 --> 00:26:41,830
 

845
00:26:39,390 --> 00:26:43,679
 furthermore the terms in that sum

846
00:26:41,820 --> 00:26:43,679
 

847
00:26:41,830 --> 00:26:45,909
 defining the empirical risk are

848
00:26:43,669 --> 00:26:45,909
 

849
00:26:43,679 --> 00:26:48,250
 independent random variables that's

850
00:26:45,899 --> 00:26:48,250
 

851
00:26:45,909 --> 00:26:51,070
 because of the iid assumption and

852
00:26:48,240 --> 00:26:51,070
 

853
00:26:48,250 --> 00:26:54,250
 because we are looking at one known data

854
00:26:51,060 --> 00:26:54,250
 

855
00:26:51,070 --> 00:26:56,230
 dependent function only if the function

856
00:26:54,240 --> 00:26:56,230
 

857
00:26:54,250 --> 00:26:57,820
 was learn from data then correlations

858
00:26:56,220 --> 00:26:57,820
 

859
00:26:56,230 --> 00:26:59,650
 are introduced and they this reasoning

860
00:26:57,810 --> 00:26:59,650
 

861
00:26:57,820 --> 00:27:03,130
 does not work anymore but we are looking

862
00:26:59,640 --> 00:27:03,130
 

863
00:26:59,650 --> 00:27:05,650
 at a non data dependent function so if

864
00:27:03,120 --> 00:27:05,650
 

865
00:27:03,130 --> 00:27:07,419
 they are also bounded if we assume that

866
00:27:05,640 --> 00:27:07,419
 

867
00:27:05,650 --> 00:27:09,760
 those functions are bounded then we can

868
00:27:07,409 --> 00:27:09,760
 

869
00:27:07,419 --> 00:27:12,070
 use what's known as Houghton's

870
00:27:09,750 --> 00:27:12,070
 

871
00:27:09,760 --> 00:27:15,070
 inequality to bound the probability of

872
00:27:12,060 --> 00:27:15,070
 

873
00:27:12,070 --> 00:27:19,390
 having a large gap it's exponential in

874
00:27:15,060 --> 00:27:19,390
 

875
00:27:15,070 --> 00:27:23,020
 the sample size and in the and in the

876
00:27:19,380 --> 00:27:23,020
 

877
00:27:19,390 --> 00:27:24,610
 amount of deviation so the usual trick

878
00:27:23,010 --> 00:27:24,610
 

879
00:27:23,020 --> 00:27:26,740
 probably a lot of you're familiar with

880
00:27:24,600 --> 00:27:26,740
 

881
00:27:24,610 --> 00:27:28,840
 this if you make the right-hand side

882
00:27:26,730 --> 00:27:28,840
 

883
00:27:26,740 --> 00:27:31,210
 equal to Delta so for a given confidence

884
00:27:28,830 --> 00:27:31,210
 

885
00:27:28,840 --> 00:27:34,059
 parameter Delta you equate the right

886
00:27:31,200 --> 00:27:34,059
 

887
00:27:31,210 --> 00:27:37,120
 hand side to Delta and then solve for

888
00:27:34,049 --> 00:27:37,120
 

889
00:27:34,059 --> 00:27:41,230
 Epsilon what you end up with is the

890
00:27:37,110 --> 00:27:41,230
 

891
00:27:37,120 --> 00:27:43,750
 inequality below so with probability

892
00:27:41,220 --> 00:27:43,750
 

893
00:27:41,230 --> 00:27:47,649
 at least one minus Delta that's what we

894
00:27:43,740 --> 00:27:47,649
 

895
00:27:43,750 --> 00:27:51,340
 mean by with high probability we have an

896
00:27:47,639 --> 00:27:51,340
 

897
00:27:47,649 --> 00:27:54,070
 inequality for the true risk the true

898
00:27:51,330 --> 00:27:54,070
 

899
00:27:51,340 --> 00:27:57,130
 but unknown risk in terms of the

900
00:27:54,060 --> 00:27:57,130
 

901
00:27:54,070 --> 00:27:59,620
 empirical risk which is computable from

902
00:27:57,120 --> 00:27:59,620
 

903
00:27:57,130 --> 00:28:01,630
 sample data and a function that the

904
00:27:59,610 --> 00:28:01,630
 

905
00:27:59,620 --> 00:28:04,240
 parents just on the sample size and the

906
00:28:01,620 --> 00:28:04,240
 

907
00:28:01,630 --> 00:28:06,490
 confidence parameter so I would like you

908
00:28:04,230 --> 00:28:06,490
 

909
00:28:04,240 --> 00:28:08,350
 to to pay attention to that frame at you

910
00:28:06,480 --> 00:28:08,350
 

911
00:28:06,490 --> 00:28:10,600
 know to that frame part at the bottom

912
00:28:08,340 --> 00:28:10,600
 

913
00:28:08,350 --> 00:28:14,230
 does a typical picture of a

914
00:28:10,590 --> 00:28:14,230
 

915
00:28:10,600 --> 00:28:19,330
 generalization bound so scaling up a

916
00:28:14,220 --> 00:28:19,330
 

917
00:28:14,230 --> 00:28:23,830
 little bit another kind of simple

918
00:28:19,320 --> 00:28:23,830
 

919
00:28:19,330 --> 00:28:25,870
 scenario if you have a finite hypothesis

920
00:28:23,820 --> 00:28:25,870
 

921
00:28:23,830 --> 00:28:32,679
 class a finite set of examples from

922
00:28:25,860 --> 00:28:32,679
 

923
00:28:25,870 --> 00:28:39,070
 which to choose from then then your

924
00:28:32,669 --> 00:28:39,070
 

925
00:28:32,679 --> 00:28:41,289
 algorithm let's say that we use let's

926
00:28:39,060 --> 00:28:41,289
 

927
00:28:39,070 --> 00:28:44,289
 say that we look at the say that we aim

928
00:28:41,279 --> 00:28:44,289
 

929
00:28:41,289 --> 00:28:46,690
 for a uniform kind of bound if I use any

930
00:28:44,279 --> 00:28:46,690
 

931
00:28:44,289 --> 00:28:48,909
 function from this class how about and

932
00:28:46,680 --> 00:28:48,909
 

933
00:28:46,690 --> 00:28:50,470
 the generalization Gabbi

934
00:28:48,899 --> 00:28:50,470
 

935
00:28:48,909 --> 00:28:53,710
 that means bounded the probability that

936
00:28:50,460 --> 00:28:53,710
 

937
00:28:50,470 --> 00:29:01,929
 for all functions in the class the

938
00:28:53,700 --> 00:29:01,929
 

939
00:28:53,710 --> 00:29:03,820
 generalization gap is small so the tool

940
00:29:01,919 --> 00:29:03,820
 

941
00:29:01,929 --> 00:29:06,340
 to use in this case since we're dealing

942
00:29:03,810 --> 00:29:06,340
 

943
00:29:03,820 --> 00:29:09,309
 with a function the finite function

944
00:29:06,330 --> 00:29:09,309
 

945
00:29:06,340 --> 00:29:11,980
 class is a probability tool called the

946
00:29:09,299 --> 00:29:11,980
 

947
00:29:09,309 --> 00:29:14,220
 Union bound also known as countable

948
00:29:11,970 --> 00:29:14,220
 

949
00:29:11,980 --> 00:29:17,110
 subadditivity of the probability measure

950
00:29:14,210 --> 00:29:17,110
 

951
00:29:14,220 --> 00:29:18,820
 so look at the complement the

952
00:29:17,100 --> 00:29:18,820
 

953
00:29:17,110 --> 00:29:21,059
 probability that there is at least one

954
00:29:18,810 --> 00:29:21,059
 

955
00:29:18,820 --> 00:29:23,950
 function in the class that has a large

956
00:29:21,049 --> 00:29:23,950
 

957
00:29:21,059 --> 00:29:27,250
 generalization gap can be bounded from

958
00:29:23,940 --> 00:29:27,250
 

959
00:29:23,950 --> 00:29:30,490
 above by Assam in which all of the terms

960
00:29:27,240 --> 00:29:30,490
 

961
00:29:27,250 --> 00:29:32,740
 can be bounded just as in the previous

962
00:29:30,480 --> 00:29:32,740
 

963
00:29:30,490 --> 00:29:35,620
 slide we bounded the probability for a

964
00:29:32,730 --> 00:29:35,620
 

965
00:29:32,740 --> 00:29:37,899
 single function and you equate that to

966
00:29:35,610 --> 00:29:37,899
 

967
00:29:35,620 --> 00:29:41,380
 Delta then you end up with a

968
00:29:37,889 --> 00:29:41,380
 

969
00:29:37,899 --> 00:29:43,570
 generalization inequality with a

970
00:29:41,370 --> 00:29:43,570
 

971
00:29:41,380 --> 00:29:46,630
 generalization bound that holds with

972
00:29:43,560 --> 00:29:46,630
 

973
00:29:43,570 --> 00:29:50,340
 high probability uniformly over the

974
00:29:46,620 --> 00:29:50,340
 

975
00:29:46,630 --> 00:29:50,340
 finite function class

976
00:29:50,870 --> 00:29:50,870
 

977
00:29:50,880 --> 00:29:54,750
 the size of the function class comes in

978
00:29:52,880 --> 00:29:54,750
 

979
00:29:52,890 --> 00:29:58,980
 in that inequality inside the log term

980
00:29:54,740 --> 00:29:58,980
 

981
00:29:54,750 --> 00:30:01,260
 and the idea is that for more

982
00:29:58,970 --> 00:30:01,260
 

983
00:29:58,980 --> 00:30:03,120
 complicated cases so if now you want to

984
00:30:01,250 --> 00:30:03,120
 

985
00:30:01,260 --> 00:30:06,650
 scale further up and you don't have a

986
00:30:03,110 --> 00:30:06,650
 

987
00:30:03,120 --> 00:30:09,630
 finite number of functions anymore but

988
00:30:06,640 --> 00:30:09,630
 

989
00:30:06,650 --> 00:30:11,210
 may be infinite may be countably

990
00:30:09,620 --> 00:30:11,210
 

991
00:30:09,630 --> 00:30:15,300
 infinite possibly uncountably infinite

992
00:30:11,200 --> 00:30:15,300
 

993
00:30:11,210 --> 00:30:16,880
 then that number inside the log which

994
00:30:15,290 --> 00:30:16,880
 

995
00:30:15,300 --> 00:30:19,170
 right now is the size the number of

996
00:30:16,870 --> 00:30:19,170
 

997
00:30:16,880 --> 00:30:20,850
 elements in the function class that has

998
00:30:19,160 --> 00:30:20,850
 

999
00:30:19,170 --> 00:30:23,640
 to be replaced by something different

1000
00:30:20,840 --> 00:30:23,640
 

1001
00:30:20,850 --> 00:30:27,660
 ideally something that replaces the the

1002
00:30:23,630 --> 00:30:27,660
 

1003
00:30:23,640 --> 00:30:29,910
 notion of size how big the class is not

1004
00:30:27,650 --> 00:30:29,910
 

1005
00:30:27,660 --> 00:30:32,250
 just the cardinality because for

1006
00:30:29,900 --> 00:30:32,250
 

1007
00:30:29,910 --> 00:30:36,270
 infinite sets the cardinality well we

1008
00:30:32,240 --> 00:30:36,270
 

1009
00:30:32,250 --> 00:30:39,900
 would have a very informative bound on

1010
00:30:36,260 --> 00:30:39,900
 

1011
00:30:36,270 --> 00:30:43,170
 the right-hand side so the next question

1012
00:30:39,890 --> 00:30:43,170
 

1013
00:30:39,900 --> 00:30:50,760
 is what if uncountably infinite function

1014
00:30:43,160 --> 00:30:50,760
 

1015
00:30:43,170 --> 00:30:53,790
 class I'm going to go kind of just

1016
00:30:50,750 --> 00:30:53,790
 

1017
00:30:50,760 --> 00:30:56,250
 describe the main the main steps of what

1018
00:30:53,780 --> 00:30:56,250
 

1019
00:30:53,790 --> 00:30:59,640
 is done in this case one approach that's

1020
00:30:56,240 --> 00:30:59,640
 

1021
00:30:56,250 --> 00:31:02,700
 used to handle this case uses what's

1022
00:30:59,630 --> 00:31:02,700
 

1023
00:30:59,640 --> 00:31:05,640
 called the double sample trick so you

1024
00:31:02,690 --> 00:31:05,640
 

1025
00:31:02,700 --> 00:31:10,650
 use a ghost sample an independent copy

1026
00:31:05,630 --> 00:31:10,650
 

1027
00:31:05,640 --> 00:31:12,120
 of the sample and doing that you kind of

1028
00:31:10,640 --> 00:31:12,120
 

1029
00:31:10,650 --> 00:31:14,100
 pass from looking at the theoretical

1030
00:31:12,110 --> 00:31:14,100
 

1031
00:31:12,120 --> 00:31:17,010
 error to look in at the empirical error

1032
00:31:14,090 --> 00:31:17,010
 

1033
00:31:14,100 --> 00:31:20,790
 on the go sample then instead of dealing

1034
00:31:17,000 --> 00:31:20,790
 

1035
00:31:17,010 --> 00:31:22,110
 with a huge space you know where the

1036
00:31:20,780 --> 00:31:22,110
 

1037
00:31:20,790 --> 00:31:23,760
 theoretical error leaves you're

1038
00:31:22,100 --> 00:31:23,760
 

1039
00:31:22,110 --> 00:31:26,310
 basically looking at a finite number of

1040
00:31:23,750 --> 00:31:26,310
 

1041
00:31:23,760 --> 00:31:28,590
 possible behaviors determined by the

1042
00:31:26,300 --> 00:31:28,590
 

1043
00:31:26,310 --> 00:31:31,530
 second sample and then with a Union

1044
00:31:28,580 --> 00:31:31,530
 

1045
00:31:28,590 --> 00:31:33,660
 bound but a clever use of a Union bound

1046
00:31:31,520 --> 00:31:33,660
 

1047
00:31:31,530 --> 00:31:35,640
 where they overlap some being handled in

1048
00:31:33,650 --> 00:31:35,640
 

1049
00:31:33,660 --> 00:31:37,290
 a smart way then you can end up with

1050
00:31:35,630 --> 00:31:37,290
 

1051
00:31:35,640 --> 00:31:40,050
 still with the probability inequality

1052
00:31:37,280 --> 00:31:40,050
 

1053
00:31:37,290 --> 00:31:42,810
 that that is informative a

1054
00:31:40,040 --> 00:31:42,810
 

1055
00:31:40,050 --> 00:31:45,600
 symmetrization trick comes in two so

1056
00:31:42,800 --> 00:31:45,600
 

1057
00:31:42,810 --> 00:31:47,280
 that one bounds the performance the

1058
00:31:45,590 --> 00:31:47,280
 

1059
00:31:45,600 --> 00:31:48,510
 probability of having good performance

1060
00:31:47,270 --> 00:31:48,510
 

1061
00:31:47,280 --> 00:31:51,650
 in one of the samples but bad

1062
00:31:48,500 --> 00:31:51,650
 

1063
00:31:48,510 --> 00:31:53,880
 performance in the other samples and

1064
00:31:51,640 --> 00:31:53,880
 

1065
00:31:51,650 --> 00:31:55,710
 something called the growth function of

1066
00:31:53,870 --> 00:31:55,710
 

1067
00:31:53,880 --> 00:31:59,970
 the hypothesis class comes in it

1068
00:31:55,700 --> 00:31:59,970
 

1069
00:31:55,710 --> 00:32:03,620
 basically tells you for a fixed number

1070
00:31:59,960 --> 00:32:03,620
 

1071
00:31:59,970 --> 00:32:05,570
 of examples if I have m sample size

1072
00:32:03,610 --> 00:32:05,570
 

1073
00:32:03,620 --> 00:32:08,780
 and I'm going to use these hypothesis

1074
00:32:05,560 --> 00:32:08,780
 

1075
00:32:05,570 --> 00:32:10,070
 class how many different labels how many

1076
00:32:08,770 --> 00:32:10,070
 

1077
00:32:08,780 --> 00:32:11,720
 different label is remember that we're

1078
00:32:10,060 --> 00:32:11,720
 

1079
00:32:10,070 --> 00:32:14,960
 looking at the binary classification

1080
00:32:11,710 --> 00:32:14,960
 

1081
00:32:11,720 --> 00:32:16,790
 case so how many different plus or minus

1082
00:32:14,950 --> 00:32:16,790
 

1083
00:32:14,960 --> 00:32:22,190
 one label is turning labels can I do

1084
00:32:16,780 --> 00:32:22,190
 

1085
00:32:16,790 --> 00:32:25,270
 with this hypothesis class and the VC

1086
00:32:22,180 --> 00:32:25,270
 

1087
00:32:22,190 --> 00:32:28,730
 dimension is defined as the largest

1088
00:32:25,260 --> 00:32:28,730
 

1089
00:32:25,270 --> 00:32:29,870
 sample size for which you can have all

1090
00:32:28,720 --> 00:32:29,870
 

1091
00:32:28,730 --> 00:32:31,550
 possible labelings

1092
00:32:29,860 --> 00:32:31,550
 

1093
00:32:29,870 --> 00:32:36,410
 the largest possible labelings on

1094
00:32:31,540 --> 00:32:36,410
 

1095
00:32:31,550 --> 00:32:39,470
 endpoints so let's have a look at what

1096
00:32:36,400 --> 00:32:39,470
 

1097
00:32:36,410 --> 00:32:42,140
 the upper bounds looks like what was

1098
00:32:39,460 --> 00:32:42,140
 

1099
00:32:39,470 --> 00:32:44,920
 proved by vapnik and Shervin Enki's is

1100
00:32:42,130 --> 00:32:44,920
 

1101
00:32:42,140 --> 00:32:48,470
 inequality shown above it's a uniform

1102
00:32:44,910 --> 00:32:48,470
 

1103
00:32:44,920 --> 00:32:52,490
 bound on the generalization gap is

1104
00:32:48,460 --> 00:32:52,490
 

1105
00:32:48,470 --> 00:32:56,420
 uniform over the function class and what

1106
00:32:52,480 --> 00:32:56,420
 

1107
00:32:52,490 --> 00:33:00,260
 can what comes up inside the log is now

1108
00:32:56,410 --> 00:33:00,260
 

1109
00:32:56,420 --> 00:33:02,300
 the growth function of the given

1110
00:33:00,250 --> 00:33:02,300
 

1111
00:33:00,260 --> 00:33:05,090
 hypothesis class of a hypothesis class

1112
00:33:02,290 --> 00:33:05,090
 

1113
00:33:02,300 --> 00:33:06,890
 that you're dealing with in principle

1114
00:33:05,080 --> 00:33:06,890
 

1115
00:33:05,090 --> 00:33:08,480
 that growth function can be bounded it's

1116
00:33:06,880 --> 00:33:08,480
 

1117
00:33:06,890 --> 00:33:13,190
 always fine it can be bounded by you

1118
00:33:08,470 --> 00:33:13,190
 

1119
00:33:08,480 --> 00:33:14,720
 know to to the sample size however if

1120
00:33:13,180 --> 00:33:14,720
 

1121
00:33:13,190 --> 00:33:17,110
 you do that you end up with a very

1122
00:33:14,710 --> 00:33:17,110
 

1123
00:33:14,720 --> 00:33:20,720
 uninformative

1124
00:33:17,100 --> 00:33:20,720
 

1125
00:33:17,110 --> 00:33:22,460
 upper bound because you know the log of

1126
00:33:20,710 --> 00:33:22,460
 

1127
00:33:20,720 --> 00:33:24,560
 the exponential then the low-kicks

1128
00:33:22,450 --> 00:33:24,560
 

1129
00:33:22,460 --> 00:33:26,420
 exponential and then you end up with a

1130
00:33:24,550 --> 00:33:26,420
 

1131
00:33:24,560 --> 00:33:28,880
 bound that basically does not depend on

1132
00:33:26,410 --> 00:33:28,880
 

1133
00:33:26,420 --> 00:33:32,180
 the sample size kind of constant in in

1134
00:33:28,870 --> 00:33:32,180
 

1135
00:33:28,880 --> 00:33:35,630
 sample size things things don't work all

1136
00:33:32,170 --> 00:33:35,630
 

1137
00:33:32,180 --> 00:33:39,200
 that nicely but fortunately there is a

1138
00:33:35,620 --> 00:33:39,200
 

1139
00:33:35,630 --> 00:33:42,320
 tool to handle the growth function that

1140
00:33:39,190 --> 00:33:42,320
 

1141
00:33:39,200 --> 00:33:44,900
 says that as long as the VC dimension of

1142
00:33:42,310 --> 00:33:44,900
 

1143
00:33:42,320 --> 00:33:47,270
 the function class is finite then the

1144
00:33:44,890 --> 00:33:47,270
 

1145
00:33:44,900 --> 00:33:49,640
 growth function can be upper bounded by

1146
00:33:47,260 --> 00:33:49,640
 

1147
00:33:47,270 --> 00:33:51,680
 a combinatorial quantity as sum of

1148
00:33:49,630 --> 00:33:51,680
 

1149
00:33:49,640 --> 00:33:55,880
 binomial coefficients and that works for

1150
00:33:51,670 --> 00:33:55,880
 

1151
00:33:51,680 --> 00:33:58,520
 all sample sizes which makes the growth

1152
00:33:55,870 --> 00:33:58,520
 

1153
00:33:55,880 --> 00:34:00,860
 function P a polynomial in sample size

1154
00:33:58,510 --> 00:34:00,860
 

1155
00:33:58,520 --> 00:34:05,900
 and then you end up with a

1156
00:34:00,850 --> 00:34:05,900
 

1157
00:34:00,860 --> 00:34:08,500
 generalization bound that decreases with

1158
00:34:05,890 --> 00:34:08,500
 

1159
00:34:05,900 --> 00:34:08,500
 sample size

1160
00:34:16,900 --> 00:34:16,900
 

1161
00:34:16,910 --> 00:34:24,720
 turns out that the VC Theory kind of

1162
00:34:22,550 --> 00:34:24,720
 

1163
00:34:22,560 --> 00:34:27,330
 characterizes learnability and they

1164
00:34:24,710 --> 00:34:27,330
 

1165
00:34:24,720 --> 00:34:30,180
 probably approximately correct setting

1166
00:34:27,320 --> 00:34:30,180
 

1167
00:34:27,330 --> 00:34:32,760
 in the sense that well first let me

1168
00:34:30,170 --> 00:34:32,760
 

1169
00:34:30,180 --> 00:34:35,490
 mention the bounds that you yes the

1170
00:34:32,750 --> 00:34:35,490
 

1171
00:34:32,760 --> 00:34:39,030
 bound the VC bound the VC upper bound is

1172
00:34:35,480 --> 00:34:39,030
 

1173
00:34:35,490 --> 00:34:41,190
 a uniform kind of bound but it's like in

1174
00:34:39,020 --> 00:34:41,190
 

1175
00:34:39,030 --> 00:34:43,950
 a double sense uniform kind of bound it

1176
00:34:41,180 --> 00:34:43,950
 

1177
00:34:41,190 --> 00:34:47,100
 is uniform over the hypothesis class it

1178
00:34:43,940 --> 00:34:47,100
 

1179
00:34:43,950 --> 00:34:50,550
 tells you that the generalization gap of

1180
00:34:47,090 --> 00:34:50,550
 

1181
00:34:47,100 --> 00:34:52,470
 all functions in the class are bounded

1182
00:34:50,540 --> 00:34:52,470
 

1183
00:34:50,550 --> 00:34:54,810
 in such and such way with high

1184
00:34:52,460 --> 00:34:54,810
 

1185
00:34:52,470 --> 00:35:00,480
 probability but it is also uniform in

1186
00:34:54,800 --> 00:35:00,480
 

1187
00:34:54,810 --> 00:35:03,950
 the data generating distribution and we

1188
00:35:00,470 --> 00:35:03,950
 

1189
00:35:00,480 --> 00:35:08,280
 have a lower bound that tells you that

1190
00:35:03,940 --> 00:35:08,280
 

1191
00:35:03,950 --> 00:35:11,010
 you know there exists data generating

1192
00:35:08,270 --> 00:35:11,010
 

1193
00:35:08,280 --> 00:35:15,570
 distributions for which with high

1194
00:35:11,000 --> 00:35:15,570
 

1195
00:35:11,010 --> 00:35:17,730
 probability the difference between the

1196
00:35:15,560 --> 00:35:17,730
 

1197
00:35:15,570 --> 00:35:20,670
 risk and the smallest possible risk in

1198
00:35:17,720 --> 00:35:20,670
 

1199
00:35:17,730 --> 00:35:23,310
 the class is bounded below by a quantity

1200
00:35:20,660 --> 00:35:23,310
 

1201
00:35:20,670 --> 00:35:28,980
 of the same order as the upper bound in

1202
00:35:23,300 --> 00:35:28,980
 

1203
00:35:23,310 --> 00:35:31,110
 the sample size so there are good and

1204
00:35:28,970 --> 00:35:31,110
 

1205
00:35:28,980 --> 00:35:36,090
 bad things about the VC framework let me

1206
00:35:31,100 --> 00:35:36,090
 

1207
00:35:31,110 --> 00:35:37,800
 mention what are the limitations so the

1208
00:35:36,080 --> 00:35:37,800
 

1209
00:35:36,090 --> 00:35:41,370
 bounds are tight in the sense that we

1210
00:35:37,790 --> 00:35:41,370
 

1211
00:35:37,800 --> 00:35:45,180
 have matching upper and lower bounds at

1212
00:35:41,360 --> 00:35:45,180
 

1213
00:35:41,370 --> 00:35:47,340
 the lower bounds of matching order these

1214
00:35:45,170 --> 00:35:47,340
 

1215
00:35:45,180 --> 00:35:54,800
 are this is a theory that was developed

1216
00:35:47,330 --> 00:35:54,800
 

1217
00:35:47,340 --> 00:35:58,220
 to analyze the output of empirical risk

1218
00:35:54,790 --> 00:35:58,220
 

1219
00:35:54,800 --> 00:36:02,790
 minimizer's and this is something that

1220
00:35:58,210 --> 00:36:02,790
 

1221
00:35:58,220 --> 00:36:05,930
 applies to uniform liberal hypothesis

1222
00:36:02,780 --> 00:36:05,930
 

1223
00:36:02,790 --> 00:36:12,450
 space so this is something that is not

1224
00:36:05,920 --> 00:36:12,450
 

1225
00:36:05,930 --> 00:36:15,720
 tuned to a particular function however

1226
00:36:12,440 --> 00:36:15,720
 

1227
00:36:12,450 --> 00:36:18,300
 practical algorithms usually do not

1228
00:36:15,710 --> 00:36:18,300
 

1229
00:36:15,720 --> 00:36:19,500
 search or at least this is what is

1230
00:36:18,290 --> 00:36:19,500
 

1231
00:36:18,300 --> 00:36:21,600
 observed in practice the practical

1232
00:36:19,490 --> 00:36:21,600
 

1233
00:36:19,500 --> 00:36:25,860
 algorithms usually do not search the

1234
00:36:21,590 --> 00:36:25,860
 

1235
00:36:21,600 --> 00:36:28,470
 whole hypothesis class as examples you

1236
00:36:25,850 --> 00:36:28,470
 

1237
00:36:25,860 --> 00:36:32,060
 have nearest neighbors rules support

1238
00:36:28,460 --> 00:36:32,060
 

1239
00:36:28,470 --> 00:36:36,390
 vector machines or deep neural networks

1240
00:36:32,050 --> 00:36:36,390
 

1241
00:36:32,060 --> 00:36:39,960
 so sort of like there is a mismatch

1242
00:36:36,380 --> 00:36:39,960
 

1243
00:36:36,390 --> 00:36:44,160
 between this theory and what is observed

1244
00:36:39,950 --> 00:36:44,160
 

1245
00:36:39,960 --> 00:36:45,540
 in practice just to give an idea I'm

1246
00:36:44,150 --> 00:36:45,540
 

1247
00:36:44,160 --> 00:36:48,000
 going to illustrate this with a picture

1248
00:36:45,530 --> 00:36:48,000
 

1249
00:36:45,540 --> 00:36:51,330
 for F VMs kind of following up on the

1250
00:36:47,990 --> 00:36:51,330
 

1251
00:36:48,000 --> 00:36:56,520
 pictures that Joan showed at the

1252
00:36:51,320 --> 00:36:56,520
 

1253
00:36:51,330 --> 00:36:59,490
 beginning so on the same data set for

1254
00:36:56,510 --> 00:36:59,490
 

1255
00:36:56,520 --> 00:37:02,430
 who those pictures were created you

1256
00:36:59,480 --> 00:37:02,430
 

1257
00:36:59,490 --> 00:37:04,349
 still run a person window that's the

1258
00:37:02,420 --> 00:37:04,349
 

1259
00:37:02,430 --> 00:37:08,160
 blue algorithm but instead of a linear

1260
00:37:04,339 --> 00:37:08,160
 

1261
00:37:04,349 --> 00:37:11,490
 SVM use a kernel SVM with a Gaussian

1262
00:37:08,150 --> 00:37:11,490
 

1263
00:37:08,160 --> 00:37:16,290
 kernel and turns out that the

1264
00:37:11,480 --> 00:37:16,290
 

1265
00:37:11,490 --> 00:37:23,550
 performance of the kernel SVM pretty

1266
00:37:16,280 --> 00:37:23,550
 

1267
00:37:16,290 --> 00:37:27,510
 much beat I mean even in expectation the

1268
00:37:23,540 --> 00:37:27,510
 

1269
00:37:23,550 --> 00:37:31,560
 error the tester of SVM is much much

1270
00:37:27,500 --> 00:37:31,560
 

1271
00:37:27,510 --> 00:37:35,790
 lower so what does this say keep in mind

1272
00:37:31,550 --> 00:37:35,790
 

1273
00:37:31,560 --> 00:37:42,900
 that keep in mind that for the Gaussian

1274
00:37:35,780 --> 00:37:42,900
 

1275
00:37:35,790 --> 00:37:44,119
 kernel the VC dimension of the space is

1276
00:37:42,890 --> 00:37:44,119
 

1277
00:37:42,900 --> 00:37:48,990
 infinite

1278
00:37:44,109 --> 00:37:48,990
 

1279
00:37:44,119 --> 00:37:50,760
 so looks like the theory is not been

1280
00:37:48,980 --> 00:37:50,760
 

1281
00:37:48,990 --> 00:37:53,300
 able to say much or at least the VC

1282
00:37:50,750 --> 00:37:53,300
 

1283
00:37:50,760 --> 00:37:56,369
 bounds are not been able to say much as

1284
00:37:53,290 --> 00:37:56,369
 

1285
00:37:53,300 --> 00:38:02,190
 as far as the performance of kernel SVM

1286
00:37:56,359 --> 00:38:02,190
 

1287
00:37:56,369 --> 00:38:05,130
 with the Gaussian kernel goes so I kind

1288
00:38:02,180 --> 00:38:05,130
 

1289
00:38:02,190 --> 00:38:07,440
 of want to mention this as a as a case

1290
00:38:05,120 --> 00:38:07,440
 

1291
00:38:05,130 --> 00:38:09,480
 study here is an example that has

1292
00:38:07,430 --> 00:38:09,480
 

1293
00:38:07,440 --> 00:38:13,920
 infinite VC dimension however the

1294
00:38:09,470 --> 00:38:13,920
 

1295
00:38:09,480 --> 00:38:16,280
 observe performance is quite good VC

1296
00:38:13,910 --> 00:38:16,280
 

1297
00:38:13,920 --> 00:38:19,170
 bounds don't seem able to explain this

1298
00:38:16,270 --> 00:38:19,170
 

1299
00:38:16,280 --> 00:38:21,180
 in fact the VC lower bounds seem to kind

1300
00:38:19,160 --> 00:38:21,180
 

1301
00:38:19,170 --> 00:38:25,290
 of be in a contradiction with what comes

1302
00:38:21,170 --> 00:38:25,290
 

1303
00:38:21,180 --> 00:38:26,580
 out of this of this experiment and the

1304
00:38:25,280 --> 00:38:26,580
 

1305
00:38:25,290 --> 00:38:29,700
 question is how to resolve this apparent

1306
00:38:26,570 --> 00:38:29,700
 

1307
00:38:26,580 --> 00:38:32,040
 contradiction so I'm not going to answer

1308
00:38:29,690 --> 00:38:32,040
 

1309
00:38:29,700 --> 00:38:34,109
 that question right away I'm going to

1310
00:38:32,030 --> 00:38:34,109
 

1311
00:38:32,040 --> 00:38:38,670
 leave it's a motivation for what's

1312
00:38:34,099 --> 00:38:38,670
 

1313
00:38:34,109 --> 00:38:39,960
 coming up after the break which is not

1314
00:38:38,660 --> 00:38:39,960
 

1315
00:38:38,670 --> 00:38:45,520
 right away

1316
00:38:39,950 --> 00:38:45,520
 

1317
00:38:39,960 --> 00:38:49,119
 so the point is that the VC bounds are

1318
00:38:45,510 --> 00:38:49,119
 

1319
00:38:45,520 --> 00:38:50,920
 worse-case kind of bounds and the

1320
00:38:49,109 --> 00:38:50,920
 

1321
00:38:49,119 --> 00:38:53,470
 example that you just saw the gravity so

1322
00:38:50,910 --> 00:38:53,470
 

1323
00:38:50,920 --> 00:38:55,270
 maybe suggesting that you know sometimes

1324
00:38:53,460 --> 00:38:55,270
 

1325
00:38:53,470 --> 00:38:58,510
 we may not be dealing with a worst case

1326
00:38:55,260 --> 00:38:58,510
 

1327
00:38:55,270 --> 00:39:02,859
 data generating distribution and how

1328
00:38:58,500 --> 00:39:02,859
 

1329
00:38:58,510 --> 00:39:07,380
 does one then exploit that

1330
00:39:02,849 --> 00:39:07,380
 

1331
00:39:02,859 --> 00:39:09,670
 well answers will come very soon for now

1332
00:39:07,370 --> 00:39:09,670
 

1333
00:39:07,380 --> 00:39:11,859
 what is kind of like The Hitchhiker's

1334
00:39:09,660 --> 00:39:11,859
 

1335
00:39:09,670 --> 00:39:14,559
 Guide summary of the first part of the

1336
00:39:11,849 --> 00:39:14,559
 

1337
00:39:11,859 --> 00:39:21,339
 talk well it seems that the theory is

1338
00:39:14,549 --> 00:39:21,339
 

1339
00:39:14,559 --> 00:39:24,910
 quite nice quite elegant and complete in

1340
00:39:21,329 --> 00:39:24,910
 

1341
00:39:21,339 --> 00:39:28,630
 a way it's right but wrong so it's like

1342
00:39:24,900 --> 00:39:28,630
 

1343
00:39:24,910 --> 00:39:30,880
 a nice topic of conversation but what do

1344
00:39:28,620 --> 00:39:30,880
 

1345
00:39:28,630 --> 00:39:34,630
 how do we you know how does it match the

1346
00:39:30,870 --> 00:39:34,630
 

1347
00:39:30,880 --> 00:39:37,839
 practice we don't know so it's practical

1348
00:39:34,620 --> 00:39:37,839
 

1349
00:39:34,630 --> 00:39:42,880
 usefulness it seems like something

1350
00:39:37,829 --> 00:39:42,880
 

1351
00:39:37,839 --> 00:39:46,109
 different has to be has to be used in

1352
00:39:42,870 --> 00:39:46,109
 

1353
00:39:42,880 --> 00:39:48,789
 order to much theory and practice and

1354
00:39:46,099 --> 00:39:48,789
 

1355
00:39:46,109 --> 00:39:52,150
 with that we're going to make a short

1356
00:39:48,779 --> 00:39:52,150
 

1357
00:39:48,789 --> 00:39:55,660
 break of like five minutes that's fine

1358
00:39:52,140 --> 00:39:55,660
 

1359
00:39:52,150 --> 00:39:57,990
 five minutes ten minutes ten minutes

1360
00:39:55,650 --> 00:39:57,990
 

1361
00:39:55,660 --> 00:39:57,990
 break

1362
00:41:49,920 --> 00:41:49,920
 

1363
00:41:49,930 --> 00:41:51,990
 you

1364
00:45:55,410 --> 00:45:55,410
 

1365
00:45:55,420 --> 00:45:58,610
[Music]

1366
00:46:38,250 --> 00:46:38,250
 

1367
00:46:38,260 --> 00:46:41,469
[Music]

1368
00:47:10,020 --> 00:47:10,020
 

1369
00:47:10,030 --> 00:47:30,489
[Music]

1370
00:47:43,120 --> 00:47:43,120
 

1371
00:47:43,130 --> 00:47:46,419
[Music]

1372
00:48:01,310 --> 00:48:01,310
 

1373
00:48:01,320 --> 00:48:04,570
[Music]

1374
00:51:48,170 --> 00:51:48,170
 

1375
00:51:48,180 --> 00:51:53,489
 okay let's get restart

1376
00:52:04,710 --> 00:52:04,710
 

1377
00:52:04,720 --> 00:52:11,200
 so we're going to be begin the second

1378
00:52:07,290 --> 00:52:11,200
 

1379
00:52:07,300 --> 00:52:15,450
 part with this presentation on second

1380
00:52:11,190 --> 00:52:15,450
 

1381
00:52:11,200 --> 00:52:15,450
 generation statistical learning theory

1382
00:52:15,770 --> 00:52:15,770
 

1383
00:52:15,780 --> 00:52:23,740
 so what we're just to recap we looked at

1384
00:52:21,599 --> 00:52:23,740
 

1385
00:52:21,609 --> 00:52:25,000
 statistical learning bounds and the fact

1386
00:52:23,730 --> 00:52:25,000
 

1387
00:52:23,740 --> 00:52:27,220
 that they bound the tail of the

1388
00:52:24,990 --> 00:52:27,220
 

1389
00:52:25,000 --> 00:52:28,420
 distribution that's critical in terms of

1390
00:52:27,210 --> 00:52:28,420
 

1391
00:52:27,220 --> 00:52:31,720
 understanding what they're trying to

1392
00:52:28,410 --> 00:52:31,720
 

1393
00:52:28,420 --> 00:52:33,609
 achieve and through that they give what

1394
00:52:31,710 --> 00:52:33,609
 

1395
00:52:31,720 --> 00:52:38,140
 are called high confidence bounds on the

1396
00:52:33,599 --> 00:52:38,140
 

1397
00:52:33,609 --> 00:52:42,010
 performance on test data and the VC

1398
00:52:38,130 --> 00:52:42,010
 

1399
00:52:38,140 --> 00:52:43,210
 Theory gave uniform bounds over a set of

1400
00:52:42,000 --> 00:52:43,210
 

1401
00:52:42,010 --> 00:52:45,760
 classifiers so you basically get the

1402
00:52:43,200 --> 00:52:45,760
 

1403
00:52:43,210 --> 00:52:52,030
 same bound for each classifier in the

1404
00:52:45,750 --> 00:52:52,030
 

1405
00:52:45,760 --> 00:52:54,819
 class which motivates as Omar was saying

1406
00:52:52,020 --> 00:52:54,819
 

1407
00:52:52,030 --> 00:52:57,010
 the empirical risk minimization clearly

1408
00:52:54,809 --> 00:52:57,010
 

1409
00:52:54,819 --> 00:52:58,690
 then to get the best test performance

1410
00:52:57,000 --> 00:52:58,690
 

1411
00:52:57,010 --> 00:53:01,569
 you have to minimize the error on the

1412
00:52:58,680 --> 00:53:01,569
 

1413
00:52:58,690 --> 00:53:04,420
 training set and then the error on the

1414
00:53:01,559 --> 00:53:04,420
 

1415
00:53:01,569 --> 00:53:06,099
 test is that training error plus the gap

1416
00:53:04,410 --> 00:53:06,099
 

1417
00:53:04,420 --> 00:53:08,859
 which is the same for all of the

1418
00:53:06,089 --> 00:53:08,859
 

1419
00:53:06,099 --> 00:53:11,290
 functions in the class but the point

1420
00:53:08,849 --> 00:53:11,290
 

1421
00:53:08,859 --> 00:53:14,619
 that was made at the end there was that

1422
00:53:11,280 --> 00:53:14,619
 

1423
00:53:11,290 --> 00:53:17,650
 the it's also worse case over the data

1424
00:53:14,609 --> 00:53:17,650
 

1425
00:53:14,619 --> 00:53:20,530
 generating distributions so it holds for

1426
00:53:17,640 --> 00:53:20,530
 

1427
00:53:17,650 --> 00:53:23,250
 all data generating distributions and

1428
00:53:20,520 --> 00:53:23,250
 

1429
00:53:20,530 --> 00:53:27,520
 this is I think the key weakness that

1430
00:53:23,240 --> 00:53:27,520
 

1431
00:53:23,250 --> 00:53:32,170
 was being shown up by the performance of

1432
00:53:27,510 --> 00:53:32,170
 

1433
00:53:27,520 --> 00:53:35,200
 the SVM and but there seems to be a

1434
00:53:32,160 --> 00:53:35,200
 

1435
00:53:32,170 --> 00:53:37,569
 contradiction because the VC Theory

1436
00:53:35,190 --> 00:53:37,569
 

1437
00:53:35,200 --> 00:53:39,069
 characterizes learnability so there

1438
00:53:37,559 --> 00:53:39,069
 

1439
00:53:37,569 --> 00:53:40,599
 seems to be kind of we're getting good

1440
00:53:39,059 --> 00:53:40,599
 

1441
00:53:39,069 --> 00:53:41,980
 performance but hey you know the

1442
00:53:40,589 --> 00:53:41,980
 

1443
00:53:40,599 --> 00:53:43,690
 performance can't be good because it's

1444
00:53:41,970 --> 00:53:43,690
 

1445
00:53:41,980 --> 00:53:46,960
 characterized so what we're going to

1446
00:53:43,680 --> 00:53:46,960
 

1447
00:53:43,690 --> 00:53:50,200
 look at is exploiting the non worst-case

1448
00:53:46,950 --> 00:53:50,200
 

1449
00:53:46,960 --> 00:53:52,359
 distributions so this is quite a subtle

1450
00:53:50,190 --> 00:53:52,359
 

1451
00:53:50,200 --> 00:53:55,000
 point that actually those lower bounds

1452
00:53:52,349 --> 00:53:55,000
 

1453
00:53:52,359 --> 00:53:58,150
 depend on particular distributions that

1454
00:53:54,990 --> 00:53:58,150
 

1455
00:53:55,000 --> 00:54:00,400
 force us to make errors and actually the

1456
00:53:58,140 --> 00:54:00,400
 

1457
00:53:58,150 --> 00:54:03,069
 distribution that we're facing in a real

1458
00:54:00,390 --> 00:54:03,069
 

1459
00:54:00,400 --> 00:54:05,890
 application may not actually be of that

1460
00:54:03,059 --> 00:54:05,890
 

1461
00:54:03,069 --> 00:54:09,640
 worst-case kind in fact it mustn't be if

1462
00:54:05,880 --> 00:54:09,640
 

1463
00:54:05,890 --> 00:54:11,710
 we're able to perform better so the

1464
00:54:09,630 --> 00:54:11,710
 

1465
00:54:09,640 --> 00:54:14,140
 bounds then become dependent on the

1466
00:54:11,700 --> 00:54:14,140
 

1467
00:54:11,710 --> 00:54:16,800
 chosen function and that function is in

1468
00:54:14,130 --> 00:54:16,800
 

1469
00:54:14,140 --> 00:54:19,780
 some way able to

1470
00:54:16,790 --> 00:54:19,780
 

1471
00:54:16,800 --> 00:54:24,850
 give evidence that the distribution is

1472
00:54:19,770 --> 00:54:24,850
 

1473
00:54:19,780 --> 00:54:27,640
 not as bad it'll mean a set of new proof

1474
00:54:24,840 --> 00:54:27,640
 

1475
00:54:24,850 --> 00:54:29,140
 techniques so we'll try and give you an

1476
00:54:27,630 --> 00:54:29,140
 

1477
00:54:27,640 --> 00:54:31,380
 overview of the proof techniques that

1478
00:54:29,130 --> 00:54:31,380
 

1479
00:54:29,140 --> 00:54:34,450
 arise in order to deliver these results

1480
00:54:31,370 --> 00:54:34,450
 

1481
00:54:31,380 --> 00:54:36,490
 and then we'll finish up with the

1482
00:54:34,440 --> 00:54:36,490
 

1483
00:54:34,450 --> 00:54:38,140
 approaches to deep learning and some

1484
00:54:36,480 --> 00:54:38,140
 

1485
00:54:36,490 --> 00:54:40,030
 future directions so that's what we're

1486
00:54:38,130 --> 00:54:40,030
 

1487
00:54:38,140 --> 00:54:42,970
 going to be looking at in this second

1488
00:54:40,020 --> 00:54:42,970
 

1489
00:54:40,030 --> 00:54:44,890
 half okay so the first I'm going to

1490
00:54:42,960 --> 00:54:44,890
 

1491
00:54:42,970 --> 00:54:48,400
 start with perhaps the simplest way of

1492
00:54:44,880 --> 00:54:48,400
 

1493
00:54:44,890 --> 00:54:52,810
 making the bounds distribution dependent

1494
00:54:48,390 --> 00:54:52,810
 

1495
00:54:48,400 --> 00:54:54,880
 or function dependent at least and it's

1496
00:54:52,800 --> 00:54:54,880
 

1497
00:54:52,810 --> 00:54:56,160
 the first step towards a non-uniform

1498
00:54:54,870 --> 00:54:56,160
 

1499
00:54:54,880 --> 00:55:01,080
 learnability

1500
00:54:56,150 --> 00:55:01,080
 

1501
00:54:56,160 --> 00:55:06,160
 so the way this is set up is we take a a

1502
00:55:01,070 --> 00:55:06,160
 

1503
00:55:01,080 --> 00:55:09,670
 set of functions sorry of classes H 1 up

1504
00:55:06,150 --> 00:55:09,670
 

1505
00:55:06,160 --> 00:55:11,830
 to may be an infinite accountable

1506
00:55:09,660 --> 00:55:11,830
 

1507
00:55:09,670 --> 00:55:14,980
 infinite countable union of function

1508
00:55:11,820 --> 00:55:14,980
 

1509
00:55:11,830 --> 00:55:19,660
 classes and perhaps they might have VC

1510
00:55:14,970 --> 00:55:19,660
 

1511
00:55:14,980 --> 00:55:22,990
 dimension DK for the case in that

1512
00:55:19,650 --> 00:55:22,990
 

1513
00:55:19,660 --> 00:55:26,470
 sequence and these VC dimensions are all

1514
00:55:22,980 --> 00:55:26,470
 

1515
00:55:22,990 --> 00:55:30,220
 finite and we imagine there's also a

1516
00:55:26,460 --> 00:55:30,220
 

1517
00:55:26,470 --> 00:55:33,160
 weighting scheme that weights are kind

1518
00:55:30,210 --> 00:55:33,160
 

1519
00:55:30,220 --> 00:55:35,980
 of prior belief if you like in that

1520
00:55:33,150 --> 00:55:35,980
 

1521
00:55:33,160 --> 00:55:38,320
 class being the the class in which the

1522
00:55:35,970 --> 00:55:38,320
 

1523
00:55:35,980 --> 00:55:40,300
 function will be likely to come that

1524
00:55:38,310 --> 00:55:40,300
 

1525
00:55:38,320 --> 00:55:45,430
 we're learning and what we would like to

1526
00:55:40,290 --> 00:55:45,430
 

1527
00:55:40,300 --> 00:55:48,010
 have is a bound that holds for for each

1528
00:55:45,420 --> 00:55:48,010
 

1529
00:55:45,430 --> 00:55:52,180
 K we're going to have the bound hold but

1530
00:55:48,000 --> 00:55:52,180
 

1531
00:55:48,010 --> 00:55:54,460
 with the weighting on the right hand

1532
00:55:52,170 --> 00:55:54,460
 

1533
00:55:52,180 --> 00:55:57,820
 side so the probability of being misled

1534
00:55:54,450 --> 00:55:57,820
 

1535
00:55:54,460 --> 00:56:01,000
 is now weighted according to that WK

1536
00:55:57,810 --> 00:56:01,000
 

1537
00:55:57,820 --> 00:56:02,620
 associated with that class and remember

1538
00:56:00,990 --> 00:56:02,620
 

1539
00:56:01,000 --> 00:56:06,580
 the sum of the W K's is less than or

1540
00:56:02,610 --> 00:56:06,580
 

1541
00:56:02,620 --> 00:56:07,480
 equal to 1 so we can therefore apply our

1542
00:56:06,570 --> 00:56:07,480
 

1543
00:56:06,580 --> 00:56:12,100
 Union bound

1544
00:56:07,470 --> 00:56:12,100
 

1545
00:56:07,480 --> 00:56:14,170
 trick again and what we have is the fact

1546
00:56:12,090 --> 00:56:14,170
 

1547
00:56:12,100 --> 00:56:16,870
 that with probability 1 minus Delta then

1548
00:56:14,160 --> 00:56:16,870
 

1549
00:56:14,170 --> 00:56:19,870
 for all of the classes and for all of

1550
00:56:16,860 --> 00:56:19,870
 

1551
00:56:16,870 --> 00:56:22,960
 the functions in each class the gap is

1552
00:56:19,860 --> 00:56:22,960
 

1553
00:56:19,870 --> 00:56:25,810
 bounded but the gap that we get is now

1554
00:56:22,950 --> 00:56:25,810
 

1555
00:56:22,960 --> 00:56:27,490
 class dependent so it's uniform the same

1556
00:56:25,800 --> 00:56:27,490
 

1557
00:56:25,810 --> 00:56:30,970
 gap for everybody in

1558
00:56:27,480 --> 00:56:30,970
 

1559
00:56:27,490 --> 00:56:32,800
 class but it will be different for

1560
00:56:30,960 --> 00:56:32,800
 

1561
00:56:30,970 --> 00:56:35,980
 different classes so this means that the

1562
00:56:32,790 --> 00:56:35,980
 

1563
00:56:32,800 --> 00:56:39,369
 function gap will be a function of the

1564
00:56:35,970 --> 00:56:39,369
 

1565
00:56:35,980 --> 00:56:44,710
 class within which the actual hypothesis

1566
00:56:39,359 --> 00:56:44,710
 

1567
00:56:39,369 --> 00:56:47,350
 that we've chosen lies so it's a first

1568
00:56:44,700 --> 00:56:47,350
 

1569
00:56:44,710 --> 00:56:50,230
 attempt very simple attempt to introduce

1570
00:56:47,340 --> 00:56:50,230
 

1571
00:56:47,350 --> 00:56:52,660
 hypothesis dependence and as I said the

1572
00:56:50,220 --> 00:56:52,660
 

1573
00:56:50,230 --> 00:56:57,160
 complexity depends on the chart on the

1574
00:56:52,650 --> 00:56:57,160
 

1575
00:56:52,660 --> 00:56:58,840
 chosen function and it drives as I you

1576
00:56:57,150 --> 00:56:58,840
 

1577
00:56:57,160 --> 00:57:00,700
 know pointed out every bound potentially

1578
00:56:58,830 --> 00:57:00,700
 

1579
00:56:58,840 --> 00:57:04,060
 drives a new algorithm this drives an

1580
00:57:00,690 --> 00:57:04,060
 

1581
00:57:00,700 --> 00:57:06,850
 algorithm that returns the function that

1582
00:57:04,050 --> 00:57:06,850
 

1583
00:57:04,060 --> 00:57:09,010
 you see here on the right it's the Arg

1584
00:57:06,840 --> 00:57:09,010
 

1585
00:57:06,850 --> 00:57:10,630
 min over all of the functions in all of

1586
00:57:09,000 --> 00:57:10,630
 

1587
00:57:09,010 --> 00:57:13,600
 the classes the union of the classes

1588
00:57:10,620 --> 00:57:13,600
 

1589
00:57:10,630 --> 00:57:16,720
 which minimizes the empirical error plus

1590
00:57:13,590 --> 00:57:16,720
 

1591
00:57:13,600 --> 00:57:19,570
 the gap associated with the minimum

1592
00:57:16,710 --> 00:57:19,570
 

1593
00:57:16,720 --> 00:57:21,880
 class that that function so this K K of

1594
00:57:19,560 --> 00:57:21,880
 

1595
00:57:19,570 --> 00:57:26,830
 H is the minimum class which that

1596
00:57:21,870 --> 00:57:26,830
 

1597
00:57:21,880 --> 00:57:30,760
 function is contained in so we're

1598
00:57:26,820 --> 00:57:30,760
 

1599
00:57:26,830 --> 00:57:32,680
 actually in way regularizing according

1600
00:57:30,750 --> 00:57:32,680
 

1601
00:57:30,760 --> 00:57:34,810
 to the class index

1602
00:57:32,670 --> 00:57:34,810
 

1603
00:57:32,680 --> 00:57:37,500
 we're regularizing the choice of

1604
00:57:34,800 --> 00:57:37,500
 

1605
00:57:34,810 --> 00:57:40,240
 function that we so we're trying to

1606
00:57:37,490 --> 00:57:40,240
 

1607
00:57:37,500 --> 00:57:43,480
 prioritize functions with low indices

1608
00:57:40,230 --> 00:57:43,480
 

1609
00:57:40,240 --> 00:57:47,560
 but trading that index against the

1610
00:57:43,470 --> 00:57:47,560
 

1611
00:57:43,480 --> 00:57:50,350
 empirical loss so it's a kind of initial

1612
00:57:47,550 --> 00:57:50,350
 

1613
00:57:47,560 --> 00:57:54,040
 form of regularization and essentially

1614
00:57:50,340 --> 00:57:54,040
 

1615
00:57:50,350 --> 00:57:56,230
 this idea of you know measuring

1616
00:57:54,030 --> 00:57:56,230
 

1617
00:57:54,040 --> 00:58:00,609
 complexity of the function is about

1618
00:57:56,220 --> 00:58:00,609
 

1619
00:57:56,230 --> 00:58:03,430
 regularization so this leads us to this

1620
00:58:00,599 --> 00:58:03,430
 

1621
00:58:00,609 --> 00:58:06,130
 idea of detecting benign distributions

1622
00:58:03,420 --> 00:58:06,130
 

1623
00:58:03,430 --> 00:58:09,900
 so the structural risk minimization

1624
00:58:06,120 --> 00:58:09,900
 

1625
00:58:06,130 --> 00:58:12,340
 approach is sort of detecting the

1626
00:58:09,890 --> 00:58:12,340
 

1627
00:58:09,900 --> 00:58:14,350
 complexity of the class that is needed

1628
00:58:12,330 --> 00:58:14,350
 

1629
00:58:12,340 --> 00:58:16,800
 to solve the particular problem so in

1630
00:58:14,340 --> 00:58:16,800
 

1631
00:58:14,350 --> 00:58:19,240
 that case is sort of detecting the

1632
00:58:16,790 --> 00:58:19,240
 

1633
00:58:16,800 --> 00:58:23,130
 complexity of functional complexity that

1634
00:58:19,230 --> 00:58:23,130
 

1635
00:58:19,240 --> 00:58:25,720
 is required for that particular problem

1636
00:58:23,120 --> 00:58:25,720
 

1637
00:58:23,130 --> 00:58:28,300
 but the problem is that you must define

1638
00:58:25,710 --> 00:58:28,300
 

1639
00:58:25,720 --> 00:58:30,490
 the hierarchy a priori so you in some

1640
00:58:28,290 --> 00:58:30,490
 

1641
00:58:28,300 --> 00:58:34,150
 way it's a little clunky in the way that

1642
00:58:30,480 --> 00:58:34,150
 

1643
00:58:30,490 --> 00:58:35,890
 you have to set this thing up and so

1644
00:58:34,140 --> 00:58:35,890
 

1645
00:58:34,150 --> 00:58:37,990
 it's not

1646
00:58:35,880 --> 00:58:37,990
 

1647
00:58:35,890 --> 00:58:40,660
 effective in practice what we would like

1648
00:58:37,980 --> 00:58:40,660
 

1649
00:58:37,990 --> 00:58:42,789
 is to have more nuanced ways to detect

1650
00:58:40,650 --> 00:58:42,789
 

1651
00:58:40,660 --> 00:58:47,380
 how wouldn't benign a particular

1652
00:58:42,779 --> 00:58:47,380
 

1653
00:58:42,789 --> 00:58:49,900
 distribution is and this is where the

1654
00:58:47,370 --> 00:58:49,900
 

1655
00:58:47,380 --> 00:58:53,490
 SVM comes in which uses this margin

1656
00:58:49,890 --> 00:58:53,490
 

1657
00:58:49,900 --> 00:58:57,369
 measure which appears to Dannette detect

1658
00:58:53,480 --> 00:58:57,369
 

1659
00:58:53,490 --> 00:59:00,760
 how benign the distribution is in the

1660
00:58:57,359 --> 00:59:00,760
 

1661
00:58:57,369 --> 00:59:02,559
 sense that the data is if you have a

1662
00:59:00,750 --> 00:59:02,559
 

1663
00:59:00,760 --> 00:59:04,240
 margin it's sort of implying that

1664
00:59:02,549 --> 00:59:04,240
 

1665
00:59:02,559 --> 00:59:06,760
 there's some region close to the

1666
00:59:04,230 --> 00:59:06,760
 

1667
00:59:04,240 --> 00:59:10,960
 boundary where the data is less likely

1668
00:59:06,750 --> 00:59:10,960
 

1669
00:59:06,760 --> 00:59:12,990
 to actually occur and that makes it

1670
00:59:10,950 --> 00:59:12,990
 

1671
00:59:10,960 --> 00:59:14,769
 easier to separate the two classes

1672
00:59:12,980 --> 00:59:14,769
 

1673
00:59:12,990 --> 00:59:18,420
 naturally because there's a bit of

1674
00:59:14,759 --> 00:59:18,420
 

1675
00:59:14,769 --> 00:59:21,369
 wiggle room in the region between the

1676
00:59:18,410 --> 00:59:21,369
 

1677
00:59:18,420 --> 00:59:23,740
 two classes and you've got a little bit

1678
00:59:21,359 --> 00:59:23,740
 

1679
00:59:21,369 --> 00:59:25,930
 of flexibility and it's easier to find

1680
00:59:23,730 --> 00:59:25,930
 

1681
00:59:23,740 --> 00:59:29,260
 that separating hyperplane in that sense

1682
00:59:25,920 --> 00:59:29,260
 

1683
00:59:25,930 --> 00:59:33,000
 so this is the kind of intuition and

1684
00:59:29,250 --> 00:59:33,000
 

1685
00:59:29,260 --> 00:59:36,640
 it's actually been formulated into

1686
00:59:32,990 --> 00:59:36,640
 

1687
00:59:33,000 --> 00:59:39,760
 minimax asymptotic rates for a class of

1688
00:59:36,630 --> 00:59:39,760
 

1689
00:59:36,640 --> 00:59:42,640
 distributions with this a definition of

1690
00:59:39,750 --> 00:59:42,640
 

1691
00:59:39,760 --> 00:59:44,440
 reduced margin density by OD Barron C

1692
00:59:42,630 --> 00:59:44,440
 

1693
00:59:42,640 --> 00:59:49,029
 burghoff so this this idea has been

1694
00:59:44,430 --> 00:59:49,029
 

1695
00:59:44,440 --> 00:59:51,549
 characterized in that sense but while

1696
00:59:49,019 --> 00:59:51,549
 

1697
00:59:49,029 --> 00:59:54,730
 that's really kind of a very useful

1698
00:59:51,539 --> 00:59:54,730
 

1699
00:59:51,549 --> 00:59:57,190
 insight it's making assumptions that we

1700
00:59:54,720 --> 00:59:57,190
 

1701
00:59:54,730 --> 00:59:59,410
 prefer not to make and we don't know

1702
00:59:57,180 --> 00:59:59,410
 

1703
00:59:57,190 --> 01:00:02,559
 whether they are valid to make when we

1704
00:59:59,400 --> 01:00:02,559
 

1705
00:59:59,410 --> 01:00:04,450
 actually apply our algorithm to a

1706
01:00:02,549 --> 01:00:04,450
 

1707
01:00:02,559 --> 01:00:06,190
 particular training set but we would

1708
01:00:04,440 --> 01:00:06,190
 

1709
01:00:04,450 --> 01:00:09,579
 like to be able to do is have the

1710
01:00:06,180 --> 01:00:09,579
 

1711
01:00:06,190 --> 01:00:12,759
 algorithm automatically detect that

1712
01:00:09,569 --> 01:00:12,759
 

1713
01:00:09,579 --> 01:00:16,029
 there's benign distribution measured by

1714
01:00:12,749 --> 01:00:16,029
 

1715
01:00:12,759 --> 01:00:18,759
 the a margin and exploit that to get

1716
01:00:16,019 --> 01:00:18,759
 

1717
01:00:16,029 --> 01:00:21,339
 good performance and that's exactly what

1718
01:00:18,749 --> 01:00:21,339
 

1719
01:00:18,759 --> 01:00:24,670
 the SVM does and we'll be showing how

1720
01:00:21,329 --> 01:00:24,670
 

1721
01:00:21,339 --> 01:00:26,859
 that arises but more generally there are

1722
01:00:24,660 --> 01:00:26,859
 

1723
01:00:24,670 --> 01:00:29,920
 other techniques that you can and other

1724
01:00:26,849 --> 01:00:29,920
 

1725
01:00:26,859 --> 01:00:32,349
 if you like benign ways of detecting a

1726
01:00:29,910 --> 01:00:32,349
 

1727
01:00:29,920 --> 01:00:34,779
 benign distribution and Mary Omar sure

1728
01:00:32,339 --> 01:00:34,779
 

1729
01:00:32,349 --> 01:00:37,240
 myself looked at how sparsity can be

1730
01:00:34,769 --> 01:00:37,240
 

1731
01:00:34,779 --> 01:00:40,150
 used as an indicator of a benign

1732
01:00:37,230 --> 01:00:40,150
 

1733
01:00:37,240 --> 01:00:42,369
 distribution so this is just to show

1734
01:00:40,140 --> 01:00:42,369
 

1735
01:00:40,150 --> 01:00:44,710
 that there are other methods other

1736
01:00:42,359 --> 01:00:44,710
 

1737
01:00:42,369 --> 01:00:47,980
 things and it's they are examples of

1738
01:00:44,700 --> 01:00:47,980
 

1739
01:00:44,710 --> 01:00:51,580
 what was set up as the so called

1740
01:00:47,970 --> 01:00:51,580
 

1741
01:00:47,980 --> 01:00:54,670
 luckiness framework which is showing how

1742
01:00:51,570 --> 01:00:54,670
 

1743
01:00:51,580 --> 01:00:56,200
 you can detect these measures of if you

1744
01:00:54,660 --> 01:00:56,200
 

1745
01:00:54,670 --> 01:00:59,670
 like luckiness benign nests at the

1746
01:00:56,190 --> 01:00:59,670
 

1747
01:00:56,200 --> 01:01:02,859
 distribution and how they can be made

1748
01:00:59,660 --> 01:01:02,859
 

1749
01:00:59,670 --> 01:01:05,140
 Dayton is essentially making structural

1750
01:01:02,849 --> 01:01:05,140
 

1751
01:01:02,859 --> 01:01:07,900
 risk minimization data dependent so you

1752
01:01:05,130 --> 01:01:07,900
 

1753
01:01:05,140 --> 01:01:10,510
 don't have to a priori set up this

1754
01:01:07,890 --> 01:01:10,510
 

1755
01:01:07,900 --> 01:01:12,340
 hierarchy of classes that we do with

1756
01:01:10,500 --> 01:01:12,340
 

1757
01:01:10,510 --> 01:01:16,480
 structural risk minimization but they

1758
01:01:12,330 --> 01:01:16,480
 

1759
01:01:12,340 --> 01:01:18,970
 are actually arise from the observations

1760
01:01:16,470 --> 01:01:18,970
 

1761
01:01:16,480 --> 01:01:22,450
 we make during the training of our

1762
01:01:18,960 --> 01:01:22,450
 

1763
01:01:18,970 --> 01:01:25,420
 classifier so what we're going to do now

1764
01:01:22,440 --> 01:01:25,420
 

1765
01:01:22,450 --> 01:01:30,430
 is use the margin example as a case

1766
01:01:25,410 --> 01:01:30,430
 

1767
01:01:25,420 --> 01:01:32,170
 study so maximizing the margin

1768
01:01:30,420 --> 01:01:32,170
 

1769
01:01:30,430 --> 01:01:34,210
 frequently makes it possible to obtain

1770
01:01:32,160 --> 01:01:34,210
 

1771
01:01:32,170 --> 01:01:36,970
 good generalization despite a VC

1772
01:01:34,200 --> 01:01:36,970
 

1773
01:01:34,210 --> 01:01:41,290
 dimension as we saw the point Omar was

1774
01:01:36,960 --> 01:01:41,290
 

1775
01:01:36,970 --> 01:01:46,359
 making in that plot of the you know the

1776
01:01:41,280 --> 01:01:46,359
 

1777
01:01:41,290 --> 01:01:48,970
 generalization distributions for the

1778
01:01:46,349 --> 01:01:48,970
 

1779
01:01:46,359 --> 01:01:51,730
 case where the Gaussian kernel was used

1780
01:01:48,960 --> 01:01:51,730
 

1781
01:01:48,970 --> 01:01:54,730
 the Gaussian kernel has infinite VC

1782
01:01:51,720 --> 01:01:54,730
 

1783
01:01:51,730 --> 01:01:57,400
 dimension so clearly you know we would

1784
01:01:54,720 --> 01:01:57,400
 

1785
01:01:54,730 --> 01:02:01,119
 not expect the based on PC Theory

1786
01:01:57,390 --> 01:02:01,119
 

1787
01:01:57,400 --> 01:02:03,520
 phlearn him to be possible and indeed

1788
01:02:01,109 --> 01:02:03,520
 

1789
01:02:01,119 --> 01:02:05,530
 the lower bound implies that SVM's must

1790
01:02:03,510 --> 01:02:05,530
 

1791
01:02:03,520 --> 01:02:08,320
 be taking advantage of the benign

1792
01:02:05,520 --> 01:02:08,320
 

1793
01:02:05,530 --> 01:02:10,480
 distributions since we know that in the

1794
01:02:08,310 --> 01:02:10,480
 

1795
01:02:08,320 --> 01:02:13,000
 worst case generalization will be bad we

1796
01:02:10,470 --> 01:02:13,000
 

1797
01:02:10,480 --> 01:02:15,160
 can create distributions that would

1798
01:02:12,990 --> 01:02:15,160
 

1799
01:02:13,000 --> 01:02:18,130
 force an SVM with the Gaussian kernel to

1800
01:02:15,150 --> 01:02:18,130
 

1801
01:02:15,160 --> 01:02:20,109
 make high error that's the consequence

1802
01:02:18,120 --> 01:02:20,109
 

1803
01:02:18,130 --> 01:02:22,180
 of that theory the fact that that

1804
01:02:20,099 --> 01:02:22,180
 

1805
01:02:20,109 --> 01:02:24,580
 doesn't happen in practice is because

1806
01:02:22,170 --> 01:02:24,580
 

1807
01:02:22,180 --> 01:02:27,940
 the distributions we experience in the

1808
01:02:24,570 --> 01:02:27,940
 

1809
01:02:24,580 --> 01:02:29,770
 real world are actually you might not

1810
01:02:27,930 --> 01:02:29,770
 

1811
01:02:27,940 --> 01:02:34,210
 think so but they are actually very

1812
01:02:29,760 --> 01:02:34,210
 

1813
01:02:29,770 --> 01:02:35,980
 lucky or benign in that sense so and we

1814
01:02:34,200 --> 01:02:35,980
 

1815
01:02:34,210 --> 01:02:37,060
 require a theory that can give bounds

1816
01:02:35,970 --> 01:02:37,060
 

1817
01:02:35,980 --> 01:02:40,840
 that are sensitive to these

1818
01:02:37,050 --> 01:02:40,840
 

1819
01:02:37,060 --> 01:02:42,910
 serendipitous distributions and the in

1820
01:02:40,830 --> 01:02:42,910
 

1821
01:02:40,840 --> 01:02:46,450
 this case we're considering the margin

1822
01:02:42,900 --> 01:02:46,450
 

1823
01:02:42,910 --> 01:02:50,800
 is this indicator of this serendipitous

1824
01:02:46,440 --> 01:02:50,800
 

1825
01:02:46,450 --> 01:02:52,900
 situation or luckiness so the first

1826
01:02:50,790 --> 01:02:52,900
 

1827
01:02:50,800 --> 01:02:56,800
 intuition about how we might approach

1828
01:02:52,890 --> 01:02:56,800
 

1829
01:02:52,900 --> 01:03:00,550
 this is that if we use real valued

1830
01:02:56,790 --> 01:03:00,550
 

1831
01:02:56,800 --> 01:03:01,990
 function classes and the margin in a

1832
01:03:00,540 --> 01:03:01,990
 

1833
01:03:00,550 --> 01:03:05,310
 sense will give us some Indy

1834
01:03:01,980 --> 01:03:05,310
 

1835
01:03:01,990 --> 01:03:08,440
 the accuracy that with which we need to

1836
01:03:05,300 --> 01:03:08,440
 

1837
01:03:05,310 --> 01:03:11,020
 approximate the function that we're

1838
01:03:08,430 --> 01:03:11,020
 

1839
01:03:08,440 --> 01:03:13,660
 learning because if there's a nice

1840
01:03:11,010 --> 01:03:13,660
 

1841
01:03:11,020 --> 01:03:15,970
 margin then if we are finding a function

1842
01:03:13,650 --> 01:03:15,970
 

1843
01:03:13,660 --> 01:03:18,340
 that's not exactly the function but

1844
01:03:15,960 --> 01:03:18,340
 

1845
01:03:15,970 --> 01:03:20,170
 close to it it will have similar

1846
01:03:18,330 --> 01:03:20,170
 

1847
01:03:18,340 --> 01:03:22,270
 performance to the function so that's

1848
01:03:20,160 --> 01:03:22,270
 

1849
01:03:20,170 --> 01:03:25,870
 the kind of intuition that motivates the

1850
01:03:22,260 --> 01:03:25,870
 

1851
01:03:22,270 --> 01:03:27,640
 first approach but we're going to

1852
01:03:25,860 --> 01:03:27,640
 

1853
01:03:25,870 --> 01:03:28,630
 actually consider three proof techniques

1854
01:03:27,630 --> 01:03:28,630
 

1855
01:03:27,640 --> 01:03:31,090
 because I think it'll be a nice

1856
01:03:28,620 --> 01:03:31,090
 

1857
01:03:28,630 --> 01:03:33,040
 illustration of the different methods

1858
01:03:31,080 --> 01:03:33,040
 

1859
01:03:31,090 --> 01:03:33,640
 that are arising in statistical learning

1860
01:03:33,030 --> 01:03:33,640
 

1861
01:03:33,040 --> 01:03:36,250
 theory

1862
01:03:33,630 --> 01:03:36,250
 

1863
01:03:33,640 --> 01:03:39,310
 so the first will be based on this

1864
01:03:36,240 --> 01:03:39,310
 

1865
01:03:36,250 --> 01:03:41,370
 approximation accuracy idea and reduces

1866
01:03:39,300 --> 01:03:41,370
 

1867
01:03:39,310 --> 01:03:44,440
 to something known as covering numbers

1868
01:03:41,360 --> 01:03:44,440
 

1869
01:03:41,370 --> 01:03:48,760
 so I'll first talk about that but then

1870
01:03:44,430 --> 01:03:48,760
 

1871
01:03:44,440 --> 01:03:50,410
 I'll follow with again using real valued

1872
01:03:48,750 --> 01:03:50,410
 

1873
01:03:48,760 --> 01:03:52,900
 functions so it's sort of similar idea

1874
01:03:50,400 --> 01:03:52,900
 

1875
01:03:50,410 --> 01:03:55,150
 but reducing to how well the class can

1876
01:03:52,890 --> 01:03:55,150
 

1877
01:03:52,900 --> 01:03:58,420
 align with random labels and this is

1878
01:03:55,140 --> 01:03:58,420
 

1879
01:03:55,150 --> 01:04:00,600
 known as random eka complexity so I'll

1880
01:03:58,410 --> 01:04:00,600
 

1881
01:03:58,420 --> 01:04:03,580
 talk a little more in detail about that

1882
01:04:00,590 --> 01:04:03,580
 

1883
01:04:00,600 --> 01:04:06,700
 and even sort of give a kind of sketch

1884
01:04:03,570 --> 01:04:06,700
 

1885
01:04:03,580 --> 01:04:09,070
 of the proof and then we'll turn to

1886
01:04:06,690 --> 01:04:09,070
 

1887
01:04:06,700 --> 01:04:12,970
 looking at an approach inspired by

1888
01:04:09,060 --> 01:04:12,970
 

1889
01:04:09,070 --> 01:04:16,390
 Bayesian inference and that maintains a

1890
01:04:12,960 --> 01:04:16,390
 

1891
01:04:12,970 --> 01:04:18,730
 distribution over the function class and

1892
01:04:16,380 --> 01:04:18,730
 

1893
01:04:16,390 --> 01:04:21,310
 will give rise to what's known as the

1894
01:04:18,720 --> 01:04:21,310
 

1895
01:04:18,730 --> 01:04:23,590
 PAC Bayes analysis so a combination of

1896
01:04:21,300 --> 01:04:23,590
 

1897
01:04:21,310 --> 01:04:25,630
 sort of a Bayesian approach in that

1898
01:04:23,580 --> 01:04:25,630
 

1899
01:04:23,590 --> 01:04:27,070
 there are distributions but a PAC

1900
01:04:25,620 --> 01:04:27,070
 

1901
01:04:25,630 --> 01:04:29,950
 analysis and this we're giving

1902
01:04:27,060 --> 01:04:29,950
 

1903
01:04:27,070 --> 01:04:31,030
 approximately probability approximate

1904
01:04:29,940 --> 01:04:31,030
 

1905
01:04:29,950 --> 01:04:34,930
 and probably approximately correct

1906
01:04:31,020 --> 01:04:34,930
 

1907
01:04:31,030 --> 01:04:36,780
 bounds so that's the sort of the the

1908
01:04:34,920 --> 01:04:36,780
 

1909
01:04:34,930 --> 01:04:42,270
 three approaches we're going to consider

1910
01:04:36,770 --> 01:04:42,270
 

1911
01:04:36,780 --> 01:04:42,270
 so I'll start now with covering numbers

1912
01:04:42,380 --> 01:04:42,380
 

1913
01:04:42,390 --> 01:04:48,700
 now the proof here follows very much

1914
01:04:45,120 --> 01:04:48,700
 

1915
01:04:45,130 --> 01:04:53,560
 that of the VC bound that Omar described

1916
01:04:48,690 --> 01:04:53,560
 

1917
01:04:48,700 --> 01:04:55,870
 we replace the test error by a proxy

1918
01:04:53,550 --> 01:04:55,870
 

1919
01:04:53,560 --> 01:05:00,280
 which is the error we would get on a

1920
01:04:55,860 --> 01:05:00,280
 

1921
01:04:55,870 --> 01:05:06,370
 random second sample and that now gives

1922
01:05:00,270 --> 01:05:06,370
 

1923
01:05:00,280 --> 01:05:08,470
 us a fixed double sample of data and

1924
01:05:06,360 --> 01:05:08,470
 

1925
01:05:06,370 --> 01:05:10,630
 we're only interested in the performance

1926
01:05:08,460 --> 01:05:10,630
 

1927
01:05:08,470 --> 01:05:13,240
 of functions on that double sample and

1928
01:05:10,620 --> 01:05:13,240
 

1929
01:05:10,630 --> 01:05:14,070
 so we reduce suddenly to something we

1930
01:05:13,230 --> 01:05:14,070
 

1931
01:05:13,240 --> 01:05:17,280
 can handle

1932
01:05:14,060 --> 01:05:17,280
 

1933
01:05:14,070 --> 01:05:19,350
 very carefully and we can there are only

1934
01:05:17,270 --> 01:05:19,350
 

1935
01:05:17,280 --> 01:05:22,500
 finitely many performances that we might

1936
01:05:19,340 --> 01:05:22,500
 

1937
01:05:19,350 --> 01:05:25,830
 be able to get or we can handle the

1938
01:05:22,490 --> 01:05:25,830
 

1939
01:05:22,500 --> 01:05:27,900
 performance that we can get and what we

1940
01:05:25,820 --> 01:05:27,900
 

1941
01:05:25,830 --> 01:05:29,700
 do here in this case in the case of the

1942
01:05:27,890 --> 01:05:29,700
 

1943
01:05:27,900 --> 01:05:32,640
 VC it was a finite set of performance

1944
01:05:29,690 --> 01:05:32,640
 

1945
01:05:29,700 --> 01:05:35,580
 here we have actually infinite set of

1946
01:05:32,630 --> 01:05:35,580
 

1947
01:05:32,640 --> 01:05:39,060
 performances but we think of covering

1948
01:05:35,570 --> 01:05:39,060
 

1949
01:05:35,580 --> 01:05:41,550
 the performances that we can get on that

1950
01:05:39,050 --> 01:05:41,550
 

1951
01:05:39,060 --> 01:05:44,130
 set of points up to an accuracy that is

1952
01:05:41,540 --> 01:05:44,130
 

1953
01:05:41,550 --> 01:05:46,440
 given by the margin so we've got two two

1954
01:05:44,120 --> 01:05:46,440
 

1955
01:05:44,130 --> 01:05:48,300
 end points the EM training points and M

1956
01:05:46,430 --> 01:05:48,300
 

1957
01:05:46,440 --> 01:05:49,980
 test points and we're going to pick a

1958
01:05:48,290 --> 01:05:49,980
 

1959
01:05:48,300 --> 01:05:52,230
 set of functions that if you like

1960
01:05:49,970 --> 01:05:52,230
 

1961
01:05:49,980 --> 01:05:55,950
 characterizes the performance we can

1962
01:05:52,220 --> 01:05:55,950
 

1963
01:05:52,230 --> 01:05:59,310
 expect on that two M examples up to the

1964
01:05:55,940 --> 01:05:59,310
 

1965
01:05:55,950 --> 01:06:01,560
 accuracy of the samples so given any

1966
01:05:59,300 --> 01:06:01,560
 

1967
01:05:59,310 --> 01:06:03,840
 function in the class we can find an

1968
01:06:01,550 --> 01:06:03,840
 

1969
01:06:01,560 --> 01:06:06,150
 element in the cover that matches the

1970
01:06:03,830 --> 01:06:06,150
 

1971
01:06:03,840 --> 01:06:09,180
 performance of that function within the

1972
01:06:06,140 --> 01:06:09,180
 

1973
01:06:06,150 --> 01:06:12,000
 epsilon of the accuracy or the margin

1974
01:06:09,170 --> 01:06:12,000
 

1975
01:06:09,180 --> 01:06:15,060
 accuracy that we're interested in

1976
01:06:11,990 --> 01:06:15,060
 

1977
01:06:12,000 --> 01:06:18,450
 approximating to so that's the idea of a

1978
01:06:15,050 --> 01:06:18,450
 

1979
01:06:15,060 --> 01:06:21,510
 cover that we find per set of functions

1980
01:06:18,440 --> 01:06:21,510
 

1981
01:06:18,450 --> 01:06:23,520
 that if you like describes the the

1982
01:06:21,500 --> 01:06:23,520
 

1983
01:06:21,510 --> 01:06:26,550
 richness of performances that we can

1984
01:06:23,510 --> 01:06:26,550
 

1985
01:06:23,520 --> 01:06:29,820
 expect we therefore confined in that

1986
01:06:26,540 --> 01:06:29,820
 

1987
01:06:26,550 --> 01:06:31,170
 cover a function that is closed to the

1988
01:06:29,810 --> 01:06:31,170
 

1989
01:06:29,820 --> 01:06:34,800
 function that was output by our

1990
01:06:31,160 --> 01:06:34,800
 

1991
01:06:31,170 --> 01:06:36,390
 algorithm it's within the sort of margin

1992
01:06:34,790 --> 01:06:36,390
 

1993
01:06:34,800 --> 01:06:39,210
 accuracy of the function on all of the

1994
01:06:36,380 --> 01:06:39,210
 

1995
01:06:36,390 --> 01:06:41,880
 points and so that function actually

1996
01:06:39,200 --> 01:06:41,880
 

1997
01:06:39,210 --> 01:06:44,580
 will have similar performance in on the

1998
01:06:41,870 --> 01:06:44,580
 

1999
01:06:41,880 --> 01:06:46,050
 test and training sorry the first sample

2000
01:06:44,570 --> 01:06:46,050
 

2001
01:06:44,580 --> 01:06:48,780
 and the second sample it will have

2002
01:06:46,040 --> 01:06:48,780
 

2003
01:06:46,050 --> 01:06:51,240
 similar performance because it's within

2004
01:06:48,770 --> 01:06:51,240
 

2005
01:06:48,780 --> 01:06:54,180
 the margin accuracy it's giving the same

2006
01:06:51,230 --> 01:06:54,180
 

2007
01:06:51,240 --> 01:06:56,730
 values so when we threshold we'll have a

2008
01:06:54,170 --> 01:06:56,730
 

2009
01:06:54,180 --> 01:06:58,890
 very similar performance and so we can

2010
01:06:56,720 --> 01:06:58,890
 

2011
01:06:56,730 --> 01:07:01,170
 apply this idea of symmetry sation which

2012
01:06:58,880 --> 01:07:01,170
 

2013
01:06:58,890 --> 01:07:03,030
 is basically saying it's very unlikely

2014
01:07:01,160 --> 01:07:03,030
 

2015
01:07:01,170 --> 01:07:05,490
 you'll get poor performance on the first

2016
01:07:03,020 --> 01:07:05,490
 

2017
01:07:03,030 --> 01:07:07,590
 half and good performance on the second

2018
01:07:05,480 --> 01:07:07,590
 

2019
01:07:05,490 --> 01:07:10,040
 the first half corresponds to the

2020
01:07:07,580 --> 01:07:10,040
 

2021
01:07:07,590 --> 01:07:11,960
 training set so we're getting poor

2022
01:07:10,030 --> 01:07:11,960
 

2023
01:07:10,040 --> 01:07:14,220
 performance on the training set but

2024
01:07:11,950 --> 01:07:14,220
 

2025
01:07:11,960 --> 01:07:16,140
 sorry poor performance on training set

2026
01:07:14,210 --> 01:07:16,140
 

2027
01:07:14,220 --> 01:07:19,470
 but but good performance on the test set

2028
01:07:16,130 --> 01:07:19,470
 

2029
01:07:16,140 --> 01:07:21,560
 and symmetry zation just says it's

2030
01:07:19,460 --> 01:07:21,560
 

2031
01:07:19,470 --> 01:07:24,300
 equally likely if you swap those before

2032
01:07:21,550 --> 01:07:24,300
 

2033
01:07:21,560 --> 01:07:25,410
 points and so you would then get a very

2034
01:07:24,290 --> 01:07:25,410
 

2035
01:07:24,300 --> 01:07:28,860
 unlikely if

2036
01:07:25,400 --> 01:07:28,860
 

2037
01:07:25,410 --> 01:07:32,970
 the errors end up in one half of the of

2038
01:07:28,850 --> 01:07:32,970
 

2039
01:07:28,860 --> 01:07:35,340
 the of the pen of the paring and then we

2040
01:07:32,960 --> 01:07:35,340
 

2041
01:07:32,970 --> 01:07:38,340
 apply the Union bound over the cover so

2042
01:07:35,330 --> 01:07:38,340
 

2043
01:07:35,340 --> 01:07:41,040
 here we've actually taken functions

2044
01:07:38,330 --> 01:07:41,040
 

2045
01:07:38,340 --> 01:07:45,090
 together that are sort of close in this

2046
01:07:41,030 --> 01:07:45,090
 

2047
01:07:41,040 --> 01:07:46,890
 cover sense and the effective complexity

2048
01:07:45,080 --> 01:07:46,890
 

2049
01:07:45,090 --> 01:07:49,200
 is the log of the covering numbers so

2050
01:07:46,880 --> 01:07:49,200
 

2051
01:07:46,890 --> 01:07:54,900
 the critical measure you need to have

2052
01:07:49,190 --> 01:07:54,900
 

2053
01:07:49,200 --> 01:07:56,190
 here is the size of this cover and this

2054
01:07:54,890 --> 01:07:56,190
 

2055
01:07:54,900 --> 01:07:58,440
 can actually be bounded by a

2056
01:07:56,180 --> 01:07:58,440
 

2057
01:07:56,190 --> 01:08:00,150
 generalization of the VC dimension known

2058
01:07:58,430 --> 01:08:00,150
 

2059
01:07:58,440 --> 01:08:02,610
 as the fact shattering dimension so this

2060
01:08:00,140 --> 01:08:02,610
 

2061
01:08:00,150 --> 01:08:06,000
 is the outline of this if you like first

2062
01:08:02,600 --> 01:08:06,000
 

2063
01:08:02,610 --> 01:08:10,410
 approach to proving generalization

2064
01:08:05,990 --> 01:08:10,410
 

2065
01:08:06,000 --> 01:08:12,390
 bounds for large margin classifiers so

2066
01:08:10,400 --> 01:08:12,390
 

2067
01:08:10,410 --> 01:08:15,510
 I'll now move to the random echo

2068
01:08:12,380 --> 01:08:15,510
 

2069
01:08:12,390 --> 01:08:19,470
 complexity and that starts from

2070
01:08:15,500 --> 01:08:19,470
 

2071
01:08:15,510 --> 01:08:22,470
 considering a uniform bound on the gap

2072
01:08:19,460 --> 01:08:22,470
 

2073
01:08:19,470 --> 01:08:25,830
 over the class so if we think about the

2074
01:08:22,460 --> 01:08:25,830
 

2075
01:08:22,470 --> 01:08:27,900
 this is the probability and I've put P

2076
01:08:25,820 --> 01:08:27,900
 

2077
01:08:25,830 --> 01:08:30,480
 to the M here which means over the M

2078
01:08:27,890 --> 01:08:30,480
 

2079
01:08:27,900 --> 01:08:35,250
 sample this is our random probability of

2080
01:08:30,470 --> 01:08:35,250
 

2081
01:08:30,480 --> 01:08:37,170
 the the sample that the all of the

2082
01:08:35,240 --> 01:08:37,170
 

2083
01:08:35,250 --> 01:08:39,150
 functions in the class have a gap less

2084
01:08:37,160 --> 01:08:39,150
 

2085
01:08:37,170 --> 01:08:41,250
 than or equal to epsilon is less than or

2086
01:08:39,140 --> 01:08:41,250
 

2087
01:08:39,150 --> 01:08:45,570
 equal to the probability that the worst

2088
01:08:41,240 --> 01:08:45,570
 

2089
01:08:41,250 --> 01:08:47,280
 class has the worse sorry function the

2090
01:08:45,560 --> 01:08:47,280
 

2091
01:08:45,570 --> 01:08:51,840
 worst gap is less than or equal to

2092
01:08:47,270 --> 01:08:51,840
 

2093
01:08:47,280 --> 01:08:54,150
 Epsilon trick here is to again use this

2094
01:08:51,830 --> 01:08:54,150
 

2095
01:08:51,840 --> 01:08:57,300
 idea of a ghost sample so we have the

2096
01:08:54,140 --> 01:08:57,300
 

2097
01:08:54,150 --> 01:08:59,730
 original sample the gap is the test

2098
01:08:57,290 --> 01:08:59,730
 

2099
01:08:57,300 --> 01:09:03,330
 error minus the empirical in sample

2100
01:08:59,720 --> 01:09:03,330
 

2101
01:08:59,730 --> 01:09:06,900
 error but now we use a ghost sample and

2102
01:09:03,320 --> 01:09:06,900
 

2103
01:09:03,330 --> 01:09:09,990
 the out the out-of-sample error is just

2104
01:09:06,890 --> 01:09:09,990
 

2105
01:09:06,900 --> 01:09:11,550
 equal to the expected value of the

2106
01:09:09,980 --> 01:09:11,550
 

2107
01:09:09,990 --> 01:09:14,060
 performance of the ghost sample this is

2108
01:09:11,540 --> 01:09:14,060
 

2109
01:09:11,550 --> 01:09:15,540
 the trick of replacing the

2110
01:09:14,050 --> 01:09:15,540
 

2111
01:09:14,060 --> 01:09:18,060
 generalization error with the

2112
01:09:15,530 --> 01:09:18,060
 

2113
01:09:15,540 --> 01:09:21,380
 performance on a ghost sample so now if

2114
01:09:18,050 --> 01:09:21,380
 

2115
01:09:18,060 --> 01:09:23,580
 we take the expected value of this soup

2116
01:09:21,370 --> 01:09:23,580
 

2117
01:09:21,380 --> 01:09:25,560
 remember we were looking at this soup

2118
01:09:23,570 --> 01:09:25,560
 

2119
01:09:23,580 --> 01:09:30,210
 here take the expected value of that

2120
01:09:25,550 --> 01:09:30,210
 

2121
01:09:25,560 --> 01:09:32,130
 soup of the supremum over the gaps it's

2122
01:09:30,200 --> 01:09:32,130
 

2123
01:09:30,210 --> 01:09:34,500
 less than or equal to and this is really

2124
01:09:32,120 --> 01:09:34,500
 

2125
01:09:32,130 --> 01:09:37,259
 just replacing that

2126
01:09:34,490 --> 01:09:37,259
 

2127
01:09:34,500 --> 01:09:41,190
 out part here with this expected value

2128
01:09:37,249 --> 01:09:41,190
 

2129
01:09:37,259 --> 01:09:45,450
 and then moving the expected value out

2130
01:09:41,180 --> 01:09:45,450
 

2131
01:09:41,190 --> 01:09:47,970
 through the through the average and that

2132
01:09:45,440 --> 01:09:47,970
 

2133
01:09:45,450 --> 01:09:49,859
 brings an inequality at less than or

2134
01:09:47,960 --> 01:09:49,859
 

2135
01:09:47,970 --> 01:09:52,200
 equal to so it's less than or equal to

2136
01:09:49,849 --> 01:09:52,200
 

2137
01:09:49,859 --> 01:09:56,640
 the expected value over this to sample

2138
01:09:52,190 --> 01:09:56,640
 

2139
01:09:52,200 --> 01:09:58,590
 double sample of the supremum of the if

2140
01:09:56,630 --> 01:09:58,590
 

2141
01:09:56,640 --> 01:10:00,540
 you like sort of gap between the

2142
01:09:58,580 --> 01:10:00,540
 

2143
01:09:58,590 --> 01:10:02,310
 performance on one sample and the

2144
01:10:00,530 --> 01:10:02,310
 

2145
01:10:00,540 --> 01:10:06,900
 performance on the other so it's a very

2146
01:10:02,300 --> 01:10:06,900
 

2147
01:10:02,310 --> 01:10:09,570
 kind of intuitive idea and here we again

2148
01:10:06,890 --> 01:10:09,570
 

2149
01:10:06,900 --> 01:10:12,720
 apply symmetrization but now in this

2150
01:10:09,560 --> 01:10:12,720
 

2151
01:10:09,570 --> 01:10:15,030
 formulation this corresponds to swapping

2152
01:10:12,710 --> 01:10:15,030
 

2153
01:10:12,720 --> 01:10:17,820
 the elements and we do that by actually

2154
01:10:15,020 --> 01:10:17,820
 

2155
01:10:15,030 --> 01:10:20,580
 adding in a variable here Sigma I which

2156
01:10:17,810 --> 01:10:20,580
 

2157
01:10:17,820 --> 01:10:22,560
 is a plus minus one variable which if

2158
01:10:20,570 --> 01:10:22,560
 

2159
01:10:20,580 --> 01:10:24,480
 it's plus one it leaves them in the same

2160
01:10:22,550 --> 01:10:24,480
 

2161
01:10:22,560 --> 01:10:27,030
 order of its minus one it swaps them and

2162
01:10:24,470 --> 01:10:27,030
 

2163
01:10:24,480 --> 01:10:29,490
 the because of the fact that they're

2164
01:10:27,020 --> 01:10:29,490
 

2165
01:10:27,030 --> 01:10:31,350
 drawn from the same distribution if we

2166
01:10:29,480 --> 01:10:31,350
 

2167
01:10:29,490 --> 01:10:33,240
 swap them there should be no change in

2168
01:10:31,340 --> 01:10:33,240
 

2169
01:10:31,350 --> 01:10:35,360
 the probability so the expectation is

2170
01:10:33,230 --> 01:10:35,360
 

2171
01:10:33,240 --> 01:10:39,350
 identical when we introduced this

2172
01:10:35,350 --> 01:10:39,350
 

2173
01:10:35,360 --> 01:10:42,330
 expectation over swapping and these are

2174
01:10:39,340 --> 01:10:42,330
 

2175
01:10:39,350 --> 01:10:44,010
 plus minus one random variables so that

2176
01:10:42,320 --> 01:10:44,010
 

2177
01:10:42,330 --> 01:10:46,320
 is why those are known as random eka

2178
01:10:44,000 --> 01:10:46,320
 

2179
01:10:44,010 --> 01:10:48,510
 Rademacher random variables and this is

2180
01:10:46,310 --> 01:10:48,510
 

2181
01:10:46,320 --> 01:10:52,850
 why this is known as the random akka

2182
01:10:48,500 --> 01:10:52,850
 

2183
01:10:48,510 --> 01:10:56,990
 analysis and so now we can actually

2184
01:10:52,840 --> 01:10:56,990
 

2185
01:10:52,850 --> 01:11:03,090
 separate out these two parts and we

2186
01:10:56,980 --> 01:11:03,090
 

2187
01:10:56,990 --> 01:11:04,890
 actually can take the some you know a

2188
01:11:03,080 --> 01:11:04,890
 

2189
01:11:03,090 --> 01:11:07,230
 bound them by the sum of the two parts

2190
01:11:04,880 --> 01:11:07,230
 

2191
01:11:04,890 --> 01:11:11,640
 rather than the difference and we get

2192
01:11:07,220 --> 01:11:11,640
 

2193
01:11:07,230 --> 01:11:14,490
 twice the expectation of this of this

2194
01:11:11,630 --> 01:11:14,490
 

2195
01:11:11,640 --> 01:11:19,410
 estimator here which is basically taking

2196
01:11:14,480 --> 01:11:19,410
 

2197
01:11:14,490 --> 01:11:21,570
 the supremum of the function correlation

2198
01:11:19,400 --> 01:11:21,570
 

2199
01:11:19,410 --> 01:11:24,600
 that can be achieved by the function

2200
01:11:21,560 --> 01:11:24,600
 

2201
01:11:21,570 --> 01:11:27,510
 with random noise so it's kind of a very

2202
01:11:24,590 --> 01:11:27,510
 

2203
01:11:24,600 --> 01:11:31,470
 intuitive concept here it's sort of

2204
01:11:27,500 --> 01:11:31,470
 

2205
01:11:27,510 --> 01:11:36,570
 saying okay I'm looking at how well I

2206
01:11:31,460 --> 01:11:36,570
 

2207
01:11:31,470 --> 01:11:40,320
 can align my function loss with random

2208
01:11:36,560 --> 01:11:40,320
 

2209
01:11:36,570 --> 01:11:42,600
 noise and this is known as the red emaki

2210
01:11:40,310 --> 01:11:42,600
 

2211
01:11:40,320 --> 01:11:45,210
 complexity of the class this quantity

2212
01:11:42,590 --> 01:11:45,210
 

2213
01:11:42,600 --> 01:11:47,880
 here so I'll show that on the next slide

2214
01:11:45,200 --> 01:11:47,880
 

2215
01:11:45,210 --> 01:11:51,179
 I'll just take this final thing here

2216
01:11:47,870 --> 01:11:51,179
 

2217
01:11:47,880 --> 01:11:53,340
 put it on to the next slide this is the

2218
01:11:51,169 --> 01:11:53,340
 

2219
01:11:51,179 --> 01:11:56,010
 empirical Rademacher complexity is the

2220
01:11:53,330 --> 01:11:56,010
 

2221
01:11:53,340 --> 01:11:58,050
 expected value for the given training

2222
01:11:56,000 --> 01:11:58,050
 

2223
01:11:56,010 --> 01:12:01,380
 set so this is for a particular training

2224
01:11:58,040 --> 01:12:01,380
 

2225
01:11:58,050 --> 01:12:03,540
 set and the rad emaki complexity is the

2226
01:12:01,370 --> 01:12:03,540
 

2227
01:12:01,380 --> 01:12:08,370
 expectation of a random training sets of

2228
01:12:03,530 --> 01:12:08,370
 

2229
01:12:03,540 --> 01:12:10,260
 that quantity so what we showed on the

2230
01:12:08,360 --> 01:12:10,260
 

2231
01:12:08,370 --> 01:12:12,300
 previous slide was this bound here the

2232
01:12:10,250 --> 01:12:12,300
 

2233
01:12:10,260 --> 01:12:15,900
 expected value of the supremum is less

2234
01:12:12,290 --> 01:12:15,900
 

2235
01:12:12,300 --> 01:12:17,730
 than twice the Rademacher complexity now

2236
01:12:15,890 --> 01:12:17,730
 

2237
01:12:15,900 --> 01:12:19,770
 there's a couple more steps that are

2238
01:12:17,720 --> 01:12:19,770
 

2239
01:12:17,730 --> 01:12:23,370
 quite straightforward using McDermott's

2240
01:12:19,760 --> 01:12:23,370
 

2241
01:12:19,770 --> 01:12:26,580
 inequality firstly to show that the

2242
01:12:23,360 --> 01:12:26,580
 

2243
01:12:23,370 --> 01:12:27,960
 supremum of this gap is bounded by its

2244
01:12:26,570 --> 01:12:27,960
 

2245
01:12:26,580 --> 01:12:32,040
 expectation which is what we've just

2246
01:12:27,950 --> 01:12:32,040
 

2247
01:12:27,960 --> 01:12:35,070
 bounded plus the the amount that we can

2248
01:12:32,030 --> 01:12:35,070
 

2249
01:12:32,040 --> 01:12:37,139
 change essentially with swapping a

2250
01:12:35,060 --> 01:12:37,139
 

2251
01:12:35,070 --> 01:12:40,460
 single element this is the sort of rep

2252
01:12:37,129 --> 01:12:40,460
 

2253
01:12:37,139 --> 01:12:42,600
 mike diameter inequality approach and

2254
01:12:40,450 --> 01:12:42,600
 

2255
01:12:40,460 --> 01:12:44,460
 finally the also the McDermott

2256
01:12:42,590 --> 01:12:44,460
 

2257
01:12:42,600 --> 01:12:46,800
 inequality can be used to bound the

2258
01:12:44,450 --> 01:12:46,800
 

2259
01:12:44,460 --> 01:12:49,739
 random act of complexity in terms of its

2260
01:12:46,790 --> 01:12:49,739
 

2261
01:12:46,800 --> 01:12:51,270
 empirical quantity this one that depends

2262
01:12:49,729 --> 01:12:51,270
 

2263
01:12:49,739 --> 01:12:53,730
 on the particular training set rather

2264
01:12:51,260 --> 01:12:53,730
 

2265
01:12:51,270 --> 01:12:56,670
 than the expectation over training sets

2266
01:12:53,720 --> 01:12:56,670
 

2267
01:12:53,730 --> 01:12:59,639
 so combining all of that together we

2268
01:12:56,660 --> 01:12:59,639
 

2269
01:12:56,670 --> 01:13:02,429
 arrive at the following bound which is

2270
01:12:59,629 --> 01:13:02,429
 

2271
01:12:59,639 --> 01:13:05,489
 the core Rademacher bond which says with

2272
01:13:02,419 --> 01:13:05,489
 

2273
01:13:02,429 --> 01:13:07,949
 high probability for all functions in

2274
01:13:05,479 --> 01:13:07,949
 

2275
01:13:05,489 --> 01:13:10,320
 the class the gap is bounded by twice

2276
01:13:07,939 --> 01:13:10,320
 

2277
01:13:07,949 --> 01:13:12,480
 the empirical Rademacher complexity plus

2278
01:13:10,310 --> 01:13:12,480
 

2279
01:13:10,320 --> 01:13:14,100
 a term that is very similar to this sort

2280
01:13:12,470 --> 01:13:14,100
 

2281
01:13:12,480 --> 01:13:15,960
 of term we would get with a single

2282
01:13:14,090 --> 01:13:15,960
 

2283
01:13:14,100 --> 01:13:17,850
 function that omar showed at the

2284
01:13:15,950 --> 01:13:17,850
 

2285
01:13:15,960 --> 01:13:20,540
 beginning so the rand american

2286
01:13:17,840 --> 01:13:20,540
 

2287
01:13:17,850 --> 01:13:24,440
 complexity is really capturing the key

2288
01:13:20,530 --> 01:13:24,440
 

2289
01:13:20,540 --> 01:13:29,100
 complexity of the function class and

2290
01:13:24,430 --> 01:13:29,100
 

2291
01:13:24,440 --> 01:13:31,710
 it's this measure here of our ability to

2292
01:13:29,090 --> 01:13:31,710
 

2293
01:13:29,100 --> 01:13:33,960
 correlate with Randa our expected

2294
01:13:31,700 --> 01:13:33,960
 

2295
01:13:31,710 --> 01:13:38,010
 ability to correlate with random noise

2296
01:13:33,950 --> 01:13:38,010
 

2297
01:13:33,960 --> 01:13:43,710
 so it's key idea now if we can apply

2298
01:13:38,000 --> 01:13:43,710
 

2299
01:13:38,010 --> 01:13:44,730
 that quite readily to SVM's so here it

2300
01:13:43,700 --> 01:13:44,730
 

2301
01:13:43,710 --> 01:13:46,650
 is for SVM's

2302
01:13:44,720 --> 01:13:46,650
 

2303
01:13:44,730 --> 01:13:48,179
 firstly we need to measure the

2304
01:13:46,640 --> 01:13:48,179
 

2305
01:13:46,650 --> 01:13:51,420
 rate-o-matic a Compaq city of the

2306
01:13:48,169 --> 01:13:51,420
 

2307
01:13:48,179 --> 01:13:54,480
 function class and here the the trick is

2308
01:13:51,410 --> 01:13:54,480
 

2309
01:13:51,420 --> 01:13:56,280
 to use the functions you can generate

2310
01:13:54,470 --> 01:13:56,280
 

2311
01:13:54,480 --> 01:13:59,699
 with bounded norm

2312
01:13:56,270 --> 01:13:59,699
 

2313
01:13:56,280 --> 01:14:03,869
 and with a fixed kernel so I'm taking

2314
01:13:59,689 --> 01:14:03,869
 

2315
01:13:59,699 --> 01:14:06,090
 Colonel Kappa and norm bound be then

2316
01:14:03,859 --> 01:14:06,090
 

2317
01:14:03,869 --> 01:14:08,489
 where this is the band on the two norm

2318
01:14:06,080 --> 01:14:08,489
 

2319
01:14:06,090 --> 01:14:12,090
 of the weight vector so this is the

2320
01:14:08,479 --> 01:14:12,090
 

2321
01:14:08,489 --> 01:14:14,520
 function class I'm interested in and the

2322
01:14:12,080 --> 01:14:14,520
 

2323
01:14:12,090 --> 01:14:19,440
 red Omega complexity of that is bounded

2324
01:14:14,510 --> 01:14:19,440
 

2325
01:14:14,520 --> 01:14:23,520
 by the norm of the the norm bound B

2326
01:14:19,430 --> 01:14:23,520
 

2327
01:14:19,440 --> 01:14:26,760
 divided by M times the square root of

2328
01:14:23,510 --> 01:14:26,760
 

2329
01:14:23,520 --> 01:14:28,829
 the trace of the kernel matrix okay so

2330
01:14:26,750 --> 01:14:28,829
 

2331
01:14:26,760 --> 01:14:30,480
 that's I'm not giving you a proof but

2332
01:14:28,819 --> 01:14:30,480
 

2333
01:14:28,829 --> 01:14:34,320
 it's very straight it's relatively

2334
01:14:30,470 --> 01:14:34,320
 

2335
01:14:30,480 --> 01:14:37,699
 straightforward to proof to show that so

2336
01:14:34,310 --> 01:14:37,699
 

2337
01:14:34,320 --> 01:14:40,469
 this motivates controlling the

2338
01:14:37,689 --> 01:14:40,469
 

2339
01:14:37,699 --> 01:14:42,929
 regularizing with the two norm while

2340
01:14:40,459 --> 01:14:42,929
 

2341
01:14:40,469 --> 01:14:46,039
 keeping the outputs at plus and minus

2342
01:14:42,919 --> 01:14:46,039
 

2343
01:14:42,929 --> 01:14:48,840
 one and this gives the SVM optimization

2344
01:14:46,029 --> 01:14:48,840
 

2345
01:14:46,039 --> 01:14:51,570
 where the hinge loss is used to take the

2346
01:14:48,830 --> 01:14:51,570
 

2347
01:14:48,840 --> 01:14:54,809
 real valued approximation that we're

2348
01:14:51,560 --> 01:14:54,809
 

2349
01:14:51,570 --> 01:14:58,139
 using to classification and we've had to

2350
01:14:54,799 --> 01:14:58,139
 

2351
01:14:54,809 --> 01:15:01,380
 use the fact that the if we apply a

2352
01:14:58,129 --> 01:15:01,380
 

2353
01:14:58,139 --> 01:15:05,159
 function like the hinge loss to the real

2354
01:15:01,370 --> 01:15:05,159
 

2355
01:15:01,380 --> 01:15:09,690
 valued functions then the ratamacue

2356
01:15:05,149 --> 01:15:09,690
 

2357
01:15:05,159 --> 01:15:12,329
 complexity is is only affected by the

2358
01:15:09,680 --> 01:15:12,329
 

2359
01:15:09,690 --> 01:15:14,190
 Lipschitz function sorry Lipschitz

2360
01:15:12,319 --> 01:15:14,190
 

2361
01:15:12,329 --> 01:15:17,460
 constant of the function and in this

2362
01:15:14,180 --> 01:15:17,460
 

2363
01:15:14,190 --> 01:15:19,590
 case the Lipschitz constant is 1 so the

2364
01:15:17,450 --> 01:15:19,590
 

2365
01:15:17,460 --> 01:15:22,280
 rad emaki complexity well it's a factor

2366
01:15:19,580 --> 01:15:22,280
 

2367
01:15:19,590 --> 01:15:25,260
 2 comes in but it's is roughly the same

2368
01:15:22,270 --> 01:15:25,260
 

2369
01:15:22,280 --> 01:15:27,239
 putting all the pieces together this

2370
01:15:25,250 --> 01:15:27,239
 

2371
01:15:25,260 --> 01:15:29,639
 gives a bound that motivates the SVM

2372
01:15:27,229 --> 01:15:29,639
 

2373
01:15:27,239 --> 01:15:32,730
 algorithm where we use the slack

2374
01:15:29,629 --> 01:15:32,730
 

2375
01:15:29,639 --> 01:15:35,670
 variables III standard notation and the

2376
01:15:32,720 --> 01:15:35,670
 

2377
01:15:32,730 --> 01:15:38,460
 margin gamma is 1 over the norm because

2378
01:15:35,660 --> 01:15:38,460
 

2379
01:15:35,670 --> 01:15:41,460
 we've used the outputs to be plus and

2380
01:15:38,450 --> 01:15:41,460
 

2381
01:15:38,460 --> 01:15:44,039
 minus 1 so this is the upper bound on

2382
01:15:41,450 --> 01:15:44,039
 

2383
01:15:41,460 --> 01:15:45,449
 the generalization error so I hopefully

2384
01:15:44,029 --> 01:15:45,449
 

2385
01:15:44,039 --> 01:15:47,489
 given you just an intuition about how

2386
01:15:45,439 --> 01:15:47,489
 

2387
01:15:45,449 --> 01:15:49,020
 this there's a lot of detail I've

2388
01:15:47,479 --> 01:15:49,020
 

2389
01:15:47,489 --> 01:15:52,559
 skipped over but hopefully it gives you

2390
01:15:49,010 --> 01:15:52,559
 

2391
01:15:49,020 --> 01:15:54,690
 an understanding so here the this is the

2392
01:15:52,549 --> 01:15:54,690
 

2393
01:15:52,559 --> 01:15:57,030
 equivalent of the empirical error the

2394
01:15:54,680 --> 01:15:57,030
 

2395
01:15:54,690 --> 01:16:00,300
 slack variable measurement so if we were

2396
01:15:57,020 --> 01:16:00,300
 

2397
01:15:57,030 --> 01:16:02,940
 getting perfect separation with a margin

2398
01:16:00,290 --> 01:16:02,940
 

2399
01:16:00,300 --> 01:16:04,590
 of gamma this would be 0 this is the

2400
01:16:02,930 --> 01:16:04,590
 

2401
01:16:02,940 --> 01:16:06,480
 complexity term that came from the

2402
01:16:04,580 --> 01:16:06,480
 

2403
01:16:04,590 --> 01:16:09,070
 Rademacher complexity and this is the

2404
01:16:06,470 --> 01:16:09,070
 

2405
01:16:06,480 --> 01:16:11,350
 equivalent of the term weak

2406
01:16:09,060 --> 01:16:11,350
 

2407
01:16:09,070 --> 01:16:13,750
 that controls the likelihood of being

2408
01:16:11,340 --> 01:16:13,750
 

2409
01:16:11,350 --> 01:16:16,870
 misled which was similar to the term we

2410
01:16:13,740 --> 01:16:16,870
 

2411
01:16:13,750 --> 01:16:20,110
 got with a single function if we

2412
01:16:16,860 --> 01:16:20,110
 

2413
01:16:16,870 --> 01:16:21,970
 consider a Gaussian kernel it actually

2414
01:16:20,100 --> 01:16:21,970
 

2415
01:16:20,110 --> 01:16:25,030
 simplifies down because the trace of the

2416
01:16:21,960 --> 01:16:25,030
 

2417
01:16:21,970 --> 01:16:28,630
 kernel matrix is just root m and so we

2418
01:16:25,020 --> 01:16:28,630
 

2419
01:16:25,030 --> 01:16:31,590
 get four over root M gamma so clearly

2420
01:16:28,620 --> 01:16:31,590
 

2421
01:16:28,630 --> 01:16:35,980
 you can see this is motivating the

2422
01:16:31,580 --> 01:16:35,980
 

2423
01:16:31,590 --> 01:16:37,870
 minimization sorry the maximization the

2424
01:16:35,970 --> 01:16:37,870
 

2425
01:16:35,980 --> 01:16:40,390
 margin minimization of the slack

2426
01:16:37,860 --> 01:16:40,390
 

2427
01:16:37,870 --> 01:16:45,670
 variables which is the at the heart of

2428
01:16:40,380 --> 01:16:45,670
 

2429
01:16:40,390 --> 01:16:48,850
 the SVM algorithm so this is assuring us

2430
01:16:45,660 --> 01:16:48,850
 

2431
01:16:45,670 --> 01:16:53,140
 that indeed the margin is an indicator

2432
01:16:48,840 --> 01:16:53,140
 

2433
01:16:48,850 --> 01:16:57,190
 of the bananas of distribution and that

2434
01:16:53,130 --> 01:16:57,190
 

2435
01:16:53,140 --> 01:17:02,530
 the SVM algorithm is able to not only

2436
01:16:57,180 --> 01:17:02,530
 

2437
01:16:57,190 --> 01:17:04,270
 find a function that is performing well

2438
01:17:02,520 --> 01:17:04,270
 

2439
01:17:02,530 --> 01:17:07,030
 on the training data but at the same

2440
01:17:04,260 --> 01:17:07,030
 

2441
01:17:04,270 --> 01:17:09,760
 time it's detecting that the

2442
01:17:07,020 --> 01:17:09,760
 

2443
01:17:07,030 --> 01:17:12,820
 distribution is buying and benign in the

2444
01:17:09,750 --> 01:17:12,820
 

2445
01:17:09,760 --> 01:17:17,500
 sense that there is this sort of low

2446
01:17:12,810 --> 01:17:17,500
 

2447
01:17:12,820 --> 01:17:20,410
 probability region close to the close to

2448
01:17:17,490 --> 01:17:20,410
 

2449
01:17:17,500 --> 01:17:25,810
 the decision boundary so in the sense of

2450
01:17:20,400 --> 01:17:25,810
 

2451
01:17:20,410 --> 01:17:28,960
 seebik off and the the bound that the

2452
01:17:25,800 --> 01:17:28,960
 

2453
01:17:25,810 --> 01:17:33,510
 the sort of just the thing I mentioned

2454
01:17:28,950 --> 01:17:33,510
 

2455
01:17:28,960 --> 01:17:37,030
 in terms of this distribution a class of

2456
01:17:33,500 --> 01:17:37,030
 

2457
01:17:33,510 --> 01:17:39,520
 this is actually a detecting that there

2458
01:17:37,020 --> 01:17:39,520
 

2459
01:17:37,030 --> 01:17:42,070
 is something of that nature going on but

2460
01:17:39,510 --> 01:17:42,070
 

2461
01:17:39,520 --> 01:17:44,350
 it's not in any sense saying it's in

2462
01:17:42,060 --> 01:17:44,350
 

2463
01:17:42,070 --> 01:17:46,060
 those classes that were defined by OD

2464
01:17:44,340 --> 01:17:46,060
 

2465
01:17:44,350 --> 01:17:48,340
 Baron and C back off it's not saying

2466
01:17:46,050 --> 01:17:48,340
 

2467
01:17:46,060 --> 01:17:51,580
 that it's just saying that it's actually

2468
01:17:48,330 --> 01:17:51,580
 

2469
01:17:48,340 --> 01:17:54,070
 detecting that it's in some sense there

2470
01:17:51,570 --> 01:17:54,070
 

2471
01:17:51,580 --> 01:17:56,410
 is this low probability region that is

2472
01:17:54,060 --> 01:17:56,410
 

2473
01:17:54,070 --> 01:18:00,340
 and it's able to detect that and find a

2474
01:17:56,400 --> 01:18:00,340
 

2475
01:17:56,410 --> 01:18:02,440
 corresponding bound so just a few

2476
01:18:00,330 --> 01:18:02,440
 

2477
01:18:00,340 --> 01:18:07,210
 comments on the on the random echo

2478
01:18:02,430 --> 01:18:07,210
 

2479
01:18:02,440 --> 01:18:10,390
 complexity approach it kind of motivates

2480
01:18:07,200 --> 01:18:10,390
 

2481
01:18:07,210 --> 01:18:12,910
 a general plug-and-play to derive bands

2482
01:18:10,380 --> 01:18:12,910
 

2483
01:18:10,390 --> 01:18:16,060
 based on random echo complexity for

2484
01:18:12,900 --> 01:18:16,060
 

2485
01:18:12,910 --> 01:18:18,670
 other kernel-based and I mean general to

2486
01:18:16,050 --> 01:18:18,670
 

2487
01:18:16,060 --> 01:18:21,040
 norm regularized algorithms

2488
01:18:18,660 --> 01:18:21,040
 

2489
01:18:18,670 --> 01:18:22,750
 as you can see the rat American

2490
01:18:21,030 --> 01:18:22,750
 

2491
01:18:21,040 --> 01:18:26,380
 complexity was sort of characterized by

2492
01:18:22,740 --> 01:18:26,380
 

2493
01:18:22,750 --> 01:18:28,150
 this to norm quantity so you can sort of

2494
01:18:26,370 --> 01:18:28,150
 

2495
01:18:26,380 --> 01:18:29,920
 plug that into any algorithm and

2496
01:18:28,140 --> 01:18:29,920
 

2497
01:18:28,150 --> 01:18:31,810
 typically you know kernel based

2498
01:18:29,910 --> 01:18:31,810
 

2499
01:18:29,920 --> 01:18:34,980
 algorithms do regularize with the two

2500
01:18:31,800 --> 01:18:34,980
 

2501
01:18:31,810 --> 01:18:38,500
 norm so you can apply it to kernel pca

2502
01:18:34,970 --> 01:18:38,500
 

2503
01:18:34,980 --> 01:18:41,370
 you can apply it to kernel CCA one class

2504
01:18:38,490 --> 01:18:41,370
 

2505
01:18:38,500 --> 01:18:44,080
 SVM indeed multiple kernel learning

2506
01:18:41,360 --> 01:18:44,080
 

2507
01:18:41,370 --> 01:18:47,770
 regression and so on so there's sort of

2508
01:18:44,070 --> 01:18:47,770
 

2509
01:18:44,080 --> 01:18:51,190
 a generic thing going on here however

2510
01:18:47,760 --> 01:18:51,190
 

2511
01:18:47,770 --> 01:18:54,370
 it's also applicable to one norm

2512
01:18:51,180 --> 01:18:54,370
 

2513
01:18:51,190 --> 01:18:56,350
 regularized methods and this actually

2514
01:18:54,360 --> 01:18:56,350
 

2515
01:18:54,370 --> 01:18:59,230
 arises from a very nice property of

2516
01:18:56,340 --> 01:18:59,230
 

2517
01:18:56,350 --> 01:19:01,300
 recommend Rademacher complexity that

2518
01:18:59,220 --> 01:19:01,300
 

2519
01:18:59,230 --> 01:19:03,310
 it's actually not changed if we take a

2520
01:19:01,290 --> 01:19:03,310
 

2521
01:19:01,300 --> 01:19:05,980
 convex hull of a set of functions so if

2522
01:19:03,300 --> 01:19:05,980
 

2523
01:19:03,310 --> 01:19:07,930
 you think of you know the the functions

2524
01:19:05,970 --> 01:19:07,930
 

2525
01:19:05,980 --> 01:19:10,030
 you can generate and say with boosting

2526
01:19:07,920 --> 01:19:10,030
 

2527
01:19:07,930 --> 01:19:14,710
 if we can imagine a sort of one norm

2528
01:19:10,020 --> 01:19:14,710
 

2529
01:19:10,030 --> 01:19:17,560
 regularized boosting then the actual one

2530
01:19:14,700 --> 01:19:17,560
 

2531
01:19:14,710 --> 01:19:20,860
 norm coefficients again we effectively

2532
01:19:17,550 --> 01:19:20,860
 

2533
01:19:17,560 --> 01:19:23,280
 have a multiplicative factor of the norm

2534
01:19:20,850 --> 01:19:23,280
 

2535
01:19:20,860 --> 01:19:25,870
 the one norm of that linear combination

2536
01:19:23,270 --> 01:19:25,870
 

2537
01:19:23,280 --> 01:19:28,240
 based on the random echo complexity of

2538
01:19:25,860 --> 01:19:28,240
 

2539
01:19:25,870 --> 01:19:29,860
 the weak learners that we're using in

2540
01:19:28,230 --> 01:19:29,860
 

2541
01:19:28,240 --> 01:19:33,580
 that boosting algorithm so again a very

2542
01:19:29,850 --> 01:19:33,580
 

2543
01:19:29,860 --> 01:19:36,640
 nice motivation for using that kind of

2544
01:19:33,570 --> 01:19:36,640
 

2545
01:19:33,580 --> 01:19:39,630
 approach and for example a lasso

2546
01:19:36,630 --> 01:19:39,630
 

2547
01:19:36,640 --> 01:19:42,130
 regression would fall into that or

2548
01:19:39,620 --> 01:19:42,130
 

2549
01:19:39,630 --> 01:19:43,750
 different flavors of boosting you know

2550
01:19:42,120 --> 01:19:43,750
 

2551
01:19:42,130 --> 01:19:46,240
 linear programming boosting or one norm

2552
01:19:43,740 --> 01:19:46,240
 

2553
01:19:43,750 --> 01:19:49,300
 SVM as it's sometimes called would would

2554
01:19:46,230 --> 01:19:49,300
 

2555
01:19:46,240 --> 01:19:52,120
 fall into this class of functions so

2556
01:19:49,290 --> 01:19:52,120
 

2557
01:19:49,300 --> 01:19:55,600
 it's a very nice kind of motivator for

2558
01:19:52,110 --> 01:19:55,600
 

2559
01:19:52,120 --> 01:19:58,540
 algorithms in that sense however the

2560
01:19:55,590 --> 01:19:58,540
 

2561
01:19:55,600 --> 01:20:01,150
 actual bounds are still in themselves

2562
01:19:58,530 --> 01:20:01,150
 

2563
01:19:58,540 --> 01:20:05,470
 not very tight so you wouldn't actually

2564
01:20:01,140 --> 01:20:05,470
 

2565
01:20:01,150 --> 01:20:07,360
 be able to you know get an idea of the

2566
01:20:05,460 --> 01:20:07,360
 

2567
01:20:05,470 --> 01:20:09,790
 real performance real test performance

2568
01:20:07,350 --> 01:20:09,790
 

2569
01:20:07,360 --> 01:20:11,800
 so they motivate algorithms but I don't

2570
01:20:09,780 --> 01:20:11,800
 

2571
01:20:09,790 --> 01:20:13,480
 think we could consider them to be you

2572
01:20:11,790 --> 01:20:13,480
 

2573
01:20:11,800 --> 01:20:15,640
 know if you like giving us a real

2574
01:20:13,470 --> 01:20:15,640
 

2575
01:20:13,480 --> 01:20:19,360
 insight into the actual test performance

2576
01:20:15,630 --> 01:20:19,360
 

2577
01:20:15,640 --> 01:20:23,170
 of particular learned algorithm so what

2578
01:20:19,350 --> 01:20:23,170
 

2579
01:20:19,360 --> 01:20:28,630
 I'm going to turn to now is the Bayes

2580
01:20:23,160 --> 01:20:28,630
 

2581
01:20:23,170 --> 01:20:30,460
 approach and this will lead to what I

2582
01:20:28,620 --> 01:20:30,460
 

2583
01:20:28,630 --> 01:20:30,750
 think are some of the tightest bounds

2584
01:20:30,450 --> 01:20:30,750
 

2585
01:20:30,460 --> 01:20:32,850
 that

2586
01:20:30,740 --> 01:20:32,850
 

2587
01:20:30,750 --> 01:20:34,890
 are available but I think also

2588
01:20:32,840 --> 01:20:34,890
 

2589
01:20:32,850 --> 01:20:37,800
 illustrates some very nice other

2590
01:20:34,880 --> 01:20:37,800
 

2591
01:20:34,890 --> 01:20:39,210
 properties that link to Bayesian

2592
01:20:37,790 --> 01:20:39,210
 

2593
01:20:37,800 --> 01:20:42,330
 learning so I think it's an interesting

2594
01:20:39,200 --> 01:20:42,330
 

2595
01:20:39,210 --> 01:20:45,450
 study in itself so perhaps also just to

2596
01:20:42,320 --> 01:20:45,450
 

2597
01:20:42,330 --> 01:20:49,110
 set in context the progression is all

2598
01:20:45,440 --> 01:20:49,110
 

2599
01:20:45,450 --> 01:20:51,180
 about trying to get further away from

2600
01:20:49,100 --> 01:20:51,180
 

2601
01:20:49,110 --> 01:20:52,980
 the Union bound in the sense the Union

2602
01:20:51,170 --> 01:20:52,980
 

2603
01:20:51,180 --> 01:20:55,920
 bound is very weak we could make it work

2604
01:20:52,970 --> 01:20:55,920
 

2605
01:20:52,980 --> 01:20:57,750
 for finite when we move to infinite we

2606
01:20:55,910 --> 01:20:57,750
 

2607
01:20:55,920 --> 01:21:00,150
 had to kind of clump functions together

2608
01:20:57,740 --> 01:21:00,150
 

2609
01:20:57,750 --> 01:21:02,220
 according to their behavior on a double

2610
01:21:00,140 --> 01:21:02,220
 

2611
01:21:00,150 --> 01:21:04,740
 sample and we could get reasonable

2612
01:21:02,210 --> 01:21:04,740
 

2613
01:21:02,220 --> 01:21:06,570
 performance but we're gain significantly

2614
01:21:04,730 --> 01:21:06,570
 

2615
01:21:04,740 --> 01:21:08,490
 over counting if you like the

2616
01:21:06,560 --> 01:21:08,490
 

2617
01:21:06,570 --> 01:21:11,700
 contributions of different errors and

2618
01:21:08,480 --> 01:21:11,700
 

2619
01:21:08,490 --> 01:21:14,760
 the Rademacher complexity similarly over

2620
01:21:11,690 --> 01:21:14,760
 

2621
01:21:11,700 --> 01:21:16,860
 counts the contribution of different

2622
01:21:14,750 --> 01:21:16,860
 

2623
01:21:14,760 --> 01:21:20,070
 errors in in some sense they're being

2624
01:21:16,850 --> 01:21:20,070
 

2625
01:21:16,860 --> 01:21:22,050
 counted twice and the Pancrase framework

2626
01:21:20,060 --> 01:21:22,050
 

2627
01:21:20,070 --> 01:21:24,300
 is able to nuance that a little more

2628
01:21:22,040 --> 01:21:24,300
 

2629
01:21:22,050 --> 01:21:28,110
 tightly and I think for that reason is

2630
01:21:24,290 --> 01:21:28,110
 

2631
01:21:24,300 --> 01:21:33,290
 able to get better bounds so without

2632
01:21:28,100 --> 01:21:33,290
 

2633
01:21:28,110 --> 01:21:36,500
 further ado I'll now introduce the key

2634
01:21:33,280 --> 01:21:36,500
 

2635
01:21:33,290 --> 01:21:40,740
 definitions for the PAC Bayes approach

2636
01:21:36,490 --> 01:21:40,740
 

2637
01:21:36,500 --> 01:21:42,990
 so what one house here is a prior

2638
01:21:40,730 --> 01:21:42,990
 

2639
01:21:40,740 --> 01:21:45,030
 distribution over the functions so

2640
01:21:42,980 --> 01:21:45,030
 

2641
01:21:42,990 --> 01:21:46,200
 you're thinking now this is a separate

2642
01:21:45,020 --> 01:21:46,200
 

2643
01:21:45,030 --> 01:21:48,720
 distribution remember we've had a

2644
01:21:46,190 --> 01:21:48,720
 

2645
01:21:46,200 --> 01:21:51,600
 distribution up until now on the inputs

2646
01:21:48,710 --> 01:21:51,600
 

2647
01:21:48,720 --> 01:21:54,150
 or input/output pairs that's our data

2648
01:21:51,590 --> 01:21:54,150
 

2649
01:21:51,600 --> 01:21:55,740
 generating distribution so we park that

2650
01:21:54,140 --> 01:21:55,740
 

2651
01:21:54,150 --> 01:21:57,870
 we're going to have that we're going to

2652
01:21:55,730 --> 01:21:57,870
 

2653
01:21:55,740 --> 01:21:59,970
 continue to have that however we now

2654
01:21:57,860 --> 01:21:59,970
 

2655
01:21:57,870 --> 01:22:01,860
 have another distribution which is a

2656
01:21:59,960 --> 01:22:01,860
 

2657
01:21:59,970 --> 01:22:03,180
 distribution over the functions in fact

2658
01:22:01,850 --> 01:22:03,180
 

2659
01:22:01,860 --> 01:22:04,020
 we're going to have two distributions

2660
01:22:03,170 --> 01:22:04,020
 

2661
01:22:03,180 --> 01:22:06,150
 over the functions

2662
01:22:04,010 --> 01:22:06,150
 

2663
01:22:04,020 --> 01:22:12,210
 there's the prior which we're denoting

2664
01:22:06,140 --> 01:22:12,210
 

2665
01:22:06,150 --> 01:22:14,490
 here q0 which is some some initial

2666
01:22:12,200 --> 01:22:14,490
 

2667
01:22:12,210 --> 01:22:16,200
 estimate of the likelihood of different

2668
01:22:14,480 --> 01:22:16,200
 

2669
01:22:14,490 --> 01:22:19,230
 functions think of it in terms of the

2670
01:22:16,190 --> 01:22:19,230
 

2671
01:22:16,200 --> 01:22:20,220
 prior in Bayesian learning and then

2672
01:22:19,220 --> 01:22:20,220
 

2673
01:22:19,230 --> 01:22:23,250
 we're going to have a posterior

2674
01:22:20,210 --> 01:22:23,250
 

2675
01:22:20,220 --> 01:22:26,670
 distribution which is again a similar

2676
01:22:23,240 --> 01:22:26,670
 

2677
01:22:23,250 --> 01:22:29,490
 function over the over the sorry

2678
01:22:26,660 --> 01:22:29,490
 

2679
01:22:26,670 --> 01:22:32,280
 distribution over the functions but now

2680
01:22:29,480 --> 01:22:32,280
 

2681
01:22:29,490 --> 01:22:36,360
 can be informed by the data so the data

2682
01:22:32,270 --> 01:22:36,360
 

2683
01:22:32,280 --> 01:22:39,630
 can be used to define that posterior

2684
01:22:36,350 --> 01:22:39,630
 

2685
01:22:36,360 --> 01:22:42,860
 distribution but the data must not be

2686
01:22:39,620 --> 01:22:42,860
 

2687
01:22:39,630 --> 01:22:42,860
 used to define the prior

2688
01:22:42,930 --> 01:22:42,930
 

2689
01:22:42,940 --> 01:22:48,400
 when we make predictions we draw a

2690
01:22:45,870 --> 01:22:48,400
 

2691
01:22:45,880 --> 01:22:51,700
 function according to the posterior

2692
01:22:48,390 --> 01:22:51,700
 

2693
01:22:48,400 --> 01:22:53,350
 distribution and we predict with that

2694
01:22:51,690 --> 01:22:53,350
 

2695
01:22:51,700 --> 01:22:55,810
 function

2696
01:22:53,340 --> 01:22:55,810
 

2697
01:22:53,350 --> 01:22:58,180
 so each prediction we generate a random

2698
01:22:55,800 --> 01:22:58,180
 

2699
01:22:55,810 --> 01:23:02,530
 distribution so it's not the Bayesian

2700
01:22:58,170 --> 01:23:02,530
 

2701
01:22:58,180 --> 01:23:07,000
 average of the posterior it's a random

2702
01:23:02,520 --> 01:23:07,000
 

2703
01:23:02,530 --> 01:23:10,660
 function drawn newly each time the risk

2704
01:23:06,990 --> 01:23:10,660
 

2705
01:23:07,000 --> 01:23:12,340
 measures are the generated from the

2706
01:23:10,650 --> 01:23:12,340
 

2707
01:23:10,660 --> 01:23:14,410
 risks that we've had before which are

2708
01:23:12,330 --> 01:23:14,410
 

2709
01:23:12,340 --> 01:23:17,800
 based on an individual function but now

2710
01:23:14,400 --> 01:23:17,800
 

2711
01:23:14,410 --> 01:23:19,840
 we average that risk over the over this

2712
01:23:17,790 --> 01:23:19,840
 

2713
01:23:17,800 --> 01:23:22,870
 distribution Q so here we have the

2714
01:23:19,830 --> 01:23:22,870
 

2715
01:23:19,840 --> 01:23:26,050
 in-sample risk is the average over Q of

2716
01:23:22,860 --> 01:23:26,050
 

2717
01:23:22,870 --> 01:23:28,450
 the risk of the individual functions and

2718
01:23:26,040 --> 01:23:28,450
 

2719
01:23:26,050 --> 01:23:31,410
 the out-of-sample is a similar average

2720
01:23:28,440 --> 01:23:31,410
 

2721
01:23:28,450 --> 01:23:34,920
 over Q again this is all posterior

2722
01:23:31,400 --> 01:23:34,920
 

2723
01:23:31,410 --> 01:23:37,300
 distributions of the out-of-sample risk

2724
01:23:34,910 --> 01:23:37,300
 

2725
01:23:34,920 --> 01:23:40,480
 you might be wondering how we're going

2726
01:23:37,290 --> 01:23:40,480
 

2727
01:23:37,300 --> 01:23:44,020
 to handle this but it it works out quite

2728
01:23:40,470 --> 01:23:44,020
 

2729
01:23:40,480 --> 01:23:46,240
 nicely in the end at least for the case

2730
01:23:44,010 --> 01:23:46,240
 

2731
01:23:44,020 --> 01:23:49,330
 of linear function classes such as the

2732
01:23:46,230 --> 01:23:49,330
 

2733
01:23:46,240 --> 01:23:51,880
 SVM so here's a typical packed Bayes

2734
01:23:49,320 --> 01:23:51,880
 

2735
01:23:49,330 --> 01:23:54,910
 bound this is not now applied in any way

2736
01:23:51,870 --> 01:23:54,910
 

2737
01:23:51,880 --> 01:23:58,860
 this is just the sort of vanilla bound

2738
01:23:54,900 --> 01:23:58,860
 

2739
01:23:54,910 --> 01:24:02,050
 so if we fix Q 0 so we fix our prior

2740
01:23:58,850 --> 01:24:02,050
 

2741
01:23:58,860 --> 01:24:05,760
 before we see any data then we generate

2742
01:24:02,040 --> 01:24:05,760
 

2743
01:24:02,050 --> 01:24:10,240
 some random data of size sample size M

2744
01:24:05,750 --> 01:24:10,240
 

2745
01:24:05,760 --> 01:24:12,160
 for any Delta greater between 0 and 1

2746
01:24:10,230 --> 01:24:12,160
 

2747
01:24:10,240 --> 01:24:14,680
 with high probability greater than 1

2748
01:24:12,150 --> 01:24:14,680
 

2749
01:24:12,160 --> 01:24:17,560
 minus Delta for all posterior

2750
01:24:14,670 --> 01:24:17,560
 

2751
01:24:14,680 --> 01:24:20,710
 distributions the KL divergence between

2752
01:24:17,550 --> 01:24:20,710
 

2753
01:24:17,560 --> 01:24:22,480
 the in-sample error now here you're

2754
01:24:20,700 --> 01:24:22,480
 

2755
01:24:20,710 --> 01:24:25,090
 saying how can we do ok although it's

2756
01:24:22,470 --> 01:24:25,090
 

2757
01:24:22,480 --> 01:24:28,090
 think of the error as being a

2758
01:24:25,080 --> 01:24:28,090
 

2759
01:24:25,090 --> 01:24:30,970
 distribution on getting it right getting

2760
01:24:28,080 --> 01:24:30,970
 

2761
01:24:28,090 --> 01:24:33,730
 it wrong so it's a binomial distribution

2762
01:24:30,960 --> 01:24:33,730
 

2763
01:24:30,970 --> 01:24:35,110
 and we're thinking of the KL divergence

2764
01:24:33,720 --> 01:24:35,110
 

2765
01:24:33,730 --> 01:24:37,390
 between the binomial distribution

2766
01:24:35,100 --> 01:24:37,390
 

2767
01:24:35,110 --> 01:24:39,670
 associated with the error rate of the

2768
01:24:37,380 --> 01:24:39,670
 

2769
01:24:37,390 --> 01:24:41,920
 in-sample error rates and the

2770
01:24:39,660 --> 01:24:41,920
 

2771
01:24:39,670 --> 01:24:43,920
 out-of-sample error rate so this is a KL

2772
01:24:41,910 --> 01:24:43,920
 

2773
01:24:41,920 --> 01:24:46,690
 divergence between those two

2774
01:24:43,910 --> 01:24:46,690
 

2775
01:24:43,920 --> 01:24:48,370
 distributions binomial distributions

2776
01:24:46,680 --> 01:24:48,370
 

2777
01:24:46,690 --> 01:24:50,220
 it's less than or equal to the KL

2778
01:24:48,360 --> 01:24:50,220
 

2779
01:24:48,370 --> 01:24:54,160
 divergence between

2780
01:24:50,210 --> 01:24:54,160
 

2781
01:24:50,220 --> 01:24:55,450
 posterior and prior plus log n plus 1

2782
01:24:54,150 --> 01:24:55,450
 

2783
01:24:54,160 --> 01:24:58,210
 over Delta divided

2784
01:24:55,440 --> 01:24:58,210
 

2785
01:24:55,450 --> 01:25:03,490
 by EM okay so that's the form of the

2786
01:24:58,200 --> 01:25:03,490
 

2787
01:24:58,210 --> 01:25:08,530
 bound and let's now think about applying

2788
01:25:03,480 --> 01:25:08,530
 

2789
01:25:03,490 --> 01:25:12,850
 it to SVM's so we think of the weight

2790
01:25:08,520 --> 01:25:12,850
 

2791
01:25:08,530 --> 01:25:15,730
 vector output by an SVM first normalize

2792
01:25:12,840 --> 01:25:15,730
 

2793
01:25:12,850 --> 01:25:18,310
 it so divided by its norm to make a unit

2794
01:25:15,720 --> 01:25:18,310
 

2795
01:25:15,730 --> 01:25:21,370
 vector in the same direction we're

2796
01:25:18,300 --> 01:25:21,370
 

2797
01:25:18,310 --> 01:25:24,960
 thinking here SVM's without a threshold

2798
01:25:21,360 --> 01:25:24,960
 

2799
01:25:21,370 --> 01:25:28,690
 so think of a threshold zero SVM's

2800
01:25:24,950 --> 01:25:28,690
 

2801
01:25:24,960 --> 01:25:31,180
 then for any M and any Delta with high

2802
01:25:28,680 --> 01:25:31,180
 

2803
01:25:28,690 --> 01:25:37,540
 probability the KL divergence between

2804
01:25:31,170 --> 01:25:37,540
 

2805
01:25:31,180 --> 01:25:39,700
 this in-sample error distribution right

2806
01:25:37,530 --> 01:25:39,700
 

2807
01:25:37,540 --> 01:25:41,920
 wrong distribution and out-of-sample

2808
01:25:39,690 --> 01:25:41,920
 

2809
01:25:39,700 --> 01:25:46,840
 right wrong distribution is less than or

2810
01:25:41,910 --> 01:25:46,840
 

2811
01:25:41,920 --> 01:25:52,060
 equal to 1/2 mu squared plus log n plus

2812
01:25:46,830 --> 01:25:52,060
 

2813
01:25:46,840 --> 01:25:55,450
 1 sorry what's happened okay

2814
01:25:52,050 --> 01:25:55,450
 

2815
01:25:52,060 --> 01:25:58,030
 log n plus 1 over Delta divided by M now

2816
01:25:55,440 --> 01:25:58,030
 

2817
01:25:55,450 --> 01:26:00,400
 mu here I haven't shown you what it is

2818
01:25:58,020 --> 01:26:00,400
 

2819
01:25:58,030 --> 01:26:04,660
 but what we're doing is scaling that

2820
01:26:00,390 --> 01:26:04,660
 

2821
01:26:00,400 --> 01:26:06,130
 unit vector by mu okay so we're scaling

2822
01:26:04,650 --> 01:26:06,130
 

2823
01:26:04,660 --> 01:26:09,460
 that up

2824
01:26:06,120 --> 01:26:09,460
 

2825
01:26:06,130 --> 01:26:11,410
 and we're allowing that as a variable

2826
01:26:09,450 --> 01:26:11,410
 

2827
01:26:09,460 --> 01:26:14,260
 here for the time being so this will

2828
01:26:11,400 --> 01:26:14,260
 

2829
01:26:11,410 --> 01:26:16,240
 hold for all mu because where mu will

2830
01:26:14,250 --> 01:26:16,240
 

2831
01:26:14,260 --> 01:26:21,730
 just be choosing our posterior

2832
01:26:16,230 --> 01:26:21,730
 

2833
01:26:16,240 --> 01:26:27,010
 distribution so the the idea is that the

2834
01:26:21,720 --> 01:26:27,010
 

2835
01:26:21,730 --> 01:26:29,590
 prior here is chosen to be a Gaussian

2836
01:26:27,000 --> 01:26:29,590
 

2837
01:26:27,010 --> 01:26:31,750
 distribution will at the origin with

2838
01:26:29,580 --> 01:26:31,750
 

2839
01:26:29,590 --> 01:26:37,210
 covariance equal to the identity and the

2840
01:26:31,740 --> 01:26:37,210
 

2841
01:26:31,750 --> 01:26:38,980
 cost area is as I said mu times the unit

2842
01:26:37,200 --> 01:26:38,980
 

2843
01:26:37,210 --> 01:26:43,030
 vector output by the algorithm so a

2844
01:26:38,970 --> 01:26:43,030
 

2845
01:26:38,980 --> 01:26:44,770
 scaling of the weight vector again with

2846
01:26:43,020 --> 01:26:44,770
 

2847
01:26:43,030 --> 01:26:47,530
 unit variance so now the KL divergence

2848
01:26:44,760 --> 01:26:47,530
 

2849
01:26:44,770 --> 01:26:50,920
 between posterior and prior is just 1/2

2850
01:26:47,520 --> 01:26:50,920
 

2851
01:26:47,530 --> 01:26:54,580
 mu squared the KL divergence between two

2852
01:26:50,910 --> 01:26:54,580
 

2853
01:26:50,920 --> 01:26:57,760
 gaussians and here we have actually a

2854
01:26:54,570 --> 01:26:57,760
 

2855
01:26:54,580 --> 01:27:00,240
 way of computing the care of this KL

2856
01:26:57,750 --> 01:27:00,240
 

2857
01:26:57,760 --> 01:27:03,840
 divergence between the computing the

2858
01:27:00,230 --> 01:27:03,840
 

2859
01:27:00,240 --> 01:27:07,630
 in-sample error which is basically the

2860
01:27:03,830 --> 01:27:07,630
 

2861
01:27:03,840 --> 01:27:08,950
 expected value of the 1 minus the

2862
01:27:07,620 --> 01:27:08,950
 

2863
01:27:07,630 --> 01:27:11,920
 cumulative normal district

2864
01:27:08,940 --> 01:27:11,920
 

2865
01:27:08,950 --> 01:27:15,460
 fusion of MU times the margin of that

2866
01:27:11,910 --> 01:27:15,460
 

2867
01:27:11,920 --> 01:27:19,570
 point so this is the empirical average

2868
01:27:15,450 --> 01:27:19,570
 

2869
01:27:15,460 --> 01:27:22,620
 of this sort of function of the margin

2870
01:27:19,560 --> 01:27:22,620
 

2871
01:27:19,570 --> 01:27:25,360
 so it's sort of saying as the number of

2872
01:27:22,610 --> 01:27:25,360
 

2873
01:27:22,620 --> 01:27:28,180
 you know margin errors or margin

2874
01:27:25,350 --> 01:27:28,180
 

2875
01:27:25,360 --> 01:27:31,090
 quantities goes down we're actually

2876
01:27:28,170 --> 01:27:31,090
 

2877
01:27:28,180 --> 01:27:34,150
 going to get a very low empirical error

2878
01:27:31,080 --> 01:27:34,150
 

2879
01:27:31,090 --> 01:27:37,060
 so it's a very close to our Chi I that

2880
01:27:34,140 --> 01:27:37,060
 

2881
01:27:34,150 --> 01:27:39,130
 we had in the in the Rademacher bound

2882
01:27:37,050 --> 01:27:39,130
 

2883
01:27:37,060 --> 01:27:43,720
 but it's a it's a slightly more nuanced

2884
01:27:39,120 --> 01:27:43,720
 

2885
01:27:39,130 --> 01:27:45,130
 version of that so this is the the

2886
01:27:43,710 --> 01:27:45,130
 

2887
01:27:43,720 --> 01:27:46,720
 empirical error we can measure it

2888
01:27:45,120 --> 01:27:46,720
 

2889
01:27:45,130 --> 01:27:49,870
 exactly with this particular

2890
01:27:46,710 --> 01:27:49,870
 

2891
01:27:46,720 --> 01:27:52,600
 distribution and finally the SVM

2892
01:27:49,860 --> 01:27:52,600
 

2893
01:27:49,870 --> 01:27:55,510
 generalization error is twice the

2894
01:27:52,590 --> 01:27:55,510
 

2895
01:27:52,600 --> 01:27:59,080
 minimum over mu of this corresponding

2896
01:27:55,500 --> 01:27:59,080
 

2897
01:27:55,510 --> 01:28:02,020
 bound on our the empirical average of

2898
01:27:59,070 --> 01:28:02,020
 

2899
01:27:59,080 --> 01:28:03,700
 this commute so you know quite a few

2900
01:28:02,010 --> 01:28:03,700
 

2901
01:28:02,020 --> 01:28:05,620
 things are happening here sorry if it's

2902
01:28:03,690 --> 01:28:05,620
 

2903
01:28:03,700 --> 01:28:08,620
 being a bit confusing but what's

2904
01:28:05,610 --> 01:28:08,620
 

2905
01:28:05,620 --> 01:28:10,840
 actually surprising is two point despite

2906
01:28:08,610 --> 01:28:10,840
 

2907
01:28:08,620 --> 01:28:13,690
 the fact that this Q mu is some sort of

2908
01:28:10,830 --> 01:28:13,690
 

2909
01:28:10,840 --> 01:28:15,430
 you know random classification we're

2910
01:28:13,680 --> 01:28:15,430
 

2911
01:28:13,690 --> 01:28:17,560
 actually able to bound the the

2912
01:28:15,420 --> 01:28:17,560
 

2913
01:28:15,430 --> 01:28:20,680
 deterministic classification we sort of

2914
01:28:17,550 --> 01:28:20,680
 

2915
01:28:17,560 --> 01:28:23,970
 D randomized this with this factor two

2916
01:28:20,670 --> 01:28:23,970
 

2917
01:28:20,680 --> 01:28:26,770
 to the general SVM generalization error

2918
01:28:23,960 --> 01:28:26,770
 

2919
01:28:23,970 --> 01:28:31,240
 so it's basically quite a simple

2920
01:28:26,760 --> 01:28:31,240
 

2921
01:28:26,770 --> 01:28:33,850
 argument that if this thing it makes if

2922
01:28:31,230 --> 01:28:33,850
 

2923
01:28:31,240 --> 01:28:36,190
 the SVM is wrong then the probability of

2924
01:28:33,840 --> 01:28:36,190
 

2925
01:28:33,850 --> 01:28:39,310
 this thing making an error is at least a

2926
01:28:36,180 --> 01:28:39,310
 

2927
01:28:36,190 --> 01:28:42,070
 half so we're actually able to to get

2928
01:28:39,300 --> 01:28:42,070
 

2929
01:28:39,310 --> 01:28:45,250
 that kind of bound so putting this all

2930
01:28:42,060 --> 01:28:45,250
 

2931
01:28:42,070 --> 01:28:47,920
 together what we have is effectively a

2932
01:28:45,240 --> 01:28:47,920
 

2933
01:28:45,250 --> 01:28:50,230
 bound on this which is implied by this K

2934
01:28:47,910 --> 01:28:50,230
 

2935
01:28:47,920 --> 01:28:53,350
 although vergence you can sort of invert

2936
01:28:50,220 --> 01:28:53,350
 

2937
01:28:50,230 --> 01:28:55,930
 that based on the empirical error which

2938
01:28:53,340 --> 01:28:55,930
 

2939
01:28:53,350 --> 01:28:59,140
 is computed by this empirical average

2940
01:28:55,920 --> 01:28:59,140
 

2941
01:28:55,930 --> 01:29:00,730
 and this quantity here which is based on

2942
01:28:59,130 --> 01:29:00,730
 

2943
01:28:59,140 --> 01:29:03,790
 so there's a trade-off between the

2944
01:29:00,720 --> 01:29:03,790
 

2945
01:29:00,730 --> 01:29:05,800
 scaling which will decrease this error

2946
01:29:03,780 --> 01:29:05,800
 

2947
01:29:03,790 --> 01:29:08,440
 but as we sort of scale up the margin

2948
01:29:05,790 --> 01:29:08,440
 

2949
01:29:05,800 --> 01:29:11,980
 but increase the cost associated with

2950
01:29:08,430 --> 01:29:11,980
 

2951
01:29:08,440 --> 01:29:16,320
 this KL divergence term and but we can

2952
01:29:11,970 --> 01:29:16,320
 

2953
01:29:11,980 --> 01:29:16,320
 choose mu because of the fact that the

2954
01:29:17,700 --> 01:29:17,700
 

2955
01:29:17,710 --> 01:29:23,840
 the thing holds for all posterior

2956
01:29:21,130 --> 01:29:23,840
 

2957
01:29:21,140 --> 01:29:27,830
 distributions we can choose Mew to

2958
01:29:23,830 --> 01:29:27,830
 

2959
01:29:23,840 --> 01:29:31,190
 optimize this bound okay so that's

2960
01:29:27,820 --> 01:29:31,190
 

2961
01:29:27,830 --> 01:29:34,070
 that's the form of the bound and I just

2962
01:29:31,180 --> 01:29:34,070
 

2963
01:29:31,190 --> 01:29:37,640
 wanted to show you some performances

2964
01:29:34,060 --> 01:29:37,640
 

2965
01:29:34,070 --> 01:29:39,200
 that we've obtained on some simple data

2966
01:29:37,630 --> 01:29:39,200
 

2967
01:29:37,640 --> 01:29:40,820
 sets but just to give you a flavor of

2968
01:29:39,190 --> 01:29:40,820
 

2969
01:29:39,200 --> 01:29:46,340
 the quality of the bound that you can

2970
01:29:40,810 --> 01:29:46,340
 

2971
01:29:40,820 --> 01:29:48,170
 get so these are the data sets and what

2972
01:29:46,330 --> 01:29:48,170
 

2973
01:29:46,340 --> 01:29:50,420
 we've applied here is two fold cross

2974
01:29:48,160 --> 01:29:50,420
 

2975
01:29:48,170 --> 01:29:52,610
 validation 10-fold cross-validation

2976
01:29:50,410 --> 01:29:52,610
 

2977
01:29:50,420 --> 01:29:55,100
 so we're also using the bound here to do

2978
01:29:52,600 --> 01:29:55,100
 

2979
01:29:52,610 --> 01:29:56,900
 model selection so these are the this is

2980
01:29:55,090 --> 01:29:56,900
 

2981
01:29:55,100 --> 01:30:01,220
 the PAC Bayes bound that I've just shown

2982
01:29:56,890 --> 01:30:01,220
 

2983
01:29:56,900 --> 01:30:03,860
 you and this is the actual bound that

2984
01:30:01,210 --> 01:30:03,860
 

2985
01:30:01,220 --> 01:30:06,110
 you get and this is the performance you

2986
01:30:03,850 --> 01:30:06,110
 

2987
01:30:03,860 --> 01:30:09,710
 get when you do model selection over the

2988
01:30:06,100 --> 01:30:09,710
 

2989
01:30:06,110 --> 01:30:13,130
 kernel width and the C parameter the

2990
01:30:09,700 --> 01:30:13,130
 

2991
01:30:09,710 --> 01:30:15,050
 regularization parameter the performance

2992
01:30:13,120 --> 01:30:15,050
 

2993
01:30:13,130 --> 01:30:17,990
 that you get compared to the performance

2994
01:30:15,040 --> 01:30:17,990
 

2995
01:30:15,050 --> 01:30:21,020
 you would get using ten fold or to fold

2996
01:30:17,980 --> 01:30:21,020
 

2997
01:30:17,990 --> 01:30:23,710
 cross validation so first I think the

2998
01:30:21,010 --> 01:30:23,710
 

2999
01:30:21,020 --> 01:30:26,650
 you know several things to observe

3000
01:30:23,700 --> 01:30:26,650
 

3001
01:30:23,710 --> 01:30:29,630
 firstly it the model selection is doing

3002
01:30:26,640 --> 01:30:29,630
 

3003
01:30:26,650 --> 01:30:32,540
 in many cases as well or even better

3004
01:30:29,620 --> 01:30:32,540
 

3005
01:30:29,630 --> 01:30:35,930
 than 10-fold cross-validation

3006
01:30:32,530 --> 01:30:35,930
 

3007
01:30:32,540 --> 01:30:38,420
 so here's slightly better here not in

3008
01:30:35,920 --> 01:30:38,420
 

3009
01:30:35,930 --> 01:30:40,310
 significantly better this case very

3010
01:30:38,410 --> 01:30:40,310
 

3011
01:30:38,420 --> 01:30:42,350
 slightly worse and this case very

3012
01:30:40,300 --> 01:30:42,350
 

3013
01:30:40,310 --> 01:30:44,150
 slightly worse but certainly very

3014
01:30:42,340 --> 01:30:44,150
 

3015
01:30:42,350 --> 01:30:47,120
 competitive across all of these data

3016
01:30:44,140 --> 01:30:47,120
 

3017
01:30:44,150 --> 01:30:50,630
 sets I say they're all quite small data

3018
01:30:47,110 --> 01:30:50,630
 

3019
01:30:47,120 --> 01:30:53,870
 sets they're all you know standard you

3020
01:30:50,620 --> 01:30:53,870
 

3021
01:30:50,630 --> 01:30:55,730
 know UCI data sets but the other thing

3022
01:30:53,860 --> 01:30:55,730
 

3023
01:30:53,870 --> 01:30:58,550
 to observe is actually that the bounds

3024
01:30:55,720 --> 01:30:58,550
 

3025
01:30:55,730 --> 01:31:00,830
 are surprisingly well maybe not for you

3026
01:30:58,540 --> 01:31:00,830
 

3027
01:30:58,550 --> 01:31:04,490
 but for me it's surprisingly tight and

3028
01:31:00,820 --> 01:31:04,490
 

3029
01:31:00,830 --> 01:31:06,650
 realistic so in this case you know this

3030
01:31:04,480 --> 01:31:06,650
 

3031
01:31:04,490 --> 01:31:10,400
 is just a factor less than a factor of

3032
01:31:06,640 --> 01:31:10,400
 

3033
01:31:06,650 --> 01:31:11,930
 three two and a half roughly here less

3034
01:31:10,390 --> 01:31:11,930
 

3035
01:31:10,400 --> 01:31:14,780
 than two even here

3036
01:31:11,920 --> 01:31:14,780
 

3037
01:31:11,930 --> 01:31:18,650
 maybe bigger but you know a factor of

3038
01:31:14,770 --> 01:31:18,650
 

3039
01:31:14,780 --> 01:31:23,600
 ten or so and here as you see you know

3040
01:31:18,640 --> 01:31:23,600
 

3041
01:31:18,650 --> 01:31:26,449
 a reasonably small factor so these are

3042
01:31:23,590 --> 01:31:26,449
 

3043
01:31:23,600 --> 01:31:29,620
 you know real bounds that are actually

3044
01:31:26,439 --> 01:31:29,620
 

3045
01:31:26,449 --> 01:31:32,179
 delivered by this procedure and they're

3046
01:31:29,610 --> 01:31:32,179
 

3047
01:31:29,620 --> 01:31:35,300
 reasonably tonight and they are driving

3048
01:31:32,169 --> 01:31:35,300
 

3049
01:31:32,179 --> 01:31:37,489
 reasonably good model selection so I

3050
01:31:35,290 --> 01:31:37,489
 

3051
01:31:35,300 --> 01:31:40,540
 think you know there's this idea that I

3052
01:31:37,479 --> 01:31:40,540
 

3053
01:31:37,489 --> 01:31:44,000
 just want to sort of break this kind of

3054
01:31:40,530 --> 01:31:44,000
 

3055
01:31:40,540 --> 01:31:46,010
 idea that s LTS statistical learning

3056
01:31:43,990 --> 01:31:46,010
 

3057
01:31:44,000 --> 01:31:48,170
 theory is doing worst-case bounds and

3058
01:31:46,000 --> 01:31:48,170
 

3059
01:31:46,010 --> 01:31:50,840
 they're not practical in this case

3060
01:31:48,160 --> 01:31:50,840
 

3061
01:31:48,170 --> 01:31:52,880
 they're certainly you know are practical

3062
01:31:50,830 --> 01:31:52,880
 

3063
01:31:50,840 --> 01:31:55,159
 and they are delivering reasonable

3064
01:31:52,870 --> 01:31:55,159
 

3065
01:31:52,880 --> 01:31:58,550
 performance I also just want to

3066
01:31:55,149 --> 01:31:58,550
 

3067
01:31:55,159 --> 01:32:01,340
 highlight this is I won't I haven't you

3068
01:31:58,540 --> 01:32:01,340
 

3069
01:31:58,550 --> 01:32:02,780
 know done the showing you what is

3070
01:32:01,330 --> 01:32:02,780
 

3071
01:32:01,340 --> 01:32:05,929
 involved here but this is basically

3072
01:32:02,770 --> 01:32:05,929
 

3073
01:32:02,780 --> 01:32:09,140
 using half the training data to learn a

3074
01:32:05,919 --> 01:32:09,140
 

3075
01:32:05,929 --> 01:32:11,780
 prior and then using that in terms of

3076
01:32:09,130 --> 01:32:11,780
 

3077
01:32:09,140 --> 01:32:16,760
 the KL divergence and so you can

3078
01:32:11,770 --> 01:32:16,760
 

3079
01:32:11,780 --> 01:32:18,650
 actually get tighter bounds and in every

3080
01:32:16,750 --> 01:32:18,650
 

3081
01:32:16,760 --> 01:32:22,659
 case you know quite significantly

3082
01:32:18,640 --> 01:32:22,659
 

3083
01:32:18,650 --> 01:32:25,790
 tighter bounds but interestingly the

3084
01:32:22,649 --> 01:32:25,790
 

3085
01:32:22,659 --> 01:32:28,250
 model selection performs less well in

3086
01:32:25,780 --> 01:32:28,250
 

3087
01:32:25,790 --> 01:32:30,679
 most cases okay in this case it's

3088
01:32:28,240 --> 01:32:30,679
 

3089
01:32:28,250 --> 01:32:33,230
 similar in this case it's quite a bit

3090
01:32:30,669 --> 01:32:33,230
 

3091
01:32:30,679 --> 01:32:34,840
 worse in this case it's slightly worse

3092
01:32:33,220 --> 01:32:34,840
 

3093
01:32:33,230 --> 01:32:37,429
 and in this case it's quite a bit worse

3094
01:32:34,830 --> 01:32:37,429
 

3095
01:32:34,840 --> 01:32:40,719
 you can actually then turn that into an

3096
01:32:37,419 --> 01:32:40,719
 

3097
01:32:37,429 --> 01:32:44,179
 algorithm which actually optimizes this

3098
01:32:40,709 --> 01:32:44,179
 

3099
01:32:40,719 --> 01:32:46,100
 bound that would be given by the sort of

3100
01:32:44,169 --> 01:32:46,100
 

3101
01:32:44,179 --> 01:32:48,380
 KL divergence between the prior now

3102
01:32:46,090 --> 01:32:48,380
 

3103
01:32:46,100 --> 01:32:50,600
 nuance prior that you've learned from

3104
01:32:48,370 --> 01:32:50,600
 

3105
01:32:48,380 --> 01:32:52,159
 half the data you're still using all the

3106
01:32:50,590 --> 01:32:52,159
 

3107
01:32:50,600 --> 01:32:54,409
 data for training but you're just using

3108
01:32:52,149 --> 01:32:54,409
 

3109
01:32:52,159 --> 01:32:56,449
 them half the data for the prior and

3110
01:32:54,399 --> 01:32:56,449
 

3111
01:32:54,409 --> 01:32:58,159
 therefore you you're bound only depends

3112
01:32:56,439 --> 01:32:58,159
 

3113
01:32:56,449 --> 01:33:03,050
 on the second half of the data but

3114
01:32:58,149 --> 01:33:03,050
 

3115
01:32:58,159 --> 01:33:04,550
 you're still coentrÃ£o and this is these

3116
01:33:03,040 --> 01:33:04,550
 

3117
01:33:03,050 --> 01:33:06,530
 are two versions but if I look at this

3118
01:33:04,540 --> 01:33:06,530
 

3119
01:33:04,550 --> 01:33:10,070
 one look at the tightness of these

3120
01:33:06,520 --> 01:33:10,070
 

3121
01:33:06,530 --> 01:33:12,140
 bounds point zero four seven okay a game

3122
01:33:10,060 --> 01:33:12,140
 

3123
01:33:10,070 --> 01:33:15,140
 model selection is not performing well

3124
01:33:12,130 --> 01:33:15,140
 

3125
01:33:12,140 --> 01:33:17,900
 in this case it it is happens to be the

3126
01:33:15,130 --> 01:33:17,900
 

3127
01:33:15,140 --> 01:33:20,510
 best but the actual bounds are really

3128
01:33:17,890 --> 01:33:20,510
 

3129
01:33:17,900 --> 01:33:24,020
 significantly tighter in many cases than

3130
01:33:20,500 --> 01:33:24,020
 

3131
01:33:20,510 --> 01:33:25,699
 say this one so it's kind of ironic that

3132
01:33:24,010 --> 01:33:25,699
 

3133
01:33:24,020 --> 01:33:28,560
 you're getting better bounds but worse

3134
01:33:25,689 --> 01:33:28,560
 

3135
01:33:25,699 --> 01:33:30,450
 model selection anyway just

3136
01:33:28,550 --> 01:33:30,450
 

3137
01:33:28,560 --> 01:33:32,490
 I think the main take-home message I

3138
01:33:30,440 --> 01:33:32,490
 

3139
01:33:30,450 --> 01:33:35,070
 wanted to say is the bounds are

3140
01:33:32,480 --> 01:33:35,070
 

3141
01:33:32,490 --> 01:33:36,750
 surprisingly tight they can drive model

3142
01:33:35,060 --> 01:33:36,750
 

3143
01:33:35,070 --> 01:33:38,430
 selection but there's not always a good

3144
01:33:36,740 --> 01:33:38,430
 

3145
01:33:36,750 --> 01:33:41,760
 correlation between the tightness and

3146
01:33:38,420 --> 01:33:41,760
 

3147
01:33:38,430 --> 01:33:44,430
 the quality of the model selection okay

3148
01:33:41,750 --> 01:33:44,430
 

3149
01:33:41,760 --> 01:33:46,950
 so I'm now going to move on to say just

3150
01:33:44,420 --> 01:33:46,950
 

3151
01:33:44,430 --> 01:33:49,440
 a few points about the relationship

3152
01:33:46,940 --> 01:33:49,440
 

3153
01:33:46,950 --> 01:33:52,380
 between plant based bounds and Bayesian

3154
01:33:49,430 --> 01:33:52,380
 

3155
01:33:49,440 --> 01:33:56,130
 learning which will also inform some of

3156
01:33:52,370 --> 01:33:56,130
 

3157
01:33:52,380 --> 01:34:01,040
 the work that we presented later so

3158
01:33:56,120 --> 01:34:01,040
 

3159
01:33:56,130 --> 01:34:07,410
 firstly let's think about the prior so

3160
01:34:01,030 --> 01:34:07,410
 

3161
01:34:01,040 --> 01:34:08,880
 impact Bayes bounds we don't we didn't

3162
01:34:07,400 --> 01:34:08,880
 

3163
01:34:07,410 --> 01:34:10,470
 have to say anything about the prior

3164
01:34:08,870 --> 01:34:10,470
 

3165
01:34:08,880 --> 01:34:12,660
 being true or not

3166
01:34:10,460 --> 01:34:12,660
 

3167
01:34:10,470 --> 01:34:14,670
 there wasn't any sense in which all we

3168
01:34:12,650 --> 01:34:14,670
 

3169
01:34:12,660 --> 01:34:17,790
 had to know was the bound the prior was

3170
01:34:14,660 --> 01:34:17,790
 

3171
01:34:14,670 --> 01:34:20,460
 defined before we saw the data so the

3172
01:34:17,780 --> 01:34:20,460
 

3173
01:34:17,790 --> 01:34:25,500
 bound still hold even if the prior is

3174
01:34:20,450 --> 01:34:25,500
 

3175
01:34:20,460 --> 01:34:27,840
 wrong obviously there must be some cost

3176
01:34:25,490 --> 01:34:27,840
 

3177
01:34:25,500 --> 01:34:29,640
 and the cost will be simply in the

3178
01:34:27,830 --> 01:34:29,640
 

3179
01:34:27,840 --> 01:34:32,580
 quality of the bound that is given if

3180
01:34:29,630 --> 01:34:32,580
 

3181
01:34:29,640 --> 01:34:36,630
 you pick a very poor quiet prior you may

3182
01:34:32,570 --> 01:34:36,630
 

3183
01:34:32,580 --> 01:34:39,120
 get weaker bounds as a result I contrast

3184
01:34:36,620 --> 01:34:39,120
 

3185
01:34:36,630 --> 01:34:41,910
 that with the Bayesian inference in

3186
01:34:39,110 --> 01:34:41,910
 

3187
01:34:39,120 --> 01:34:45,420
 which there's some difficulty I mean

3188
01:34:41,900 --> 01:34:45,420
 

3189
01:34:41,910 --> 01:34:47,820
 that you would need to assume that the

3190
01:34:45,410 --> 01:34:47,820
 

3191
01:34:45,420 --> 01:34:49,410
 prior is correct in order to have some

3192
01:34:47,810 --> 01:34:49,410
 

3193
01:34:47,820 --> 01:34:51,810
 confidence that the inference was

3194
01:34:49,400 --> 01:34:51,810
 

3195
01:34:49,410 --> 01:34:55,320
 correct so there's a sort of difference

3196
01:34:51,800 --> 01:34:55,320
 

3197
01:34:51,810 --> 01:34:59,160
 in quality of what's going on here in

3198
01:34:55,310 --> 01:34:59,160
 

3199
01:34:55,320 --> 01:35:02,250
 relation to the prior if we look at the

3200
01:34:59,150 --> 01:35:02,250
 

3201
01:34:59,160 --> 01:35:04,620
 posterior distribution again there's a

3202
01:35:02,240 --> 01:35:04,620
 

3203
01:35:02,250 --> 01:35:06,990
 difference the bank based bounds as I've

3204
01:35:04,610 --> 01:35:06,990
 

3205
01:35:04,620 --> 01:35:10,370
 emphasized they hold for all posterior

3206
01:35:06,980 --> 01:35:10,370
 

3207
01:35:06,990 --> 01:35:13,380
 distributions but Bayesian inference

3208
01:35:10,360 --> 01:35:13,380
 

3209
01:35:10,370 --> 01:35:16,100
 would like you to I mean it can only

3210
01:35:13,370 --> 01:35:16,100
 

3211
01:35:13,380 --> 01:35:18,510
 really say anything about using the

3212
01:35:16,090 --> 01:35:18,510
 

3213
01:35:16,100 --> 01:35:20,370
 posterior computed by Bayesian inference

3214
01:35:18,500 --> 01:35:20,370
 

3215
01:35:18,510 --> 01:35:24,630
 of course there are approximations and

3216
01:35:20,360 --> 01:35:24,630
 

3217
01:35:20,370 --> 01:35:26,550
 those are nice to do and work in

3218
01:35:24,620 --> 01:35:26,550
 

3219
01:35:24,630 --> 01:35:28,680
 practice but there isn't there's a

3220
01:35:26,540 --> 01:35:28,680
 

3221
01:35:26,550 --> 01:35:30,270
 little bit of a mismatch between what

3222
01:35:28,670 --> 01:35:30,270
 

3223
01:35:28,680 --> 01:35:32,100
 the theory is saying you should do and

3224
01:35:30,260 --> 01:35:32,100
 

3225
01:35:30,270 --> 01:35:34,290
 what you're actually doing but in the

3226
01:35:32,090 --> 01:35:34,290
 

3227
01:35:32,100 --> 01:35:35,910
 PAC Bayes Kent's that's no problem you

3228
01:35:34,280 --> 01:35:35,910
 

3229
01:35:34,290 --> 01:35:38,130
 can use whatever posterior you want

3230
01:35:35,900 --> 01:35:38,130
 

3231
01:35:35,910 --> 01:35:41,270
 again it will simply affect the quality

3232
01:35:38,120 --> 01:35:41,270
 

3233
01:35:38,130 --> 01:35:44,090
 of the bound

3234
01:35:41,260 --> 01:35:44,090
 

3235
01:35:41,270 --> 01:35:46,160
 but what you trade here is ability to

3236
01:35:44,080 --> 01:35:46,160
 

3237
01:35:44,090 --> 01:35:48,770
 compute the bound with the quality of

3238
01:35:46,150 --> 01:35:48,770
 

3239
01:35:46,160 --> 01:35:54,340
 the bound and finally the data

3240
01:35:48,760 --> 01:35:54,340
 

3241
01:35:48,770 --> 01:35:56,570
 distribution the bank based bounds

3242
01:35:54,330 --> 01:35:56,570
 

3243
01:35:54,340 --> 01:35:58,130
 there's a kind of twist here that's

3244
01:35:56,560 --> 01:35:58,130
 

3245
01:35:56,570 --> 01:36:02,000
 quite interesting and we'll come back to

3246
01:35:58,120 --> 01:36:02,000
 

3247
01:35:58,130 --> 01:36:04,130
 in that the the prior can be you can be

3248
01:36:01,990 --> 01:36:04,130
 

3249
01:36:02,000 --> 01:36:07,220
 defined in terms of the data generating

3250
01:36:04,120 --> 01:36:07,220
 

3251
01:36:04,130 --> 01:36:12,980
 distribution provided it doesn't depend

3252
01:36:07,210 --> 01:36:12,980
 

3253
01:36:07,220 --> 01:36:14,930
 on the particular sample so and you

3254
01:36:12,970 --> 01:36:14,930
 

3255
01:36:12,980 --> 01:36:16,220
 might say well wait a minute you if you

3256
01:36:14,920 --> 01:36:16,220
 

3257
01:36:14,930 --> 01:36:17,750
 define it in terms of the data

3258
01:36:16,210 --> 01:36:17,750
 

3259
01:36:16,220 --> 01:36:19,400
 generating distribution you don't know

3260
01:36:17,740 --> 01:36:19,400
 

3261
01:36:17,750 --> 01:36:21,410
 what that is so how can you you don't

3262
01:36:19,390 --> 01:36:21,410
 

3263
01:36:19,400 --> 01:36:24,230
 even know what the prior is but if you

3264
01:36:21,400 --> 01:36:24,230
 

3265
01:36:21,410 --> 01:36:26,570
 look at the pack Bayes bound the prior

3266
01:36:24,220 --> 01:36:26,570
 

3267
01:36:24,230 --> 01:36:30,020
 only enters in in that KL divergence

3268
01:36:26,560 --> 01:36:30,020
 

3269
01:36:26,570 --> 01:36:32,930
 term so what we can show is in many

3270
01:36:30,010 --> 01:36:32,930
 

3271
01:36:30,020 --> 01:36:35,230
 cases if you can upper bound that in

3272
01:36:32,920 --> 01:36:35,230
 

3273
01:36:32,930 --> 01:36:38,660
 other words estimate that KL divergence

3274
01:36:35,220 --> 01:36:38,660
 

3275
01:36:35,230 --> 01:36:40,970
 from information you have about the

3276
01:36:38,650 --> 01:36:40,970
 

3277
01:36:38,660 --> 01:36:42,680
 prior in the way it's defined the fact

3278
01:36:40,960 --> 01:36:42,680
 

3279
01:36:40,970 --> 01:36:46,370
 that you don't know the prior doesn't

3280
01:36:42,670 --> 01:36:46,370
 

3281
01:36:42,680 --> 01:36:49,640
 matter so it's a kind of interesting

3282
01:36:46,360 --> 01:36:49,640
 

3283
01:36:46,370 --> 01:36:53,660
 sweet trick okay so will will show you

3284
01:36:49,630 --> 01:36:53,660
 

3285
01:36:49,640 --> 01:36:56,180
 some examples of that so in the Bayesian

3286
01:36:53,650 --> 01:36:56,180
 

3287
01:36:53,660 --> 01:36:57,830
 inference the data distribution actually

3288
01:36:56,170 --> 01:36:57,830
 

3289
01:36:56,180 --> 01:37:00,470
 only enters through the noise model

3290
01:36:57,820 --> 01:37:00,470
 

3291
01:36:57,830 --> 01:37:03,110
 generating the output so effectively the

3292
01:37:00,460 --> 01:37:03,110
 

3293
01:37:00,470 --> 01:37:05,690
 input part of the distribution does not

3294
01:37:03,100 --> 01:37:05,690
 

3295
01:37:03,110 --> 01:37:07,610
 arise which you know can be seen as a

3296
01:37:05,680 --> 01:37:07,610
 

3297
01:37:05,690 --> 01:37:09,290
 strength in some sense so I don't want

3298
01:37:07,600 --> 01:37:09,290
 

3299
01:37:07,610 --> 01:37:11,480
 to I'm not implying any criticism there

3300
01:37:09,280 --> 01:37:11,480
 

3301
01:37:09,290 --> 01:37:17,660
 it's just you know a difference in the

3302
01:37:11,470 --> 01:37:17,660
 

3303
01:37:11,480 --> 01:37:19,730
 way it applies okay so I'm kind of

3304
01:37:17,650 --> 01:37:19,730
 

3305
01:37:17,660 --> 01:37:23,480
 drawing to a close of this second

3306
01:37:19,720 --> 01:37:23,480
 

3307
01:37:19,730 --> 01:37:25,730
 generation I think it leads to practical

3308
01:37:23,470 --> 01:37:25,730
 

3309
01:37:23,480 --> 01:37:30,860
 algorithms and I said even practical

3310
01:37:25,720 --> 01:37:30,860
 

3311
01:37:25,730 --> 01:37:33,590
 model selection it motivates and

3312
01:37:30,850 --> 01:37:33,590
 

3313
01:37:30,860 --> 01:37:37,780
 explains I think known heuristics which

3314
01:37:33,580 --> 01:37:37,780
 

3315
01:37:33,590 --> 01:37:42,260
 is nice kind of it accords with those

3316
01:37:37,770 --> 01:37:42,260
 

3317
01:37:37,780 --> 01:37:44,390
 the proof techniques are refined and you

3318
01:37:42,250 --> 01:37:44,390
 

3319
01:37:42,260 --> 01:37:47,150
 know I've gone through a number of

3320
01:37:44,380 --> 01:37:47,150
 

3321
01:37:44,390 --> 01:37:49,910
 iterations let's say which I think are

3322
01:37:47,140 --> 01:37:49,910
 

3323
01:37:47,150 --> 01:37:53,000
 also interesting and have led to

3324
01:37:49,900 --> 01:37:53,000
 

3325
01:37:49,910 --> 01:37:54,840
 significantly tighter bounds so I think

3326
01:37:52,990 --> 01:37:54,840
 

3327
01:37:53,000 --> 01:37:58,469
 there's you know quite a rounded

3328
01:37:54,830 --> 01:37:58,469
 

3329
01:37:54,840 --> 01:38:00,570
 we hear but I think we're you know we

3330
01:37:58,459 --> 01:38:00,570
 

3331
01:37:58,469 --> 01:38:03,150
 are now is we're actually seeing this

3332
01:38:00,560 --> 01:38:03,150
 

3333
01:38:00,570 --> 01:38:06,530
 very impressive performance from deep

3334
01:38:03,140 --> 01:38:06,530
 

3335
01:38:03,150 --> 01:38:10,699
 learning and many of these techniques

3336
01:38:06,520 --> 01:38:10,699
 

3337
01:38:06,530 --> 01:38:13,650
 are unable to provide let's say

3338
01:38:10,689 --> 01:38:13,650
 

3339
01:38:10,699 --> 01:38:16,290
 appropriate bounds explanations for the

3340
01:38:13,640 --> 01:38:16,290
 

3341
01:38:13,650 --> 01:38:18,210
 performance of deep learning and for

3342
01:38:16,280 --> 01:38:18,210
 

3343
01:38:16,290 --> 01:38:20,429
 that reason I think we may need a

3344
01:38:18,200 --> 01:38:20,429
 

3345
01:38:18,210 --> 01:38:22,530
 next-generation statistical learning

3346
01:38:20,419 --> 01:38:22,530
 

3347
01:38:20,429 --> 01:38:25,230
 theory at this point so I'm going to

3348
01:38:22,520 --> 01:38:25,230
 

3349
01:38:22,530 --> 01:38:30,260
 hand over to Omar to take you into

3350
01:38:25,220 --> 01:38:30,260
 

3351
01:38:25,230 --> 01:38:30,260
 hyperdrive on the Hitchhiker's Guide

3352
01:38:48,580 --> 01:38:48,580
 

3353
01:38:48,590 --> 01:38:57,380
 anyone heard of this thing called deep

3354
01:38:50,290 --> 01:38:57,380
 

3355
01:38:50,300 --> 01:38:58,969
 learning so it turns out deep neural

3356
01:38:57,370 --> 01:38:58,969
 

3357
01:38:57,380 --> 01:39:01,670
 networks have placed some sort of a

3358
01:38:58,959 --> 01:39:01,670
 

3359
01:38:58,969 --> 01:39:05,239
 challenge you know when was think of

3360
01:39:01,660 --> 01:39:05,239
 

3361
01:39:01,670 --> 01:39:08,860
 statistical any theory they show pretty

3362
01:39:05,229 --> 01:39:08,860
 

3363
01:39:05,239 --> 01:39:13,849
 good performance despite the fact that

3364
01:39:08,850 --> 01:39:13,849
 

3365
01:39:08,860 --> 01:39:17,800
 they deal with function classes of you

3366
01:39:13,839 --> 01:39:17,800
 

3367
01:39:13,849 --> 01:39:17,800
 know very complex functional classes

3368
01:39:19,109 --> 01:39:19,109
 

3369
01:39:19,119 --> 01:39:24,110
 some team that came up while John was

3370
01:39:21,430 --> 01:39:24,110
 

3371
01:39:21,440 --> 01:39:25,849
 speaking was this relation this

3372
01:39:24,100 --> 01:39:25,849
 

3373
01:39:24,110 --> 01:39:29,380
 connection between the margin and the

3374
01:39:25,839 --> 01:39:29,380
 

3375
01:39:25,849 --> 01:39:38,449
 accuracy with which one has to estimate

3376
01:39:29,370 --> 01:39:38,449
 

3377
01:39:29,380 --> 01:39:40,039
 the weights so think of think of the

3378
01:39:38,439 --> 01:39:40,039
 

3379
01:39:38,449 --> 01:39:42,949
 output of a neural network the weights

3380
01:39:40,029 --> 01:39:42,949
 

3381
01:39:40,039 --> 01:39:48,170
 output but a neural network if those

3382
01:39:42,939 --> 01:39:48,170
 

3383
01:39:42,949 --> 01:39:52,010
 weights somehow leave you know in a

3384
01:39:48,160 --> 01:39:52,010
 

3385
01:39:48,170 --> 01:39:54,860
 region in a wide region sometimes we

3386
01:39:52,000 --> 01:39:54,860
 

3387
01:39:52,010 --> 01:39:58,219
 have been called a flat wide region with

3388
01:39:54,850 --> 01:39:58,219
 

3389
01:39:54,860 --> 01:40:01,010
 good generalization then that some

3390
01:39:58,209 --> 01:40:01,010
 

3391
01:39:58,219 --> 01:40:02,510
 property that has been connected to two

3392
01:40:01,000 --> 01:40:02,510
 

3393
01:40:01,010 --> 01:40:04,940
 the ability to generalize we would like

3394
01:40:02,500 --> 01:40:04,940
 

3395
01:40:02,510 --> 01:40:07,639
 to point out here that in the pack based

3396
01:40:04,930 --> 01:40:07,639
 

3397
01:40:04,940 --> 01:40:11,440
 approach once you throw in randomization

3398
01:40:07,629 --> 01:40:11,440
 

3399
01:40:07,639 --> 01:40:13,460
 so if you do have this kind of output

3400
01:40:11,430 --> 01:40:13,460
 

3401
01:40:11,440 --> 01:40:14,539
 from your neural network

3402
01:40:13,450 --> 01:40:14,539
 

3403
01:40:13,460 --> 01:40:16,849
 you know weights that live in a wide

3404
01:40:14,529 --> 01:40:16,849
 

3405
01:40:14,539 --> 01:40:19,480
 region of good performance thrown in

3406
01:40:16,839 --> 01:40:19,480
 

3407
01:40:16,849 --> 01:40:22,039
 randomization it seemed that performance

3408
01:40:19,470 --> 01:40:22,039
 

3409
01:40:19,480 --> 01:40:25,130
 it's unlikely it's unexpected that it

3410
01:40:22,029 --> 01:40:25,130
 

3411
01:40:22,039 --> 01:40:27,260
 will go any worse it's quite to be

3412
01:40:25,120 --> 01:40:27,260
 

3413
01:40:25,130 --> 01:40:28,579
 expected that it will actually be be

3414
01:40:27,250 --> 01:40:28,579
 

3415
01:40:27,260 --> 01:40:31,340
 better I found this thing about

3416
01:40:28,569 --> 01:40:31,340
 

3417
01:40:28,579 --> 01:40:34,840
 randomness fascinating it seems that

3418
01:40:31,330 --> 01:40:34,840
 

3419
01:40:31,340 --> 01:40:38,059
 randomness has some sort of smoothing

3420
01:40:34,830 --> 01:40:38,059
 

3421
01:40:34,840 --> 01:40:43,219
 effect but that's more like a personal

3422
01:40:38,049 --> 01:40:43,219
 

3423
01:40:38,059 --> 01:40:46,369
 note so you gotta enjoy have derived

3424
01:40:43,209 --> 01:40:46,369
 

3425
01:40:43,219 --> 01:40:49,429
 useful bounds in this way and they have

3426
01:40:46,359 --> 01:40:49,429
 

3427
01:40:46,369 --> 01:40:52,159
 also been suggestions that stability of

3428
01:40:49,419 --> 01:40:52,159
 

3429
01:40:49,429 --> 01:40:55,219
 stochastic gradient descent leads to

3430
01:40:52,149 --> 01:40:55,219
 

3431
01:40:52,159 --> 01:40:57,469
 good generalization what we're going to

3432
01:40:55,209 --> 01:40:57,469
 

3433
01:40:55,219 --> 01:40:59,860
 do or what I'm going to see in the next

3434
01:40:57,459 --> 01:40:59,860
 

3435
01:40:57,469 --> 01:41:05,560
 few slides five slides left after

3436
01:40:59,850 --> 01:41:05,560
 

3437
01:40:59,860 --> 01:41:06,940
 this one by the way is we are going to

3438
01:41:05,550 --> 01:41:06,940
 

3439
01:41:05,560 --> 01:41:09,730
 present an approach that combines

3440
01:41:06,930 --> 01:41:09,730
 

3441
01:41:06,940 --> 01:41:11,880
 stability with a park based framework

3442
01:41:09,720 --> 01:41:11,880
 

3443
01:41:09,730 --> 01:41:15,070
 that you just saw John described and

3444
01:41:11,870 --> 01:41:15,070
 

3445
01:41:11,880 --> 01:41:17,020
 argue that this make a connection with a

3446
01:41:15,060 --> 01:41:17,020
 

3447
01:41:15,070 --> 01:41:21,340
 learning principle that somehow trolls

3448
01:41:17,010 --> 01:41:21,340
 

3449
01:41:17,020 --> 01:41:26,889
 from information theoretic principles

3450
01:41:21,330 --> 01:41:26,889
 

3451
01:41:21,340 --> 01:41:32,440
 and point out some possible directions

3452
01:41:26,879 --> 01:41:32,440
 

3453
01:41:26,889 --> 01:41:35,290
 of research so let me first talk about

3454
01:41:32,430 --> 01:41:35,290
 

3455
01:41:32,440 --> 01:41:37,199
 stability I like to describe it so

3456
01:41:35,280 --> 01:41:37,199
 

3457
01:41:35,290 --> 01:41:42,040
 what's shown in the slide here is

3458
01:41:37,189 --> 01:41:42,040
 

3459
01:41:37,199 --> 01:41:43,929
 capital a is my algorithm the argument

3460
01:41:42,030 --> 01:41:43,929
 

3461
01:41:42,040 --> 01:41:46,360
 for kata capital a is a particular

3462
01:41:43,919 --> 01:41:46,360
 

3463
01:41:43,929 --> 01:41:48,820
 realization of a sample let's say that

3464
01:41:46,350 --> 01:41:48,820
 

3465
01:41:46,360 --> 01:41:51,159
 were looking at a fixed sample size mmm

3466
01:41:48,810 --> 01:41:51,159
 

3467
01:41:48,820 --> 01:41:53,530
 and I want to see if I feed different

3468
01:41:51,149 --> 01:41:53,530
 

3469
01:41:51,159 --> 01:41:55,150
 samples to my algorithm how different

3470
01:41:53,520 --> 01:41:55,150
 

3471
01:41:53,530 --> 01:41:57,460
 and the output be that's the basic idea

3472
01:41:55,140 --> 01:41:57,460
 

3473
01:41:55,150 --> 01:41:58,989
 of stability somehow measure the

3474
01:41:57,450 --> 01:41:58,989
 

3475
01:41:57,460 --> 01:42:02,170
 difference between the outputs of my

3476
01:41:58,979 --> 01:42:02,170
 

3477
01:41:58,989 --> 01:42:06,550
 algorithm when you feed it with

3478
01:42:02,160 --> 01:42:06,550
 

3479
01:42:02,170 --> 01:42:08,820
 different training samples so in order

3480
01:42:06,540 --> 01:42:08,820
 

3481
01:42:06,550 --> 01:42:11,679
 to make this a little bit more precise

3482
01:42:08,810 --> 01:42:11,679
 

3483
01:42:08,820 --> 01:42:13,330
 and just to clarify notation to the

3484
01:42:11,669 --> 01:42:13,330
 

3485
01:42:11,679 --> 01:42:15,000
 argument it's the arguments are

3486
01:42:13,320 --> 01:42:15,000
 

3487
01:42:13,330 --> 01:42:16,810
 Interpol's

3488
01:42:14,990 --> 01:42:16,810
 

3489
01:42:15,000 --> 01:42:19,450
 and to make this statement

3490
01:42:16,800 --> 01:42:19,450
 

3491
01:42:16,810 --> 01:42:20,610
 mathematically precise let me say that

3492
01:42:19,440 --> 01:42:20,610
 

3493
01:42:19,450 --> 01:42:23,170
[Music]

3494
01:42:20,600 --> 01:42:23,170
 

3495
01:42:20,610 --> 01:42:26,350
 the algorithm has uniform hypothesis

3496
01:42:23,160 --> 01:42:26,350
 

3497
01:42:23,170 --> 01:42:29,730
 stability Beta Beta is a coefficient the

3498
01:42:26,340 --> 01:42:29,730
 

3499
01:42:26,350 --> 01:42:33,130
 factor showing in the left hand side

3500
01:42:29,720 --> 01:42:33,130
 

3501
01:42:29,730 --> 01:42:39,310
 other specific sample size when that

3502
01:42:33,120 --> 01:42:39,310
 

3503
01:42:33,130 --> 01:42:42,670
 inequality is satisfied so I would like

3504
01:42:39,300 --> 01:42:42,670
 

3505
01:42:39,310 --> 01:42:47,860
 you to think you know that the algorithm

3506
01:42:42,660 --> 01:42:47,860
 

3507
01:42:42,670 --> 01:42:49,389
 outputs elements of norm space that's

3508
01:42:47,850 --> 01:42:49,389
 

3509
01:42:47,860 --> 01:42:51,219
 how it makes sense to talk about the

3510
01:42:49,379 --> 01:42:51,219
 

3511
01:42:49,389 --> 01:42:53,850
 norm difference between the outputs

3512
01:42:51,209 --> 01:42:53,850
 

3513
01:42:51,219 --> 01:42:57,250
 based from two different training sets

3514
01:42:53,840 --> 01:42:57,250
 

3515
01:42:53,850 --> 01:43:00,040
 so these kind of models cases where your

3516
01:42:57,240 --> 01:43:00,040
 

3517
01:42:57,250 --> 01:43:02,830
 algorithm outputs a weight vector this

3518
01:43:00,030 --> 01:43:02,830
 

3519
01:43:00,040 --> 01:43:05,260
 applies to SVM's more generally to our

3520
01:43:02,820 --> 01:43:05,260
 

3521
01:43:02,830 --> 01:43:10,060
 KHS this applies also to neural networks

3522
01:43:05,250 --> 01:43:10,060
 

3523
01:43:05,260 --> 01:43:11,920
 and the property of sensitivity

3524
01:43:10,050 --> 01:43:11,920
 

3525
01:43:10,060 --> 01:43:15,760
 or the property of stability I'd like to

3526
01:43:11,910 --> 01:43:15,760
 

3527
01:43:11,920 --> 01:43:18,070
 see it as a property of smoothness of

3528
01:43:15,750 --> 01:43:18,070
 

3529
01:43:15,760 --> 01:43:20,530
 the output of the algorithm in fact this

3530
01:43:18,060 --> 01:43:20,530
 

3531
01:43:18,070 --> 01:43:22,240
 ellipsis fatto if you look at the

3532
01:43:20,520 --> 01:43:22,240
 

3533
01:43:20,530 --> 01:43:24,280
 left-hand side on the left-hand side you

3534
01:43:22,230 --> 01:43:24,280
 

3535
01:43:22,240 --> 01:43:27,970
 have the Hamming distance between the

3536
01:43:24,270 --> 01:43:27,970
 

3537
01:43:24,280 --> 01:43:31,750
 two samples so how different can the

3538
01:43:27,960 --> 01:43:31,750
 

3539
01:43:27,970 --> 01:43:33,760
 output of the algorithm be on two on two

3540
01:43:31,740 --> 01:43:33,760
 

3541
01:43:31,750 --> 01:43:37,810
 different training sets if my algorithm

3542
01:43:33,750 --> 01:43:37,810
 

3543
01:43:33,760 --> 01:43:39,070
 has stability better never mind about if

3544
01:43:37,800 --> 01:43:39,070
 

3545
01:43:37,810 --> 01:43:41,590
 we know that the algorithm has stability

3546
01:43:39,060 --> 01:43:41,590
 

3547
01:43:39,070 --> 01:43:43,570
 better then that difference can be at

3548
01:43:41,580 --> 01:43:43,570
 

3549
01:43:41,590 --> 01:43:50,770
 most beta times the number of entries in

3550
01:43:43,560 --> 01:43:50,770
 

3551
01:43:43,570 --> 01:43:52,270
 which the two samples are different this

3552
01:43:50,760 --> 01:43:52,270
 

3553
01:43:50,770 --> 01:43:53,980
 is not the only case in which stability

3554
01:43:52,260 --> 01:43:53,980
 

3555
01:43:52,270 --> 01:43:55,410
 has been studied it has will be studied

3556
01:43:53,970 --> 01:43:55,410
 

3557
01:43:53,980 --> 01:43:59,580
 through the lenses of a loss function

3558
01:43:55,400 --> 01:43:59,580
 

3559
01:43:55,410 --> 01:44:01,480
 this is a little bit you know weights

3560
01:43:59,570 --> 01:44:01,480
 

3561
01:43:59,580 --> 01:44:03,340
 you can call it a little bit more

3562
01:44:01,470 --> 01:44:03,340
 

3563
01:44:01,480 --> 01:44:05,260
 generally in the recovers cases not only

3564
01:44:03,330 --> 01:44:05,260
 

3565
01:44:03,340 --> 01:44:07,360
 cases where the output lives in

3566
01:44:05,250 --> 01:44:07,360
 

3567
01:44:05,260 --> 01:44:17,680
 especially in a space with a special

3568
01:44:07,350 --> 01:44:17,680
 

3569
01:44:07,360 --> 01:44:18,850
 structure so the reason stability study

3570
01:44:17,670 --> 01:44:18,850
 

3571
01:44:17,680 --> 01:44:20,590
 and I'm going to show this in the next

3572
01:44:18,840 --> 01:44:20,590
 

3573
01:44:18,850 --> 01:44:23,440
 slide is in connection with obtain a

3574
01:44:20,580 --> 01:44:23,440
 

3575
01:44:20,590 --> 01:44:26,200
 generalization bounds the topic of the

3576
01:44:23,430 --> 01:44:26,200
 

3577
01:44:23,440 --> 01:44:27,820
 topic of the whole tutorial but I want

3578
01:44:26,190 --> 01:44:27,820
 

3579
01:44:26,200 --> 01:44:30,160
 to point out that this particular notion

3580
01:44:27,810 --> 01:44:30,160
 

3581
01:44:27,820 --> 01:44:31,810
 of uniform stability either a uniform

3582
01:44:30,150 --> 01:44:31,810
 

3583
01:44:30,160 --> 01:44:34,180
 stability of the hypothesis also the

3584
01:44:31,800 --> 01:44:34,180
 

3585
01:44:31,810 --> 01:44:38,110
 loss function it's a worst-case notion

3586
01:44:34,170 --> 01:44:38,110
 

3587
01:44:34,180 --> 01:44:40,770
 of stability in the sense that it is

3588
01:44:38,100 --> 01:44:40,770
 

3589
01:44:38,110 --> 01:44:43,210
 data insensitive it is even distribution

3590
01:44:40,760 --> 01:44:43,210
 

3591
01:44:40,770 --> 01:44:44,710
 insensitive it is the finisher has

3592
01:44:43,200 --> 01:44:44,710
 

3593
01:44:43,210 --> 01:44:48,910
 nothing to do with the jetan interest in

3594
01:44:44,700 --> 01:44:48,910
 

3595
01:44:44,710 --> 01:44:50,950
 distribution and it is kind of like an

3596
01:44:48,900 --> 01:44:50,950
 

3597
01:44:48,910 --> 01:44:53,080
 ongoing topic of research keep in mind a

3598
01:44:50,940 --> 01:44:53,080
 

3599
01:44:50,950 --> 01:44:55,770
 good a good thing to look at how to

3600
01:44:53,070 --> 01:44:55,770
 

3601
01:44:53,080 --> 01:44:59,520
 bring in notions of stability that

3602
01:44:55,760 --> 01:44:59,520
 

3603
01:44:55,770 --> 01:45:02,200
 captures something more about the data

3604
01:44:59,510 --> 01:45:02,200
 

3605
01:44:59,520 --> 01:45:04,540
 so how does the bility come in for

3606
01:45:02,190 --> 01:45:04,540
 

3607
01:45:02,200 --> 01:45:08,080
 generalization I just want to show you

3608
01:45:04,530 --> 01:45:08,080
 

3609
01:45:04,540 --> 01:45:10,030
 the form of bounds base of stability if

3610
01:45:08,070 --> 01:45:10,030
 

3611
01:45:08,080 --> 01:45:12,420
 you have an algorithm that somehow you

3612
01:45:10,020 --> 01:45:12,420
 

3613
01:45:10,030 --> 01:45:14,580
 have obtained an estimate of the

3614
01:45:12,410 --> 01:45:14,580
 

3615
01:45:12,420 --> 01:45:17,620
 stability coefficient for that algorithm

3616
01:45:14,570 --> 01:45:17,620
 

3617
01:45:14,580 --> 01:45:19,330
 then you can get a generalization bound

3618
01:45:17,610 --> 01:45:19,330
 

3619
01:45:17,620 --> 01:45:21,790
 for that algorithm more or less of this

3620
01:45:19,320 --> 01:45:21,790
 

3621
01:45:19,330 --> 01:45:23,710
 form where the true risk is bounded from

3622
01:45:21,780 --> 01:45:23,710
 

3623
01:45:21,790 --> 01:45:26,410
 above by the empirical risk

3624
01:45:23,700 --> 01:45:26,410
 

3625
01:45:23,710 --> 01:45:28,120
 a term that depends not just on the

3626
01:45:26,400 --> 01:45:28,120
 

3627
01:45:26,410 --> 01:45:31,300
 sample size and the confuse parameter

3628
01:45:28,110 --> 01:45:31,300
 

3629
01:45:28,120 --> 01:45:33,460
 also on the stability factor one thing

3630
01:45:31,290 --> 01:45:33,460
 

3631
01:45:31,300 --> 01:45:37,870
 that I want to emphasize here why do we

3632
01:45:33,450 --> 01:45:37,870
 

3633
01:45:33,460 --> 01:45:39,580
 care about generalization bounds so the

3634
01:45:37,860 --> 01:45:39,580
 

3635
01:45:37,870 --> 01:45:41,890
 goal should be to control to put a leash

3636
01:45:39,570 --> 01:45:41,890
 

3637
01:45:39,580 --> 01:45:47,020
 on the true error the goal is to have a

3638
01:45:41,880 --> 01:45:47,020
 

3639
01:45:41,890 --> 01:45:49,180
 small true error the problem the true

3640
01:45:47,010 --> 01:45:49,180
 

3641
01:45:47,020 --> 01:45:50,700
 error is not a computable thing we don't

3642
01:45:49,170 --> 01:45:50,700
 

3643
01:45:49,180 --> 01:45:52,480
 have access to the true errors this is

3644
01:45:50,690 --> 01:45:52,480
 

3645
01:45:50,700 --> 01:45:54,730
 probably something that has been

3646
01:45:52,470 --> 01:45:54,730
 

3647
01:45:52,480 --> 01:45:57,969
 mentioned a few times that it is it is

3648
01:45:54,720 --> 01:45:57,969
 

3649
01:45:54,730 --> 01:45:59,760
 sinking in so given that that is the

3650
01:45:57,959 --> 01:45:59,760
 

3651
01:45:57,969 --> 01:46:01,989
 case it makes sense to aim for

3652
01:45:59,750 --> 01:46:01,989
 

3653
01:45:59,760 --> 01:46:04,540
 minimizing upper bounds on the true

3654
01:46:01,979 --> 01:46:04,540
 

3655
01:46:01,989 --> 01:46:06,340
 error if we minimize upper bounds on

3656
01:46:04,530 --> 01:46:06,340
 

3657
01:46:04,540 --> 01:46:08,770
 that where or then we're guaranteed to

3658
01:46:06,330 --> 01:46:08,770
 

3659
01:46:06,340 --> 01:46:12,520
 you know put the true error on a short

3660
01:46:08,760 --> 01:46:12,520
 

3661
01:46:08,770 --> 01:46:14,860
 leash so minimizing expressions of the

3662
01:46:12,510 --> 01:46:14,860
 

3663
01:46:12,520 --> 01:46:16,690
 form on the left hand side of this of

3664
01:46:14,850 --> 01:46:16,690
 

3665
01:46:14,860 --> 01:46:19,420
 this on the right on the right hand side

3666
01:46:16,680 --> 01:46:19,420
 

3667
01:46:16,690 --> 01:46:23,230
 of this inequality makes a lot of sense

3668
01:46:19,410 --> 01:46:23,230
 

3669
01:46:19,420 --> 01:46:24,640
 minimizing just empirical risk doesn't

3670
01:46:23,220 --> 01:46:24,640
 

3671
01:46:23,230 --> 01:46:26,860
 make sense a lot doesn't make a lot of

3672
01:46:24,630 --> 01:46:26,860
 

3673
01:46:24,640 --> 01:46:29,590
 sense to me because the pyrrha calories

3674
01:46:26,850 --> 01:46:29,590
 

3675
01:46:26,860 --> 01:46:33,940
 by itself is not an upper bound on the

3676
01:46:29,580 --> 01:46:33,940
 

3677
01:46:29,590 --> 01:46:36,270
 true risk minimizing some cooked up

3678
01:46:33,930 --> 01:46:36,270
 

3679
01:46:33,940 --> 01:46:39,219
 formula involved in the empirical risk

3680
01:46:36,260 --> 01:46:39,219
 

3681
01:46:36,270 --> 01:46:41,560
 doesn't make much sense unless there is

3682
01:46:39,209 --> 01:46:41,560
 

3683
01:46:39,219 --> 01:46:44,710
 very good indication that such a thing

3684
01:46:41,550 --> 01:46:44,710
 

3685
01:46:41,560 --> 01:46:46,780
 is an upper bound on the true risk so

3686
01:46:44,700 --> 01:46:46,780
 

3687
01:46:44,710 --> 01:46:48,370
 this is what we care about upper bounds

3688
01:46:46,770 --> 01:46:48,370
 

3689
01:46:46,780 --> 01:46:50,860
 on the true risk this is what we care

3690
01:46:48,360 --> 01:46:50,860
 

3691
01:46:48,370 --> 01:46:52,150
 about generalization bounds by the way

3692
01:46:50,850 --> 01:46:52,150
 

3693
01:46:50,860 --> 01:46:57,850
 the notion of stability was just

3694
01:46:52,140 --> 01:46:57,850
 

3695
01:46:52,150 --> 01:47:01,300
 mentioned it was introduced by Bousquet

3696
01:46:57,840 --> 01:47:01,300
 

3697
01:46:57,850 --> 01:47:04,840
 analyses and a few comments so the idea

3698
01:47:01,290 --> 01:47:04,840
 

3699
01:47:01,300 --> 01:47:06,400
 is that if individual examples would not

3700
01:47:04,830 --> 01:47:06,400
 

3701
01:47:04,840 --> 01:47:09,100
 affect too much the output of an

3702
01:47:06,390 --> 01:47:09,100
 

3703
01:47:06,400 --> 01:47:11,790
 algorithm then the output of the

3704
01:47:09,090 --> 01:47:11,790
 

3705
01:47:09,100 --> 01:47:14,370
 algorithm should be somehow concentrated

3706
01:47:11,780 --> 01:47:14,370
 

3707
01:47:11,790 --> 01:47:16,780
 this can be applied to kernel methods

3708
01:47:14,360 --> 01:47:16,780
 

3709
01:47:14,370 --> 01:47:19,750
 where the stability coefficient can be

3710
01:47:16,770 --> 01:47:19,750
 

3711
01:47:16,780 --> 01:47:23,320
 estimated and it is related in fact to

3712
01:47:19,740 --> 01:47:23,320
 

3713
01:47:19,750 --> 01:47:26,920
 the regularization factor although those

3714
01:47:23,310 --> 01:47:26,920
 

3715
01:47:23,320 --> 01:47:30,460
 bounds are rather weak and a question is

3716
01:47:26,910 --> 01:47:30,460
 

3717
01:47:26,920 --> 01:47:32,560
 if if we know that the output of an

3718
01:47:30,450 --> 01:47:32,560
 

3719
01:47:30,460 --> 01:47:34,530
 algorithm is highly concentrated can we

3720
01:47:32,550 --> 01:47:34,530
 

3721
01:47:32,560 --> 01:47:37,530
 do better can we obtain better

3722
01:47:34,520 --> 01:47:37,530
 

3723
01:47:34,530 --> 01:47:37,530
 generalization

3724
01:47:41,419 --> 01:47:41,419
 

3725
01:47:41,429 --> 01:47:46,300
 just kind of like to set the stage for

3726
01:47:43,620 --> 01:47:46,300
 

3727
01:47:43,630 --> 01:47:48,190
 the next for the next slide I'm going to

3728
01:47:46,290 --> 01:47:48,190
 

3729
01:47:46,300 --> 01:47:50,349
 mention something about distribution

3730
01:47:48,180 --> 01:47:50,349
 

3731
01:47:48,190 --> 01:47:53,530
 dependent priorities is an idea that was

3732
01:47:50,339 --> 01:47:53,530
 

3733
01:47:50,349 --> 01:47:55,530
 introduced by Kotani Park in seven if

3734
01:47:53,520 --> 01:47:55,530
 

3735
01:47:53,530 --> 01:48:02,739
 I'm not wrong

3736
01:47:55,520 --> 01:48:02,739
 

3737
01:47:55,530 --> 01:48:05,349
 he looked Gibbs or Boltzmann kinds of

3738
01:48:02,729 --> 01:48:05,349
 

3739
01:48:02,739 --> 01:48:08,650
 distributions so for a prior he looked

3740
01:48:05,339 --> 01:48:08,650
 

3741
01:48:05,349 --> 01:48:12,809
 at basically the normalized exponential

3742
01:48:08,640 --> 01:48:12,809
 

3743
01:48:08,650 --> 01:48:15,040
 of the risk and for a posterior

3744
01:48:12,799 --> 01:48:15,040
 

3745
01:48:12,809 --> 01:48:16,270
 exponential of the empirical risk

3746
01:48:15,030 --> 01:48:16,270
 

3747
01:48:15,040 --> 01:48:19,179
 notice that the posterior is something

3748
01:48:16,260 --> 01:48:19,179
 

3749
01:48:16,270 --> 01:48:21,699
 computable from data the prior does not

3750
01:48:19,169 --> 01:48:21,699
 

3751
01:48:19,179 --> 01:48:24,190
 a pain on sample data also the prior

3752
01:48:21,689 --> 01:48:24,190
 

3753
01:48:21,699 --> 01:48:26,500
 does depend on the data generating

3754
01:48:24,180 --> 01:48:26,500
 

3755
01:48:24,190 --> 01:48:28,210
 distribution in a sense it is not

3756
01:48:26,490 --> 01:48:28,210
 

3757
01:48:26,500 --> 01:48:32,860
 computable because we don't know the

3758
01:48:28,200 --> 01:48:32,860
 

3759
01:48:28,210 --> 01:48:34,510
 data generating distribution but as Joan

3760
01:48:32,850 --> 01:48:34,510
 

3761
01:48:32,860 --> 01:48:40,420
 was pointing out in the park based

3762
01:48:34,500 --> 01:48:40,420
 

3763
01:48:34,510 --> 01:48:42,520
 framework that is okay since in the end

3764
01:48:40,410 --> 01:48:42,520
 

3765
01:48:40,420 --> 01:48:44,559
 all that you care about is how far is

3766
01:48:42,510 --> 01:48:44,559
 

3767
01:48:42,520 --> 01:48:46,239
 the posterior from the prior in terms of

3768
01:48:44,549 --> 01:48:46,239
 

3769
01:48:44,559 --> 01:48:48,070
 the KL divergence so as long as you're

3770
01:48:46,229 --> 01:48:48,070
 

3771
01:48:46,239 --> 01:48:49,510
 able to control the KL divergence of

3772
01:48:48,060 --> 01:48:49,510
 

3773
01:48:48,070 --> 01:48:51,940
 posterior to prior then you're doing

3774
01:48:49,500 --> 01:48:51,940
 

3775
01:48:49,510 --> 01:48:54,000
 good even if you don't know explicitly

3776
01:48:51,930 --> 01:48:54,000
 

3777
01:48:51,940 --> 01:48:58,929
 even if you cannot calculate explicitly

3778
01:48:53,990 --> 01:48:58,929
 

3779
01:48:54,000 --> 01:49:01,659
 the prior but so the point of this so

3780
01:48:58,919 --> 01:49:01,659
 

3781
01:48:58,929 --> 01:49:03,429
 look at Q 0 is defined in terms of the

3782
01:49:01,649 --> 01:49:03,429
 

3783
01:49:01,659 --> 01:49:05,349
 race is defined in terms of the data

3784
01:49:03,419 --> 01:49:05,349
 

3785
01:49:03,429 --> 01:49:06,880
 generating distribution that's the idea

3786
01:49:05,339 --> 01:49:06,880
 

3787
01:49:05,349 --> 01:49:10,270
 that was introduced about how to bring

3788
01:49:06,870 --> 01:49:10,270
 

3789
01:49:06,880 --> 01:49:11,800
 in the data generating distribution to

3790
01:49:10,260 --> 01:49:11,800
 

3791
01:49:10,270 --> 01:49:15,150
 form priors for the park based framework

3792
01:49:11,790 --> 01:49:15,150
 

3793
01:49:11,800 --> 01:49:19,960
 and here is an example of a park base

3794
01:49:15,140 --> 01:49:19,960
 

3795
01:49:15,150 --> 01:49:22,960
 pound on the KL divergence of average

3796
01:49:19,950 --> 01:49:22,960
 

3797
01:49:19,960 --> 01:49:24,460
 empirical risk to average true risk of

3798
01:49:22,950 --> 01:49:24,460
 

3799
01:49:22,960 --> 01:49:27,969
 course once you have a bount of this

3800
01:49:24,450 --> 01:49:27,969
 

3801
01:49:24,460 --> 01:49:31,469
 form you can invert it you can invert

3802
01:49:27,959 --> 01:49:31,469
 

3803
01:49:27,969 --> 01:49:37,989
 the KL divergence to get a bound of the

3804
01:49:31,459 --> 01:49:37,989
 

3805
01:49:31,469 --> 01:49:39,699
 average true risk that's that so I'm

3806
01:49:37,979 --> 01:49:39,699
 

3807
01:49:37,989 --> 01:49:42,190
 going to try to you know give you a

3808
01:49:39,689 --> 01:49:42,190
 

3809
01:49:39,699 --> 01:49:43,809
 little bit of an impression for what

3810
01:49:42,180 --> 01:49:43,809
 

3811
01:49:42,190 --> 01:49:46,980
 stability combined with PAC based

3812
01:49:43,799 --> 01:49:46,980
 

3813
01:49:43,809 --> 01:49:46,980
 framework can do

3814
01:49:47,090 --> 01:49:47,090
 

3815
01:49:47,100 --> 01:49:52,830
 if you have an algorithm that has

3816
01:49:50,240 --> 01:49:52,830
 

3817
01:49:50,250 --> 01:49:54,300
 uniforms hypothesis stability say

3818
01:49:52,820 --> 01:49:54,300
 

3819
01:49:52,830 --> 01:49:57,690
 somehow you know that that properly

3820
01:49:54,290 --> 01:49:57,690
 

3821
01:49:54,300 --> 01:50:01,530
 satisfied for your algorithm then for a

3822
01:49:57,680 --> 01:50:01,530
 

3823
01:49:57,690 --> 01:50:05,210
 given sample size you have the following

3824
01:50:01,520 --> 01:50:05,210
 

3825
01:50:01,530 --> 01:50:08,520
 bound the KL divergence between the

3826
01:50:05,200 --> 01:50:08,520
 

3827
01:50:05,210 --> 01:50:11,310
 average empirical risk and the true

3828
01:50:08,510 --> 01:50:11,310
 

3829
01:50:08,520 --> 01:50:14,720
 empirical risk is bounded from above by

3830
01:50:11,300 --> 01:50:14,720
 

3831
01:50:11,310 --> 01:50:14,720
 a quantity that

3832
01:50:23,790 --> 01:50:23,790
 

3833
01:50:23,800 --> 01:50:30,849
 depends on the stability coefficient

3834
01:50:26,959 --> 01:50:30,849
 

3835
01:50:26,969 --> 01:50:34,719
 besides sample size and confidence

3836
01:50:30,839 --> 01:50:34,719
 

3837
01:50:30,849 --> 01:50:36,760
 parameter how this bound is obtained it

3838
01:50:34,709 --> 01:50:36,760
 

3839
01:50:34,719 --> 01:50:40,000
 was obtained using Gaussian random

3840
01:50:36,750 --> 01:50:40,000
 

3841
01:50:36,760 --> 01:50:42,820
 ization with a prior being a Gaussian

3842
01:50:39,990 --> 01:50:42,820
 

3843
01:50:40,000 --> 01:50:46,449
 centered at the expected weight vector

3844
01:50:42,810 --> 01:50:46,449
 

3845
01:50:42,820 --> 01:50:48,610
 output by the algorithm and special

3846
01:50:46,439 --> 01:50:48,610
 

3847
01:50:46,449 --> 01:50:51,699
 covariance structure and the posterior

3848
01:50:48,600 --> 01:50:51,699
 

3849
01:50:48,610 --> 01:50:54,730
 is a Gaussian Center at the actual

3850
01:50:51,689 --> 01:50:54,730
 

3851
01:50:51,699 --> 01:51:00,550
 random output of the algorithm same

3852
01:50:54,720 --> 01:51:00,550
 

3853
01:50:54,730 --> 01:51:03,010
 covariance structure as a prior using

3854
01:51:00,540 --> 01:51:03,010
 

3855
01:51:00,550 --> 01:51:04,510
 gaussians then one controls the KL

3856
01:51:03,000 --> 01:51:04,510
 

3857
01:51:03,010 --> 01:51:06,130
 divergence I'm indicate the veges

3858
01:51:04,500 --> 01:51:06,130
 

3859
01:51:04,510 --> 01:51:09,190
 between two - non distributions is

3860
01:51:06,120 --> 01:51:09,190
 

3861
01:51:06,130 --> 01:51:13,300
 computable explicitly and this is how in

3862
01:51:09,180 --> 01:51:13,300
 

3863
01:51:09,190 --> 01:51:15,849
 the end we care only about the norm

3864
01:51:13,290 --> 01:51:15,849
 

3865
01:51:13,300 --> 01:51:18,040
 distance between the random output and

3866
01:51:15,839 --> 01:51:18,040
 

3867
01:51:15,849 --> 01:51:22,270
 its expectation this is how in the end a

3868
01:51:18,030 --> 01:51:22,270
 

3869
01:51:18,040 --> 01:51:24,849
 concentration inequality comes in and so

3870
01:51:22,260 --> 01:51:24,849
 

3871
01:51:22,270 --> 01:51:28,480
 those are the main components of of the

3872
01:51:24,839 --> 01:51:28,480
 

3873
01:51:24,849 --> 01:51:33,790
 proof of this bound first the typical

3874
01:51:28,470 --> 01:51:33,790
 

3875
01:51:28,480 --> 01:51:35,050
 PAC based bound which says that to bound

3876
01:51:33,780 --> 01:51:35,050
 

3877
01:51:33,790 --> 01:51:38,710
 the K in the various between average

3878
01:51:35,040 --> 01:51:38,710
 

3879
01:51:35,050 --> 01:51:40,360
 risk empirical and true you need to

3880
01:51:38,700 --> 01:51:40,360
 

3881
01:51:38,710 --> 01:51:43,980
 bound the KL divergence between

3882
01:51:40,350 --> 01:51:43,980
 

3883
01:51:40,360 --> 01:51:46,270
 posterior and prior and we bounded but

3884
01:51:43,970 --> 01:51:46,270
 

3885
01:51:43,980 --> 01:51:50,170
 we bounded that in view that we were

3886
01:51:46,260 --> 01:51:50,170
 

3887
01:51:46,270 --> 01:51:52,090
 using Gaussian distributions we bounded

3888
01:51:50,160 --> 01:51:52,090
 

3889
01:51:50,170 --> 01:51:55,239
 that by using a concentration inequality

3890
01:51:52,080 --> 01:51:55,239
 

3891
01:51:52,090 --> 01:51:57,670
 for the output weight vector from its

3892
01:51:55,229 --> 01:51:57,670
 

3893
01:51:55,239 --> 01:52:00,150
 expectation just in case that you're

3894
01:51:57,660 --> 01:52:00,150
 

3895
01:51:57,670 --> 01:52:00,150
 wondering

3896
01:52:04,450 --> 01:52:04,450
 

3897
01:52:04,460 --> 01:52:09,159
 the naan being used is our KHS norm

3898
01:52:14,899 --> 01:52:14,899
 

3899
01:52:14,909 --> 01:52:20,400
 last but not least there is something

3900
01:52:18,680 --> 01:52:20,400
 

3901
01:52:18,690 --> 01:52:22,380
 some tools coming in from information

3902
01:52:20,390 --> 01:52:22,380
 

3903
01:52:20,400 --> 01:52:24,420
 theory that have been used by Akili and

3904
01:52:22,370 --> 01:52:24,420
 

3905
01:52:22,380 --> 01:52:26,370
 so otto to somehow capture the idea of

3906
01:52:24,410 --> 01:52:26,370
 

3907
01:52:24,420 --> 01:52:29,550
 how much information about the

3908
01:52:26,360 --> 01:52:29,550
 

3909
01:52:26,370 --> 01:52:31,440
 particular training set is stored in the

3910
01:52:29,540 --> 01:52:31,440
 

3911
01:52:29,550 --> 01:52:36,710
 weight vectors learned by a needle

3912
01:52:31,430 --> 01:52:36,710
 

3913
01:52:31,440 --> 01:52:39,540
 network the idea is that over fitting

3914
01:52:36,700 --> 01:52:39,540
 

3915
01:52:36,710 --> 01:52:42,690
 that thing that we learn to kind of

3916
01:52:39,530 --> 01:52:42,690
 

3917
01:52:39,540 --> 01:52:44,040
 avoid is related to think that your

3918
01:52:42,680 --> 01:52:44,040
 

3919
01:52:42,690 --> 01:52:45,690
 weight vector stores too much

3920
01:52:44,030 --> 01:52:45,690
 

3921
01:52:44,040 --> 01:52:48,420
 information about the particular data

3922
01:52:45,680 --> 01:52:48,420
 

3923
01:52:45,690 --> 01:52:50,460
 set then that's what we call overfitting

3924
01:52:48,410 --> 01:52:50,460
 

3925
01:52:48,420 --> 01:52:52,679
 ideally your weight vector stores

3926
01:52:50,450 --> 01:52:52,679
 

3927
01:52:50,460 --> 01:52:55,409
 information about the data generating

3928
01:52:52,669 --> 01:52:55,409
 

3929
01:52:52,679 --> 01:53:03,840
 distribution itself then that's one we

3930
01:52:55,399 --> 01:53:03,840
 

3931
01:52:55,409 --> 01:53:05,310
 have good generalization so storing too

3932
01:53:03,830 --> 01:53:05,310
 

3933
01:53:03,840 --> 01:53:07,830
 much information in the particular

3934
01:53:05,300 --> 01:53:07,830
 

3935
01:53:05,310 --> 01:53:11,070
 language vector is kind of like a way of

3936
01:53:07,820 --> 01:53:11,070
 

3937
01:53:07,830 --> 01:53:16,520
 weakening the concentration of the

3938
01:53:11,060 --> 01:53:16,520
 

3939
01:53:11,070 --> 01:53:18,960
 weight vector and the argument is that

3940
01:53:16,510 --> 01:53:18,960
 

3941
01:53:16,520 --> 01:53:24,139
 something kind of like based on the

3942
01:53:18,950 --> 01:53:24,139
 

3943
01:53:18,960 --> 01:53:24,139
 information bottle net principle that

3944
01:53:26,050 --> 01:53:26,050
 

3945
01:53:26,060 --> 01:53:31,739
 achilles Watteau come up with a specific

3946
01:53:28,040 --> 01:53:31,739
 

3947
01:53:28,050 --> 01:53:33,449
 function to minimize that then if you

3948
01:53:31,729 --> 01:53:33,449
 

3949
01:53:31,739 --> 01:53:37,050
 use the boundary demising algorithm the

3950
01:53:33,439 --> 01:53:37,050
 

3951
01:53:33,449 --> 01:53:38,639
 output is a weight vector that stores

3952
01:53:37,040 --> 01:53:38,639
 

3953
01:53:37,050 --> 01:53:41,600
 minimal information about the particular

3954
01:53:38,629 --> 01:53:41,600
 

3955
01:53:38,639 --> 01:53:45,120
 training set in a sense that they made

3956
01:53:41,590 --> 01:53:45,120
 

3957
01:53:41,600 --> 01:53:46,980
 specific this could potentially lead to

3958
01:53:45,110 --> 01:53:46,980
 

3959
01:53:45,120 --> 01:53:48,270
 title-pack based bounds this is

3960
01:53:46,970 --> 01:53:48,270
 

3961
01:53:46,980 --> 01:53:52,020
 something to be studied this is

3962
01:53:48,260 --> 01:53:52,020
 

3963
01:53:48,270 --> 01:53:55,070
 something to be seen and with that I'm

3964
01:53:52,010 --> 01:53:55,070
 

3965
01:53:52,020 --> 01:53:55,070
 closed in the tutorial

3966
01:54:02,259 --> 01:54:02,259
 

3967
01:54:02,269 --> 01:54:09,949
 so statistical learning theory we say

3968
01:54:06,819 --> 01:54:09,949
 

3969
01:54:06,829 --> 01:54:14,210
 that it might be offering a nice

3970
01:54:09,939 --> 01:54:14,210
 

3971
01:54:09,949 --> 01:54:20,380
 hyperlift put the thumb up wo sometime

3972
01:54:14,200 --> 01:54:20,380
 

3973
01:54:14,210 --> 01:54:20,380
 soon kind of ish thank you very much

3974
01:54:28,990 --> 01:54:28,990
 

3975
01:54:29,000 --> 01:54:34,200
 sometimes don't run away don't run away

3976
01:54:31,160 --> 01:54:34,200
 

3977
01:54:31,170 --> 01:54:35,280
 we have some time for questions in case

3978
01:54:34,190 --> 01:54:35,280
 

3979
01:54:34,200 --> 01:54:38,970
 there are any questions from the

3980
01:54:35,270 --> 01:54:38,970
 

3981
01:54:35,280 --> 01:54:41,670
 audience we have also John did any

3982
01:54:38,960 --> 01:54:41,670
 

3983
01:54:38,970 --> 01:54:43,770
 questions also John did promise that we

3984
01:54:41,660 --> 01:54:43,770
 

3985
01:54:41,670 --> 01:54:46,680
 would have some references for you these

3986
01:54:43,760 --> 01:54:46,680
 

3987
01:54:43,770 --> 01:54:49,470
 are at the back of the slide so if

3988
01:54:46,670 --> 01:54:49,470
 

3989
01:54:46,680 --> 01:54:51,090
 anyone wants to see them available yeah

3990
01:54:49,460 --> 01:54:51,090
 

3991
01:54:49,470 --> 01:54:56,100
 so I guess there are some questions from

3992
01:54:51,080 --> 01:54:56,100
 

3993
01:54:51,090 --> 01:54:58,890
 the audience so yeah yeah hello I have a

3994
01:54:56,090 --> 01:54:58,890
 

3995
01:54:56,100 --> 01:55:02,030
 question with respect to the stability

3996
01:54:58,880 --> 01:55:02,030
 

3997
01:54:58,890 --> 01:55:05,670
 plus back by this bound I think that

3998
01:55:02,020 --> 01:55:05,670
 

3999
01:55:02,030 --> 01:55:07,650
 actually doesn't converge to 0 when n

4000
01:55:05,660 --> 01:55:07,650
 

4001
01:55:05,670 --> 01:55:16,320
 goes to infinity I don't know if you can

4002
01:55:07,640 --> 01:55:16,320
 

4003
01:55:07,650 --> 01:55:19,380
 go back to the to the slide so be nice

4004
01:55:16,310 --> 01:55:19,380
 

4005
01:55:16,320 --> 01:55:21,650
 actually if you could wait for five

4006
01:55:19,370 --> 01:55:21,650
 

4007
01:55:19,380 --> 01:55:21,650
 minutes

4008
01:55:35,770 --> 01:55:35,770
 

4009
01:55:35,780 --> 01:55:39,670
 we're waiting for some slide to come up

4010
01:55:58,300 --> 01:55:58,300
 

4011
01:55:58,310 --> 01:56:01,210
 I wasn't sure

4012
01:56:06,090 --> 01:56:06,090
 

4013
01:56:06,100 --> 01:56:13,900
 okay I think you can ask so please ask

4014
01:56:10,830 --> 01:56:13,900
 

4015
01:56:10,840 --> 01:56:16,630
 question yeah oh yeah

4016
01:56:13,890 --> 01:56:16,630
 

4017
01:56:13,900 --> 01:56:20,440
 so in this light where do you have the

4018
01:56:16,620 --> 01:56:20,440
 

4019
01:56:16,630 --> 01:56:23,590
 stability plus the PAC buyers bound the

4020
01:56:20,430 --> 01:56:23,590
 

4021
01:56:20,440 --> 01:56:26,140
 generalization bound there is okay there

4022
01:56:23,580 --> 01:56:26,140
 

4023
01:56:23,590 --> 01:56:30,130
 is one term that depends linearly in n I

4024
01:56:26,130 --> 01:56:30,130
 

4025
01:56:26,140 --> 01:56:34,000
 think it's n bits square times 1 plus

4026
01:56:30,120 --> 01:56:34,000
 

4027
01:56:30,130 --> 01:56:37,630
 log over 1 Delta square but you do if

4028
01:56:33,990 --> 01:56:37,630
 

4029
01:56:34,000 --> 01:56:39,400
 you divide that by n that means that the

4030
01:56:37,620 --> 01:56:39,400
 

4031
01:56:37,630 --> 01:56:43,120
 bound doesn't go to 0 as n goes to

4032
01:56:39,390 --> 01:56:43,120
 

4033
01:56:39,400 --> 01:56:44,620
 infinity right that's how that what's

4034
01:56:43,110 --> 01:56:44,620
 

4035
01:56:43,120 --> 01:56:46,180
 the meaning of that it has to do

4036
01:56:44,610 --> 01:56:46,180
 

4037
01:56:44,620 --> 01:56:55,300
 something with the prior in the function

4038
01:56:46,170 --> 01:56:55,300
 

4039
01:56:46,180 --> 01:56:57,970
 space on the hypothesis space so I as I

4040
01:56:55,290 --> 01:56:57,970
 

4041
01:56:55,300 --> 01:57:00,190
 understood maybe I can repeat back what

4042
01:56:57,960 --> 01:57:00,190
 

4043
01:56:57,970 --> 01:57:04,260
 you were saying the beta would be

4044
01:57:00,180 --> 01:57:04,260
 

4045
01:57:00,190 --> 01:57:08,560
 independent would be dependent on N or

4046
01:57:04,250 --> 01:57:08,560
 

4047
01:57:04,260 --> 01:57:11,650
 potentially they--they so in practice it

4048
01:57:08,550 --> 01:57:11,650
 

4049
01:57:08,560 --> 01:57:13,570
 is but not as strongly as that I mean in

4050
01:57:11,640 --> 01:57:13,570
 

4051
01:57:11,650 --> 01:57:15,220
 our experience of applying this bound we

4052
01:57:13,560 --> 01:57:15,220
 

4053
01:57:13,570 --> 01:57:19,150
 have actually you know generated these

4054
01:57:15,210 --> 01:57:19,150
 

4055
01:57:15,220 --> 01:57:23,260
 the actual rate of increase of beta is

4056
01:57:19,140 --> 01:57:23,260
 

4057
01:57:19,150 --> 01:57:26,530
 is is less sharp and you actually do

4058
01:57:23,250 --> 01:57:26,530
 

4059
01:57:23,260 --> 01:57:30,420
 generate what we are not so happy about

4060
01:57:26,520 --> 01:57:30,420
 

4061
01:57:26,530 --> 01:57:34,600
 is the fact that as you raise the

4062
01:57:30,410 --> 01:57:34,600
 

4063
01:57:30,420 --> 01:57:37,570
 generally the regularization this does

4064
01:57:34,590 --> 01:57:37,570
 

4065
01:57:34,600 --> 01:57:39,820
 become weak and we believe that is as a

4066
01:57:37,560 --> 01:57:39,820
 

4067
01:57:37,570 --> 01:57:45,420
 result of too much you know there's sort

4068
01:57:39,810 --> 01:57:45,420
 

4069
01:57:39,820 --> 01:57:47,410
 of a if you like a uniform bound on the

4070
01:57:45,410 --> 01:57:47,410
 

4071
01:57:45,420 --> 01:57:49,240
 stability which doesn't take into

4072
01:57:47,400 --> 01:57:49,240
 

4073
01:57:47,410 --> 01:57:52,210
 account the specifics so it's a bit like

4074
01:57:49,230 --> 01:57:52,210
 

4075
01:57:49,240 --> 01:57:53,710
 you know our VC thing this is like VC

4076
01:57:52,200 --> 01:57:53,710
 

4077
01:57:52,210 --> 01:57:55,930
 for stability we need to have a more

4078
01:57:53,700 --> 01:57:55,930
 

4079
01:57:53,710 --> 01:57:57,700
 nuanced understanding of how stability

4080
01:57:55,920 --> 01:57:57,700
 

4081
01:57:55,930 --> 01:58:03,190
 actually operates in a particular case

4082
01:57:57,690 --> 01:58:03,190
 

4083
01:57:57,700 --> 01:58:04,450
 and so that's I think there's one more

4084
01:58:03,180 --> 01:58:04,450
 

4085
01:58:03,190 --> 01:58:06,490
 question it would be really nice if

4086
01:58:04,440 --> 01:58:06,490
 

4087
01:58:04,450 --> 01:58:08,110
 everybody else could try to be quiet

4088
01:58:06,480 --> 01:58:08,110
 

4089
01:58:06,490 --> 01:58:11,650
 I'm just for the time that the question

4090
01:58:08,100 --> 01:58:11,650
 

4091
01:58:08,110 --> 01:58:13,110
 is asked so thanks for the great talk so

4092
01:58:11,640 --> 01:58:13,110
 

4093
01:58:11,650 --> 01:58:15,449
 in the beginning of

4094
01:58:13,100 --> 01:58:15,449
 

4095
01:58:13,110 --> 01:58:19,400
 I think the first generation right there

4096
01:58:15,439 --> 01:58:19,400
 

4097
01:58:15,449 --> 01:58:21,989
 was some criticisms that the bounds were

4098
01:58:19,390 --> 01:58:21,989
 

4099
01:58:19,400 --> 01:58:25,020
 not realistic for real data that you see

4100
01:58:21,979 --> 01:58:25,020
 

4101
01:58:21,989 --> 01:58:27,210
 in in real-world settings right and so

4102
01:58:25,010 --> 01:58:27,210
 

4103
01:58:25,020 --> 01:58:30,449
 you worked you know to make them work

4104
01:58:27,200 --> 01:58:30,449
 

4105
01:58:27,210 --> 01:58:33,199
 tight but then now that we have this all

4106
01:58:30,439 --> 01:58:33,199
 

4107
01:58:30,449 --> 01:58:35,550
 these adversarial examples popping up

4108
01:58:33,189 --> 01:58:35,550
 

4109
01:58:33,199 --> 01:58:38,219
 potentially maybe those original bounds

4110
01:58:35,540 --> 01:58:38,219
 

4111
01:58:35,550 --> 01:58:39,600
 are actually you know they are doing

4112
01:58:38,209 --> 01:58:39,600
 

4113
01:58:38,219 --> 01:58:41,790
 what you would expect to see if you

4114
01:58:39,590 --> 01:58:41,790
 

4115
01:58:39,600 --> 01:58:44,369
 consider adversarial distributions which

4116
01:58:41,780 --> 01:58:44,369
 

4117
01:58:41,790 --> 01:58:46,500
 according to this morning's tutorial are

4118
01:58:44,359 --> 01:58:46,500
 

4119
01:58:44,369 --> 01:58:49,350
 abundant and they're all over the place

4120
01:58:46,490 --> 01:58:49,350
 

4121
01:58:46,500 --> 01:58:51,449
 so sometimes I hear what your thoughts

4122
01:58:49,340 --> 01:58:51,449
 

4123
01:58:49,350 --> 01:58:53,070
 are on kind of these other adversarial

4124
01:58:51,439 --> 01:58:53,070
 

4125
01:58:51,449 --> 01:58:55,980
 distributions that are that supposedly

4126
01:58:53,060 --> 01:58:55,980
 

4127
01:58:53,070 --> 01:58:58,560
 are realistic in the real world so just

4128
01:58:55,970 --> 01:58:58,560
 

4129
01:58:55,980 --> 01:59:00,570
 to repeat what I think you asking I mean

4130
01:58:58,550 --> 01:59:00,570
 

4131
01:58:58,560 --> 01:59:03,449
 were you asking about the problem with

4132
01:59:00,560 --> 01:59:03,449
 

4133
01:59:00,570 --> 01:59:05,940
 let's say adversarial inputs into a deep

4134
01:59:03,439 --> 01:59:05,940
 

4135
01:59:03,449 --> 01:59:12,060
 network and the fact that they can be

4136
01:59:05,930 --> 01:59:12,060
 

4137
01:59:05,940 --> 01:59:13,320
 used to actually cause the network to

4138
01:59:12,050 --> 01:59:13,320
 

4139
01:59:12,060 --> 01:59:16,170
 make an error or you're just thinking

4140
01:59:13,310 --> 01:59:16,170
 

4141
01:59:13,320 --> 01:59:17,969
 generally adversarial situations where

4142
01:59:16,160 --> 01:59:17,969
 

4143
01:59:16,170 --> 01:59:21,179
 you don't have iid data but you're

4144
01:59:17,959 --> 01:59:21,179
 

4145
01:59:17,969 --> 01:59:26,119
 having bounds that somehow worst case

4146
01:59:21,169 --> 01:59:26,119
 

4147
01:59:21,179 --> 01:59:29,610
 over all possible inputs the former

4148
01:59:26,109 --> 01:59:29,610
 

4149
01:59:26,119 --> 01:59:32,130
 inputs have been manipulated to miss

4150
01:59:29,600 --> 01:59:32,130
 

4151
01:59:29,610 --> 01:59:35,070
 class you know to increase the error

4152
01:59:32,120 --> 01:59:35,070
 

4153
01:59:32,130 --> 01:59:37,650
 yeah right and so and some some of the

4154
01:59:35,060 --> 01:59:37,650
 

4155
01:59:35,070 --> 01:59:39,360
 bounds that were shown we're quite large

4156
01:59:37,640 --> 01:59:39,360
 

4157
01:59:37,650 --> 01:59:41,210
 if you consider adversarial examples

4158
01:59:39,350 --> 01:59:41,210
 

4159
01:59:39,360 --> 01:59:45,060
 which I think reflects those early

4160
01:59:41,200 --> 01:59:45,060
 

4161
01:59:41,210 --> 01:59:48,449
 bounds so yeah I mean the these bounds

4162
01:59:45,050 --> 01:59:48,449
 

4163
01:59:45,060 --> 01:59:50,639
 based on iid data the chances of you

4164
01:59:48,439 --> 01:59:50,639
 

4165
01:59:48,449 --> 01:59:52,199
 hitting those adversarial inputs are so

4166
01:59:50,629 --> 01:59:52,199
 

4167
01:59:50,639 --> 01:59:54,210
 low that the fact that they make an

4168
01:59:52,189 --> 01:59:54,210
 

4169
01:59:52,199 --> 01:59:56,250
 error will not appear in you know will

4170
01:59:54,200 --> 01:59:56,250
 

4171
01:59:54,210 --> 01:59:59,219
 not affect these bounds I think we would

4172
01:59:56,240 --> 01:59:59,219
 

4173
01:59:56,250 --> 02:00:01,560
 need to have a much more I think the the

4174
01:59:59,209 --> 02:00:01,560
 

4175
01:59:59,219 --> 02:00:03,869
 key there will be learning properties of

4176
02:00:01,550 --> 02:00:03,869
 

4177
02:00:01,560 --> 02:00:07,739
 the distribution so in some sense the

4178
02:00:03,859 --> 02:00:07,739
 

4179
02:00:03,869 --> 02:00:09,590
 manifold which is I think the key to if

4180
02:00:07,729 --> 02:00:09,590
 

4181
02:00:07,739 --> 02:00:11,909
 you like the difference between those

4182
02:00:09,580 --> 02:00:11,909
 

4183
02:00:09,590 --> 02:00:14,760
 errors that are made you know through

4184
02:00:11,899 --> 02:00:14,760
 

4185
02:00:11,909 --> 02:00:17,280
 sort of pointing very slightly

4186
02:00:14,750 --> 02:00:17,280
 

4187
02:00:14,760 --> 02:00:19,260
 differences as opposed to perhaps

4188
02:00:17,270 --> 02:00:19,260
 

4189
02:00:17,280 --> 02:00:21,659
 understanding some structure in the data

4190
02:00:19,250 --> 02:00:21,659
 

4191
02:00:19,260 --> 02:00:23,460
 which would you know for example in an

4192
02:00:21,649 --> 02:00:23,460
 

4193
02:00:21,659 --> 02:00:25,230
 animal you would identify eyes you would

4194
02:00:23,450 --> 02:00:25,230
 

4195
02:00:23,460 --> 02:00:26,460
 identify the type of fur you would

4196
02:00:25,220 --> 02:00:26,460
 

4197
02:00:25,230 --> 02:00:29,130
 identify a number of things

4198
02:00:26,450 --> 02:00:29,130
 

4199
02:00:26,460 --> 02:00:30,750
 and you'd have to cause errors in all of

4200
02:00:29,120 --> 02:00:30,750
 

4201
02:00:29,130 --> 02:00:33,210
 these factors in order to make an error

4202
02:00:30,740 --> 02:00:33,210
 

4203
02:00:30,750 --> 02:00:36,300
 in the classification rather than just

4204
02:00:33,200 --> 02:00:36,300
 

4205
02:00:33,210 --> 02:00:38,430
 you know somehow some gushed out of the

4206
02:00:36,290 --> 02:00:38,430
 

4207
02:00:36,300 --> 02:00:40,680
 image that is used in the deep network

4208
02:00:38,420 --> 02:00:40,680
 

4209
02:00:38,430 --> 02:00:42,690
 case to do the classification so I think

4210
02:00:40,670 --> 02:00:42,690
 

4211
02:00:40,680 --> 02:00:45,240
 it's more about that the way to approach

4212
02:00:42,680 --> 02:00:45,240
 

4213
02:00:42,690 --> 02:00:47,310
 that problem is to try and pose the

4214
02:00:45,230 --> 02:00:47,310
 

4215
02:00:45,240 --> 02:00:49,680
 problem as a learning problem about

4216
02:00:47,300 --> 02:00:49,680
 

4217
02:00:47,310 --> 02:00:52,620
 structuring data rather than just as a

4218
02:00:49,670 --> 02:00:52,620
 

4219
02:00:49,680 --> 02:00:54,270
 classification problem per se but I

4220
02:00:52,610 --> 02:00:54,270
 

4221
02:00:52,620 --> 02:00:55,800
 think you know that doesn't mean SLT

4222
02:00:54,260 --> 02:00:55,800
 

4223
02:00:54,270 --> 02:00:57,840
 wouldn't have a role to play in doing

4224
02:00:55,790 --> 02:00:57,840
 

4225
02:00:55,800 --> 02:01:00,210
 that but I think it would be a more

4226
02:00:57,830 --> 02:01:00,210
 

4227
02:00:57,840 --> 02:01:02,130
 structured role in learning different

4228
02:01:00,200 --> 02:01:02,130
 

4229
02:01:00,210 --> 02:01:06,240
 sort of elements if you like of the of

4230
02:01:02,120 --> 02:01:06,240
 

4231
02:01:02,130 --> 02:01:07,230
 the structure of the input Thanks any

4232
02:01:06,230 --> 02:01:07,230
 

4233
02:01:06,240 --> 02:01:10,710
 more questions

4234
02:01:07,220 --> 02:01:10,710
 

4235
02:01:07,230 --> 02:01:13,140
 well that's because yet so regarding

4236
02:01:10,700 --> 02:01:13,140
 

4237
02:01:10,710 --> 02:01:16,640
 your last slide could you elaborate a

4238
02:01:13,130 --> 02:01:16,640
 

4239
02:01:13,140 --> 02:01:20,790
 bit more on this information and how

4240
02:01:16,630 --> 02:01:20,790
 

4241
02:01:16,640 --> 02:01:23,100
 information complexity and how it can be

4242
02:01:20,780 --> 02:01:23,100
 

4243
02:01:20,790 --> 02:01:26,160
 used to optimize the bound on them so

4244
02:01:23,090 --> 02:01:26,160
 

4245
02:01:23,100 --> 02:01:29,850
 the the argument made this isn't you

4246
02:01:26,150 --> 02:01:29,850
 

4247
02:01:26,160 --> 02:01:30,270
 know obviously our work Achille and

4248
02:01:29,840 --> 02:01:30,270
 

4249
02:01:29,850 --> 02:01:33,030
 Swatow

4250
02:01:30,260 --> 02:01:33,030
 

4251
02:01:30,270 --> 02:01:34,890
 have looked at this idea of measuring

4252
02:01:33,020 --> 02:01:34,890
 

4253
02:01:33,030 --> 02:01:37,500
 the amount of information in the weights

4254
02:01:34,880 --> 02:01:37,500
 

4255
02:01:34,890 --> 02:01:39,450
 so that you obviously the information in

4256
02:01:37,490 --> 02:01:39,450
 

4257
02:01:37,500 --> 02:01:41,400
 the weights has two sources one is the

4258
02:01:39,440 --> 02:01:41,400
 

4259
02:01:39,450 --> 02:01:43,350
 underlying distribution but then there's

4260
02:01:41,390 --> 02:01:43,350
 

4261
02:01:41,400 --> 02:01:45,060
 also the question of whether given the

4262
02:01:43,340 --> 02:01:45,060
 

4263
02:01:43,350 --> 02:01:46,830
 underlying distribution is there more

4264
02:01:45,050 --> 02:01:46,830
 

4265
02:01:45,060 --> 02:01:49,860
 specific information about the actual

4266
02:01:46,820 --> 02:01:49,860
 

4267
02:01:46,830 --> 02:01:51,630
 training data that was used and the

4268
02:01:49,850 --> 02:01:51,630
 

4269
02:01:49,860 --> 02:01:53,160
 point they're making is if there's too

4270
02:01:51,620 --> 02:01:53,160
 

4271
02:01:51,630 --> 02:01:54,540
 much information about the training data

4272
02:01:53,150 --> 02:01:54,540
 

4273
02:01:53,160 --> 02:01:56,880
 as opposed to the underlying

4274
02:01:54,530 --> 02:01:56,880
 

4275
02:01:54,540 --> 02:01:59,490
 distribution that implies some sort of

4276
02:01:56,870 --> 02:01:59,490
 

4277
02:01:56,880 --> 02:02:01,530
 overfitting they make the argument that

4278
02:01:59,480 --> 02:02:01,530
 

4279
02:01:59,490 --> 02:02:03,030
 Ashley's stochastic gradient descent and

4280
02:02:01,520 --> 02:02:03,030
 

4281
02:02:01,530 --> 02:02:07,050
 particularly the information bottleneck

4282
02:02:03,020 --> 02:02:07,050
 

4283
02:02:03,030 --> 02:02:09,720
 does in some way control that but not in

4284
02:02:07,040 --> 02:02:09,720
 

4285
02:02:07,050 --> 02:02:11,820
 such a specific way that you know that

4286
02:02:09,710 --> 02:02:11,820
 

4287
02:02:09,720 --> 02:02:13,530
 it might that there could be criteria

4288
02:02:11,810 --> 02:02:13,530
 

4289
02:02:11,820 --> 02:02:16,440
 you could use that would actually help

4290
02:02:13,520 --> 02:02:16,440
 

4291
02:02:13,530 --> 02:02:18,270
 you to control this more tightly than

4292
02:02:16,430 --> 02:02:18,270
 

4293
02:02:16,440 --> 02:02:20,850
 perhaps you know it's sort of almost a

4294
02:02:18,260 --> 02:02:20,850
 

4295
02:02:18,270 --> 02:02:22,620
 byproduct of the sort of information

4296
02:02:20,840 --> 02:02:22,620
 

4297
02:02:20,850 --> 02:02:24,240
 bottleneck training that is frequently

4298
02:02:22,610 --> 02:02:24,240
 

4299
02:02:22,620 --> 02:02:26,220
 used and with stochastic gradient

4300
02:02:24,230 --> 02:02:26,220
 

4301
02:02:24,240 --> 02:02:28,460
 descent I think there's also this

4302
02:02:26,210 --> 02:02:28,460
 

4303
02:02:26,220 --> 02:02:31,080
 possibility there's a relationship with

4304
02:02:28,450 --> 02:02:31,080
 

4305
02:02:28,460 --> 02:02:32,790
 you know differential privacy and the

4306
02:02:31,070 --> 02:02:32,790
 

4307
02:02:31,080 --> 02:02:34,170
 fact that you introduce randomness and

4308
02:02:32,780 --> 02:02:34,170
 

4309
02:02:32,790 --> 02:02:35,130
 so you know the specific information

4310
02:02:34,160 --> 02:02:35,130
 

4311
02:02:34,170 --> 02:02:37,080
 about

4312
02:02:35,120 --> 02:02:37,080
 

4313
02:02:35,130 --> 02:02:39,090
 element's the training center get lost

4314
02:02:37,070 --> 02:02:39,090
 

4315
02:02:37,080 --> 02:02:41,220
 and are not encoded in it so there's a

4316
02:02:39,080 --> 02:02:41,220
 

4317
02:02:39,090 --> 02:02:43,740
 connection there potentially as well so

4318
02:02:41,210 --> 02:02:43,740
 

4319
02:02:41,220 --> 02:02:46,280
 I think these are all ideas that we

4320
02:02:43,730 --> 02:02:46,280
 

4321
02:02:43,740 --> 02:02:49,320
 believe could be used during some way

4322
02:02:46,270 --> 02:02:49,320
 

4323
02:02:46,280 --> 02:02:51,450
 improve the bounds on concentration that

4324
02:02:49,310 --> 02:02:51,450
 

4325
02:02:49,320 --> 02:02:53,370
 you get which will then improve the

4326
02:02:51,440 --> 02:02:53,370
 

4327
02:02:51,450 --> 02:02:55,710
 bounds on the generalization because

4328
02:02:53,360 --> 02:02:55,710
 

4329
02:02:53,370 --> 02:02:58,170
 these depend on the concentration if you

4330
02:02:55,700 --> 02:02:58,170
 

4331
02:02:55,710 --> 02:03:00,090
 like of the actual weight vectors you

4332
02:02:58,160 --> 02:03:00,090
 

4333
02:02:58,170 --> 02:03:02,130
 observe relative to the expected value

4334
02:03:00,080 --> 02:03:02,130
 

4335
02:03:00,090 --> 02:03:05,610
 of what you would it serve over a set of

4336
02:03:02,120 --> 02:03:05,610
 

4337
02:03:02,130 --> 02:03:07,530
 random training sets so that's the idea

4338
02:03:05,600 --> 02:03:07,530
 

4339
02:03:05,610 --> 02:03:09,330
 but I'd say it's it's quite vague at

4340
02:03:07,520 --> 02:03:09,330
 

4341
02:03:07,530 --> 02:03:11,690
 this stage but it's I think an

4342
02:03:09,320 --> 02:03:11,690
 

4343
02:03:09,330 --> 02:03:14,010
 intriguing approach and I kind of was

4344
02:03:11,680 --> 02:03:14,010
 

4345
02:03:11,690 --> 02:03:16,650
 feeling this is actually potentially a

4346
02:03:14,000 --> 02:03:16,650
 

4347
02:03:14,010 --> 02:03:18,810
 new way of thinking about generalization

4348
02:03:16,640 --> 02:03:18,810
 

4349
02:03:16,650 --> 02:03:20,850
 which is different from the margin or

4350
02:03:18,800 --> 02:03:20,850
 

4351
02:03:18,810 --> 02:03:22,590
 the sort of you know region of weight

4352
02:03:20,840 --> 02:03:22,590
 

4353
02:03:20,850 --> 02:03:24,150
 space I think margin and region and

4354
02:03:22,580 --> 02:03:24,150
 

4355
02:03:22,590 --> 02:03:26,310
 weight space are very similar you know

4356
02:03:24,140 --> 02:03:26,310
 

4357
02:03:24,150 --> 02:03:29,190
 in some sense this room in the safe in

4358
02:03:26,300 --> 02:03:29,190
 

4359
02:03:26,310 --> 02:03:32,010
 around the solution to do similar stuff

4360
02:03:29,180 --> 02:03:32,010
 

4361
02:03:29,190 --> 02:03:34,520
 and you kind of if that's a lot of room

4362
02:03:32,000 --> 02:03:34,520
 

4363
02:03:32,010 --> 02:03:36,810
 then you kind of almost divide the total

4364
02:03:34,510 --> 02:03:36,810
 

4365
02:03:34,520 --> 02:03:38,820
 complexity by the complexity of that

4366
02:03:36,800 --> 02:03:38,820
 

4367
02:03:36,810 --> 02:03:41,130
 space and that gives you some sort of

4368
02:03:38,810 --> 02:03:41,130
 

4369
02:03:38,820 --> 02:03:42,990
 quotient which would be the complexity

4370
02:03:41,120 --> 02:03:42,990
 

4371
02:03:41,130 --> 02:03:43,920
 of your algorithms which I think it but

4372
02:03:42,980 --> 02:03:43,920
 

4373
02:03:42,990 --> 02:03:45,930
 I think this is different

4374
02:03:43,910 --> 02:03:45,930
 

4375
02:03:43,920 --> 02:03:49,980
 in some sense so that's that's the I

4376
02:03:45,920 --> 02:03:49,980
 

4377
02:03:45,930 --> 02:03:53,040
 think intriguing direction Thanks any

4378
02:03:49,970 --> 02:03:53,040
 

4379
02:03:49,980 --> 02:03:54,590
 more questions if not let's thank the

4380
02:03:53,030 --> 02:03:54,590
 

4381
02:03:53,040 --> 02:04:01,850
 speakers again okay thank you

4382
02:03:54,580 --> 02:04:01,850
 

4383
02:03:54,590 --> 02:04:01,850
[Applause]

4384
02:04:33,740 --> 02:04:33,740
 

4385
02:04:33,750 --> 02:04:37,140
[Music]